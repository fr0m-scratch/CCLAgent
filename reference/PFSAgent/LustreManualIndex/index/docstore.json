{"docstore/metadata": {"ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb": {"doc_hash": "55d55b31a088d3d367ff03172f04f99c0eb048da7ef4aa30bacda43c01d11edb"}, "bf8a1ec0-568b-47b0-a7c5-21a48cec8b13": {"doc_hash": "7a530bf76b267fa1d24d94f51c71502e386fe76e906c0e7575df56b0f9b32e66"}, "a65ed932-3b3c-455b-b428-909f95d3dbc2": {"doc_hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88"}, "47977575-fffa-4357-a1c0-cff50282b3fe": {"doc_hash": "1fe4c21fcc0fff5e09fb4ad22a903f330ebb1d697b96b7a51cbdb1961376626d"}, "22a09abf-8c64-40ef-b0e4-3f0c680c6528": {"doc_hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22"}, "777b7a2a-a9f3-4979-b600-eec8a49b8e7d": {"doc_hash": "f0c6ec40904b8e04b81a43aceb05c286d74c9588d44143e4db9c4cb930da6298"}, "67237423-af56-4120-aa24-70eb93f29297": {"doc_hash": "e2ffe3a3bbc461a0aa775e064e6564a509ede829e4c844f0ee87ad712b8b2dee"}, "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2": {"doc_hash": "1fc8ea6f4b26b94e01c4665ea641603c25ea2c18272ca4d6c7b8b617cb6ed9d7"}, "2f911bd0-c44f-44b5-b226-560182781bd4": {"doc_hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6"}, "a9366dfb-6705-4f16-8ac5-76e1945f8dc8": {"doc_hash": "7d3a5b59a7eaa2e9f311ca24344bc5f1aa2b36ca75c1e1f2b30b9e13313230d2"}, "0bbbec1f-efdf-4525-ba8a-93c7d136290f": {"doc_hash": "4cc77dcbda1a28ee15cc302eb43d77beeae6496c4b2211187cc14ab88355a01f"}, "bb3d8a82-59bb-4143-bfee-fdc2379b11d6": {"doc_hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8"}, "6c068f47-9062-4cc9-803d-e6acfb6c66a0": {"doc_hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6"}, "645b2691-3c1f-4a87-b41a-d620be00ec85": {"doc_hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d"}, "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a": {"doc_hash": "2c541e46aafde89add9a6c9ed87c866a12f338b8ddb04609cb3928b949e5f4aa"}, "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb": {"doc_hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8"}, "0888add2-b651-4598-9893-5922248a97c9": {"doc_hash": "bf0f598e59f15a348f8755c26bf1809118050a124e874498372aa5be0046dc3c"}, "761cdce3-575f-4bdb-94ac-ae187e5ef7e9": {"doc_hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035"}, "5b3ff7df-5b84-46ae-b374-90e4f2cfed78": {"doc_hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27"}, "a83a9ded-1bf5-47a3-b6d1-9aba91141757": {"doc_hash": "d2da4308c4c2038893c82e26d86f2d48a61d2c0bb621064910b010b55bae63a5"}, "fa0d9664-d941-45d1-8d3c-afe24618fbb8": {"doc_hash": "e5b36b6e8a99229817f5aee6738f7124ffcee0c046aa5af6d8d32ae5f26acf05"}, "37222704-0a59-4da0-925a-e2f4ab7c0729": {"doc_hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47"}, "a35258f4-eb58-4e5a-b66b-4814eca7f5b0": {"doc_hash": "934347200c3346ae799feceff93b7acf9b02814452049c7667cfb47038e60497"}, "67c64319-6584-4bdc-b600-62c718831df6": {"doc_hash": "cc5722147e9cc7f7b1e30c10dbfa724f643674114b733aab0f5bbfaad1ccb737"}, "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf": {"doc_hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f"}, "f01c8240-32d1-4591-ac57-6a7a39cc34a8": {"doc_hash": "3445594814cd167537ba0d74b2637b6e324f1044231f3ef0572628ca0da292e0"}, "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29": {"doc_hash": "8de845787a4ce46a25f25adc36b8ae865519e8ac501e0403174ee696ac285e80"}, "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2": {"doc_hash": "450e77dd6c8b606cffbe2f76c9ec0035deb2f5eeff9037b8c45aa014ea4a7d46"}, "8fe9bbcf-6085-4523-a8c0-b662c60ba81d": {"doc_hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a"}, "5b986ca8-7311-4014-8513-80f735259c79": {"doc_hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d"}, "b88264eb-e9c4-4f08-9be9-eb423dd36b19": {"doc_hash": "47974e4509ee5bb6084613eb6f7bd301008642bfc91aef3fb6d74aed452ce87e"}, "7d617e61-6c89-4bb9-a949-0006b8fd89f5": {"doc_hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42"}, "dea0950f-ca76-4df0-9c7c-70e604fd5f14": {"doc_hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07"}, "dcc81bc0-236c-4ad3-a95f-a0ec7a928645": {"doc_hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50"}, "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18": {"doc_hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4"}, "cff75fac-b53b-4236-96bd-53e27bb85409": {"doc_hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa"}, "c98af996-876a-4161-b507-b7f760731c52": {"doc_hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2"}, "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1": {"doc_hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b"}, "f13aa227-5c7b-43f8-8e13-41665aef4267": {"doc_hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534"}, "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479": {"doc_hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802"}, "2a4f6215-823c-41aa-9fdb-a39e2944433f": {"doc_hash": "16618e445361b1e0a1852febdb6b836b7a8f5c188f0d60657a182183787b9f39"}, "4abc006a-7fe9-4567-a652-25157dbb2a76": {"doc_hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448"}, "93b33121-2589-4655-af52-aff6aa8c7f68": {"doc_hash": "0dfca469acd512a9141149e2b5ed802e464426c5f0537611c3c9594bccfd7979"}, "ea80abee-1c6e-4eea-8975-8c7fd00b547d": {"doc_hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664"}, "368eac03-b394-4c04-8d4b-1af2f2abc607": {"doc_hash": "da7b4967aeaad37cee9cd9bf590f1a0461abf26411c4753e963083c1f6d4180e"}, "3e6d6384-67cd-4be0-a9c8-de3e325d60ca": {"doc_hash": "bb0a221411848abb237ff40820539b64f6f37974dc80162ab6bf1af3467f14c6"}, "4fb6edc1-9346-4f89-a73c-9c90bb50fa63": {"doc_hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8"}, "3c0710af-28b3-4a98-8400-67e28de0a539": {"doc_hash": "ec52fc5965e5c236f947e3bf456daaee268f5ef5ef0fc85a53f298a93700a206", "ref_doc_id": "ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb"}, "39559a7a-3157-4f68-bf4c-2eb74bd984ab": {"doc_hash": "1fc283ea195de23970957552ce58706146c87f5c9b5194120beae0d3a20629af", "ref_doc_id": "ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb"}, "1a671d38-8d1e-4b2e-ae6d-79ceb2a86a5e": {"doc_hash": "14878e456129a07c998c1d1f8e725eb7f5443e04bd13e353d56d681fa8e8dfce", "ref_doc_id": "ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb"}, "70f9d054-2875-4a2e-af9b-10976de8a456": {"doc_hash": "d70e3f199c3b21382248afb8eb90e13a233713a01f450494837bdd28dc16cb7a", "ref_doc_id": "ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb"}, "16c1f261-9688-46f2-a408-fa12cb0ff908": {"doc_hash": "3ea83ae53239d213669c1a3718bd4dda8cdd72bdf28c2a941f3827c996894bd0", "ref_doc_id": "bf8a1ec0-568b-47b0-a7c5-21a48cec8b13"}, "edabca5c-8221-41cb-ab61-c15aa7d90653": {"doc_hash": "b94be7407e0e61bcd17c4f93a8df502b9ff4eccbde97eecb5ad7d2640cdc9ee7", "ref_doc_id": "bf8a1ec0-568b-47b0-a7c5-21a48cec8b13"}, "99e6d9a4-143d-4488-9c4d-b91deaa9ea7f": {"doc_hash": "8f69d6283f6d2ed3dc8200ff1832121c957f349cb04ad53c7838c0c01dfe6c18", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "990e0472-9948-4c0d-a91e-3e755b9eb13a": {"doc_hash": "037c3be5588bba82bf1c4626653042236fa093289b308ffdf3eaa3b9ae5d1738", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "84fb2fa1-598c-46c1-8441-1fed67455e51": {"doc_hash": "f51ee0599f59f82fc922b91ac0c8e81e180a8bd4a246d35505588efd26db7b0f", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "d084acc6-9fdb-409a-9709-a6f5bd2dc2f1": {"doc_hash": "edeca2bf40624b4f8780a32ddc45f4da01a119ff0abf43a4d40ba58745b3f550", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "87c980c0-e858-404a-adcc-7852870d6793": {"doc_hash": "483a13f44fb33e25e6a074afcbc4145edba9110f5117dce0f820a040a66ca74b", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "05262992-1418-4e39-8e59-f9a354c0d2c7": {"doc_hash": "492b292cc77c3cd6353afeb5820036e18a4dacf0aa5a7d36e7b74fb3f959c7b8", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "88998759-6bbf-4408-bb53-ff4522aef293": {"doc_hash": "5d003d17639b3473c8082ff80855092af0d7c59655a25ee99cbb91cd1da4a58f", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "098fb67f-4fff-4e8e-8338-9623ed9f1eb4": {"doc_hash": "6563c983a6b56d0244116210d63214dfef03937d04549387ed9c68458b2621ee", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "0a6634ef-a9c0-43d7-bdac-46e915c27c8d": {"doc_hash": "64b825126f5e15a8d1dc4c21d898b03d08911eaa7d4fd68fd348c134ec24802d", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "1eb984ae-fec3-48e9-8460-7d505fbb44b8": {"doc_hash": "a5fa884d0a44262da38fd2fd5f2ddc55e9c7dd48557256797d2fac621513f40c", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "f61e0631-69fd-49ed-9675-f2c7c6f1aa61": {"doc_hash": "2869e02b69ac8fbbfca157d60a5b1b3f12f516aaf34400da7df114f71eb625b9", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "3a62f12c-182e-48b8-b6e4-9bc24101a989": {"doc_hash": "699d5929c693b0e0bf49c3b2ea8111f5e06d2c7543945e71cdcb7cb0eee537e4", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "e72d7a24-b2fb-4a3c-a4d0-7ad61ba79564": {"doc_hash": "e34d96e691437d5fefa093337f6f2603894eb3b0be25d2efc0279591ca0df945", "ref_doc_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2"}, "60e316ed-db9f-4fbb-b44c-fb21d7823af9": {"doc_hash": "ee35a6e57e9defbee2e0ca287ffb71ed967e9c1452144a16545ba53c14fafa8c", "ref_doc_id": "47977575-fffa-4357-a1c0-cff50282b3fe"}, "8f8561b6-a25c-4035-aeb6-b7263285504b": {"doc_hash": "e801ea4e0b7547d5fd4b18c2eb8512de5c36b640fe41961d8aa0e9dcad215f3d", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "e6882cf8-971f-4033-b3e2-b60f093dd580": {"doc_hash": "cace11b4e64d295fe7fa6cd429cbf41895defa2b45da84476bb97be3eda63143", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "1e5c261f-01e6-4679-a9ba-020d587fe252": {"doc_hash": "182c13b154976d0da4e5f28096e0e9120689db430f6762d8637c2e69fec7acbe", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "997875b0-8b68-4eb2-b1d0-a77464f21cf6": {"doc_hash": "db2c856a28fa9ef0949db98ebf1430e1569fc48239c56baf1fc9a2241a5ff825", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "bd95e015-3f83-4b82-8cdf-06160241bc94": {"doc_hash": "a1fbd2de6d6232bbcc554861f85a7dd456c46d0b0346b2083299b5d693f6916b", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "56f9f000-2de4-4757-bbbb-36f169265b26": {"doc_hash": "e260762fbc526defcf801c79f1082b2bddb70085e86a665825605a2e10b69d72", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "0b805162-992c-4dbb-a3e5-cf7f71c3236e": {"doc_hash": "df2becbe613f2b5641cfdfaad6d74adf506932daaf6ea5848f3e5fe79e28a8c7", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "aa4c1ab0-c77f-4351-a209-42bc80883a18": {"doc_hash": "ae01c215f9008a4ce1dc69b813956e536fb3c88852319296326c8106189be26f", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "ac8b0e86-593f-454f-ac21-95982f436246": {"doc_hash": "e4f2808381d5b38b97eeb2f6398762838e1a3d545251b76e8acc6f69f308410d", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "4c3f0732-6917-44a4-a33e-a6956ed7de57": {"doc_hash": "f0782fc30471994d2d4de546129846e1ccfe21d2ea2016615e725210d7a83263", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "b09a0ca5-72e1-4e96-851e-d212de48b481": {"doc_hash": "f89632bd884ab163c71e213e2bb9434c345ee2ffc84faa61a54fe4c6943c84b6", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "a18efd4e-6f06-4fd7-b994-d2fb42626c28": {"doc_hash": "4a0b24e3ec0a0c0f4fb54ecea42fa8b5863c32b419f2c514d64a46bb9f81113e", "ref_doc_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528"}, "e105c176-31c7-49fd-ba76-6cbc88c67273": {"doc_hash": "f3c34fa1c9ed921fb90d2b7ab03d2a209678d80fc3ad23fffac76144ae10c310", "ref_doc_id": "777b7a2a-a9f3-4979-b600-eec8a49b8e7d"}, "fbac1618-9d37-4eaf-afef-7bad5a39609a": {"doc_hash": "71037da59c0bd26ca04959e9d947bded4adfb0138dd93b7caf428c905424e740", "ref_doc_id": "777b7a2a-a9f3-4979-b600-eec8a49b8e7d"}, "ad928673-1a9a-4b70-835b-9bd579452e5c": {"doc_hash": "71fa6dcf7a2715c442dedef618639c13e0190de798a623fdeef301771abb8ea2", "ref_doc_id": "777b7a2a-a9f3-4979-b600-eec8a49b8e7d"}, "d0aee2b2-4853-43f7-b674-3ac18a01e8d9": {"doc_hash": "33bae556b8f698593b723d173da13f52c18921965634bdb35d995c7cef62da11", "ref_doc_id": "67237423-af56-4120-aa24-70eb93f29297"}, "27217ae4-0599-4cbb-8c2d-d39754903de9": {"doc_hash": "70458207f3eda1c6c792bd9a3b4adc8db6cf4064e6540fa7800c61586235a273", "ref_doc_id": "67237423-af56-4120-aa24-70eb93f29297"}, "6a15ccb5-78ba-499f-a01a-3b43a9295015": {"doc_hash": "cdbe1025acb6447f0e7cbb2ec4159d2244947be9f080c66b770d197dd98f3f6d", "ref_doc_id": "67237423-af56-4120-aa24-70eb93f29297"}, "9ba2013c-de54-4185-9c98-baee14e2b2ed": {"doc_hash": "473bd34bc09ffd66f7030f1261dcf64f3d52ee1c8256ee335b0c0a664922ec11", "ref_doc_id": "67237423-af56-4120-aa24-70eb93f29297"}, "81966ca0-1c09-44d1-a78e-3cba76b65ef3": {"doc_hash": "b058064777fbfa2850e5b68bbf9810b9c89475012047b1a2acb52e921cd775ee", "ref_doc_id": "67237423-af56-4120-aa24-70eb93f29297"}, "bb24aeb7-99f2-404e-b3c1-47f2bc628112": {"doc_hash": "1413ccec31f10afa3b9b4e200651d8332c4aecf9f7d3d7fb10e1084616b65f47", "ref_doc_id": "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2"}, "6d9b8bbd-faf9-4797-9672-b1d414b96f9b": {"doc_hash": "067e985a823684feec09179e238040e62d0b5f0f213f52a6e4636566176de31e", "ref_doc_id": "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2"}, "3d8ab82b-4a62-4e06-9299-c7661a4834a2": {"doc_hash": "71d026d26710a55140af7ae66a5b30efdebcd933fc164fd98366487952293961", "ref_doc_id": "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2"}, "f01f2a99-fd5b-4648-aa8e-4f0c579664d9": {"doc_hash": "e7b194893c52b0da2a8bcab8f5f5360db363d33660bfce5a0886417673488136", "ref_doc_id": "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2"}, "3ff5ddcc-8cb9-4cc3-bb88-ae1986126506": {"doc_hash": "609b079ca18d31bb4b3c935c0bd59303648db51218f6c2e4faf74daf15f01ea2", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "d8466c53-c24f-4e28-a96d-352cdaa4595e": {"doc_hash": "b568da7a64f835b2e25a661209383c178d39485a3260ef54124814a774ac1ffe", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "efcb45fd-4838-4ab9-925e-deb45eeec8ca": {"doc_hash": "47c375b2cab0db09ebcf4a8d61d6afa3461ed27f6a7c168851fb007105d5d266", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "0cdbcef2-c1ef-4d54-be5b-075258d9e220": {"doc_hash": "1ac1efbb986d0c9cc7e70182a3d6d80e7e54b53475a5cfd6f4ea2f85331fd39f", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "fd807025-95a4-4292-8c9f-949027f31ad9": {"doc_hash": "3c24e42ccd2f04cf660c5b69412ddcc344665f7839f28c8474a35a12ed9b80ad", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "f286cdc6-54f9-458d-857e-071b1c754f2f": {"doc_hash": "1fed6f58655be4327c95b5924921243d6695d8ef33eacaf61d50a98ec219c71c", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "edb03e85-e1e6-4883-84c5-7f04c1f7d52e": {"doc_hash": "e38d7a4d987c062bbeabac264bd6d81652366bec2b4151986b0f13f969aee8ab", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "f58d8f3c-6a5c-4be9-a51b-579b5193fbbd": {"doc_hash": "605265cd098747993fccb9a67bf262db8a49c354271e72816e6cb2742eb2e6e7", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "1d2226d4-a3c0-4b0f-b6a8-80d6c7e5b9a7": {"doc_hash": "656f855b399c0124b54bb3b111970b674a88220e299e4384731fb02a6a83ff7f", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "e52c40f6-7d2f-4ae2-b851-16552b84da0b": {"doc_hash": "c71045ed056787deef6a747e67c69710ebe83bb84d1698db457203d254e9c4d1", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "47085eab-6f62-411e-b472-740c6000cb8f": {"doc_hash": "00d5832c1c657f3cf91fdd20cfa52e641daeb4ed49034107c18ca58254560823", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "45c925c1-251d-4c47-a62e-234c7405e77a": {"doc_hash": "54f648e0086adb8a0ca6616cb8313dc3bbb43cf98d7d1e524b374b752a8622eb", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "8007cc36-c2f8-4633-a86c-143f8511294c": {"doc_hash": "3ed0062bcca0df70de36019f0feea2177af9c4beee621126cd7e1ecce676b51e", "ref_doc_id": "2f911bd0-c44f-44b5-b226-560182781bd4"}, "45c0fee2-cc78-40e7-af23-53ec99edf098": {"doc_hash": "9376ae0bb3de5b9d93c4b06aa0169e24a589298c73225181cbee2c59a6055201", "ref_doc_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8"}, "34e65364-5276-4335-ba80-7937a322d9e9": {"doc_hash": "2cdbc7189c6b2331ed0eda1a39cc8e0cb2ddb6779ff251d282ac3ee026894586", "ref_doc_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8"}, "688b7b6e-0c00-4fa6-88d6-dcf3fbf89bb1": {"doc_hash": "7fca1cef82704c1fb55b473660d62a4d2399b8d611998f5e275f902db877dd20", "ref_doc_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8"}, "3de7168f-247a-4265-9390-1eaf79e5baf3": {"doc_hash": "03d63e843fb13876b151466f75a8b20a23729cf0ca9ee95819e621ed5a2c1407", "ref_doc_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8"}, "e069d5ff-744a-41ba-b927-8ffb9781fa71": {"doc_hash": "218230fef6bd531f028d6b9092102d309637e7d326721207232d48370d0cd64f", "ref_doc_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8"}, "f3492345-b073-4336-838a-371332162bbf": {"doc_hash": "d4351e4743c7e933f2597b2c8494c40a8078428975429d970f6d460c3bb58d92", "ref_doc_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8"}, "580f4f20-b533-451e-9f4d-41040256ad76": {"doc_hash": "0e092b1c2d0ebc1032ba2a50ae8e08726d2a0e5e5d582148936b92d7e7b6944b", "ref_doc_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8"}, "b3066eb5-9faf-476c-a0f6-d17bcd552755": {"doc_hash": "29221a2e2e773d0cdfefc2cc1e4b88e9759e7e43cfa922c01c6be58cdcfca283", "ref_doc_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8"}, "af8294bb-953e-4ba7-a128-25c0962b4deb": {"doc_hash": "601fa443748c5173431367612f206e6f09b7201e5cd9bfac41d40dc8d9b76b12", "ref_doc_id": "0bbbec1f-efdf-4525-ba8a-93c7d136290f"}, "61e1d57a-0e7f-44eb-ac51-dd2150203ec5": {"doc_hash": "d922a49554476f330e70fbcf0fd2ab2b184dd3e65a54149c7859f140bcab0216", "ref_doc_id": "0bbbec1f-efdf-4525-ba8a-93c7d136290f"}, "8df0a542-59a5-4039-b9e7-70c5ed180bb3": {"doc_hash": "2f08bafd245c56d54ffb74abdc3506f9d8ada8bfd709e940ec333f74fa16d262", "ref_doc_id": "0bbbec1f-efdf-4525-ba8a-93c7d136290f"}, "c225c7ad-7af7-47bb-b644-a6ff481ae9fd": {"doc_hash": "21c88168501f07d988b9b304ff38442a7679d1069b323ea446d3c17a53fb6822", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "cfe753d7-fc48-4b93-a301-6f81e7047022": {"doc_hash": "933a216411f8fa5d376832c66d03dbb89bfd9762d6bc29784cfcd08bb8d3afd8", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "87ba4671-b829-4c5d-8cfb-a3f6e74c1006": {"doc_hash": "197399b6377b123e06584d144982286b582f70f8e93f09954043bc0927bb199a", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "2b765fd0-82a0-44ad-980d-aec425c5d8a7": {"doc_hash": "beba06047ab508b2d918326526b996c76dcee70207a202fc95b0ef5814668c12", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "6e5f49d9-4a1e-4cf2-9044-bd2b7c53a4e1": {"doc_hash": "b40fbe1cf4f5e0365efe2760a9cda77d935ce6ca724e03e55a0f95f5c3cc2d3e", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "97ceb643-957e-4ba3-8375-750a353ea863": {"doc_hash": "7b5f68faa6c3acdad46e2a1e8b31b3af459e62ae0239e3171e25fac87be810a7", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "2f3efa13-85c7-423c-b003-cc10f0e145fd": {"doc_hash": "49025f6bf9b854e713f67cfa1004ac2f95fd12e45c5ab3b5511cbb5c2be4a732", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "41c9842f-3d0a-4a2c-b5bb-9df0586049a8": {"doc_hash": "70403d6a51808cba8664a3993eee3a67c0733a1154ac5fc95a62b21aa87de221", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "a382b835-c6ef-4ac1-89e1-dcf054ed463d": {"doc_hash": "2ef3401b84977e84bd4511115188ca54f05837775d1d2c16608707c55999afb9", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "a6a1833c-0d62-4f9c-ab24-55d14624bdd9": {"doc_hash": "3ea00e952a4c681a25acba8993ae807552156ba4bcdb09b87fb6ef77392c9350", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "0676830f-743b-40b4-af9d-0948b4818846": {"doc_hash": "9a3fa9fa2724c5900cddfd29346594460282f90a551d2b74321c63821a2ac6fd", "ref_doc_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6"}, "850eb63d-3f73-47be-a4f8-194429b25a53": {"doc_hash": "4876c416cc3d0156debf7a45190b86c4c34a6043a4e7ee2a5e53bba4d7e093b5", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "fff02b91-315c-4bda-8221-b5ee521682f3": {"doc_hash": "2f0e6c9183d76f0c8fde4b393281b2c591377207dfc224096095525ef97a5fd0", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "5a480064-3ecd-4233-8b6a-e9cd659bd8a1": {"doc_hash": "4719e233f43cef9485a57e67c5f124c4fc9f33497c07acdd7840e785476f7e56", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "4ae89cdf-3a48-417e-be0d-ae50a557964b": {"doc_hash": "b38d154c95d3fe3bb5a108ea0a32972ed6c8106f7d0f785134929d6cbbdcd637", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "1c7f22bd-f4e4-48a6-9398-339099bcd19b": {"doc_hash": "0a6a014c8ca36f5690f60ea4ec1c4e57afbaa5f9ba10f6e0c0173dbb8be7131c", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "89a49ef1-3931-4d51-a21a-0c4aa7385ee8": {"doc_hash": "0bb956953bb772ceb3a40d117db983ddc62e838c76be9e1ec2204b1a728d4eb5", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "00576cf0-08da-4d13-a604-ced094435a92": {"doc_hash": "e11fd7fc5bdad382c9e25deff713dcf498ebd73da618e7dae8d0c7123176ef4f", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "169f02aa-7d6b-4ae2-bd3e-6a78d9255178": {"doc_hash": "6d11b0a7f0d45a96702a02ea0277c5130a958888755304beeddbdbfb6791258c", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "55670111-0771-4ded-b490-742332a58dee": {"doc_hash": "5880931b3cd4692e2f2bdd78faf7efe9dd114185b5be22c83f40746670f3794d", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "85f1a93c-3f49-4333-88f6-0812e8f8df1d": {"doc_hash": "7e72ebf3100ce9ca59f89868f4ce790c07ff175b0f40f6227b8e9532e2dc84d4", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "8d094437-13ef-4076-bf8c-37dda3427653": {"doc_hash": "eae3cb0a65ade51833b40c8abb31732d091a660379cec6e9585ff6cc7bc26630", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "e8fda46a-e6bb-44fe-b59c-cc89ba5e41a0": {"doc_hash": "971ede68734cf161fc28f1b524a61fe2480706bb8f83ee72cae3db7b7fd112cf", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "d45ccdc5-24bc-4705-aa6e-c099b00aac72": {"doc_hash": "41b7be257c988e8277018fd8c44067fdbb22afdd921b731916275a098640f0f3", "ref_doc_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0"}, "78efda29-5e61-436b-aad0-505d1d6cd97d": {"doc_hash": "006c56a70e6b74d667f0866cda6ce34b866b07f48226e085662199bd7c5aa10e", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "2ae5ac95-2db8-4620-aa1f-092ac89f40ac": {"doc_hash": "235bd5605c7d3c5f251ce3f55fa7afd60101cf8bc58f859c25ce169a0f38d5b6", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "23c3446b-a8c9-4e32-9c40-27dd5bcffeab": {"doc_hash": "2328a7f8fc8543f95417eac5f751a401b1e19f4d25802a31e11bd1c6710b32da", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "324c4d1a-38ad-4a4a-995e-8879cae54099": {"doc_hash": "60310b1fd0973e42394d31c84b9c88699475e20706f60c6ca2b8647de086258d", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "36e2b789-7df7-4f6c-b28d-de01b984a2ae": {"doc_hash": "b16e0aeb4606ea9acabb1a5e5ce8075d3a78efe2bf9761f3ee58e27bbb1a29ae", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "6e96e415-7fc6-4a95-ba0c-f59072f0ffdc": {"doc_hash": "8d32d7d44f73f35139c934730dad4b3be385df0f4abc7ecd382b00150e91a620", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "a33c3387-2dd9-4966-ac12-ed75207f98a5": {"doc_hash": "49d7f4ffa43c6be43c79daae1718628da34c6afb0ec84d14e793336237957c54", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "9911285a-b813-4946-9702-07251119218c": {"doc_hash": "e3384fc4953292a95958fd563abb47755d67778af4fb1b9c548227854d8585c2", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "3c52e911-1f68-4bb8-b478-64ad1e88846e": {"doc_hash": "5d1b7f16f6b53daee2d1658fec7e134a5ff42c37afc13516b1d7422c091eac0e", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "57ee31a2-4529-40bc-8a6c-de04c79d481c": {"doc_hash": "4a3cb06536ad68270b8a17264643fd470a72fb7b4fd1497358ef999e699a96f1", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "88f18a19-e019-4711-bafe-3477f3895402": {"doc_hash": "bb8080276ec2b42e1c5c13f01c2c6fe60766e40e1b0fdcc65962ab7dd6c9bd22", "ref_doc_id": "645b2691-3c1f-4a87-b41a-d620be00ec85"}, "0c7ef801-325d-4e42-990e-9ee4c2ba11b3": {"doc_hash": "7b1aa6ef36eb6e9abe52755f9748bbff30a8c733d7b7eb458f19807254bca3bb", "ref_doc_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a"}, "fc4bb9cf-43f3-4229-8486-2eb33d659acc": {"doc_hash": "ed598f5b0a0faab4253945245c84ba25222da53038dd873a09f54900478bc261", "ref_doc_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a"}, "c235149f-381c-49c5-8309-e774a4457fb0": {"doc_hash": "d9f956af8d16c1dba66c99b375fe6b665a15399f23695516935bdac6901b0c55", "ref_doc_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a"}, "ab43e416-226a-481a-bdc2-4b92ede3a636": {"doc_hash": "2d895e992c00a1a859c11fcadf9fb6891055f19ae14696577d9347e07a5ca4c7", "ref_doc_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a"}, "8f5a2669-15f7-4dc7-a7db-3a253df3c03f": {"doc_hash": "6138568ff2b4cb0e77c007394496650dad1d70edcfcf779f0d1a4009c991e472", "ref_doc_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a"}, "1c7f8342-aff1-4a66-ab67-02c98c9aff6f": {"doc_hash": "04d934efe7cb70434e911100e4508a993400cb2a586e2b9ee6605ce7c5584f0c", "ref_doc_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb"}, "26a1788e-c342-47a4-ae23-22c00611b98c": {"doc_hash": "2cfc07232c6026161ac999557b14d5d6142fb5b8f282ff37beec4d278f5b6b39", "ref_doc_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb"}, "afcf90c1-6db2-4858-9311-cb671723f0f6": {"doc_hash": "855af38d4abc81303bc6cd82cc15901b5e5a1de5a9bbca73e143e0435de467f9", "ref_doc_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb"}, "3c919a47-5f29-41c5-b7d1-b15d78e8dd8a": {"doc_hash": "0882326cb098df5a320c4ea46578634793214caf8f99b251e8c4c01abdecfe3e", "ref_doc_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb"}, "0a0c745a-4ce6-46c7-9097-3ef9d8a4d38b": {"doc_hash": "41592c4704ff7ef31d7c7b54f5ff63b8fd51473e20d4bbfc65674d37abc6601c", "ref_doc_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb"}, "0965d9b9-8345-4ac2-9fce-fc4b9eee8918": {"doc_hash": "47c57f8157b9828f3cd0bc4c390e3b6d18cd4b76a4dd747749c7dbd4e64dfc76", "ref_doc_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb"}, "95914297-72b6-4758-a223-358c6e8ebb2d": {"doc_hash": "c71c855de3dbd1e3ed781f56ece02f98872f35c71af16d45f01fe433df540e73", "ref_doc_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb"}, "e7a41d50-04ce-4a68-9d7e-b3ec356957ea": {"doc_hash": "7ba017a4196d84d7205da2eee57806405265672750cb45bb7ac955fbbf0a26ec", "ref_doc_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb"}, "529c9648-92dd-4acb-8e9e-4abc64947169": {"doc_hash": "afa6c1c1a23e0e42c64d9c3ed10067411fd962519e81b8d2e7e5fea72b6ac5f2", "ref_doc_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb"}, "6ba914da-d7e6-4b83-bc5f-d4afc270dad4": {"doc_hash": "b5b307799975aacb7645be5f903d2966befe5124a46812eca1f2716280e8e256", "ref_doc_id": "0888add2-b651-4598-9893-5922248a97c9"}, "758c62da-76ab-480e-92bd-39b87a48a406": {"doc_hash": "29d0dcd74cedafd3be30884e624cb1fd0063dbcaf705ec86b56e41a30e02d6f5", "ref_doc_id": "0888add2-b651-4598-9893-5922248a97c9"}, "e271d3c8-a8e4-41c2-a214-c9e3fbeb81ab": {"doc_hash": "521eabf9449a9e14164dfb1604039b42edb4ea62c5a054e6d91e5de62a3af7e5", "ref_doc_id": "0888add2-b651-4598-9893-5922248a97c9"}, "04ffd1d6-c418-4587-9df1-d372db06dc6f": {"doc_hash": "8003e18237d2417afc89574657a533668c11d65580a182f0518b478d3db3bd7a", "ref_doc_id": "0888add2-b651-4598-9893-5922248a97c9"}, "97a87b7f-003a-4b97-92eb-623de84a0ada": {"doc_hash": "3fee73c9c197aaa9f44e13269353599aca0916da5c67d7ded806a42ec0fbc7c4", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "78706d88-6079-4cfa-9df0-2cc951081685": {"doc_hash": "e0f4851877fedec530159ac3382494edb08777289574ed402c8cd021bcfc8bac", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "80e00c7f-06a2-48a8-9108-7845ba032f76": {"doc_hash": "65ac885e3adf6edb0e2528ce48e12eb41ad53ffa77a3b55e9c29293971c0a231", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "42eceeee-6cc2-4570-8be8-f6e1ff1b2531": {"doc_hash": "f8a0ed63f1467379b6f7a6b8548be28c05a60d482ce26c3fc2ba72c30e628312", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "e7f781d4-ccb1-4fdf-a7e8-14684c118690": {"doc_hash": "99d6b375db3c2ca1333a64b770d3ce84ba702533f72c87f12c093c2ff598c7d7", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "6e0cf0d9-6ebf-451c-af03-a5fb0566f715": {"doc_hash": "1b487b99e63b86b7792cfaafc76307f1433efb1c7eac8b787bb043154f8a3bee", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "4928e89b-90e4-4ebd-a5f5-8fb076e35b35": {"doc_hash": "92e0672e5924e1d846b4514abf5de83c0b3acb5f916593dd6ca9993598557934", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "14af0eac-4713-406f-9899-10e23567b00d": {"doc_hash": "bb697b1773ef7bdc7f8f5e2816220e3680a67e2d0d0c1a113e9d897f3db88d2a", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "09e7ffba-5ab5-45a3-a9c7-6f06464de225": {"doc_hash": "269c88e401be6c0dc5bd0f4578af04c2bc4f20213773f332e8e6bf0ed6347c95", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "d746be72-d5ee-4fad-8fb6-185be883418b": {"doc_hash": "bfd441865ed90017aae02dd66a2cd8eede442b87cd99d51f83237154cdd866a6", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "b89bed59-d1d4-43f1-84c2-3e484f521bf8": {"doc_hash": "b2e77f2946f338e165abce30568197849986abb5eccfce8cd16b936e8c05f155", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "f70c5049-1c69-4717-b0a8-f9e55b96637b": {"doc_hash": "ffefd7036a0904bf43d5c637b85817abd59cffae9f506f04f84be0a7dd8bdfdc", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "d6077241-ef53-4122-a268-1c30a2f954d5": {"doc_hash": "e676b3d28028ec314ef6c461a59f0ae93b072501f5e282d15f2f11715be3dbaa", "ref_doc_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9"}, "4ba4f0b0-9b74-4cea-a015-9a6d8e8e1710": {"doc_hash": "1e842fb13969521d0e253fb9218ff2c1c56c833f230a13f98121c8a261dbeaac", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "e9059c12-5b98-4e65-9ded-730980f9276d": {"doc_hash": "de7599891584379fa07122b8f7ae2c644802ea79a76e937e10dfe51bcf2a7930", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "06055661-3e92-48f2-83cf-ddf96872f49c": {"doc_hash": "de69cce962bfbbb8d8aa53ab25ce635f6fb5160eb1353e334dc6947f1b086014", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "a73df900-f4b5-4c92-94a1-b35c0c18ef5d": {"doc_hash": "26dd00b9e1e86665d9d7db082ffc06eb96e13e41fc54e1a42f86609e793191e8", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "bfcc1e97-ef78-4850-b7ac-1f9ffa217a33": {"doc_hash": "a1d4683ca75e8a7d32d29ca0baf9a380f109434891945de63e80f804d9197ba7", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "5204ea24-9dd2-465e-b720-79d81bdf750b": {"doc_hash": "db3efa38a29b97422811f0697563d248fc9dc08aaad098609b433abd714d74bc", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "426b4faf-6ad8-473a-850a-4c5e3dcf22c9": {"doc_hash": "374efc8bfdc28cdc40b1b0abd6c879c93e2ca88f410bd69b0d6474ad5e1ef7e6", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "d0c601e0-ba51-45f0-b2c8-6c6cccc23221": {"doc_hash": "f02099e9fc290f92c4d841e1e67bdc6c204acfb2e539e9b2ebd12d206e0fdeb0", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "afa03b52-1847-426a-8a76-e9f29182f7c6": {"doc_hash": "89f08b5c0e7ba0ea2a4bb84430b6291baa785085d2be7e62248bdc3f4fd914d9", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "49378cae-63ec-4e61-aa89-bef5e683103b": {"doc_hash": "7df4a0cb10098c6914b75b28a8158fab028d35fb3a71817a213202dec4646200", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "f8a22a3c-0adf-4397-a82b-3657c343eb35": {"doc_hash": "5691303e807f4a033cabca4fe883c49dd4f1c6b4b1dbf84e4ad3adb60625c59e", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "f1a2369d-52cd-4b13-9254-b0d43ee89149": {"doc_hash": "e5478ffcb8e13fd89caf04e3b6bd38677eebe68369556ac766ecfdfdd2952013", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "ef7feccf-90c7-4254-8868-1b9aee2e1759": {"doc_hash": "517f0701ff226202de8b8be64a7325c8776d315fd934fe67c23aa6efa96f017f", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "5109ff20-66d7-4c3d-b305-b5688f11b425": {"doc_hash": "f9d7617081940808064a8d1b3c39b26e4992d28ceee5f34c62bb05ab2a36c001", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "6d1bcdd4-6ff1-4e18-b37e-d6a8cfad0c7f": {"doc_hash": "4a681d5875b570bc788333f733457aae36a9fe5699b206cb4ea6a17f24f01b2e", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "fa08f408-8d6c-4741-80e2-31141bde438d": {"doc_hash": "f85a5cc87f013531aafc56c9a31c3dd134fb0448823b500afa969539c6ad4a36", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "67d36f7e-f87b-452a-8490-2a1583a45a4d": {"doc_hash": "a5c5d48c029dfd052063c749bdca12fd680ebfdd32f21366b01482c31cd463ca", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "d80970f5-c462-4381-a340-5a1bf000238b": {"doc_hash": "3909dd4dfa790f6b593ca1cb904f7f336885850c1b93105da279bc5581b41889", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "f83d6f6b-5703-4cd1-bcf9-3db13dcf4a3a": {"doc_hash": "838504bda000f89e657df1ad0f7d229707341918766662f701107b1977aa9dad", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "89481538-57aa-4755-a40f-ca883786125e": {"doc_hash": "fe2c2600311ce5b77c0c0725c8133d06b413b941349769c5da5766684e705562", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "bcd1e421-103e-4735-a200-17803c811991": {"doc_hash": "2acc1626216e192971b0648ef32beca73d44ed81f587160c2348dbffb7de9bbd", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "3225dfe7-8db0-4070-afb6-7ec70b4a26de": {"doc_hash": "2a86584c6a9e38a83733fc146470352391a639d88437d598db082fe09dd9f2e8", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "5994e311-edfc-439b-939b-9df3e60d27f6": {"doc_hash": "85a46ba7959521d8e7b431faafb6d24992f5673bc9f9481436784d36d844cc59", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "ccff69a2-86d9-4206-8eae-47ab9df1fd31": {"doc_hash": "b30499fb5f605f4146a7d3c2ae80c58486ce0bb34e4f8989c572fe16cf709b01", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "59290b1e-1817-4bf7-bee4-ebfac7d14792": {"doc_hash": "932a9ac6928325dfa7760da26dc5852fc122279b899f7f1406096520b43c5555", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "5c543585-de51-4e1a-9275-308e98d43465": {"doc_hash": "483e70b7f869b95367b6a39821753e9fe57e73a993107376ef8473323568b711", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "8e2adddf-5564-47d7-812c-c57416a423b7": {"doc_hash": "4f6061959f8b09809055b4aef6b0551dd493df2bd510ae5c99d6d1feb0bc9836", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "01caa303-9e21-42b3-bff5-44b86af5fa73": {"doc_hash": "95924eff69a5a74f1e8733d0d8c1b2b655ab1b804a292e004f7bf314f5064aef", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "2178baf0-2e71-4958-9144-1216f2f1f9c9": {"doc_hash": "9d2d2f2835c91fa78ba4beda6be0ff1e1d0eea992d649aaeb05eeb6323a79435", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "bdcd0665-afef-4d12-84a3-80eb97def906": {"doc_hash": "2191e9aaf38b3f2855db7ede1c7336d74403bff17340d38d8f366f90417b7c96", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "80c0a361-b35b-40b6-b798-65ce79ea5f35": {"doc_hash": "48c2dc57d099c43c09a7b91b858a0fa5a460ab088edf99f904f31fcb0720f786", "ref_doc_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78"}, "99d3d5ba-d4d0-4259-bc8c-705a0c3678e9": {"doc_hash": "8257010ac73abe4610511a94ea78a142d28b5692a2a18cf81f6cb2dd992b43c1", "ref_doc_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757"}, "d5bd9449-2f9f-4ecd-8977-7ba6aab35435": {"doc_hash": "4677ec8f56f3f4b33148d8f64222e51ac826c462c11dd9a549ca25d5a993404c", "ref_doc_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757"}, "e24f2a49-8d1b-4530-942b-7167a89bf90b": {"doc_hash": "4b441da2e403fa501c41697a69fedd4ac21cfb57bdec30d17af6b733d37fe45d", "ref_doc_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757"}, "f0d07f09-f859-493e-b6cb-5c55c56d668c": {"doc_hash": "16044a6f1ba078391348f893b57dea7e521813d1a1ce5abd877c96900a895872", "ref_doc_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757"}, "bef8dbb8-829d-49e4-b438-81772ad0d899": {"doc_hash": "87be114f559ef9642165601cd3734fdbb2ebd82f5e93503a1ec1d477f1c31426", "ref_doc_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757"}, "f4aa9d5d-217f-4f40-9bc7-3812d8ce8aa6": {"doc_hash": "006bd02602f64b8e5edfb49cb8895f8bc20a37cd074cf7cde05a57510e3b4c9a", "ref_doc_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757"}, "543bf6bc-1eac-4ae5-bf47-56de26276ade": {"doc_hash": "57862089c3731789c3a5240d5492a3c84b56bf4bac1d98023247b41e674852f9", "ref_doc_id": "fa0d9664-d941-45d1-8d3c-afe24618fbb8"}, "5b003578-7bd1-4359-af6c-e8c6b9d49041": {"doc_hash": "5f8dc2ee9f0b6120b3f8e0f7ab899e26d3863f3f2655954bab6c096d14f3e471", "ref_doc_id": "fa0d9664-d941-45d1-8d3c-afe24618fbb8"}, "c8f8d992-cf93-4c9e-81de-9729f18de176": {"doc_hash": "39248700bd1a4302eed624d2462cda288eaa3c66e6f31576b6234ccb75a51fe9", "ref_doc_id": "fa0d9664-d941-45d1-8d3c-afe24618fbb8"}, "bbad918d-30ae-4ff3-8757-5e7eaa4cfaaf": {"doc_hash": "78f2fe4ac265b1d8ea6f51974e9e41ec1078197d8d36dba9223d8fa938f20a53", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "bee065b7-2b55-4c0f-840e-80159963033b": {"doc_hash": "eefebf8cde786f52a872edab34ae77143ca90868258127629814fb5995e59ef8", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "15170e97-1ce3-4a72-843d-4eaa53b9cfaa": {"doc_hash": "671ab83a3a6d23e69ea442a5ea0971da1abe1fcd604b34486b886df3c63bded9", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "eab2a285-0e6e-4f09-a752-c24cc6fe9a82": {"doc_hash": "ab8ba523be525e4124db01a528b34a245a266b4d98cfb0a26bcd38ffd18794a4", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "74b23b5e-9aac-44f5-825c-3abf739e1aac": {"doc_hash": "779ded8d767cc87bb64ade4b2a940c194eea284e30c0a221ce7e89c5720a96d8", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "ee3f4a17-ccb1-4467-ac74-247980d83251": {"doc_hash": "774461af606aef0eba676317863e19c91e3d5e4bc497a9ab5247f77e4740835f", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "73c8af7c-febb-4309-bd71-f09475fdfd85": {"doc_hash": "d674de73eb036e34cabaa1bde2e64548825fe1ceae5f7b66539572ea3df417d7", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "436b5466-ee41-4372-9565-a3e3131c2949": {"doc_hash": "78d3b45828a981c4e1066fdd49c3c51df6ac92ef314bc4d858d4a44839d5da5f", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "8283437d-d295-44bf-8eff-70868a7153d6": {"doc_hash": "e257795710755b210e2f14a3021e605289bc66a594b94f3e164bc00cfae8060a", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "ce4ce2f7-8911-437a-86c3-50d1e21497f3": {"doc_hash": "77615194a18e76e4c19796dedc0bca9e97a6d775679c1a883740d8af6435229f", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "41054fef-3fab-4adb-9ac4-88a24e989a66": {"doc_hash": "671feba41736abf02d1fbc479a1f5591c0f65fb8eabaddc41357a93acaf2c4a9", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "cfdd6e3e-9c2a-4e29-8c73-7564c5397aa4": {"doc_hash": "4520aa2ec4fe3a510a2df5c9ab8ea70cae28671eb924eb19089c8fc063814024", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "c7448192-91d4-4829-bfaf-af421667b63c": {"doc_hash": "3f0a3cd5d4a21c6808f8528dd6797f488321571c127483a7975b6bca72154e75", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "35fd6598-17c4-4583-8762-ac9e23dcaee4": {"doc_hash": "bc83451f1d64567d6dec5265b1f3a553f2414dad2fd4483a0b99ba5f0a40dad1", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "25514e69-c941-4475-95c5-dfa182eaee1c": {"doc_hash": "cf8a0abea3e7951817657a01bf4aa20dbacb28a51c8769234a2ee92d1cc10548", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "2c2b1a7a-31ac-49c0-a2ed-38379e4df154": {"doc_hash": "9c9b2938a62d5e042a95c9a32068be2c3a80c0319abe242de26d4fbc3ab3843a", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "9a00cc16-ae52-423a-a5a1-6d33128bd6a1": {"doc_hash": "415d49fdbcb4b787eccfc912183c4259c41096a8a75129a60b32d72472eede9f", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "7a760f3d-88a7-4517-8486-c51269a38888": {"doc_hash": "d02e39857d66241c51f2b52a9fddf6f93db2382f33ef6acd1daeefa625b6312b", "ref_doc_id": "37222704-0a59-4da0-925a-e2f4ab7c0729"}, "0cdb5fde-7a5d-442b-a88e-6ad85806fad2": {"doc_hash": "5fd48f837c704b874eead4c813498db4511dbced432721db130e7a2a67af880d", "ref_doc_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0"}, "9bc4fd67-9769-4da1-bbab-12760a299a4d": {"doc_hash": "a58b564741db6f0c1bf454f11ffe893eeec6a1e50c9cac6294f9fe6cc47b16d3", "ref_doc_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0"}, "737c143f-2576-4f45-a854-e39a5ade0454": {"doc_hash": "60621656a96b064997629a78c4fa599af85c619640256bb936d4293b34f58f87", "ref_doc_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0"}, "6ff60224-6fb9-41f0-8c34-92ce4ede4172": {"doc_hash": "de4ce48021ff3b3e1f0066f3868739207dccaf8f023fb56ff5f62dfef0f37e89", "ref_doc_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0"}, "b6e6edbe-b8e5-4c4f-bf19-7b8a1ff405b3": {"doc_hash": "0927884fa2ce778cba5c37f3e52a4e61162fcf9a49c5b4894159a3828177377f", "ref_doc_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0"}, "ac88c352-89cb-4475-8269-132388625273": {"doc_hash": "15958934a02e6850a4afbefe580656bd73ff7326ac8f2a2ae5229a2c074e302a", "ref_doc_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0"}, "6b800ea5-0623-4e7e-a5d5-ae6b0f83adb8": {"doc_hash": "6166d88f78a29979597bef2e2c953f89cf7916871e313b867efa0f0c72a62a4f", "ref_doc_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0"}, "7518710f-9ca7-4291-9ba3-f007d085ef0d": {"doc_hash": "4671cc31cb012b1d8e5f03f9d6391e97e91932447e5b655a40c7baf6b8107feb", "ref_doc_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0"}, "6ec64c04-8560-473f-b0f8-a38b2e3251fe": {"doc_hash": "db966cda99609bccaeec2ae75539da6a1e6baf206cf0c48a6d069e2b14601d55", "ref_doc_id": "67c64319-6584-4bdc-b600-62c718831df6"}, "70dfe8e7-61f0-4329-8e86-bba6c99b3384": {"doc_hash": "5d7edc048f5f3fb29f0de8a590456abdb9c6ea2072d156023eefd7a3e2c56a1e", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "c2e3055a-18ce-4694-9f0d-96b35b5732b0": {"doc_hash": "ba24d3d2b2350e0ddaecb5de88038c9cc412072e4fb23eaa78f91851167959bd", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "be88f882-9e95-4063-b60b-ea860976d866": {"doc_hash": "497b4b6675d36f647c1d3ed37a7eb4c79bd6693443115c1d1a59c7e75b82129f", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "ae7ea765-7866-4f26-9f1b-64e116e1241f": {"doc_hash": "253374b7d835ac46f6023b9c218e6af8616fb3203b5a91dc3411ae8e2392b54b", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "ac99226f-31a3-4bad-9db9-363ac8bac564": {"doc_hash": "ad7c66634c94e35814b043ac0bbef8d6686e844bb1addd4e455312a47cef97af", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "8ae994c4-2e69-45a9-bfcf-029cb6b69e20": {"doc_hash": "0020e9f050f98fad2c07d0af2e6da8107b182eb81179ffe5bf4a912c6b7e8a78", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "a64024c2-d8b2-40a9-9b41-165050b39e6b": {"doc_hash": "abe2b5a012c8e81af55aa134de35e2d665c37268599e483a0358ffa9114d0fea", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "5f4c9d8e-4d88-4a1f-90ea-d8721ff2c572": {"doc_hash": "7a180c1e62123808bdc44bcb964a8d5245513be666d5ef5bcdb4917d145e0935", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "47c151d5-aa4f-482d-8278-9923dd87a84c": {"doc_hash": "6f1bb6e1443acf7519e150d305016bdc7b6670d9c2f74b433c9714b0bbff0e83", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "d0234c91-95c9-48eb-9c53-dcdbbd4be96a": {"doc_hash": "0887029d708a85cef11f13c5b4cf0b94bd53a28321bdc6031a3ed24995d8c6f3", "ref_doc_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf"}, "4694d615-158c-4a50-9edb-cf2069562aad": {"doc_hash": "e5b0b386f247e1f678a6becdb3fba508d7ba5c8f256ef7764ad67dace232b7a4", "ref_doc_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8"}, "b8f89da6-3dfc-4280-adb3-fec694077ca5": {"doc_hash": "5897a6a4fc1f5f9aaabaa797b54c25a3fd2ebe48d2e4fce4a4adc9a9e7a0405a", "ref_doc_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8"}, "b8a9f542-64c3-4e43-8ed0-89b3db7c222b": {"doc_hash": "3735f335773101fce132a6df40dc53ed488c0108d96341906d7b37e0db72118f", "ref_doc_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8"}, "6a2a71f4-2a66-490a-9864-9ceed73b855d": {"doc_hash": "d359d024104548a296d34a7399768443cfa85f3f5641c27bf181021978417434", "ref_doc_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8"}, "a0985e58-e9bd-406a-a937-97aa977d2707": {"doc_hash": "c857645a537e5e228e806355ea8fb1067a1da82a5df7c5564982e46bcbe125db", "ref_doc_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8"}, "cd554e57-cbe8-43fd-8a3d-99e34ba1d830": {"doc_hash": "1b457d3d6ed8ab1d03eb73dc1a591c86b2c5d8eb82054077c4e407a7c21802b7", "ref_doc_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29"}, "7b331903-ef04-46bf-ad5f-2c7cabe52273": {"doc_hash": "4524d4ac13951fa799e57c768cc20d25c5ca2b5a566c29169f1a88a086886a43", "ref_doc_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29"}, "7012fb65-3f50-4799-8bd5-f011dfb15c31": {"doc_hash": "918ffbc8fff59899936b38b094ac1c6482632e895726f0c3b499749be92eb43b", "ref_doc_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29"}, "a0d96c8d-593b-4d86-add1-ef04496a7171": {"doc_hash": "1a8468661ca8237e3d1bee4ccb8d1efc9947aec786db1f0c57d4c614f0fac7ab", "ref_doc_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29"}, "14493710-ca01-4a67-aae9-b89acc28ca0b": {"doc_hash": "5b0eac559e5942579b2422f94924581be0424d552b790405c072efa57b67656d", "ref_doc_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29"}, "f93ab89e-cda3-4f5a-afde-101d5349ad25": {"doc_hash": "0553f0d847c4d9906399ed150eaa7ed774e47aebc8c1006dcf711271aef27bfe", "ref_doc_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2"}, "a2053816-b9eb-4574-8b06-7bbf7faaa21e": {"doc_hash": "a0c49ba418b9fc22617f8185303510b57ea5a6cbabdf134a8e814be3ff02623a", "ref_doc_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2"}, "374d567e-9228-4288-bc7d-9b9d5118cc3d": {"doc_hash": "54befaadd3f2232be94e88d846e50bf13071520e20ba25ca26053eca32b59d57", "ref_doc_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2"}, "ea62810b-4b18-4659-b0e5-c7d34e7ee8f4": {"doc_hash": "b1b62ffe63587337e016c0df03db6e3dc7e305d62ba8e2c28756e234b119080b", "ref_doc_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2"}, "c91a1b06-ae3e-41f3-b7d2-b3d65353b4a3": {"doc_hash": "54856668c8afe3f073b2b352c6de73f08fbcb88c3aa8ab1c9143d237930ea474", "ref_doc_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2"}, "47c61460-18e3-4c24-8063-e675db1a7b93": {"doc_hash": "802892b915b49731465d9ecf09fefa00e25293a375de687e681d550d88bfd60a", "ref_doc_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2"}, "500cba4f-d799-4781-8e18-44990af993bf": {"doc_hash": "b3039226b181e77181e0b4a84d44e23530a8157474acad6d03d173d155835315", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "97520c7d-6b47-4583-bdf0-c111eb12421b": {"doc_hash": "1a82f71c232289057ccbbc8a5aa1a114c31c2a769a354223abe463209cb726bd", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "2e53743b-fa8b-4b25-9e82-67f189d4cfbe": {"doc_hash": "2449da36f38259b4ddc4ef29deaf6111b609f9a339d2b419a5d05372a22f1ff6", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "4ecbf791-d6cf-4d54-8cd0-017baa7c34e3": {"doc_hash": "a8080a7e10e93e8a1ba66f934dc2900a6fa8b833102797af5a1a104ca19c1bd6", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "e12e8e0f-3b9c-43ec-af99-5dd69231a90b": {"doc_hash": "a0e1d3d0c072b4613f65d6b8d87cbd805fbe503f18b7094a49798607a3e68ba5", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "4da52265-8c66-44ba-be49-47e16fcf8b71": {"doc_hash": "c378382368469f2509dceddc53830153393668224c54154341f013ed8c1df828", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "955d252e-8d72-4be9-87eb-66d9f5f08f2e": {"doc_hash": "d509745669029f6e3d27b00e2a66714af6edc788dd6c8dcf26ee4b8cedd9dca5", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "4f990239-3787-458e-a0ed-09b3d54d702c": {"doc_hash": "cf0a270350fd464bc771356d049b705d07c3faec683504defe7a56046bd5fca2", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "727175eb-c8a0-421c-8c00-82eea3ed13bc": {"doc_hash": "557db07dfab26b6247db9ef862ea4c8fed16841da63e20fec5fb2f95594ac907", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "4f4d9103-e696-4ce0-8ad8-a0b921a123b3": {"doc_hash": "d17f7c1c688882cc318dc12c53697292397948afccd9a81da2ee4e12801c99d2", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "6d1f4446-4ad9-4743-9cdb-333265814353": {"doc_hash": "5a350b8908b1245d318b7d3b1e438a96edcd21b2893271cee047cf0baeaab437", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "18d32423-84bc-46dc-9aa5-046a8cdb2759": {"doc_hash": "18c426843c26adea0d268be27280fc8e758488ffa88557f19d85106d42484873", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "e67619a3-0fd3-416d-899f-58d0a2a848cf": {"doc_hash": "4bce95640446a7a300197b0da3f71e0c51981b1264518f084b857791247bf07e", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "e68caf36-73a2-4ceb-8f72-07334ca8152e": {"doc_hash": "6046e9ad116ebbd52cc68bcb65cbc2e474d1c63c5727372a80af45b497c77049", "ref_doc_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d"}, "45af6caf-a2a2-43c7-b8ec-1d951d70e6b1": {"doc_hash": "66fae22cb539a1fcaded02d0512edd445b1cecc1ae4b7b33eb358299641f585c", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "3232d1da-0ba6-40fc-905c-67797e14dedd": {"doc_hash": "07e28ce0e8097476437807aeade1ebfc4544e0f689f6588994544fae654647d7", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "b17e48e7-1e41-4576-a65a-b2141bca4d26": {"doc_hash": "c15fa60db69d0c86ec6d4eef51c9d53696898ed096a12fdd8d33d43885562363", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "1b0b8101-feaf-4b95-8b18-3bfbcb7c7576": {"doc_hash": "d7db1a30206d4387162809637ab913e42b5db07139a7b95fbfd547728ee39aa2", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "260f57d6-fdf5-4982-aeae-0088bc3a6fe9": {"doc_hash": "cfaf7498726bc06903627af3373fc8231cdbd68340dde5496b35ca3a3f8a65b8", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "7e05ad97-7c80-4f0e-9930-928a5a937f64": {"doc_hash": "a916da0db983334c2fe764b6a7ee3a772fa98819e9c70b51179283399a272d51", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "81e0d73d-72fd-4060-a924-6b0f880dcec1": {"doc_hash": "7400f99657d984d76ab574c442f134eaa26807831981faa066253be18f70bfb9", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "07494a65-fa94-43c6-bb5d-505af52af2d4": {"doc_hash": "fb0e7a5ef4ff382f875779915dfef328ff3e81efa00c040cf8a491236dacf9b4", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "79e428fe-eceb-4dcc-8653-142fadd62c84": {"doc_hash": "4631fbd683aac06449fca154b79502231d1577d6ecbad84ad3946a1597028e53", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "07914103-c94a-439a-95d2-f2f9081034d8": {"doc_hash": "043744294bf1f681992d896b92884f1e050cbdeb6d271323d1c930f8e48167c4", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "325566b4-d0f8-4a1a-a101-523b573da843": {"doc_hash": "292347a676da6c492f9e211fc326c94e18518cf6961c10bb4d2f749125b6f7bd", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "d2497dfb-87e4-469e-b917-5938bf600c1e": {"doc_hash": "a2f85587e185bc590e0fe300b5d7c9aee9ed242ff78c283fb0e792c08cff7dc7", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "e04316a3-9431-4a00-815f-eea66d935b72": {"doc_hash": "667678192d2101c74c0113d19caa5d7a14135a9cadcdbf317548d3aa0ffff1c8", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "aca219f7-5935-40fe-9e19-1336f1cc7867": {"doc_hash": "be98ce940cd173fff26be97300ae4156e9b5bf6819afc54ead81065771ec123e", "ref_doc_id": "5b986ca8-7311-4014-8513-80f735259c79"}, "20bc7b11-c15c-4eb1-91db-fd05353848af": {"doc_hash": "f8917d13cde9ecd52c4c54f3205ff32e99db42e2414aea9fad214feba795fd14", "ref_doc_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19"}, "3072836d-b160-46b4-87dc-275d9713e2a6": {"doc_hash": "e3c887cad7ea96866e87de05e127cb1dd55f71f48764a6a7e8d1ed66736058a7", "ref_doc_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19"}, "6162b0e9-1cf7-451b-86db-b807f7a0d50a": {"doc_hash": "ed6f4ef5b6d464fbc24882b900556d383f759bf9aaaa6d4060c371fe7700a611", "ref_doc_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19"}, "c17f761a-5422-43e5-8923-77324d6f9c98": {"doc_hash": "00f195610b92411279b7ce9231596f1a256e3fd457af6dfb770ce4c70cd60907", "ref_doc_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19"}, "ea23b2d1-7e3c-4af0-a085-16cbfa1cce2a": {"doc_hash": "806f07329a1e3bfb4e76cfa591a7ece69dde394f277b538ae67016bbeb3a4952", "ref_doc_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19"}, "b73853a3-4cab-444d-9984-5ebe0d6ebbc4": {"doc_hash": "9a633cd07a583d1c06b857a56e8b1cdf6f6b3dd858171998b7f7259bc2d6c0f5", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "035e36cf-d71e-413b-9e80-0285118eb183": {"doc_hash": "67d1390b8b9a4cbb9ca1060f2afae8eaa543fb34043de544aef37538314d79ba", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "0c613bba-1dde-460e-b307-9f46b7238ef5": {"doc_hash": "4a50a04a78fa8ce31f575a96d5a1fb04cd5a4742eb7925c9accdc98d52911711", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "69f23c0a-4913-42a8-98dc-e6dc594da9db": {"doc_hash": "3a7b31f49e2192eddc7f5d442a9800841ccbe94b22b4c1e656fe1ef0c60e27b8", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "29be9a0a-7c85-4056-becb-ff538025c8eb": {"doc_hash": "da6e5cdaad33d7a78940b7dc9aa6dc23edd926429413647a3213c831095e8027", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "af4d7fa8-d26a-4baa-9ed0-be56a752cfa7": {"doc_hash": "88b6eed89e67dc6f8c980dd2ec5d134070881db34f5db2ff6cbe2fee8d0f5228", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "2a8d8bc9-fe55-4333-b713-daa2a965a34e": {"doc_hash": "2965385614d79ef27494371718015fc5a1623a87e50114547b321a250c2f09c8", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "492b0191-9870-4e04-9a6d-ef25b9b490d5": {"doc_hash": "0daae90ae677b1b6b60c1cb84b3db6d23d28022129c1cbba6aad52db190c2178", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "f01f6609-2e15-4b2b-90cd-ac68bbd9c800": {"doc_hash": "b0e95506ad991ddf9885fb6259bfac2f1e91edd2bdedb512143a2fbcf9172234", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "9c781917-1bea-42f6-ba8a-619a13dcd7ad": {"doc_hash": "2ca2646b54ca114156f45a64463cec22d757745948d64a30369c1d899d14f446", "ref_doc_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5"}, "464a55e4-48af-405c-a8d6-ec93289db24f": {"doc_hash": "0a866dc2b9d784b70a438f3ca3477b09ce4d58b4d845306a44d44283c97d481b", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "bb42bff2-8e2c-4867-9311-c53e6ffa5aed": {"doc_hash": "96cebdb481ae49d8b5a8673a00b832c151e174a67d34e6a521f0efd891696ddc", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "e31f2ba0-fdb2-4dbb-9e16-a62dc367f0cc": {"doc_hash": "fad0fa3ad80e5ffe2419409a7e1d02c8c2214f4c98ec5a2cd825c2ce572bdd04", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "2c41aa1c-442c-4a2a-84cf-8cd0800036c8": {"doc_hash": "6610fdb5dd5275ed78d751971c6e366151eac5cb108673a5097f99cac6e70acc", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "f3376098-ec3d-4c5b-9f7e-10a1b51a3d52": {"doc_hash": "8f03a26a9ee2993b0886b69b93551f64050e49d5a0db595d72a9776c3a1920e4", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "61c1da22-0d52-4893-a2de-6ee03d524084": {"doc_hash": "bc8ca08d94fcf3b8a388b888f5b923aa991b28bd2833ff41ba7b2b2e4e231ce3", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "0ca26635-e8e4-488b-bf27-eae09c16899d": {"doc_hash": "8b00ac56f89ea96ab122621eddfb341c15198dc5249625287ab5783770f4e705", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "9686f81e-bf5a-4a13-9a37-f9e79a87bcb9": {"doc_hash": "9ea41210f0c04dd2fb2959fd96f05a82474b4c28c821079ea336110591baf270", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "de46db37-e19d-4f14-89a4-5bb1253f3241": {"doc_hash": "698944e2a698a18c405748ff27b3be2f83b56e5a96433cf9fe717dbd4bb093ca", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "a613ba78-a4bc-4cf6-9c1a-659ad2d7dcc3": {"doc_hash": "8e973864dfc0e2c43c64d3579e69ec093615d9522806bb74d634dde4eb76b09a", "ref_doc_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14"}, "a2a6219e-cedd-431e-bd8c-250bb45f0f18": {"doc_hash": "15e282559d9e46cf16a5ab9c63e3a3557036368c3e7952b5f0b10761ad65e5db", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "ec13075d-7b85-4b22-a524-49e499df6cde": {"doc_hash": "51fc0b624481c3d644292c23c29154b01bfedc69aa1bb6e04e80ed8dc5e9bc8a", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "53b652f1-4066-4ad9-84dd-830c5689f4e5": {"doc_hash": "a09006998a0750801a0dc3f33054e88a603da12a9146b8b55bba368289f384c9", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "77bc8d10-ae76-446f-becf-c5068e71c580": {"doc_hash": "a51889154ee07334e3ef2129c6bf6c9ae0eb1bcfb1135728ed617b016f1f2dca", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "ec42e894-1708-49ed-a740-fb33442aaa14": {"doc_hash": "3fe34427f9a32335818af99dd02515f54802a27d9528246458f8b742a60e1988", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "ae54a7f7-6482-480d-bba9-74d9a63a8d5b": {"doc_hash": "08b5a3d8d0d49e2302e64adcef1b668e8362aed1b9227b2954b54db5b8e399d8", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "4be836c2-5785-4d22-b85e-ee403b72b180": {"doc_hash": "6160c0b68ff0d22ff0ae7cd76464d79b172b3a1c7d0ff4e03a8d61d9970e7df1", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "3db5f6a8-287f-4a01-9514-4cd65c4fe0e3": {"doc_hash": "bdbc371cef6fab43743e03e712f1d2731b94a37fda9d5caaf87b5727cd9bd095", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "98cdaeb3-bb01-4f21-9979-b891f090eb0e": {"doc_hash": "53060c71bd0522c52ffab7ab8394c82bf1f4f8dc93e10d78f37d0c1aff16a898", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "b1e9a8b8-f0c1-4d86-a7c6-350eaca359b3": {"doc_hash": "d034e23274cdc641ac307cf1a74abdabc889647a1d31889d759b13d114147ad4", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "e1190fe6-b130-40a3-a14e-d54ec696349c": {"doc_hash": "f4cf31845550a0c1a7c24e29ec918f1be5ff7841363392ba4b4ee69ac1ad7c96", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "75157706-cebc-48b7-b47a-4033eaa07997": {"doc_hash": "a01cb25a46040d1e39ad58e48c8e3ae789176b37606aab9b5846f6e48e89fc0a", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "a7f8500a-8f5b-46d6-98f8-0f7b5f35b110": {"doc_hash": "670ffbf16b61c5ab18651959ebda1cf4e6162bd3e7e59dc4fb40c1feeb5abe55", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "dbc4d4d3-4a74-4974-a6b1-35f212b59ddc": {"doc_hash": "eaee0bac4b6c3371d603c89ce80e3b7bbc8a0ee984e4112ae1a68a39ffcfa993", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "c7aea255-df11-43fe-9485-423bea33c358": {"doc_hash": "911e20ee685cc311d3ccd4eaaa684b105c6cdacbf290c1a3840e3658085bcdf9", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "75128155-3564-440e-b39f-e13abf77c8e6": {"doc_hash": "5ce2e37f34c8daef41c377c98d28d70c3a5493552e6c533e338a13f708892653", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "769ef2a2-22b4-41ed-bb8d-6482c4c95574": {"doc_hash": "68d45b84c6056ebe4c29ac6fcd3a917903662a24c035065365292fb5a23bf363", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "4fa1029f-9ae1-45c1-82d9-196f4786204f": {"doc_hash": "8b782acca29f92381a638b06869c41cd7298a0fbd00066c6d800a9433c53fcb7", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "2f158caa-8625-4ee3-96ba-4789d5ee53e4": {"doc_hash": "d840be493f1b084bffd41de02a797bae97ddbe2aedcd214f66d3ec9ee887c472", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "6a59ad66-2d74-48ad-ae9c-2a458860adb8": {"doc_hash": "25f11a3206bb3ae8d5f559c0a25b831132bb03d74f9097ab71c14223094d5fa0", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "459f8772-f83c-49fb-863e-ebd29a3bdb03": {"doc_hash": "48367c488ffc4420a9e11952c8ba2614ea36b3eb3395bb0d5e57cbf67c03a0ef", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "b9d963b6-9a11-4bd3-adcf-86a510dc2ac8": {"doc_hash": "ee96e7810a74bc672d42e3b5c426a180fab86688ceda7f63bec7d714bd3e36b6", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "653444f8-2f28-4c86-84be-fa5755f9ad79": {"doc_hash": "7a6bc55c211b8baa3d7d4ada2d3a04dc8f419f8db97055ebc235ff04c80021b5", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "24b29f16-da27-4188-8e59-c27e3184fb61": {"doc_hash": "79bbb3d7b57f6f8cdf2527eb9f97fd4f0b223c41941a3519fe64a0d43ecf6fd2", "ref_doc_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645"}, "f3bfbce9-4dc3-4acd-b10e-f42c3a0c12e2": {"doc_hash": "86b084033db9e9b473a657c4d9ae2a359f34615b60668ecff444cb3dff1a1275", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "25fb1c16-d6cc-4b16-80bb-9e1f45c5a2a2": {"doc_hash": "06ca2fad31dcd47764ec7fe013881c34245d3691e1f09fbf8202532a321ff4bd", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "b345a187-792c-4486-9fc0-d0c08c7b3d1a": {"doc_hash": "9a377a088b7500a578e7509bcd628ca06475b635e12780d7f6c5db9222e00955", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "6d97ea8f-d920-425a-aae5-f1e3acfb4cf0": {"doc_hash": "eddb4cd603b53772c14b3010acc9275b742713c7280c5d5c3986c0cd9f137424", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "8c9b5cf8-8726-4a64-9026-bb3c3516edc8": {"doc_hash": "892c3a3f44ed4c5abc2ba77197a2fbc6c08f185b8057b986aef459fcd554c6e3", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "4ddb765e-bef5-46aa-810d-c49fd0426b9d": {"doc_hash": "5fdeba781c06c42b733eb60573731d4a294e9271c19e07e590cd1b98628633db", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "53a165a5-b007-4d8b-b848-d1d192a391aa": {"doc_hash": "2d1fb314fa1b231acfbb8a2b063c3ba268a7f8e3d1c59bfa128d870dabf9ab35", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "0c8b38db-8a7c-492b-879a-e70e1a4336db": {"doc_hash": "dceeea030c3c93fa5ada981ad0cc2a78299945f5df952dcee05d5f2fdb7504d2", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "093a0a9d-5f90-45fa-b381-86e79679e187": {"doc_hash": "a9a3c7a118f8c65ba9b3e32e561ca9d2dfa4e98f3ed7fa1f9d820dabd6264c25", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "e7fd9b77-7787-4c1f-a1ed-8a7afda3ffd8": {"doc_hash": "b934169a6ac690fc67ece57e060444353bda5ef386ad84a4e987b2c0439b757b", "ref_doc_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18"}, "a5fcd38d-e8ef-4c3d-b3e3-489a82ce624e": {"doc_hash": "b8e4cf52644a1bbb9ec664d0758f423d56e5b35a7eb8c80524cec29cd8b41522", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "52afaf24-82ed-453d-8253-c5839bbceddb": {"doc_hash": "cd281c61aaa1845ec144b5d405c5d92e36c0ccc9f3684442e63b6e936ad410fe", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "4280eac5-bc8c-42ec-954f-1743ecd86131": {"doc_hash": "9c67d14127bd2d3795605b141e0e6a84c2aa1df48ebc031573363ce00efbd784", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "cbc2e97c-2895-43e8-b4f6-877f684f93e5": {"doc_hash": "531e6d1998fbecae077d1b25393c069d61234708a563f1efb6d990801311549b", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "1291463a-80ab-4f81-8935-7df59e9a2c64": {"doc_hash": "dd4fff4361d638882bc5989fc6ed712c4be1b9d08210d5b0842af2888ad43ceb", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "3f189d9f-5d20-419b-841b-d7f2b85a3235": {"doc_hash": "619e3a872a1474ec9fa855639a34bdd8bc88c7237c1514055ab6926a15d4061e", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "b6eb837f-45bd-42d5-929a-82e505e2c47c": {"doc_hash": "87c1f6c5f487671a5d2065306472d92c19cb69cd8350185f7ba9dc378497f956", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "0426c5b3-ac50-4a0b-bf30-28c4ef7c0c88": {"doc_hash": "673881c4eb7186355c04d1f0cfc60652ef8645732950d12b6b754d39a1cdbe60", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "3f72eb89-bf6c-42e7-9738-de311fde642d": {"doc_hash": "598feca1e62fec82adcdcb4431a4bbd16cab35efa02e2f83ec5f9a68d97c9812", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "8f6e846f-cad9-40e6-bb21-8ffbd5615435": {"doc_hash": "0332ec63e061f3490bce3d87acfb54999fafa3ebc6d904f61d60bc68d308e88e", "ref_doc_id": "cff75fac-b53b-4236-96bd-53e27bb85409"}, "79fd59a1-0f67-4259-95d0-9593eddb2979": {"doc_hash": "b6220eb369046342f4decaa5e99a1a8ac73f116cf69d19b941189fa9c7baba91", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "e6bd3b87-622a-4b94-bd18-1cf5490321f2": {"doc_hash": "6ed8a05d6d15ac20ead2c4757100d0287bb6b5fa370a714b0fa78ff0cacf71ab", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "771620ec-5238-4304-aa8d-540848257809": {"doc_hash": "a3cdc30ab01e53d5c1cb43bd96f73b11567d426ac27288d4b5fe7798d42df582", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "fd7da413-0f58-4831-8192-83066ed725b5": {"doc_hash": "76bb2613d213a202ef261d6cd2380e319d5410477e3995457cb98d4f5dabf50f", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "4c469705-e1c5-418e-b1e0-e40a215c9857": {"doc_hash": "1f4c6dd8ef1b83fb9a6b49ebf10a819f02fbdbde447d914141949cfb33970971", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "55c2ee46-422a-47fb-ba08-a90f897c30e6": {"doc_hash": "c9a2d96749f230ea277cafd416154fc2aa18de8c06bafa40c55b85aa2f2a33d6", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "03c25273-8887-4cda-976f-c061b6d442be": {"doc_hash": "179e240a88c633910b7257f2990eedd4023136f412cacbe6fdd52af75425c19a", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "68180694-3e1d-4002-9d96-522b76ecd18f": {"doc_hash": "6c048c55a5fc0983e059903009c97dfa541bae509d3b8ffe743842c5cabf9b14", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "8ab42aa1-762a-43b6-ae4f-9241ac5b56db": {"doc_hash": "139bbf6826cfdb08aa41f705c167fd18b06c18f3920280fd8eabf0dbdc95bad6", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "94780ac9-a5d4-4faa-ac54-be9ac8408ca2": {"doc_hash": "ae8c19f849d2b59659387515da53ef657d5e20bcfe0a012baa79c20263ce7609", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "de6b6565-4653-4051-b0fd-509673094b31": {"doc_hash": "2ff9bf11e6c6a138a7fdf7d55d0f717a84bfc35d07c6370d4be047c3aadf66a9", "ref_doc_id": "c98af996-876a-4161-b507-b7f760731c52"}, "55347ab6-1108-482d-ab43-548ed534e865": {"doc_hash": "cef39b7df8b3a39255275364b8badd075f61e0df963ee5fa6ec00615faba4872", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "8575c6da-600e-4aee-9e5e-c3d974cc49b8": {"doc_hash": "fa77a5312271cf94123619eb4d74d03809a8b2de284ba9dfcb1e427a965d609b", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "8c26a9ce-334c-4348-8f53-a4d55a0b7d12": {"doc_hash": "d8986953d57e241c7bd77104f2109f8fce9fce0df35df0740ffa82ae8ab249e1", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "8069e7d9-0b7d-418f-9b52-4da09682b7d2": {"doc_hash": "af317dc43dd2f8d69c1fc5cedb0e2aba22455db6ad68ba09e0d5834043bde5de", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "158dd72e-689f-4485-b4cd-6cc55f14dffa": {"doc_hash": "e8a0729e754804e17a73e4e8a33d6b00be15341021b14510566cd94f60ca300f", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "0cddf58d-b096-4684-8060-9c47d1338342": {"doc_hash": "ea9e3a85bc86ae5b21c632c49291f95ad465848bdeadf86c0e5a0e6ca5c5a449", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "e21dba46-ecd5-4089-8416-247339c9b6b2": {"doc_hash": "17c8b1dfd7e75aff431d8aa6e3e91cd01bc70c2400e1408027a5c5f813e04fd7", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "acdcc031-14ca-4291-b81d-2312b00d3d13": {"doc_hash": "edd99d4a2ada700246ce6df6a9b9a27ac82bdc18c0d28cd17b858b048cd71633", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "dd4654a1-8ea5-4074-8cac-3d77074b1767": {"doc_hash": "075b8d13fe59c0abee5b2c141e095bebef1cc07ae2f378ae8e203ffa2734b45a", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "f043b67b-01a5-4b06-b43c-d1ee02c25170": {"doc_hash": "b41441627b4dfedf15ef7a337ae81c190a9c42cffa1cc0d452116d674449221e", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "62d79756-d8c9-422b-acd9-0ba998c6d162": {"doc_hash": "3abd383b9361bf550b9ce7fca7995a1f8f0da92074c555067196fbf4e0c4cbe7", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "fb72eaaf-ade5-4c07-8b12-df384333a32b": {"doc_hash": "592c6b358ae9bf2c826bb3dacc1b75174d1520934b1a776c8af14031986e3c71", "ref_doc_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1"}, "7a81acd2-ae13-4df8-b38b-46b2a7069503": {"doc_hash": "5d936180d25083897404a8a708522c6d26b5060be4468632da5d497bcdf18603", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "f90c1baa-80d1-4e8d-88ff-db4fa0baee38": {"doc_hash": "ffdb26c499f2f75b7b65015be530a6c81e4ef7dc05d6b4226dd269e5e57c1dbd", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "b198a0b8-1dee-44bf-8fd2-1e7a6cb721e0": {"doc_hash": "b70d2f94936517c54e98d1f4cd868d29909c0919c291b7ae33e8dbf684ddc722", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "711b0b2a-c247-48ab-b9ae-0b2f7ef3376e": {"doc_hash": "d059391a022ce7dd4efcd24fa2e9f71e2bedd4885282c59a3444f28f6e535bd3", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "f41449d3-0812-4d39-8a70-b2dc1f0baf70": {"doc_hash": "66df7f7877f6725b57301e5019de7aa253e57a98ce6e35d5c921685d5faf2477", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "bf25aa15-c988-428b-b362-e61342ad4e63": {"doc_hash": "04d764ff8e1cfa44c97c1d1c118ff1af3e0ddc90e7e408a30258132e552ad784", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "0a58cacc-7968-44c4-afcb-78c33ab1f5b6": {"doc_hash": "08d38b4989a7c7eb795f0e3728aadf396de79f7bd58cf35ef4233b30b1b59651", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "63168870-1025-44bc-b22f-87ac8190c3b8": {"doc_hash": "7ee666a9e7cb5ed7a01bedee5dfdcabc060887a6d0fbdafdc0c4bd4b1ce605ca", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "c8cecca9-eafc-4bf0-b3ed-e7f26954e26f": {"doc_hash": "b88727dc4a52e8d0d1c1963af6156071845444f200edfbbfdaf0c812f9a62558", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "d1812762-419a-45d7-876c-35ea1f2cd008": {"doc_hash": "a30ef524c503bf88a1029873481900bf0fdf7eca675d7863558ed8d386a2347e", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "7288a503-3adf-425f-b755-ede9169970dc": {"doc_hash": "2bdbc3a41e669fcbcfd32793ecd5bc08be09aa7382d5989c990c670fad8f7643", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "418eaabd-57dd-45c3-8d13-02e5a47d941d": {"doc_hash": "41df7da6f36f90c1283c32a40691bd2c9dec95a7ef6e44b8493108de260ebea4", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "c61082d9-b1cb-4af1-96cc-845e7027aae8": {"doc_hash": "81709412fc504baf356c3b1bd8487def5087602fe06b4e9e9b4506d926e5fe40", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "c212ba32-d78c-4e9e-b894-68a0fd6363a9": {"doc_hash": "cc534485d52619abbab66d3be2016a10bf464a415848cabe08a593cee3481b15", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "e36592a5-5f66-4fb7-9b35-78e111ec7192": {"doc_hash": "4798b2745940c303de1273c06a63044afc37853a3c3a6e63d1df33af853fa746", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "92f7a61e-d750-4a9e-a119-75ad62738585": {"doc_hash": "05b0d9dfae68e634e225652536f5fe0e1828261d5e45c8c447c3271e332b20ad", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "ce9db1af-3c90-40d9-bfe0-be3aaca2470d": {"doc_hash": "7221072c929148eab9a776034d412f8a61e5f1da527d2ab2e0943af06b664e09", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "e1912527-e3e0-4212-8be5-fbe640d49244": {"doc_hash": "3c557032dbb03a62e3c5d3ed5e7ee4486539ecdb5a779f753e3d329daf7e91e5", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "16042130-d987-4e09-ab98-c62f1032782a": {"doc_hash": "9f03574a74e7819c12db704629c41a5cdee19fa089b65f8fe8aab8f17c2ec60c", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "985ee1c8-15ae-4481-b610-1452034f970b": {"doc_hash": "3b0d0f6b563f100f5835ca3cf404cabf004628890319168209f92742f358441f", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "e31e850a-a069-44c2-a904-0d8f38b57c45": {"doc_hash": "0432aa9a855125ed057133db7e2bc490490dd1fd316be2c977bedf6c732acf46", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "99e9aaa6-a99c-4893-8e80-494b3ab92314": {"doc_hash": "483c50cecf3eee812e274e62c1a3fcb6a705f01fffe5f698109ef6ea5a659bfd", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "0ecf32d5-1f81-4826-a762-dd2328a8f8c0": {"doc_hash": "eb113850be1aee53b2814436fb6536dca867c2b0548111f9223529ada4a64d7d", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "afbef011-e553-4f2e-8b82-95c02380243b": {"doc_hash": "fb69f63bc82e10c904e318b6e6cc98a2422fb5d14c5ce9a3371c3686d7d17353", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "1428a947-0b91-4c1b-9042-05dc93c55c0f": {"doc_hash": "1d28e46135e4a896ec20235cc2acf7c95cbad4ce2b2cc888e87dc37e4c6487a4", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "8f98d5f2-3512-45bc-8316-047df59c3bfe": {"doc_hash": "93912e96f93f592eec3c27c4ed4690dad21e9030c3b6106777f977631a4a9210", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "a3d99e67-2928-4caf-8e15-b2e796767ce5": {"doc_hash": "992b2718fc3c2a15718af5c9fa9bcee6b294f40204dda6cc3df8b7e6478e9fba", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "8421f599-df9f-4940-b7e8-102d2fe80f2a": {"doc_hash": "ea8b0ef69d644fa123682ba4b6f3e0d6f8f3fae32a8bdd797f0bb012af2a1942", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "f8dbfe61-bcfd-43c4-a0dc-892717c00341": {"doc_hash": "39da3a97efd3bd84405d81589db12de7f3a20429d96d2436ec2ee5e0ea1bdcab", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "ef810dae-afea-41ef-9c96-acb5c588b910": {"doc_hash": "8570004d84b79d406e5d902411a5bcb34b7aad8b9637d0946d4af50992f356fe", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "341f7f39-897d-431f-a822-05346c2f06a9": {"doc_hash": "a21954f9af4e0b809de17fc6b10a6e67bc21fcf172af4e6798fb09e6f0f02ef3", "ref_doc_id": "f13aa227-5c7b-43f8-8e13-41665aef4267"}, "cf8e5da9-8aff-43dd-a423-661803a1bf5c": {"doc_hash": "8157a7d0576d7c2db8053431176e1ac4b376270982b5c0622e441356191cf077", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "ef22e630-71f4-4fb3-9927-54e882dc31d0": {"doc_hash": "3ed6e4b8c48ccaa3e680ab5135b7d7a779b2b47f13b69d7d9febff6f930682fe", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "bff3eb95-0e97-49d4-b008-e836adaa76b2": {"doc_hash": "4195c55d2cb78950f07b5ef5d4861747490fb7736cea87560933276f517457b4", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "a2f4a512-beb1-4d9a-8509-6d0f9a4c4d48": {"doc_hash": "e60ef3507d5a7364e60232601d670d5100f1eab2cd4eab64971a89795bf9f295", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "b3291a1b-e1f9-438f-be38-11bc31513976": {"doc_hash": "d69686fa1154fe77984f6330ab9c558aff86257d191f42a8c4075145978ceca7", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "8bd52d44-4e46-401e-bf97-41825fcb2955": {"doc_hash": "f9d223cdd7324014b15b29385b660e91dcd079b643035712f56971149c577e3e", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "40d42af7-e390-4057-a7d4-d9e25e6d22b3": {"doc_hash": "2b28a6d679e41b6c0869918274ca76bc008bd49e865b01dd3ded11f28b01a738", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "7a9c618f-fd1e-4ead-98e4-58cfbb669a04": {"doc_hash": "24334b036984508736a83f5dafa33a60cc88835f30f42376f42596634df7aba5", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "074dd052-0636-443f-9ac8-53eaa76ddfdd": {"doc_hash": "dc7f87db191bddb260f0cf1d5c8b2cdcb00defdcdd57a7cbb9e6a066e06590fd", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "10e6fb7d-a3ca-4e23-91e2-288b52556e32": {"doc_hash": "cd4d711fc517bc57c4f270e9ceb97612c81d2dbc85e8f06b29c1567d8b5d29b0", "ref_doc_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479"}, "f9a34abb-43de-4671-b277-a483dacf3813": {"doc_hash": "83a10b485a49594aa73cf185619e5fcf3a004a935942dc19ead68fc6c2b2845f", "ref_doc_id": "2a4f6215-823c-41aa-9fdb-a39e2944433f"}, "50681729-1b26-434e-a7a1-b3374e83150b": {"doc_hash": "6fcaaa3552e03fe24421eb5e1e47d7501256e8d08c8f1972c6570ad51ed7a6b3", "ref_doc_id": "2a4f6215-823c-41aa-9fdb-a39e2944433f"}, "9e570bbb-dda6-43c6-a96e-7482602f3c63": {"doc_hash": "2fb61a40f6e3a4939b21c75e1a1c6745594c90df322f50789d497bb5674d65a3", "ref_doc_id": "4abc006a-7fe9-4567-a652-25157dbb2a76"}, "6ed96385-8787-408a-89de-630559f85dde": {"doc_hash": "ffafd385f9c52fa4fb47c75f05edc5b4c53d2e9427c53c2e4e06e79852f09974", "ref_doc_id": "4abc006a-7fe9-4567-a652-25157dbb2a76"}, "805e0cb0-a020-4fab-88d1-3575f69b6684": {"doc_hash": "d18add60cb742c75b0b7fb4bc1df2856aabbe10cfdf5f0cc9ba96e0dae7bdf9f", "ref_doc_id": "4abc006a-7fe9-4567-a652-25157dbb2a76"}, "ed6544c1-11ec-45a5-bf0d-9d4b7ebfcbb3": {"doc_hash": "dcecfcf9576c050cbafb28cfec10c9676b2b0c83a2b645307cd6edbd8b0e9f23", "ref_doc_id": "4abc006a-7fe9-4567-a652-25157dbb2a76"}, "f2890be5-78c3-4a3b-a90d-816749ed643c": {"doc_hash": "1c0fb047da72923fd068174e73c7d57ea7ba686afea7019d2cb1b585e5362baf", "ref_doc_id": "4abc006a-7fe9-4567-a652-25157dbb2a76"}, "97ce037c-aa05-4ddf-b60f-406f02a30279": {"doc_hash": "c6924067c80fa767980bc17c2e8dbc3258200a2249d4ac2bba995a415639bc6c", "ref_doc_id": "4abc006a-7fe9-4567-a652-25157dbb2a76"}, "083b98a1-e73e-45b1-a255-ba723e128e20": {"doc_hash": "d5587b1ce6d729f6caedc3d4cd58081182b51d73797e151827817bed83d6f411", "ref_doc_id": "4abc006a-7fe9-4567-a652-25157dbb2a76"}, "6b5fb6b8-1035-4281-b23d-892df0040323": {"doc_hash": "4cb38d6320ffc39d9c51b34d3ac43dd1fa6778dffca8103bc9d769bae017c8be", "ref_doc_id": "4abc006a-7fe9-4567-a652-25157dbb2a76"}, "0c62bf41-01d7-4136-b751-2040b5a5bd96": {"doc_hash": "d90ac714e595cb4c25e88b7c0031df5f094aba1c619d7c9e638aa79e677a3011", "ref_doc_id": "4abc006a-7fe9-4567-a652-25157dbb2a76"}, "16401bc9-28ee-4f62-8a85-4da9fbcc8032": {"doc_hash": "dfcd793e43036c8f8a038aa091551ce685bd0135dac9d68e340c3caf5087b2b6", "ref_doc_id": "93b33121-2589-4655-af52-aff6aa8c7f68"}, "82d40da6-b720-4b2d-81e6-aa3894e7d04e": {"doc_hash": "ec8dc9f41c062d2c30ba2e02dc30a07dafe2429fd68c5f28b79301f3edd8c426", "ref_doc_id": "93b33121-2589-4655-af52-aff6aa8c7f68"}, "67def0d8-13cc-4f6a-9a89-84a851c12563": {"doc_hash": "808185df11bb9fdeaca94702f81b4487dd7877a0d30bee75dd2e7fb16fee2c14", "ref_doc_id": "93b33121-2589-4655-af52-aff6aa8c7f68"}, "bb996c0a-6f56-49f0-850a-6bb628f2acbe": {"doc_hash": "8f1606dbbae52a0caf476160ab0cb1b922268442d297e40c91f3410bc1734576", "ref_doc_id": "93b33121-2589-4655-af52-aff6aa8c7f68"}, "6c479a44-b30b-4aab-9044-346302dd87c6": {"doc_hash": "2da720cae3cae9363693582bb812f72f75080f84051b0ce58a77fc58cc04c98f", "ref_doc_id": "93b33121-2589-4655-af52-aff6aa8c7f68"}, "88c420f8-1687-4fb9-b6d6-43ae22ea68f1": {"doc_hash": "e02405d34a2df46b1548e23f1491704ead3acf4c1cfb2eb70cc9fd3f668b5dad", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "6e1b7b28-d382-4173-9b3d-b397c307eacf": {"doc_hash": "54189029aaab363dc0f6cdee35557d043254150190144b4b8d96a6e6c028f47a", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "01b7c2b2-afa3-420b-ad2c-24e6007ad5af": {"doc_hash": "dd70ce29878d402b36bdd0ae086932f9e41fe7a9b63472b88954f4de690d1cd0", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "342e5a68-3eb2-4671-a622-cdb2a9682656": {"doc_hash": "dcb3470fc51a2448ecda3e94e99424dda1195d8d0738e3d6e555c424078e9857", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "fc30232d-1547-4cf2-97db-faf8ef9d49da": {"doc_hash": "f160bf6e692543a9a0037ff4e3f3f51aeeb98de6bbd4a8feed5339c9f71b629d", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "4db9c634-3499-48e7-962b-3ff8f136ad39": {"doc_hash": "764191c117e5c864f37636b9a743f97b502939bae3b6931c3d4e456228387048", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "da9f107b-e586-4aed-8079-16cb8f982e18": {"doc_hash": "0557533af3ce7c3d452e0c4b3d64f71beb7df81ed55cb8fc99302a4af6cfd1dd", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "c8075030-7004-4aa5-bd47-c77608eecda7": {"doc_hash": "82c2bd9f818e14a2aa89819af0b84fa995ea0224bcfd2cfc7383b8be8ba67773", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "8342151c-1130-438b-be50-20a5fef97bfc": {"doc_hash": "2f31d08307e6a31aa8cf711f2f4ec9f27f986ddcaebc2c7dc8b3bf075f9a390b", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "7f5485ff-2186-4340-a583-84eae6d55c51": {"doc_hash": "ee480a23adf7bff230729cc52f7d5ec50729508791ecd765c03703e8dd381256", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "168e9538-c94a-4889-bbc2-c6b9cb100883": {"doc_hash": "c58cc8ce7b35baf03421c9654def85a1b0b3ff81270ac7c721259af046440240", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "0664337e-3549-4889-afd1-c0292651989d": {"doc_hash": "f8084c1288a37bf926fb8263ed6401d2bfb255151279050b0a21f12be8b73943", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "69d3cd26-bc8d-41f0-99cd-d54bfc9a0517": {"doc_hash": "20333cdeaf6b6ac5ccbc3e90c8f6b1ca2fb2290eff530300ccfdce381ec149d2", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "8bca2bd5-6a4e-48e3-b1e5-f9afdace211b": {"doc_hash": "a465e88eec21458f2a73ffc51e3c6a51c79375021585b6c1db37ddf6bed1f100", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "7e785e8c-6599-4349-ac2a-d9579f023c83": {"doc_hash": "32c717f0266fb91d9e792fa3005dca38491926848484603699d1fe0a2464de48", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "af0b4c87-d136-4e38-862d-9592d10f5d73": {"doc_hash": "7375eaf9541b81cf9b553857a8050ac61f3ec7a7f80ca13422c0927c7b7b9f81", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "fc5908e2-5cf8-49a0-bb59-c1fc6b404c91": {"doc_hash": "5435e82c56a78443ff4295c58a97980f950a2c63e11edd6dd0cf8649dc4001dc", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "1c2bad15-8530-4158-8cf2-3a04faa7baad": {"doc_hash": "86ea08ace2a05a7e9293e4ea00f773ad6165c50e1c3416161c5a08576332ffd2", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "7d84abdb-0609-4f2f-84f2-d7bc7af44eb7": {"doc_hash": "73d6132df7992b9358dcb5e5e4857a8f28c2aa829980960c49169988a4a15303", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "c525478c-1612-4cf2-b299-e636e2c0f8fd": {"doc_hash": "babd1ae1b84d232b0d3bf1a16a52e61ccf70b652baf385b169295e46b439c3b3", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "1bcb5c2f-8833-4c64-868e-9f74ab54d0c4": {"doc_hash": "026c43a26a0dd96b1813a40f3001952db2a3c41c2a067ad5462a19c882d93abb", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "86c097b9-3b5c-434a-87d9-d0b81a3db21e": {"doc_hash": "7d51d9295ecba1897b2914638dd35f6bfd7cda7ffafd7ddf3820f8fc03c12cbe", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "77b25d47-0888-437d-ab87-3b6995d19628": {"doc_hash": "e29d4e63a373176e3612b9ef864c06defaca4eaac807de8b199a311a5679f1af", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "df319bff-9b78-482b-9e10-9d9340ac65c8": {"doc_hash": "0eeb26893f055eaf5eedeb00666874f04095b92d2fb3fd89d242c9659c081f21", "ref_doc_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d"}, "3168d81b-22e0-475b-b4b5-bafb597a4158": {"doc_hash": "8212e71304486b4e3aa04a0adbc4a3edffc00a421116b8a03a699a9bbe40d123", "ref_doc_id": "368eac03-b394-4c04-8d4b-1af2f2abc607"}, "ceccb3ae-d26a-446d-bece-8c2231aa98a9": {"doc_hash": "d5ec8e759f9b70fed2add67c424f1f60a0e1272b442d7ae82b1774aa274f19c0", "ref_doc_id": "368eac03-b394-4c04-8d4b-1af2f2abc607"}, "0b71f8bd-15aa-4263-be78-c0e85fda6d71": {"doc_hash": "a93d50544e462dd64ab0b99a4bf13b032a4826abb5ab5bba372c894549592ae3", "ref_doc_id": "368eac03-b394-4c04-8d4b-1af2f2abc607"}, "835aa024-1a5d-422c-b548-fb5b6aaa1f5c": {"doc_hash": "f54acdf065974a9a9d612beeb770944d1cb7c07a805f6adb98f3b3972a2686dc", "ref_doc_id": "368eac03-b394-4c04-8d4b-1af2f2abc607"}, "68cb63d7-e086-4811-99fc-151ed16c7c03": {"doc_hash": "5bc4718b6b15eee36308a8192b79a3cdd3efca930b38da8d3d1e0ab748e33d09", "ref_doc_id": "368eac03-b394-4c04-8d4b-1af2f2abc607"}, "1d561428-2c47-4640-a9a0-032ad23b0f28": {"doc_hash": "82d4f48f040ba9220945ba228a96872cf9cf8cf5c11a6b1ab2bd3fe224accd6c", "ref_doc_id": "368eac03-b394-4c04-8d4b-1af2f2abc607"}, "49111234-4189-4cd2-a5b9-cd7694303271": {"doc_hash": "c83a5ea00c6d0371ad5c80c8c9eb7bd4b30a22df0610d0919fc4da3d949da807", "ref_doc_id": "368eac03-b394-4c04-8d4b-1af2f2abc607"}, "4e7af7dc-09db-4b5e-b7c0-bdc780cc5dc5": {"doc_hash": "25c9984d3387976142dcca7658f47728d2bcac5c2686a38c6372767ba95989a8", "ref_doc_id": "368eac03-b394-4c04-8d4b-1af2f2abc607"}, "9767b782-3f7e-4d39-8cbb-ba918fa1018a": {"doc_hash": "bfbaa3eb2affb0798942370f4b19e8ffa8db31148afca1e8e62f5a7ef141c719", "ref_doc_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca"}, "d2a9f83b-5765-4eeb-a7aa-3171d86d8b09": {"doc_hash": "7f1ab29ba88c8ba83cc376cfc1e8342bd8db1ba9b1d85be2b0cb1666b17d0695", "ref_doc_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca"}, "e11c8992-262b-4b3c-91a7-422ff0264593": {"doc_hash": "cd8f58c1d49c0e8e35b20068d4d8cd87ef654e81b928f952534eb14050cf7e09", "ref_doc_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca"}, "38040e85-accc-42a8-8d37-fd7f6d603588": {"doc_hash": "b751889b17c80d17e946e2c2abdffc42dd8bc751b443a25983c969702ec46023", "ref_doc_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca"}, "eb938ff6-e589-43f7-999a-fb8f1457defb": {"doc_hash": "7c43bbcf194221faac08fafc8dadfe84b2252d60473766a9897acd205143e106", "ref_doc_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca"}, "0b0a89a1-1320-43ba-a4e6-1b840702a77d": {"doc_hash": "0fef275881c8ddca3e0baefdeb6d006a45e9554b32a0c400a0d2226e3826df13", "ref_doc_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca"}, "4130f604-b195-4f36-a755-143f4b00764f": {"doc_hash": "4ba9227382bf1e51a78773ac8e4069262f5abcd1da241d98a9dd60382c7f2136", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "b4cc1a85-b6f8-4d82-975a-9969cbf942a9": {"doc_hash": "d02547a49016a0dc270d32bc7010f33e5591a8776196ecd14bc612176dddae40", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "d24a66e8-f7ed-4873-9ffb-b73434bf7f30": {"doc_hash": "958ff73a8810b57b4fc8644f4a2acd6d095d3633098c138c5fc0ed2a8cce9380", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "514bd2a8-4598-4b42-ae08-98c77cfe99ab": {"doc_hash": "03fa1c93e8048f35d7a91172dc6b0f1e9c5f2f4e69796d048e72a2617eb48397", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "d5b40e25-57f4-4984-afa1-5b32ca670bcc": {"doc_hash": "73f9a13af544403ee1dc8cf93f096725bb3155297716880ae6a0b3594701a638", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "acb05411-eff8-48a8-b879-6807ca2fa975": {"doc_hash": "6ebe8c2b06713be74e61319d95aff538136dcde184e91a74384a5d267473ec5f", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "6c1e4c3d-a0c2-4a73-a9be-c689603744d8": {"doc_hash": "67c160130887d4b9eaa705bd00913edb64f0e8a7de812c6bdfb290236aab8e77", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "3ef9277f-62d4-47f4-ad44-1d92a5c1f6ba": {"doc_hash": "e15246ef20de7e9231d3c95bd75c54156e9fd6646453cb7aec81e765a614c3d7", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "006af9de-dfb0-47bf-9e9b-ab1ee67b4d8c": {"doc_hash": "335b9d716d20b58cc04bd01cfeda0c63dc051620abb7dd266d6552ef43369f52", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "8be77cec-7088-48d8-a12e-3fd61f754702": {"doc_hash": "55c47726e8c536e7fb04a9c8c2e086895eeab37ad6f559d2e89f9a06667d633c", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "88f3d31d-b1ae-453c-9195-e10c6c6cbaa8": {"doc_hash": "55e4560c82a5b0736f66da6a07bfad8925a4c16c1fbc93eacf2317ddfaaf1cf3", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "1dd968c3-c348-49f2-ab30-ed398572267c": {"doc_hash": "d028e904f2ddc24fed374e01f8aa6bcb989018205748c4936fc5f8eb25481a89", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "f3de678d-4b9e-44eb-a310-595d5b2b4e81": {"doc_hash": "2886781ee45b879e0d502b39f27f525ee0ecda300aa9c70d9c5155d04e62710b", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "3b96eef8-c761-4789-a35a-95b2caa517de": {"doc_hash": "085d223b8a22222085fafb46b4eecedcf31dd15f95466dc2c1b112274ce74793", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "6af00e97-5fb3-4028-87aa-8fac7142816c": {"doc_hash": "71654a23b7694ab7035e415ae2e0164f70d7e31d2e24bb51dd24eaaa3c1a1b93", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "d38d39e2-e92b-4206-9281-b1ad4d509fc0": {"doc_hash": "f7df3f975156cfe0c2011e04d4c6221eff3775a1e409fdee90e2dba90a64061f", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "ba305577-4101-4af5-94e7-b241d115030e": {"doc_hash": "223af306f8556339e0f30cbbced3d404f2b8b83bbc98b07ba546d787aaeb274c", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "c3faf249-779b-4a78-a51e-c73518da42b6": {"doc_hash": "ba13e03dfc190f92f14a843e2f7f3e2f8d6ca04eaff7dbed2b6db87b37d071c9", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "79a5f98d-8759-43ac-b2fd-7bc8b2b7dd6a": {"doc_hash": "79d437ffb457d049292f5a9bd19786f0a650c705833405167334d4da68ad79bd", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "1160ec06-94a7-486c-9d76-8e781dbf2e45": {"doc_hash": "0e4beafd379f3193f13c74ab621b2def1e8dd09ad2d602d671bcf860835d043a", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "326e82a4-2e10-47e1-b272-bb135115ad8a": {"doc_hash": "93c5f6374b80a0e92acf2c9711ffe1863adcbbacddb6e60a8ff37f64ab248dd6", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "2ce74c66-f170-4878-8541-cd18ad94df33": {"doc_hash": "6035fd6f8ef16ab221a9f5d882d4b30c8fd3e03a7a50bc40ba409b7d87a59ede", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "91919c20-4fb5-4920-8685-cf6c29cfd2b7": {"doc_hash": "373ebbf73d4ab5588459e391f9d7cd48bf48126ea4a79d4ec898357e95bce694", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "598d2917-1902-48c4-9d74-0804cd1ba6c5": {"doc_hash": "da73d250041f18b04f9a9657e5f9d99e6c5e09030e9395f3ab8e21a6b336d5fe", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "7171d91d-d981-457d-a6a8-ee96bd810183": {"doc_hash": "3e26165c1ef5dd527c79485b87d5a5121b420a6477a55c7b05308340263ec3a6", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "dc92b5ab-90a4-400b-9568-5673c771565b": {"doc_hash": "f4c076f9cf1bb1590f37b97bf44c494955a3dbb8ff656b71cd882023638dad32", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "8ff0ad3f-e0c6-425e-b74b-446a0da0a2f7": {"doc_hash": "7bf109bb50d11a1da94dbd51d9364f57af049c06734e618209079f855a7acd3d", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "c252df82-3700-408f-ac0b-f869fb5c27b9": {"doc_hash": "b691dad13add40bfb394ccc1ce27887d8dcfa72639a024b523fe4ce56676d610", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "d654f4a1-0e2b-403d-963c-60923e3152d5": {"doc_hash": "cb576c1b33bd1346ee76648a6be4749f09a3cb2ff1657ea161c7f2362af25ff7", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}, "ba75c3ae-77e7-4503-a0d1-e3021ad87dc4": {"doc_hash": "26619586ab0144d0693b990a0b45b60e22c17d8b15582c8a8978b4a711506d54", "ref_doc_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63"}}, "docstore/data": {"3c0710af-28b3-4a98-8400-67e28de0a539": {"__data__": {"id_": "3c0710af-28b3-4a98-8400-67e28de0a539", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "55d55b31a088d3d367ff03172f04f99c0eb048da7ef4aa30bacda43c01d11edb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39559a7a-3157-4f68-bf4c-2eb74bd984ab", "node_type": "1", "metadata": {}, "hash": "7d70a0f3948097313cc51b26fd2a869c880f8742b65c2ae357b6fd6707cb43f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Lustre* Software Release 2.x\n\n## Operations Manual\n\nCopyright \u00a9 2010, 2011 Oracle and/or its affiliates. (The original version of this Operations Manual without the Intel modifications.)\n\nCopyright \u00a9 2011, 2017 Intel Corporation. (Intel modifications to the original version of this Operations Manual.)\n\nNotwithstanding Intel\u2019s ownership of the copyright in the modifications to the original version of this Operations Manual, as between Intel and Oracle, Oracle and/or its affiliates retain sole ownership of the copyright in the unmodified portions of this Operations Manual.\n\n**Important Notice from Intel**\n\nINFORMATION IN THIS DOCUMENT IS PROVIDED IN CONNECTION WITH INTEL PRODUCTS. NO LICENSE, EXPRESS OR IMPLIED, BY ESTOPPEL OR OTHERWISE, TO ANY INTELLECTUAL PROPERTY RIGHTS IS GRANTED BY THIS DOCUMENT. EXCEPT AS PROVIDED IN INTEL'S TERMS AND CONDITIONS OF SALE FOR SUCH PRODUCTS, INTEL ASSUMES NO LIABILITY WHATSOEVER AND INTEL DISCLAIMS ANY EXPRESS OR IMPLIED WARRANTY, RELATING TO SALE AND/OR USE OF INTEL PRODUCTS INCLUDING LIABILITY OR WARRANTIES RELATING TO FITNESS FOR A PARTICULAR PURPOSE, MERCHANTABILITY, OR INFRINGEMENT OF ANY PATENT, COPYRIGHT OR OTHER INTELLECTUAL PROPERTY RIGHT.\n\nA \"Mission Critical Application\" is any application in which failure of the Intel Product could result, directly or indirectly, in personal injury or death. SHOULD YOU PURCHASE OR USE INTEL'S PRODUCTS FOR ANY SUCH MISSION CRITICAL APPLICATION, YOU SHALL INDEMNIFY AND HOLD INTEL AND ITS SUBSIDIARIES, SUBCONTRACTORS AND AFFILIATES, AND THE DIRECTORS, OFFICERS, AND EMPLOYEES OF EACH, HARMLESS AGAINST ALL CLAIMS COSTS, DAMAGES, AND EXPENSES AND REASONABLE ATTORNEYS' FEES ARISING OUT OF, DIRECTLY OR INDIRECTLY, ANY CLAIM OF PRODUCT LIABILITY, PERSONAL INJURY, OR DEATH ARISING IN ANY WAY OUT OF SUCH MISSION CRITICAL APPLICATION, WHETHER OR NOT INTEL OR ITS SUBCONTRACTOR WAS NEGLIGENT IN THE DESIGN, MANUFACTURE, OR WARNING OF THE INTEL PRODUCT OR ANY OF ITS PARTS.\n\nIntel may make changes to specifications and product descriptions at any time, without notice. Designers must not rely on the absence or characteristics of any features or instructions marked \"reserved\" or \"undefined\". Intel reserves these for future definition and shall have no responsibility whatsoever for conflicts or incompatibilities arising from future changes to them. The information here is subject to change without notice. Do not finalize a design with this information.\n\nThe products described in this document may contain design defects or errors known as errata which may cause the product to deviate from published specifications. Current characterized errata are available on request.\n\nContact your local Intel sales office or your distributor to obtain the latest specifications and before placing your product order.\n\nCopies of documents which have an order number and are referenced in this document, or other Intel literature, may be obtained by calling 1-800-548-4725, or go to: http://www.intel.com/design/literature.htm\n\nIntel and the Intel logo are trademarks of Intel Corporation in the U.S. and/or other countries. Lustre is a registered trademark of Oracle Corporation.\n\n*Other names and brands may be claimed as the property of others.\n\nTHE ORIGINAL LUSTRE 2.x FILESYSTEM: OPERATIONS MANUAL HAS BEEN MODIFIED: THIS OPERATIONS MANUAL IS A MODIFIED VERSION OF, AND IS DERIVED FROM, THE LUSTRE 2.0 FILESYSTEM: OPERATIONS MANUAL PUBLISHED BY ORACLE AND AVAILABLE AT http://www.lustre.org/. MODIFICATIONS (collectively, the \u201cModifications\u201d) HAVE BEEN MADE BY INTEL CORPORATION (\u201cIntel\u201d). ORACLE AND ITS AFFILIATES HAVE NOT REVIEWED, APPROVED, SPONSORED, OR ENDORSED THIS MODIFIED OPERATIONS MANUAL, OR ENDORSED INTEL, AND ORACLE AND ITS AFFILIATES ARE NOT RESPONSIBLE OR LIABLE FOR ANY MODIFICATIONS THAT INTEL HAS MADE TO THE ORIGINAL OPERATIONS MANUAL.\n\nNOTHING IN THIS MODIFIED OPERATIONS MANUAL IS INTENDED TO AFFECT THE NOTICE PROVIDED BY ORACLE BELOW IN RESPECT OF THE ORIGINAL OPERATIONS MANUAL AND SUCH ORACLE NOTICE CONTINUES TO APPLY TO THIS MODIFIED OPERATIONS MANUAL EXCEPT FOR THE MODIFICATIONS; THIS INTEL NOTICE SHALL APPLY ONLY TO MODIFICATIONS MADE BY INTEL. AS BETWEEN YOU AND ORACLE: (I) NOTHING IN THIS INTEL NOTICE IS INTENDED TO AFFECT THE TERMS OF THE ORACLE NOTICE BELOW; AND (II) IN THE EVENT OF ANY CONFLICT BETWEEN THE TERMS OF THIS INTEL NOTICE AND THE TERMS OF THE ORACLE NOTICE, THE ORACLE NOTICE SHALL PREVAIL.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39559a7a-3157-4f68-bf4c-2eb74bd984ab": {"__data__": {"id_": "39559a7a-3157-4f68-bf4c-2eb74bd984ab", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "55d55b31a088d3d367ff03172f04f99c0eb048da7ef4aa30bacda43c01d11edb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c0710af-28b3-4a98-8400-67e28de0a539", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ec52fc5965e5c236f947e3bf456daaee268f5ef5ef0fc85a53f298a93700a206", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a671d38-8d1e-4b2e-ae6d-79ceb2a86a5e", "node_type": "1", "metadata": {}, "hash": "f0197d659efd1c1d9142265998abe2f2f1e55c9fd66b64d773362e02922f3b5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "NOTHING IN THIS MODIFIED OPERATIONS MANUAL IS INTENDED TO AFFECT THE NOTICE PROVIDED BY ORACLE BELOW IN RESPECT OF THE ORIGINAL OPERATIONS MANUAL AND SUCH ORACLE NOTICE CONTINUES TO APPLY TO THIS MODIFIED OPERATIONS MANUAL EXCEPT FOR THE MODIFICATIONS; THIS INTEL NOTICE SHALL APPLY ONLY TO MODIFICATIONS MADE BY INTEL. AS BETWEEN YOU AND ORACLE: (I) NOTHING IN THIS INTEL NOTICE IS INTENDED TO AFFECT THE TERMS OF THE ORACLE NOTICE BELOW; AND (II) IN THE EVENT OF ANY CONFLICT BETWEEN THE TERMS OF THIS INTEL NOTICE AND THE TERMS OF THE ORACLE NOTICE, THE ORACLE NOTICE SHALL PREVAIL.\n\nYour use of any Intel software shall be governed by separate license terms containing restrictions on use and disclosure and are protected by intellectual property laws.\n\nThe information contained herein is subject to change without notice and is not warranted to be error-free. If you find any errors, please report them to us in writing.\n\nThis work is licensed under a Creative Commons Attribution-Share Alike 3.0 United States License. To view a copy of this license and obtain more information about Creative Commons licensing, visit [Creative Commons Attribution-Share Alike 3.0 United States](http://creativecommons.org/licenses/by-sa/3.0/us) or send a letter to Creative Commons, 171 2nd Street, Suite 300, San Francisco, California 94105, USA.\n\n**Important Notice from Oracle**\n\nThis software and related documentation are provided under a license agreement containing restrictions on use and disclosure and are protected by intellectual property laws. Except as expressly permitted in your license agreement or allowed by law, you may not use, copy, reproduce, translate, broadcast, modify, license, transmit, distribute, exhibit, perform, publish, or display any part, in any form, or by any means. Reverse engineering, disassembly, or decompilation of this software, unless required by law for interoperability, is prohibited.\n\nThe information contained herein is subject to change without notice and is not warranted to be error-free. If you find any errors, please report them to us in writing.\n\nIf this is software or related software documentation that is delivered to the U.S. Government or anyone licensing it on behalf of the U.S. Government, the following notice is applicable:\n\nU.S. GOVERNMENT RIGHTS. Programs, software, databases, and related documentation and technical data delivered to U.S. Government customers are \"commercial computer software\" or \"commercial technical data\" pursuant to the applicable Federal Acquisition Regulation and agency-specific supplemental regulations. As such, the use, duplication, disclosure, modification, and adaptation shall be subject to the restrictions and license terms set forth in the applicable Government contract, and, to the extent applicable by the terms of the Government contract, the additional rights set forth in FAR 52.227-19, Commercial Computer Software License (December 2007). Oracle America, Inc., 500 Oracle Parkway, Redwood City, CA 94065.\n\nThis software or hardware is developed for general use in a variety of information management applications. It is not developed or intended for use in any inherently dangerous applications, including applications which may create a risk of personal injury. If you use this software or hardware in dangerous applications, then you shall be responsible to take all appropriate fail-safe, backup, redundancy, and other measures to ensure its safe use. Oracle Corporation and its affiliates disclaim any liability for any damages caused by use of this software or hardware in dangerous applications.\n\nOracle and Java are registered trademarks of Oracle and/or its affiliates. Other names may be trademarks of their respective owners.\n\nAMD, Opteron, the AMD logo, and the AMD Opteron logo are trademarks or registered trademarks of Advanced Micro Devices. Intel and Intel Xeon are trademarks or registered trademarks of Intel Corporation. All SPARC trademarks are used under license and are trademarks or registered trademarks of SPARC International, Inc. UNIX is a registered trademark licensed through X/Open Company, Ltd.\n\nThis software or hardware and documentation may provide access to or information on content, products, and services from third parties. Oracle Corporation and its affiliates are not responsible for and expressly disclaim all warranties of any kind with respect to third-party content, products, and services. Oracle Corporation and its affiliates will not be responsible for any loss, costs, or damages incurred due to your access to or use of third-party content, products, or services.\n\nCopyright \u00a9 2011, Oracle et/ou ses affili\u00e9s. Tous droits r\u00e9serv\u00e9s.\n\nCe logiciel et la documentation qui l\u2019accompagne sont prot\u00e9g\u00e9s par les lois sur la propri\u00e9t\u00e9 intellectuelle.", "mimetype": "text/plain", "start_char_idx": 3855, "end_char_idx": 8653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a671d38-8d1e-4b2e-ae6d-79ceb2a86a5e": {"__data__": {"id_": "1a671d38-8d1e-4b2e-ae6d-79ceb2a86a5e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "55d55b31a088d3d367ff03172f04f99c0eb048da7ef4aa30bacda43c01d11edb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39559a7a-3157-4f68-bf4c-2eb74bd984ab", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1fc283ea195de23970957552ce58706146c87f5c9b5194120beae0d3a20629af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70f9d054-2875-4a2e-af9b-10976de8a456", "node_type": "1", "metadata": {}, "hash": "30050af538da691d614dd1ca33ee07642ba66102d0743b871c40c38702fc9866", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Intel and Intel Xeon are trademarks or registered trademarks of Intel Corporation. All SPARC trademarks are used under license and are trademarks or registered trademarks of SPARC International, Inc. UNIX is a registered trademark licensed through X/Open Company, Ltd.\n\nThis software or hardware and documentation may provide access to or information on content, products, and services from third parties. Oracle Corporation and its affiliates are not responsible for and expressly disclaim all warranties of any kind with respect to third-party content, products, and services. Oracle Corporation and its affiliates will not be responsible for any loss, costs, or damages incurred due to your access to or use of third-party content, products, or services.\n\nCopyright \u00a9 2011, Oracle et/ou ses affili\u00e9s. Tous droits r\u00e9serv\u00e9s.\n\nCe logiciel et la documentation qui l\u2019accompagne sont prot\u00e9g\u00e9s par les lois sur la propri\u00e9t\u00e9 intellectuelle. Ils sont conc\u00e9d\u00e9s sous licence et soumis \u00e0 des restrictions d\u2019utilisation et de divulgation. Sauf disposition de votre contrat de licence ou de la loi, vous ne pouvez pas copier, reproduire, traduire, diffuser, modifier, breveter, transmettre, distribuer, exposer, ex\u00e9cuter, publier ou afficher le logiciel, m\u00eame partiellement, sous quelque forme et par quelque proc\u00e9d\u00e9 que ce soit. Par ailleurs, il est interdit de proc\u00e9der \u00e0 toute ing\u00e9nierie inverse du logiciel, de le d\u00e9sassembler ou de le d\u00e9compiler, except\u00e9 \u00e0 des fins d\u2019interop\u00e9rabilit\u00e9 avec des logiciels tiers ou tel que prescrit par la loi.\n\nLes informations fournies dans ce document sont susceptibles de modification sans pr\u00e9avis. Par ailleurs, Oracle Corporation ne garantit pas qu\u2019elles soient exemptes d\u2019erreurs et vous invite, le cas \u00e9ch\u00e9ant, \u00e0 lui en faire part par \u00e9crit.\n\nSi ce logiciel, ou la documentation qui l\u2019accompagne, est conc\u00e9d\u00e9 sous licence au Gouvernement des Etats-Unis, ou \u00e0 toute entit\u00e9 qui d\u00e9livre la licence de ce logiciel ou l\u2019utilise pour le compte du Gouvernement des Etats-Unis, la notice suivante s\u2019applique :\n\nU.S. GOVERNMENT RIGHTS. Programs, software, databases, and related documentation and technical data delivered to U.S. Government customers are \"commercial computer software\" or \"commercial technical data\" pursuant to the applicable Federal Acquisition Regulation and agency-specific supplemental regulations. As such, the use, duplication, disclosure, modification, and adaptation shall be subject to the restrictions and license terms set forth in the applicable Government contract, and, to the extent applicable by the terms of the Government contract, the additional rights set forth in FAR 52.227-19, Commercial Computer Software License (December 2007). Oracle America, Inc., 500 Oracle Parkway, Redwood City, CA 94065.\n\nCe logiciel ou mat\u00e9riel a \u00e9t\u00e9 d\u00e9velopp\u00e9 pour un usage g\u00e9n\u00e9ral dans le cadre d\u2019applications de gestion des informations. Ce logiciel ou mat\u00e9riel n\u2019est pas con\u00e7u ni n\u2019est destin\u00e9 \u00e0 \u00eatre utilis\u00e9 dans des applications \u00e0 risque, notamment dans des applications pouvant causer des dommages corporels. Si vous utilisez ce logiciel ou mat\u00e9riel dans le cadre d\u2019applications dangereuses, il est de votre responsabilit\u00e9 de prendre toutes les mesures de secours, de sauvegarde, de redondance et autres mesures n\u00e9cessaires \u00e0 son utilisation dans des conditions optimales de s\u00e9curit\u00e9. Oracle Corporation et ses affili\u00e9s d\u00e9clinent toute responsabilit\u00e9 quant aux dommages caus\u00e9s par l\u2019utilisation de ce logiciel ou mat\u00e9riel pour ce type d\u2019applications.\n\nOracle et Java sont des marques d\u00e9pos\u00e9es d\u2019Oracle Corporation et/ou de ses affili\u00e9s.Tout autre nom mentionn\u00e9 peut correspondre \u00e0 des marques appartenant \u00e0 d\u2019autres propri\u00e9taires qu\u2019Oracle.\n\nAMD, Opteron, le logo AMD et le logo AMD Opteron sont des marques ou des marques d\u00e9pos\u00e9es d\u2019Advanced Micro Devices. Intel et Intel Xeon sont des marques ou des marques d\u00e9pos\u00e9es d\u2019Intel Corporation. Toutes les marques SPARC sont utilis\u00e9es sous licence et sont des marques ou des marques d\u00e9pos\u00e9es de SPARC International, Inc. UNIX est une marque d\u00e9pos\u00e9e conc\u00e9d\u00e9e sous licence par X/Open Company, Ltd.", "mimetype": "text/plain", "start_char_idx": 7718, "end_char_idx": 11807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70f9d054-2875-4a2e-af9b-10976de8a456": {"__data__": {"id_": "70f9d054-2875-4a2e-af9b-10976de8a456", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "55d55b31a088d3d367ff03172f04f99c0eb048da7ef4aa30bacda43c01d11edb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a671d38-8d1e-4b2e-ae6d-79ceb2a86a5e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "14878e456129a07c998c1d1f8e725eb7f5443e04bd13e353d56d681fa8e8dfce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Oracle Corporation et ses affili\u00e9s d\u00e9clinent toute responsabilit\u00e9 quant aux dommages caus\u00e9s par l\u2019utilisation de ce logiciel ou mat\u00e9riel pour ce type d\u2019applications.\n\nOracle et Java sont des marques d\u00e9pos\u00e9es d\u2019Oracle Corporation et/ou de ses affili\u00e9s.Tout autre nom mentionn\u00e9 peut correspondre \u00e0 des marques appartenant \u00e0 d\u2019autres propri\u00e9taires qu\u2019Oracle.\n\nAMD, Opteron, le logo AMD et le logo AMD Opteron sont des marques ou des marques d\u00e9pos\u00e9es d\u2019Advanced Micro Devices. Intel et Intel Xeon sont des marques ou des marques d\u00e9pos\u00e9es d\u2019Intel Corporation. Toutes les marques SPARC sont utilis\u00e9es sous licence et sont des marques ou des marques d\u00e9pos\u00e9es de SPARC International, Inc. UNIX est une marque d\u00e9pos\u00e9e conc\u00e9d\u00e9e sous licence par X/Open Company, Ltd.\n\nCe logiciel ou mat\u00e9riel et la documentation qui l\u2019accompagne peuvent fournir des informations ou des liens donnant acc\u00e8s \u00e0 des contenus, des produits et des services \u00e9manant de tiers. Oracle Corporation et ses affili\u00e9s d\u00e9clinent toute responsabilit\u00e9 ou garantie expresse quant aux contenus, produits ou services \u00e9manant de tiers. En aucun cas, Oracle Corporation et ses affili\u00e9s ne sauraient \u00eatre tenus pour responsables des pertes subies, des co\u00fbts occasionn\u00e9s ou des dommages caus\u00e9s par l\u2019acc\u00e8s \u00e0 des contenus, produits ou services tiers, ou \u00e0 leur utilisation.\n\nThis work is licensed under a Creative Commons Attribution-Share Alike 3.0 United States License. To view a copy of this license and obtain more information about Creative Commons licensing, visit [Creative Commons Attribution-Share Alike 3.0 United States](http://creativecommons.org/licenses/by-sa/3.0/us) or send a letter to Creative Commons, 171 2nd Street, Suite 300, San Francisco, California 94105, USA.", "mimetype": "text/plain", "start_char_idx": 11052, "end_char_idx": 12784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16c1f261-9688-46f2-a408-fa12cb0ff908": {"__data__": {"id_": "16c1f261-9688-46f2-a408-fa12cb0ff908", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/00-Preface.md", "file_name": "00-Preface.md", "file_type": "text/markdown", "file_size": 4867, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf8a1ec0-568b-47b0-a7c5-21a48cec8b13", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/00-Preface.md", "file_name": "00-Preface.md", "file_type": "text/markdown", "file_size": 4867, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7a530bf76b267fa1d24d94f51c71502e386fe76e906c0e7575df56b0f9b32e66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edabca5c-8221-41cb-ab61-c15aa7d90653", "node_type": "1", "metadata": {}, "hash": "6347d21489f29184f716ce82be452afb9d195bb6c851f4cfb8f6287cd0bae117", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Preface\n\nThe Lustre*Software Release 2.x Operations Manual provides detailed information and procedures to install, configure and tune a Lustre file system. The manual covers topics such as failover, quotas, striping, and bonding. This manual also contains troubleshooting information and tips to improve the operation and performance of a Lustre file system.\n\n- [Preface](#preface)\n  * [About this Document](#about-this-document)\n    + [UNIX* Commands](#unix-commands)\n    + [Shell Prompts](#shell-prompts)\n    + [Related Documentation](#related-documentation)\n    + [Documentation and Support](#documentation-and-support)\n  * [Revisions](#revisions)\n    + [which version?](#which-version)\n\n\n\n## About this Document\n\nThis document is maintained by Whamcloud in Docbook format. The canonical version is available at <https://wiki.whamcloud.com/display/PUB/Documentation>.\n\n\n\n### UNIX* Commands\n\nThis document may not contain information about basic UNIX* operating system commands and procedures such as shutting down the system, booting the system, and configuring devices. Refer to the following for this information:\n\n- Software documentation that you received with your system\n\n- Red Hat* Enterprise Linux* documentation, which is at: <https://docs.redhat.com/docs/en-US/index.html>\n\n  ### Note\n\n  The Lustre client module is available for many different Linux* versions and distributions. The Red Hat Enterprise Linux distribution is the best supported and tested platform for Lustre servers.\n\n\n\n### Shell Prompts\n\nThe shell prompt used in the example text indicates whether a command can or should be executed by a regular user, or whether it requires superuser permission to run. Also, the machine type is often included in the prompt to indicate whether the command should be run on a client node, on an MDS node, an OSS node, or the MGS node.\n\nSome examples are listed below, but other prompt combinations are also used as needed for the example.\n\n| **Shell**                  | **Prompt** |\n| -------------------------- | ---------- |\n| Regular user               | `machine$` |\n| Superuser (root)           | `machine#` |\n| Regular user on the client | `client$`  |\n| Superuser on the MDS       | `mds#`     |\n| Superuser on the OSS       | `oss#`     |\n| Superuser on the MGS       | `mgs#`     |\n\n\n\n\n\n### Related Documentation\n\n| **Application**    | **Title**                                       | **Format** | **Location**                                                 |\n| ------------------ | ----------------------------------------------- | ---------- | ------------------------------------------------------------ |\n| Latest information | *Lustre Software Release 2.x Change Logs*       | Wiki page  | Online at <https://wiki.whamcloud.com/display/PUB/Documentation> |\n| Service            | *Lustre Software Release 2.x Operations Manual* | PDFHTML    | Online at <https://wiki.whamcloud.com/display/PUB/Documentation> |\n\n \n\n### Documentation and Support\n\nThese web sites provide additional resources:\n\n- Documentation <http://wiki.whamcloud.com/display/PUB/Documentation> <http://www.lustre.org>\n- Support <https://jira.whamcloud.com/>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "edabca5c-8221-41cb-ab61-c15aa7d90653": {"__data__": {"id_": "edabca5c-8221-41cb-ab61-c15aa7d90653", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/00-Preface.md", "file_name": "00-Preface.md", "file_type": "text/markdown", "file_size": 4867, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf8a1ec0-568b-47b0-a7c5-21a48cec8b13", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/00-Preface.md", "file_name": "00-Preface.md", "file_type": "text/markdown", "file_size": 4867, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7a530bf76b267fa1d24d94f51c71502e386fe76e906c0e7575df56b0f9b32e66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16c1f261-9688-46f2-a408-fa12cb0ff908", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/00-Preface.md", "file_name": "00-Preface.md", "file_type": "text/markdown", "file_size": 4867, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3ea83ae53239d213669c1a3718bd4dda8cdd72bdf28c2a941f3827c996894bd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Related Documentation\n\n| **Application**    | **Title**                                       | **Format** | **Location**                                                 |\n| ------------------ | ----------------------------------------------- | ---------- | ------------------------------------------------------------ |\n| Latest information | *Lustre Software Release 2.x Change Logs*       | Wiki page  | Online at <https://wiki.whamcloud.com/display/PUB/Documentation> |\n| Service            | *Lustre Software Release 2.x Operations Manual* | PDFHTML    | Online at <https://wiki.whamcloud.com/display/PUB/Documentation> |\n\n \n\n### Documentation and Support\n\nThese web sites provide additional resources:\n\n- Documentation <http://wiki.whamcloud.com/display/PUB/Documentation> <http://www.lustre.org>\n- Support <https://jira.whamcloud.com/>\n\n\n\n## Revisions\n\nThe Lustre* File System Release 2.x Operations Manual is a community maintained work. Versions of the manual are continually built as suggestions for changes and improvements arrive. Suggestions for improvements can be submitted through the ticketing system maintained at <https://jira.whamcloud.com/browse/LUDOC>. Instructions for providing a patch to the existing manual are available at: <http://wiki.lustre.org/Lustre_Manual_Changes>.\n\nThis manual covers a range of Lustre 2.x software releases, currently starting with the 2.5 release. Features specific to individual releases are identified within the table of contents using a shorthand notation (e.g. this paragraph is tagged as a Lustre 2.5 specific feature so that it will be updated when the 2.5-specific tagging is removed), and within the text using a distinct box.\n\n### which version amd I running?\n\nThe current version of Lustre that is in use on the node can be found using the command `lctl get_param version` on any Lustre client or server, for example:\n\n```\n$ lctl get_param version\nversion=2.10.5\n```\n\nOnly the latest revision of this document is made readily available because changes are continually arriving. The current and latest revision of this manual is available from links maintained at:<http://lustre.opensfs.org/documentation/>.\n\n| **Revision History**        |                                                      |      |\n| --------------------------- | ---------------------------------------------------- | ---- |\n| Revision 0                  | Built on 04 January 2022 21:27:24Z Intel Corporation |      |\n| Continuous build of Manual. |                                                      |      |", "mimetype": "text/plain", "start_char_idx": 2316, "end_char_idx": 4867, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99e6d9a4-143d-4488-9c4d-b91deaa9ea7f": {"__data__": {"id_": "99e6d9a4-143d-4488-9c4d-b91deaa9ea7f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "990e0472-9948-4c0d-a91e-3e755b9eb13a", "node_type": "1", "metadata": {}, "hash": "240c5b38849e137e2e350716870c3a8f9e10614b20657b68b6e803786d728bb7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Table of Content\n- [Introducing the Lustre* File System](#introducing-the-lustre-file-system)\n  * [1. Understanding Lustre Architecture](#understanding-lustre-architecture)\n    + [What a Lustre File System Is (and What It Isn't)](#what-a-lustre-file-system-is-and-what-it-isnt)\n      - [Lustre Features](#lustre-features)\n    + [Lustre Components](#lustre-components)\n      - [Management Server (MGS)](#management-server-mgs)\n      - [Lustre File System Components](#lustre-file-system-components)\n      - [Lustre Networking (LNet)](#lustre-networking-lnet)\n      - [Lustre Cluster](#lustre-cluster)\n    + [Lustre File System Storage and I/O](#lustre-file-system-storage-and-io)\n      - [Lustre File System and Striping](#lustre-file-system-and-striping)\n  * [Understanding Lustre Networking (LNet)](#understanding-lustre-networking-lnet)\n    + [Introducing LNet](#introducing-lnet)\n    + [Key Features of LNet](#key-features-of-lnet)\n    + [Lustre Networks](#lustre-networks)\n    + [Supported Network Types](#supported-network-types)\n  * [Understanding Failover in a Lustre File System](#understanding-failover-in-a-lustre-file-system)\n    + [What is Failover?](#what-is-failover)\n      - [Failover Capabilities](#failover-capabilities)\n      - [Types of Failover Configurations](#types-of-failover-configurations)\n    + [Failover Functionality in a Lustre File System](#failover-functionality-in-a-lustre-file-system)\n      - [MDT Failover Configuration (Active/Passive)](#mdt-failover-configuration-activepassive)\n      - [MDT Failover Configuration (Active/Active)](#mdt-failover-configuration-activeactive)L 2.4\n      - [OST Failover Configuration (Active/Active)](#ost-failover-configuration-activeactive)\n\n# Introducing the Lustre* File System\n\nPart I provides background information to help you understand the Lustre file system architecture and how the major components fit together. You will find information in this section about:\n\n- [Understanding Lustre Architecture](#understanding-lustre-architecture)\n- [Understanding Lustre Networking (LNet)](#understanding-lustre-networking-lnet)\n- [Understanding Failover in a Lustre File System](#understanding-failover-in-a-lustre-file-system)\n\n## Understanding Lustre Architecture\n\n### What a Lustre File System Is (and What It Isn't)\n\nThe Lustre architecture is a storage architecture for clusters. The central component of the Lustre architecture is the Lustre file system, which is supported on the Linux operating system and provides a POSIX *standard-compliant UNIX file system interface.\n\nThe Lustre storage architecture is used for many different kinds of clusters. It is best known for powering many of the largest high-performance computing (HPC) clusters worldwide, with tens of thousands of client systems, petabytes (PiB) of storage and hundreds of gigabytes per second (GB/sec) of I/O throughput. Many HPC sites use a Lustre file system as a site-wide global file system, serving dozens of clusters.\n\nThe ability of a Lustre file system to scale capacity and performance for any need reduces the need to deploy many separate file systems, such as one for each compute cluster. Storage management is simplified by avoiding the need to copy data between compute clusters. In addition to aggregating storage capacity of many servers, the I/O throughput is also aggregated and scales with additional servers. Moreover, throughput and/or capacity can be easily increased by adding servers dynamically.\n\nWhile a Lustre file system can function in many work environments, it is not necessarily the best choice for all applications. It is best suited for uses that exceed the capacity that a single server can provide, though in some use cases, a Lustre file system can perform better with a single server than other file systems due to its strong locking and data coherency.\n\nA Lustre file system is currently not particularly well suited for \"peer-to-peer\" usage models where clients and servers are running on the same node, each sharing a small amount of storage, due to the lack of data replication at the Lustre software level. In such uses, if one client/server fails, then the data stored on that node will not be accessible until the node is restarted.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "990e0472-9948-4c0d-a91e-3e755b9eb13a": {"__data__": {"id_": "990e0472-9948-4c0d-a91e-3e755b9eb13a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99e6d9a4-143d-4488-9c4d-b91deaa9ea7f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8f69d6283f6d2ed3dc8200ff1832121c957f349cb04ad53c7838c0c01dfe6c18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84fb2fa1-598c-46c1-8441-1fed67455e51", "node_type": "1", "metadata": {}, "hash": "149a3ddc4550afd15bcaa0c8db9fd36987699ddcdabaaa41d5125b78ad731d7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition to aggregating storage capacity of many servers, the I/O throughput is also aggregated and scales with additional servers. Moreover, throughput and/or capacity can be easily increased by adding servers dynamically.\n\nWhile a Lustre file system can function in many work environments, it is not necessarily the best choice for all applications. It is best suited for uses that exceed the capacity that a single server can provide, though in some use cases, a Lustre file system can perform better with a single server than other file systems due to its strong locking and data coherency.\n\nA Lustre file system is currently not particularly well suited for \"peer-to-peer\" usage models where clients and servers are running on the same node, each sharing a small amount of storage, due to the lack of data replication at the Lustre software level. In such uses, if one client/server fails, then the data stored on that node will not be accessible until the node is restarted.\n\n#### Lustre Features\n\nLustre file systems run on a variety of vendor's kernels. For more details, see the Lustre Test Matrix [the section called \u201c Preparing to Install the Lustre Software\u201d](02.05-Installing%20the%20Lustre%20Software.md#installing-the-lustre-software).\n\nA Lustre installation can be scaled up or down with respect to the number of client nodes, disk storage and bandwidth. Scalability and performance are dependent on available disk and network bandwidth and the processing power of the servers in the system. A Lustre file system can be deployed in a wide variety of configurations that can be scaled well beyond the size and performance observed in production systems to date.\n\n[Table 1, \u201cLustre File System Scalability and Performance\u201d](#table-1-lustre-file-system-scalability-and-performance) shows some of the scalability and performance characteristics of a Lustre file system. For a full list of Lustre file and filesystem limits see [Table 4, \u201cFile and file system limits\u201d](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#table-4-file-and-file-system-limits).\n\n##### Table 1.", "mimetype": "text/plain", "start_char_idx": 3241, "end_char_idx": 5369, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84fb2fa1-598c-46c1-8441-1fed67455e51": {"__data__": {"id_": "84fb2fa1-598c-46c1-8441-1fed67455e51", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "990e0472-9948-4c0d-a91e-3e755b9eb13a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "037c3be5588bba82bf1c4626653042236fa093289b308ffdf3eaa3b9ae5d1738", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d084acc6-9fdb-409a-9709-a6f5bd2dc2f1", "node_type": "1", "metadata": {}, "hash": "6a10c72530034cadf2382add5cda266cb82e305876a712d6f0c2a7ddbe7562f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Lustre installation can be scaled up or down with respect to the number of client nodes, disk storage and bandwidth. Scalability and performance are dependent on available disk and network bandwidth and the processing power of the servers in the system. A Lustre file system can be deployed in a wide variety of configurations that can be scaled well beyond the size and performance observed in production systems to date.\n\n[Table 1, \u201cLustre File System Scalability and Performance\u201d](#table-1-lustre-file-system-scalability-and-performance) shows some of the scalability and performance characteristics of a Lustre file system. For a full list of Lustre file and filesystem limits see [Table 4, \u201cFile and file system limits\u201d](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#table-4-file-and-file-system-limits).\n\n##### Table 1. Lustre File System Scalability and Performance\n\n| **Feature**                 | **Current Practical Range**                                  | **Known Production Usage**                                   |\n| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| **Client Scalability**      | 100-100000                                                   | 50000+ clients, many in the 10000 to 20000 range             |\n| **Client Performance**      | *Single client:*<br/>I/O 90% of network bandwidth<br/>*Aggregate:<br/>*10 TB/sec I/O | *Single client:*<br/>4.5 GB/sec I/O (FDR IB, OPA1), 1000 metadata ops/sec<br/>*Aggregate:*<br/>2.5 TB/sec I/O |\n| **OSS Scalability**         | *Single OSS:*<br/>1-32 OSTs per OSS<br/>*Single OST:<br/>*300M objects, 256TiB per OST (ldiskfs)<br/>500M objects, 256TiB per OST (ZFS)<br/>*OSS count:<br/>*1000 OSSs, with up to 4000 OSTs | *Single OSS:*<br/>32x 8TiB OSTs per OSS (ldiskfs),<br/>8x 32TiB OSTs per OSS (ldiskfs)<br/>1x 72TiB OST per OSS (ZFS)<br/>*OSS count:*<br/>450 OSSs with 1000 4TiB OSTs<br/>192 OSSs with 1344 8TiB OSTs<br/>768 OSSs with 768 72TiB OSTs |\n| **OSS Performance**         | *Single OSS:*<br/>15 GB/sec<br/>*Aggregate:<br/>*10 TB/sec   | *Single OSS:<br/>*10 GB/sec<br/>*Aggregate:*<br/>2.5 TB/sec  |\n| **MDS Scalability**         | *Single MDS:<br/>*1-4 MDTs per MDS<br/>*Single MDT<br/>:*4 billion files, 8TiB per MDT (ldiskfs)<br/>64 billion files, 64TiB per MDT (ZFS)<br/>*MDS count:*<br/>256 MDSs, with up to 256 MDTs | *Single MDS:*<br/>3 billion files<br/>*MDS count:*<br/>7 MDS with 7 2TiB MDTs in production<br/>256 MDS with 256 64GiB MDTs in testing |\n| **MDS Performance**         | 50000/s create operations,<br/>200000/s metadata stat operations | 15000/s create operations,<br/>50000/s metadata stat operations |\n| **File system Scalability** | *Single File:*<br/>32 PiB max file size (ldiskfs)<br/>2^63 bytes (ZFS)<br/>*Aggregate:*<br/>512 PiB space, 1 trillion files | *Single File:*<br/>multi-TiB max file size<br/>*Aggregate:*<br/>55 PiB space, 8 billion files |\n\n \n\nOther Lustre software features are:\n\n- **Performance-enhanced ext4 file system:** The Lustre file system uses an improved version of the ext4 journaling file system to store data and metadata. This version, called `ldiskfs` , has been enhanced to improve performance and provide additional functionality needed by the Lustre file system.\n- It is also possible to use ZFS as the backing filesystem for Lustre for the MDT, OST, and MGS storage. This allows Lustre to leverage the scalability and data integrity features of ZFS for individual storage targets.", "mimetype": "text/plain", "start_char_idx": 4496, "end_char_idx": 8080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d084acc6-9fdb-409a-9709-a6f5bd2dc2f1": {"__data__": {"id_": "d084acc6-9fdb-409a-9709-a6f5bd2dc2f1", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84fb2fa1-598c-46c1-8441-1fed67455e51", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f51ee0599f59f82fc922b91ac0c8e81e180a8bd4a246d35505588efd26db7b0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87c980c0-e858-404a-adcc-7852870d6793", "node_type": "1", "metadata": {}, "hash": "1cb8e2c5797e912e563ecbeb6670f739839d6ce12fbda6d584e4225bba8e06a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This version, called `ldiskfs` , has been enhanced to improve performance and provide additional functionality needed by the Lustre file system.\n- It is also possible to use ZFS as the backing filesystem for Lustre for the MDT, OST, and MGS storage. This allows Lustre to leverage the scalability and data integrity features of ZFS for individual storage targets.\n- **POSIX standard compliance:** The full POSIX test suite passes in an identical manner to a local ext4 file system, with limited exceptions on Lustre clients. In a cluster, most operations are atomic so that clients never see stale data or metadata. The Lustre software supports mmap() file I/O.\n- **High-performance heterogeneous networking:** The Lustre software supports a variety of high performance, low latency networks and permits Remote Direct Memory Access (RDMA) for InfiniBand *(utilizing OpenFabrics Enterprise Distribution (OFED*), Intel OmniPath\u00ae, and other advanced networks for fast and efficient network transport. Multiple RDMA networks can be bridged using Lustre routing for maximum performance. The Lustre software also includes integrated network diagnostics.\n- **High-availability:** The Lustre file system supports active/active failover using shared storage partitions for OSS targets (OSTs), and for MDS targets (MDTs). The Lustre file system can work with a variety of high availability (HA) managers to allow automated failover and has no single point of failure (NSPF). This allows application transparent recovery. Multiple mount protection (MMP) provides integrated protection from errors in highly-available systems that would otherwise cause file system corruption.\n- **Security:** By default TCP connections are only allowed from privileged ports. UNIX group membership is verified on the MDS.\n- **Access control list (ACL), extended attributes:** the Lustre security model follows that of a UNIX file system, enhanced with POSIX ACLs. Noteworthy additional features include root squash.\n- **Interoperability:** The Lustre file system runs on a variety of CPU architectures and mixed-endian clusters and is interoperable between successive major Lustre software releases.\n- **Object-based architecture:** Clients are isolated from the on-disk file structure enabling upgrading of the storage architecture without affecting the client.\n- **Byte-granular file and fine-grained metadata locking:** Many clients can read and modify the same file or directory concurrently. The Lustre distributed lock manager (LDLM) ensures that files are coherent between all clients and servers in the file system. The MDT LDLM manages locks on inode permissions and pathnames. Each OST has its own LDLM for locks on file stripes stored thereon, which scales the locking performance as the file system grows.\n- **Quotas:** User and group quotas are available for a Lustre file system.\n- **Capacity growth:**The size of a Lustre file system and aggregate cluster bandwidth can be increased without interruption by adding new OSTs and MDTs to the cluster.\n- **Controlled file layout:** The layout of files across OSTs can be configured on a per file, per directory, or per file system basis. This allows file I/O to be tuned to specific application requirements within a single file system. The Lustre file system uses RAID-0 striping and balances space usage across OSTs.\n- **Network data integrity protection:** A checksum of all data sent from the client to the OSS protects against corruption during data transfer.\n- **MPI I/O:** The Lustre architecture has a dedicated MPI ADIO layer that optimizes parallel I/O to match the underlying file system architecture.\n- **NFS and CIFS export:** Lustre files can be re-exported using NFS (via Linux knfsd or Ganesha) or CIFS (via Samba), enabling them to be shared with non-Linux clients such as Microsoft*Windows, *Apple *Mac OS X*, and others.\n- **Disaster recovery tool:** The Lustre file system provides an online distributed file system check (LFSCK) that can restore consistency between storage components in case of a major file system error. A Lustre file system can operate even in the presence of file system inconsistencies, and LFSCK can run while the filesystem is in use, so LFSCK is not required to complete before returning the file system to production.\n- **Performance monitoring:** The Lustre file system offers a variety of mechanisms to examine performance and tuning.\n- **Open source:** The Lustre software is licensed under the GPL 2.0 license for use with the Linux operating system.\n\n### Lustre Components \n\nAn installation of the Lustre software includes a management server (MGS) and one or more Lustre file systems interconnected with Lustre networking (LNet).", "mimetype": "text/plain", "start_char_idx": 7717, "end_char_idx": 12432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87c980c0-e858-404a-adcc-7852870d6793": {"__data__": {"id_": "87c980c0-e858-404a-adcc-7852870d6793", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d084acc6-9fdb-409a-9709-a6f5bd2dc2f1", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "edeca2bf40624b4f8780a32ddc45f4da01a119ff0abf43a4d40ba58745b3f550", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05262992-1418-4e39-8e59-f9a354c0d2c7", "node_type": "1", "metadata": {}, "hash": "e1d9060676861d6c2357b9fe94964a25ef578da5190815a955b0ffe62fadb9c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Disaster recovery tool:** The Lustre file system provides an online distributed file system check (LFSCK) that can restore consistency between storage components in case of a major file system error. A Lustre file system can operate even in the presence of file system inconsistencies, and LFSCK can run while the filesystem is in use, so LFSCK is not required to complete before returning the file system to production.\n- **Performance monitoring:** The Lustre file system offers a variety of mechanisms to examine performance and tuning.\n- **Open source:** The Lustre software is licensed under the GPL 2.0 license for use with the Linux operating system.\n\n### Lustre Components \n\nAn installation of the Lustre software includes a management server (MGS) and one or more Lustre file systems interconnected with Lustre networking (LNet).\n\nA basic configuration of Lustre file system components is shown in [Figure 1, \u201cLustre file system components in a basic cluster\u201d](#figure-1-lustre-file-system-components-in-a-basic-cluster).\n\n##### Figure 1. Lustre file system components in a basic cluster\n\n![Lustre file system components in a basic cluster](figures/Basic_Cluster.png)\n\n#### Management Server (MGS)\n\nThe MGS stores configuration information for all the Lustre file systems in a cluster and provides this information to other Lustre components. Each Lustre target contacts the MGS to provide information, and Lustre clients contact the MGS to retrieve information.\n\nIt is preferable that the MGS have its own storage space so that it can be managed independently. However, the MGS can be co-located and share storage space with an MDS as shown in [Figure 1, \u201cLustre file system components in a basic cluster\u201d](#figure-1-lustre-file-system-components-in-a-basic-cluster).\n\n#### Lustre File System Components\n\nEach Lustre file system consists of the following components:\n\n- **Metadata Servers (MDS)**- The MDS makes metadata stored in one or more MDTs available to Lustre clients. Each MDS manages the names and directories in the Lustre file system(s) and provides network request handling for one or more local MDTs.\n\n- **Metadata Targets (MDT**) \\- Each filesystem has at least one MDT, which holds the root directory. The MDT stores metadata (such as filenames, directories, permissions and file layout) on storage attached to an MDS. Each file system has one MDT. An MDT on a shared storage target can be available to multiple MDSs, although only one can access it at a time. If an active MDS fails, a second MDS node can serve the MDT and make it available to clients. This is referred to as MDS failover.\n\n  Multiple MDTs are supported with the Distributed Namespace Environment (Distributed Namespace Environment (DNE)). In addition to the primary MDT that holds the filesystem root, it is possible to add additional MDS nodes, each with their own MDTs, to hold sub-directory trees of the filesystem.\n\n  Introduced in Lustre 2.8\n\n  Since Lustre software release 2.8, DNE also allows the filesystem to distribute files of a single directory over multiple MDT nodes. A directory which is distributed across multiple MDTs is known as a *striped directory*.\n\n- **Object Storage Servers (OSS)**: The OSS provides file I/O service and network request handling for one or more local OSTs. Typically, an OSS serves between two and eight OSTs, up to 16 TiB each. A typical configuration is an MDT on a dedicated node, two or more OSTs on each OSS node, and a client on each of a large number of compute nodes.\n\n- **Object Storage Target (OST)**: User file data is stored in one or more objects, each object on a separate OST in a Lustre file system. The number of objects per file is configurable by the user and can be tuned to optimize performance for a given workload.\n\n- **Lustre clients**: Lustre clients are computational, visualization or desktop nodes that are running Lustre client software, allowing them to mount the Lustre file system.\n\nThe Lustre client software provides an interface between the Linux virtual file system and the Lustre servers. The client software includes a management client (MGC), a metadata client (MDC), and multiple object storage clients (OSCs), one corresponding to each OST in the file system.\n\nA logical object volume (LOV) aggregates the OSCs to provide transparent access across all the OSTs. Thus, a client with the Lustre file system mounted sees a single, coherent, synchronized namespace.", "mimetype": "text/plain", "start_char_idx": 11590, "end_char_idx": 16035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05262992-1418-4e39-8e59-f9a354c0d2c7": {"__data__": {"id_": "05262992-1418-4e39-8e59-f9a354c0d2c7", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87c980c0-e858-404a-adcc-7852870d6793", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "483a13f44fb33e25e6a074afcbc4145edba9110f5117dce0f820a040a66ca74b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88998759-6bbf-4408-bb53-ff4522aef293", "node_type": "1", "metadata": {}, "hash": "f7a414bebdd2ea693dc0ea896355ad14b3f455e87ed0224ba1a9084af9b0ac61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Object Storage Target (OST)**: User file data is stored in one or more objects, each object on a separate OST in a Lustre file system. The number of objects per file is configurable by the user and can be tuned to optimize performance for a given workload.\n\n- **Lustre clients**: Lustre clients are computational, visualization or desktop nodes that are running Lustre client software, allowing them to mount the Lustre file system.\n\nThe Lustre client software provides an interface between the Linux virtual file system and the Lustre servers. The client software includes a management client (MGC), a metadata client (MDC), and multiple object storage clients (OSCs), one corresponding to each OST in the file system.\n\nA logical object volume (LOV) aggregates the OSCs to provide transparent access across all the OSTs. Thus, a client with the Lustre file system mounted sees a single, coherent, synchronized namespace. Several clients can write to different parts of the same file simultaneously, while, at the same time, other clients can read from the file.\n\nA logical metadata volume (LMV) aggregates the MDCs to provide transparent access across all the MDTs in a similar manner as the LOV does for file access. This allows the client to see the directory tree on multiple MDTs as a single coherent namespace, and striped directories are merged on the clients to form a single visible directory to users and applications.\n\n[Table 2, \u201c Storage and hardware requirements for Lustre file system components\u201d](#table-2--storage-and-hardware-requirements-for-lustre-file-system-components) provides the requirements for attached storage for each Lustre file system component and describes desirable characteristics of the hardware used.\n\n##### Table 2.  Storage and hardware requirements for Lustre file system components\n\n|             | **Required attached storage**       | **Desirable hardware characteristics**                       |\n| ----------- | ----------------------------------- | ------------------------------------------------------------ |\n| **MDSs**    | 1-2% of file system capacity        | Adequate CPU power, plenty of memory, fast disk storage.     |\n| **OSSs**    | 1-128 TiB per OST, 1-8 OSTs per OSS | Good bus bandwidth. Recommended that storage be balanced evenly across OSSs and matched to network bandwidth. |\n| **Clients** | No local storage needed             | Low latency, high bandwidth network.                         |\n\n \n\n \n\nFor additional hardware requirements and considerations, see [*Determining Hardware Configuration Requirements and Formatting Options*](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md).\n\n#### Lustre Networking (LNet)\n\nLustre Networking (LNet) is a custom networking API that provides the communication infrastructure that handles metadata and file I/O data for the Lustre file system servers and clients. For more information about LNet, see[*Understanding Lustre Networking (LNet)*](#understanding-lustre-networking-lnet).", "mimetype": "text/plain", "start_char_idx": 15110, "end_char_idx": 18146, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88998759-6bbf-4408-bb53-ff4522aef293": {"__data__": {"id_": "88998759-6bbf-4408-bb53-ff4522aef293", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05262992-1418-4e39-8e59-f9a354c0d2c7", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "492b292cc77c3cd6353afeb5820036e18a4dacf0aa5a7d36e7b74fb3f959c7b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "098fb67f-4fff-4e8e-8338-9623ed9f1eb4", "node_type": "1", "metadata": {}, "hash": "3c78754ab603b8c775ac3b030ffa122721ba31955224147592410de6cb9b9b20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| **OSSs**    | 1-128 TiB per OST, 1-8 OSTs per OSS | Good bus bandwidth. Recommended that storage be balanced evenly across OSSs and matched to network bandwidth. |\n| **Clients** | No local storage needed             | Low latency, high bandwidth network.                         |\n\n \n\n \n\nFor additional hardware requirements and considerations, see [*Determining Hardware Configuration Requirements and Formatting Options*](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md).\n\n#### Lustre Networking (LNet)\n\nLustre Networking (LNet) is a custom networking API that provides the communication infrastructure that handles metadata and file I/O data for the Lustre file system servers and clients. For more information about LNet, see[*Understanding Lustre Networking (LNet)*](#understanding-lustre-networking-lnet).\n\n\n\n#### Lustre Cluster\n\nAt scale, a Lustre file system cluster can include hundreds of OSSs and thousands of clients (see [Figure 2, \u201c Lustre cluster at scale\u201d](#figure-2--lustre-cluster-at-scale). More than one type of network can be used in a Lustre cluster. Shared storage between OSSs enables failover capability. For more details about OSS failover, see [*Understanding Failover in a Lustre File System*](#understanding-failover-in-a-lustre-file-system).\n\n##### Figure 2.  Lustre cluster at scale\n\n![Lustre file system cluster at scale](figures/Scaled_Cluster.png)\n\n### Lustre File System Storage and I/O\n\nLustre File IDentifiers (FIDs) are used internally for identifying files or objects, similar to inode numbers in local filesystems. A FID is a 128-bit identifier, which contains a unique 64-bit sequence number (SEQ), a 32-bit object ID (OID), and a 32-bit version number. The sequence number is unique across all Lustre targets in a file system (OSTs and MDTs). This allows multiple MDTs and OSTs to uniquely identify objects without depending on identifiers in the underlying filesystem (e.g. inode numbers) that are likely to be duplicated between targets. The FID SEQ number also allows mapping a FID to a particular MDT or OST.\n\nThe LFSCK file system consistency checking tool provides functionality that enables FID-in-dirent for existing files. It includes the following functionality:   \n\n- Verifies the FID stored with each directory entry and regenerates it from the inode if it is invalid or missing.\n- Verifies the linkEA entry for each inode and regenerates it if invalid or missing. The linkEA stores of the file name and parent FID. It is stored as an extended attribute in each inode. Thus, the linkEA can be used to reconstruct the full path name of a file from only the FID.\n\nInformation about where file data is located on the OST(s) is stored as an extended attribute called layout EA in an MDT object identified by the FID for the file (see [Figure 3, \u201cLayout EA on MDT pointing to file data on OSTs\u201d](#figure-3-layout-ea-on-mdt-pointing-to-file-data-on-osts). If the file is a regular file (not a directory or symbol link), the MDT object points to 1-to-N OST object(s) on the OST(s) that contain the file data. If the MDT file points to one object, all the file data is stored in that object. If the MDT file points to more than one object, the file data is striped across the objects using RAID 0, and each object is stored on a different OST. (For more information about how striping is implemented in a Lustre file system, see [the section called \u201c Lustre File System and Striping\u201d](#lustre-file-system-and-striping).\n\n##### Figure 3. Layout EA on MDT pointing to file data on OSTs\n\n ![Layout EA on MDT pointing to file data on OSTs](figures/Metadata_File.png)", "mimetype": "text/plain", "start_char_idx": 17287, "end_char_idx": 20943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "098fb67f-4fff-4e8e-8338-9623ed9f1eb4": {"__data__": {"id_": "098fb67f-4fff-4e8e-8338-9623ed9f1eb4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88998759-6bbf-4408-bb53-ff4522aef293", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5d003d17639b3473c8082ff80855092af0d7c59655a25ee99cbb91cd1da4a58f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a6634ef-a9c0-43d7-bdac-46e915c27c8d", "node_type": "1", "metadata": {}, "hash": "b57a234e91857fc7da45fd172571397416822432c505c380c0c38b7ffa649717", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When a client wants to read from or write to a file, it first fetches the layout EA from the MDT object for the file. The client then uses this information to perform I/O on the file, directly interacting with the OSS nodes where the objects are stored. This process is illustrated in [Figure 4, \u201cLustre client requesting file data\u201d](#figure-4-lustre-client-requesting-file-data).\n\n##### Figure 4. Lustre client requesting file data\n\n ![Lustre client requesting file data](figures/File_Write.png)\n\nThe available bandwidth of a Lustre file system is determined as follows:\n\n- The *network bandwidth* equals the aggregated bandwidth of the OSSs to the targets.\n\n- The *disk bandwidth* equals the sum of the disk bandwidths of the storage targets (OSTs) up to the limit of the network bandwidth.\n\n- The *aggregate bandwidth* equals the minimum of the disk bandwidth and the network bandwidth.\n\n- The *available file system space* equals the sum of the available space of all the OSTs.\n\n\n\n\n#### Lustre File System and Striping\n\nOne of the main factors leading to the high performance of Lustre file systems is the ability to stripe data across multiple OSTs in a round-robin fashion. Users can optionally configure for each file the number of stripes, stripe size, and OSTs that are used.\n\nStriping can be used to improve performance when the aggregate bandwidth to a single file exceeds the bandwidth of a single OST. The ability to stripe is also useful when a single OST does not have enough free space to hold an entire file. For more information about benefits and drawbacks of file striping, see [the section called \u201c Lustre File Layout (Striping) Considerations\u201d](#lustre-file-layout-striping-considerations).\n\nStriping allows segments or 'chunks' of data in a file to be stored on different OSTs, as shown in [Figure 5, \u201cFile striping on a Lustre file system\u201d](#figure-5-file-striping-on-a-lustre-file-system). In the Lustre file system, a RAID 0 pattern is used in which data is \"striped\" across a certain number of objects. The number of objects in a single file is called the `stripe_count`.\n\nEach object contains a chunk of data from the file. When the chunk of data being written to a particular object exceeds the `stripe_size`, the next chunk of data in the file is stored on the next object.\n\nDefault values for `stripe_count` and `stripe_size` are set for the file system. The default value for`stripe_count` is 1 stripe for file and the default value for `stripe_size` is 1MB. The user may change these values on a per directory or per file basis. For more details, see [the section called \u201cSetting the File Layout/Striping Configuration (`lfs setstripe`)\u201d](#setting-the-file-layoutstriping-configuration-lfs-setstripe).\n\n[Figure 5, \u201cFile striping on a Lustre file system\u201d](#figure-5-file-striping-on-a-lustre-file-system), the `stripe_size` for File C is larger than the `stripe_size` for File A, allowing more data to be stored in a single stripe for File C. The `stripe_count` for File A is 3, resulting in data striped across three objects, while the `stripe_count` for File B and File C is 1.\n\nNo space is reserved on the OST for unwritten data. File A in [Figure 5, \u201cFile striping on a Lustre file system\u201d](#figure-5-file-striping-on-a-lustre-file-system).\n\n##### Figure 5. File striping on a Lustre file system\n\n ![File striping pattern across three OSTs for three different data files. The file is sparse and missing chunk 6.](figures/File_Striping.png)", "mimetype": "text/plain", "start_char_idx": 20947, "end_char_idx": 24422, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a6634ef-a9c0-43d7-bdac-46e915c27c8d": {"__data__": {"id_": "0a6634ef-a9c0-43d7-bdac-46e915c27c8d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "098fb67f-4fff-4e8e-8338-9623ed9f1eb4", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6563c983a6b56d0244116210d63214dfef03937d04549387ed9c68458b2621ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1eb984ae-fec3-48e9-8460-7d505fbb44b8", "node_type": "1", "metadata": {}, "hash": "024b79c6eb6c074386eb5a6b9741bf828b573d03bc59900e21630a9fec262919", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The maximum file size is not limited by the size of a single target. In a Lustre file system, files can be striped across multiple objects (up to 2000), and each object can be up to 16 TiB in size with ldiskfs, or up to 256PiB with ZFS. This leads to a maximum file size of 31.25 PiB for ldiskfs or 8EiB with ZFS. Note that a Lustre file system can support files up to 2^63 bytes (8EiB), limited only by the space available on the OSTs.\n\n**Note**\n\nldiskfs filesystems without the `ea_inode` feature limit the maximum stripe count for a single file to 160 OSTs.\n\nAlthough a single file can only be striped over 2000 objects, Lustre file systems can have thousands of OSTs. The I/O bandwidth to access a single file is the aggregated I/O bandwidth to the objects in a file, which can be as much as a bandwidth of up to 2000 servers. On systems with more than 2000 OSTs, clients can do I/O using multiple files to utilize the full file system bandwidth.\n\nFor more information about striping, see [*Managing File Layout (Striping) and Free Space*](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md).\n\n**Extended Attributes(xattrs)**\n\nLustre uses lov_user_md_v1/lov_user_md_v3 data-structures to maintain its file striping information under xattrs. Extended attributes are created when files and directory are created. Lustre uses `trusted` extended attributes to store its parameters which are root-only accessible. The parameters are:\n\n* **trusted.lov:** Holds layout for a regular file, or default file layout stored on a directory (also accessible as lustre.lov for non-root users). \n* **trusted.lma:** Holds FID and extra state flags for current file \n* **trusted.lmv:** Holds layout for a striped directory (DNE 2), not present otherwise \n* **trusted.link:** Holds parent directory FID + filename for each link to a file (for lfs fid2path)\n\nxattr which are stored and present in the file could be verify using:\n\n```\n# getfattr -d -m - /mnt/testfs/file>\n```\n\n\n\n## Understanding Lustre Networking (LNet)\n\nThis chapter introduces Lustre networking (LNet). It includes the following sections:\n\n- [the section called \u201c Introducing LNet\u201d](#introducing-lnet)\n- [the section called \u201cKey Features of LNet\u201d](#key-features-of-lnet)\n- [the section called \u201cLustre Networks\u201d](#lustre-networks)\n- [the section called \u201cSupported Network Types\u201d](#supported-network-types)\n\n \n\n### Introducing LNet\n\nIn a cluster using one or more Lustre file systems, the network communication infrastructure required by the Lustre file system is implemented using the Lustre networking (LNet) feature.\n\nLNet supports many commonly-used network types, such as InfiniBand and IP networks, and allows simultaneous availability across multiple network types with routing between them. Remote direct memory access (RDMA) is permitted when supported by underlying networks using the appropriate Lustre network driver (LND). High availability and recovery features enable transparent recovery in conjunction with failover servers.\n\nAn LND is a pluggable driver that provides support for a particular network type, for example `ksocklnd` is the driver which implements the TCP Socket LND that supports TCP networks. LNDs are loaded into the driver stack, with one LND for each network type in use.\n\nFor information about configuring LNet, see [*Configuring Lustre Networking (LNet)*](02.06-Configuring%20Lustre%20Networking%20(LNet).md).\n\nFor information about administering LNet, see [Administering Lustre](03-Administering%20Lustre.md).\n\n\n\n### Key Features of LNet\n\nKey features of LNet include:\n\n- RDMA, when supported by underlying networks\n- Support for many commonly-used network types\n- High availability and recovery\n- Support of multiple network types simultaneously\n- Routing among disparate networks\n\nLNet permits end-to-end read/write throughput at or near peak bandwidth rates on a variety of network interconnects.\n\n\n\n### Lustre Networks\n\nA Lustre network is comprised of clients and servers running the Lustre software. It need not be confined to one LNet subnet but can span several networks provided routing is possible between the networks. In a similar manner, a single network can have multiple LNet subnets.", "mimetype": "text/plain", "start_char_idx": 24425, "end_char_idx": 28630, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1eb984ae-fec3-48e9-8460-7d505fbb44b8": {"__data__": {"id_": "1eb984ae-fec3-48e9-8460-7d505fbb44b8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a6634ef-a9c0-43d7-bdac-46e915c27c8d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "64b825126f5e15a8d1dc4c21d898b03d08911eaa7d4fd68fd348c134ec24802d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f61e0631-69fd-49ed-9675-f2c7c6f1aa61", "node_type": "1", "metadata": {}, "hash": "daee354bef8c1987f87ca97837b108241d998c4fa66af23da6163621d8cc1d91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Key Features of LNet\n\nKey features of LNet include:\n\n- RDMA, when supported by underlying networks\n- Support for many commonly-used network types\n- High availability and recovery\n- Support of multiple network types simultaneously\n- Routing among disparate networks\n\nLNet permits end-to-end read/write throughput at or near peak bandwidth rates on a variety of network interconnects.\n\n\n\n### Lustre Networks\n\nA Lustre network is comprised of clients and servers running the Lustre software. It need not be confined to one LNet subnet but can span several networks provided routing is possible between the networks. In a similar manner, a single network can have multiple LNet subnets.\n\nThe Lustre networking stack is comprised of two layers, the LNet code module and the LND. The LNet layer operates above the LND layer in a manner similar to the way the network layer operates above the data link layer. LNet layer is connectionless, asynchronous and does not verify that data has been transmitted while the LND layer is connection oriented and typically does verify data transmission.\n\nLNets are uniquely identified by a label comprised of a string corresponding to an LND and a number, such as tcp0, o2ib0, or o2ib1, that uniquely identifies each LNet. Each node on an LNet has at least one network identifier (NID). A NID is a combination of the address of the network interface and the LNet label in the form:`*address*@*LNet_label*`.\n\nExamples:\n\n```\n192.168.1.2@tcp0\n10.13.24.90@o2ib1\n```\n\nIn certain circumstances it might be desirable for Lustre file system traffic to pass between multiple LNets. This is possible using LNet routing. It is important to realize that LNet routing is not the same as network routing. For more details about LNet routing, see [*Configuring Lustre Networking (LNet)*](02.06-Configuring%20Lustre%20Networking%20(LNet).md).\n\n### Supported Network Types\n\nThe LNet code module includes LNDs to support many network types including:\n\n- InfiniBand: OpenFabrics OFED (o2ib)\n- TCP (any network carrying TCP traffic, including GigE, 10GigE, and IPoIB)\n- RapidArray: ra\n- Quadrics: Elan\n\n## Understanding Failover in a Lustre File System\n\nThis chapter describes failover in a Lustre file system. It includes:\n\n- [the section called \u201c What is Failover?\u201d](#what-is-failover)\n- [the section called \u201c Failover Functionality in a Lustre File System\u201d](#failover-functionality-in-a-lustre-file-system)\n\n### What is Failover?\n\nIn a high-availability (HA) system, unscheduled downtime is minimized by using redundant hardware and software components and software components that automate recovery when a failure occurs. If a failure condition occurs, such as the loss of a server or storage device or a network or software fault, the system's services continue with minimal interruption. Generally, availability is specified as the percentage of time the system is required to be available.\n\nAvailability is accomplished by replicating hardware and/or software so that when a primary server fails or is unavailable, a standby server can be switched into its place to run applications and associated resources. This process, called failover, is automatic in an HA system and, in most cases, completely application-transparent.\n\nA failover hardware setup requires a pair of servers with a shared resource (typically a physical storage device, which may be based on SAN, NAS, hardware RAID, SCSI or Fibre Channel (FC) technology). The method of sharing storage should be essentially transparent at the device level; the same physical logical unit number (LUN) should be visible from both servers. To ensure high availability at the physical storage level, we encourage the use of RAID arrays to protect against drive-level failures.\n\n**Note**\n\nThe Lustre software does not provide redundancy for data; it depends exclusively on redundancy of backing storage devices. The backing OST storage should be RAID 5 or, preferably, RAID 6 storage. MDT storage should be RAID 1 or RAID 10.\n\n#### Failover Capabilities\n\nTo establish a highly-available Lustre file system, power management software or hardware and high availability (HA) software are used to provide the following failover capabilities:\n\n- **Resource fencing**- Protects physical storage from simultaneous access by two nodes.\n- **Resource management**- Starts and stops the Lustre resources as a part of failover, maintains the cluster state, and carries out other resource management tasks.", "mimetype": "text/plain", "start_char_idx": 27944, "end_char_idx": 32410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f61e0631-69fd-49ed-9675-f2c7c6f1aa61": {"__data__": {"id_": "f61e0631-69fd-49ed-9675-f2c7c6f1aa61", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1eb984ae-fec3-48e9-8460-7d505fbb44b8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a5fa884d0a44262da38fd2fd5f2ddc55e9c7dd48557256797d2fac621513f40c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a62f12c-182e-48b8-b6e4-9bc24101a989", "node_type": "1", "metadata": {}, "hash": "43acd14a8a0bce8d47a08b2e281f9f95a9b7bafd07e171c32096bf7645783a10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To ensure high availability at the physical storage level, we encourage the use of RAID arrays to protect against drive-level failures.\n\n**Note**\n\nThe Lustre software does not provide redundancy for data; it depends exclusively on redundancy of backing storage devices. The backing OST storage should be RAID 5 or, preferably, RAID 6 storage. MDT storage should be RAID 1 or RAID 10.\n\n#### Failover Capabilities\n\nTo establish a highly-available Lustre file system, power management software or hardware and high availability (HA) software are used to provide the following failover capabilities:\n\n- **Resource fencing**- Protects physical storage from simultaneous access by two nodes.\n- **Resource management**- Starts and stops the Lustre resources as a part of failover, maintains the cluster state, and carries out other resource management tasks.\n- **Health monitoring**- Verifies the availability of hardware and network resources and responds to health indications provided by the Lustre software.\n\nThese capabilities can be provided by a variety of software and/or hardware solutions. For more information about using power management software or hardware and high availability (HA) software with a Lustre file system, see [*Configuring Failover in a Lustre File System*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md).\n\nHA software is responsible for detecting failure of the primary Lustre server node and controlling the failover.The Lustre software works with any HA software that includes resource (I/O) fencing. For proper resource fencing, the HA software must be able to completely power off the failed server or disconnect it from the shared storage device. If two active nodes have access to the same storage device, data may be severely corrupted.\n\n\n\n#### Types of Failover Configurations\n\nNodes in a cluster can be configured for failover in several ways. They are often configured in pairs (for example, two OSTs attached to a shared storage device), but other failover configurations are also possible. Failover configurations include:\n\n- **Active/passive** pair - In this configuration, the active node provides resources and serves data, while the passive node is usually standing by idle. If the active node fails, the passive node takes over and becomes active.\n- **Active/active** pair - In this configuration, both nodes are active, each providing a subset of resources. In case of a failure, the second node takes over resources from the failed node.\n\nIf there is a single MDT in a filesystem, two MDSes can be configured as an active/passive pair, while pairs of OSSes can be deployed in an active/active configuration that improves OST availability without extra overhead. Often the standby MDS is the active MDS for another Lustre file system or the MGS, so no nodes are idle in the cluster. If there are multiple MDTs in a filesystem, active-active failover configurations are available for MDSs that serve MDTs on shared storage.", "mimetype": "text/plain", "start_char_idx": 31559, "end_char_idx": 34542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a62f12c-182e-48b8-b6e4-9bc24101a989": {"__data__": {"id_": "3a62f12c-182e-48b8-b6e4-9bc24101a989", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f61e0631-69fd-49ed-9675-f2c7c6f1aa61", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2869e02b69ac8fbbfca157d60a5b1b3f12f516aaf34400da7df114f71eb625b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e72d7a24-b2fb-4a3c-a4d0-7ad61ba79564", "node_type": "1", "metadata": {}, "hash": "f61b5a2e37e78e62dd9c58c43274bd077ae838f5b9cb7bbb3d61fb0a9d219a91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Failover Functionality in a Lustre File System\n\n- [MDT Failover Configuration (Active/Passive)](#mdt-failover-configuration-activepassive)\n- [MDT Failover Configuration (Active/Active)](#mdt-failover-configuration-activeactive)L 2.4\n- [OST Failover Configuration (Active/Active)](#ost-failover-configuration-activeactive)\n\nThe failover functionality provided by the Lustre software can be used for the following failover scenario. When a client attempts to do I/O to a failed Lustre target, it continues to try until it receives an answer from any of the configured failover nodes for the Lustre target. A user-space application does not detect anything unusual, except that the I/O may take longer to complete.\n\nFailover in a Lustre file system requires that two nodes be configured as a failover pair, which must share one or more storage devices. A Lustre file system can be configured to provide MDT or OST failover.\n\n- For MDT failover, two MDSs can be configured to serve the same MDT. Only one MDS node can serve an MDT at a time.By placing two or more MDT devices on storage shared by two MDSs, one MDS can fail and the remaining MDS can begin serving the unserved MDT. This is described as an active/active failover pair.\n\n- For OST failover, multiple OSS nodes can be configured to be able to serve the same OST. However, only one OSS node can serve the OST at a time. An OST can be moved between OSS nodes that have access to the same storage device using `umount/mount` commands.\n\nThe `--servicenode` option is used to set up nodes in a Lustre file system for failover at creation time (using`mkfs.lustre`) or later when the Lustre file system is active (using `tunefs.lustre`). For explanations of these utilities, see [the section called \u201c mkfs.lustre\u201d](06.07-System%20Configuration%20Utilities.md#mkfslustre)and [the section called \u201c tunefs.lustre\u201d](06.07-System%20Configuration%20Utilities.md#tunefslustre).\n\nFailover capability in a Lustre file system can be used to upgrade the Lustre software between successive minor versions without cluster downtime. For more information, see [*Upgrading a Lustre File System*](03.06-Upgrading%20a%20Lustre%20File%20System.md).\n\nFor information about configuring failover, see [*Configuring Failover in a Lustre File System*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md).\n\n**Note**\n\nThe Lustre software provides failover functionality only at the file system level. In a complete failover solution, failover functionality for system-level components, such as node failure detection or power control, must be provided by a third-party tool.\n\n**Caution**\n\nOST failover functionality does not protect against corruption caused by a disk failure. If the storage media (i.e., physical disk) used for an OST fails, it cannot be recovered by functionality provided in the Lustre software. We strongly recommend that some form of RAID be used for OSTs. Lustre functionality assumes that the storage is reliable, so it adds no extra reliability features.", "mimetype": "text/plain", "start_char_idx": 34546, "end_char_idx": 37572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e72d7a24-b2fb-4a3c-a4d0-7ad61ba79564": {"__data__": {"id_": "e72d7a24-b2fb-4a3c-a4d0-7ad61ba79564", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a65ed932-3b3c-455b-b428-909f95d3dbc2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db211a53d8cb61027a41236cdfca3f25fa082af0a948f4773595b2b930d6dd88", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a62f12c-182e-48b8-b6e4-9bc24101a989", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "699d5929c693b0e0bf49c3b2ea8111f5e06d2c7543945e71cdcb7cb0eee537e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### MDT Failover Configuration (Active/Passive)\n\nTwo MDSs are typically configured as an active/passive failover pair as shown in [Figure 6, \u201cLustre failover configuration for a active/passive MDT\u201d](#figure-6-lustre-failover-configuration-for-a-active-passive-mdt). Note that both nodes must have access to shared storage for the MDT(s) and the MGS. The primary (active) MDS manages the Lustre system metadata resources. If the primary MDS fails, the secondary (passive) MDS takes over these resources and serves the MDTs and the MGS.\n\n**Note**\n\nIn an environment with multiple file systems, the MDSs can be configured in a quasi active/active configuration, with each MDS managing metadata for a subset of the Lustre file system.\n\n##### Figure 6. Lustre failover configuration for a active/passive MDT\n\n![Lustre failover configuration for an MDT](figures/MDT_Failover.png)\n\nIntroduced in Lustre 2.4\n\n#### MDT Failover Configuration (Active/Active)\n\nMDTs can be configured as an active/active failover configuration. A failover cluster is built from two MDSs as shown in\u00a0[Figure\u00a07, \u201cLustre failover configuration for a active/active MDTs\u201d](#figure-7-lustre-failover-configuration-for-a-active-active-mdts).\n\n##### Figure\u00a07.\u00a0Lustre failover configuration for a active/active MDTs\n\n![Lustre failover configuration for two MDTs](figures/MDTs_Failover.png) \n\n#### OST Failover Configuration (Active/Active)\n\nOSTs are usually configured in a load-balanced, active/active failover configuration. A failover cluster is built from two OSSs as shown in [Figure 8, \u201cLustre failover configuration for an OSTs\u201d](#figure-8-lustre-failover-configuration-for-an-osts).\n\n**Note**\n\nOSSs configured as a failover pair must have shared disks/RAID.\n\n##### Figure 8. Lustre failover configuration for an OSTs\n\n ![Lustre failover configuration for an OSTs](figures/OST_Failover.png)\n\n \n\nIn an active configuration, 50% of the available OSTs are assigned to one OSS and the remaining OSTs are assigned to the other OSS. Each OSS serves as the primary node for half the OSTs and as a failover node for the remaining OSTs.\n\nIn this mode, if one OSS fails, the other OSS takes over all of the failed OSTs. The clients attempt to connect to each OSS serving the OST, until one of them responds. Data on the OST is written synchronously, and the clients replay transactions that were in progress and uncommitted to disk before the OST failure.\n\nFor more information about configuring failover, see [*Configuring Failover in a Lustre File System*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md).", "mimetype": "text/plain", "start_char_idx": 37576, "end_char_idx": 40163, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60e316ed-db9f-4fbb-b44c-fb21d7823af9": {"__data__": {"id_": "60e316ed-db9f-4fbb-b44c-fb21d7823af9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/04-Installation Overview.md", "file_name": "04-Installation Overview.md", "file_type": "text/markdown", "file_size": 2886, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47977575-fffa-4357-a1c0-cff50282b3fe", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/04-Installation Overview.md", "file_name": "04-Installation Overview.md", "file_type": "text/markdown", "file_size": 2886, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1fe4c21fcc0fff5e09fb4ad22a903f330ebb1d697b96b7a51cbdb1961376626d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Installation Overview\n\n**Table of Contents**\n\n- [Installation Overview](#installation-overview)\n    * [Steps to Installing the Lustre Software](#steps-to-installing-the-lustre-software)\n\nThis chapter provides on overview of the procedures required to set up, install and configure a Lustre file system.\n\n### Note\n\nIf the Lustre file system is new to you, you may find it helpful to refer to [Introducing the Lustre* File System](02-Introducing%20the%20Lustre%20File%20System.md) for a description of the Lustre architecture, file system components and terminology before proceeding with the installation procedure.\n\n## Steps to Installing the Lustre Software\n\nTo set up Lustre file system hardware and install and configure the Lustre software, refer the the chapters below in the order listed:\n\n1. *(Required)* **Set up your Lustre file system hardware.**\n\n   See [*Determining Hardware Configuration Requirements and Formatting Options*](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md) - Provides guidelines for configuring hardware for a Lustre file system including storage, memory, and networking requirements.\n\n2. *(Optional - Highly Recommended)* **Configure storage on Lustre storage devices.**\n\n   See [*Configuring Storage on a Lustre File System*](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md) - Provides instructions for setting up hardware RAID on Lustre storage devices.\n\n3. *(Optional)* **Set up network interface bonding.**\n\n   See [*Setting Up Network Interface Bonding*](02.04-Setting%20Up%20Network%20Interface%20Bonding.md) - Describes setting up network interface bonding to allow multiple network interfaces to be used in parallel to increase bandwidth or redundancy.\n\n4. *(Required)* **Install Lustre software.**\n\n   See [*Installing the Lustre Software*](02.05-Installing%20the%20Lustre%20Software.md) - Describes preparation steps and a procedure for installing the Lustre software.\n\n5. *(Optional)* **Configure Lustre Networking (LNet).**\n\n   See [*Configuring Lustre Networking (LNet)*](02.06-Configuring%20Lustre%20Networking%20(LNet).md) - Describes how to configure LNet if the default configuration is not sufficient. By default, LNet will use the first TCP/IP interface it discovers on a system. LNet configuration is required if you are using InfiniBand or multiple Ethernet interfaces.\n\n6. *(Required)* **Configure the Lustre file system.**\n\n   See [*Configuring a Lustre File System*](02.07-Configuring%20a%20Lustre%20File%20System.md) - Provides an example of a simple Lustre configuration procedure and points to tools for completing more complex configurations.\n\n7. *(Optional)* **Configure Lustre failover.**\n\n   See [*Configuring Failover in a Lustre File System*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md) - Describes how to configure Lustre failover.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f8561b6-a25c-4035-aeb6-b7263285504b": {"__data__": {"id_": "8f8561b6-a25c-4035-aeb6-b7263285504b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6882cf8-971f-4033-b3e2-b60f093dd580", "node_type": "1", "metadata": {}, "hash": "3b48b7406d65a51c01530f00dee24cd6d32f5c11562b7bc91849c65130b775c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Determining Hardware Configuration Requirements and Formatting Options\n\n**Table of Contents**\n\n- [Determining Hardware Configuration Requirements and Formatting Options](#determining-hardware-configuration-requirements-and-formatting-options)\n  * [Hardware Considerations](#hardware-considerations)\n    + [MGT and MDT Storage Hardware Considerations](#mgt-and-mdt-storage-hardware-considerations)\n    + [OST Storage Hardware Considerations](#ost-storage-hardware-considerations)\n  * [Determining Space Requirements](#determining-space-requirements)\n    + [Determining MGT Space Requirements](#determining-mgt-space-requirements)\n    + [Determining MDT Space Requirements](#determining-mdt-space-requirements)\n    + [Determining OST Space Requirements](#determining-ost-space-requirements)\n  * [Setting ldiskfs File System Formatting Options](#setting-ldiskfs-file-system-formatting-options)\n    + [Setting Formatting Options for an ldiskfs MDT](#setting-formatting-options-for-an-ldiskfs-mdt)\n    + [Setting Formatting Options for an ldiskfs OST](#setting-formatting-options-for-an-ldiskfs-ost)\n  * [File and File System Limits](#file-and-file-system-limits)\n  * [Determining Memory Requirements](#determining-memory-requirements)\n    + [Client Memory Requirements](#client-memory-requirements)\n    + [MDS Memory Requirements](#mds-memory-requirements)\n      - [Calculating MDS Memory Requirements](#calculating-mds-memory-requirements)\n    + [OSS Memory Requirements](#oss-memory-requirements)\n      - [Calculating OSS Memory Requirements](#calculating-oss-memory-requirements)\n  * [Implementing Networks To Be Used by the Lustre File System](#implementing-networks-to-be-used-by-the-lustre-file-system)\n  \n  \n\nThis chapter describes hardware configuration requirements for a Lustre file system including:\n\n- [the section called \u201c Hardware Considerations\u201d](#hardware-considerations)\n- [the section called \u201c Determining Space Requirements\u201d](#determining-space-requirements)\n- [the section called \u201c Setting ldiskfs File System Formatting Options \u201d](#setting-ldiskfs-file-system-formatting-options)\n- [the section called \u201cDetermining Memory Requirements\u201d](#determining-memory-requirements)\n- [the section called \u201cImplementing Networks To Be Used by the Lustre File System\u201d](#implementing-networks-to-be-used-by-the-lustre-file-system)\n\n## Hardware Considerations\n\n- [MGT and MDT Storage Hardware Considerations](#mgt-and-mdt-storage-hardware-considerations)\n- [OST Storage Hardware Considerations](#ost-storage-hardware-considerations)\n\nA Lustre file system can utilize any kind of block storage device such as single disks, software RAID, hardware RAID, or a logical volume manager. In contrast to some networked file systems, the block devices are only attached to the MDS and OSS nodes in a Lustre file system and are not accessed by the clients directly.\n\nSince the block devices are accessed by only one or two server nodes, a storage area network (SAN) that is accessible from all the servers is not required. Expensive switches are not needed because point-to-point connections between the servers and the storage arrays normally provide the simplest and best attachments. (If failover capability is desired, the storage must be attached to multiple servers.)\n\nFor a production environment, it is preferable that the MGS have separate storage to allow future expansion to multiple file systems. However, it is possible to run the MDS and MGS on the same machine and have them share the same storage device.\n\nFor best performance in a production environment, dedicated clients are required. For a non-production Lustre environment or for testing, a Lustre client and server can run on the same machine. However, dedicated clients are the only supported configuration.\n\n### Warning\n\nPerformance and recovery issues can occur if you put a client on an MDS or OSS:\n\n- Running the OSS and a client on the same machine can cause issues with low memory and memory pressure. If the client consumes all the memory and then tries to write data to the file system, the OSS will need to allocate pages to receive data from the client but will not be able to perform this operation due to low memory. This can cause the client to hang.\n- Running the MDS and a client on the same machine can cause recovery and deadlock issues and impact the performance of other Lustre clients.\n\nOnly servers running on 64-bit CPUs are tested and supported.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6882cf8-971f-4033-b3e2-b60f093dd580": {"__data__": {"id_": "e6882cf8-971f-4033-b3e2-b60f093dd580", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f8561b6-a25c-4035-aeb6-b7263285504b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e801ea4e0b7547d5fd4b18c2eb8512de5c36b640fe41961d8aa0e9dcad215f3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e5c261f-01e6-4679-a9ba-020d587fe252", "node_type": "1", "metadata": {}, "hash": "f99b5e47d1ec8040de0c673eff8f718c6c2c3d5a83114c3ef56b822787601853", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For best performance in a production environment, dedicated clients are required. For a non-production Lustre environment or for testing, a Lustre client and server can run on the same machine. However, dedicated clients are the only supported configuration.\n\n### Warning\n\nPerformance and recovery issues can occur if you put a client on an MDS or OSS:\n\n- Running the OSS and a client on the same machine can cause issues with low memory and memory pressure. If the client consumes all the memory and then tries to write data to the file system, the OSS will need to allocate pages to receive data from the client but will not be able to perform this operation due to low memory. This can cause the client to hang.\n- Running the MDS and a client on the same machine can cause recovery and deadlock issues and impact the performance of other Lustre clients.\n\nOnly servers running on 64-bit CPUs are tested and supported. 64-bit CPU clients are typically used for testing to match expected customer usage and avoid limitations due to the 4 GB limit for RAM size, 1 GB low-memory limitation, and 16 TB file size limit of 32-bit CPUs. Also, due to kernel API limitations, performing backups of Lustre filesystems on 32-bit clients may cause backup tools to confuse files that report the same 32-bit inode number, if the backup tools depend on the inode number for correct operation.\n\nThe storage attached to the servers typically uses RAID to provide fault tolerance and can optionally be organized with logical volume management (LVM), which is then formatted as a Lustre file system. Lustre OSS and MDS servers read, write and modify data in the format imposed by the file system.\n\nThe Lustre file system uses journaling file system technology on both the MDTs and OSTs. For a MDT, as much as a 20 percent performance gain can be obtained by placing the journal on a separate device.\n\nThe MDS can effectively utilize a lot of CPU cycles. A minimum of four processor cores are recommended. More are advisable for files systems with many clients.\n\n### Note\n\nLustre clients running on different CPU architectures is supported. One limitation is that the PAGE_SIZE kernel macro on the client must be as large as the PAGE_SIZE of the server. In particular, ARM or PPC clients with large pages (up to 64kB pages) can run with x86 servers (4kB pages).\n\n### MGT and MDT Storage Hardware Considerations\n\nMGT storage requirements are small (less than 100 MB even in the largest Lustre file systems), and the data on an MGT is only accessed on a server/client mount, so disk performance is not a consideration. However, this data is vital for file system access, so the MGT should be reliable storage, preferably mirrored RAID1.\n\nMDS storage is accessed in a database-like access pattern with many seeks and read-and-writes of small amounts of data. Storage types that provide much lower seek times, such as SSD or NVMe is strongly preferred for the MDT, and high-RPM SAS is acceptable.\n\nFor maximum performance, the MDT should be configured as RAID1 with an internal journal and two disks from different controllers.\n\nIf you need a larger MDT, create multiple RAID1 devices from pairs of disks, and then make a RAID0 array of the RAID1 devices. For ZFS, use `mirror` VDEVs for the MDT. This ensures maximum reliability because multiple disk failures only have a small chance of hitting both disks in the same RAID1 device.\n\nDoing the opposite (RAID1 of a pair of RAID0 devices) has a 50% chance that even two disk failures can cause the loss of the whole MDT device. The first failure disables an entire half of the mirror and the second failure has a 50% chance of disabling the remaining mirror.\n\nIntroduced in Lustre 2.4If multiple MDTs are going to be present in the system, each MDT should be specified for the anticipated usage and load. For details on how to add additional MDTs to the filesystem, see [the section called \u201cAdding a New MDT to a Lustre File System\u201d](03.03-Lustre%20Maintenance.md#adding-a-new-mdt-to-a-lustre-file-system).\n\nIntroduced in Lustre 2.4\n\n### Warning\n\nMDT0000 contains the root of the Lustre file system. If MDT0000 is unavailable for any reason, the file system cannot be used.\n\n### Note\n\nUsing the DNE feature it is possible to dedicate additional MDTs to sub-directories off the file system root directory stored on MDT0000, or arbitrarily for lower-level subdirectories.", "mimetype": "text/plain", "start_char_idx": 3515, "end_char_idx": 7911, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e5c261f-01e6-4679-a9ba-020d587fe252": {"__data__": {"id_": "1e5c261f-01e6-4679-a9ba-020d587fe252", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6882cf8-971f-4033-b3e2-b60f093dd580", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cace11b4e64d295fe7fa6cd429cbf41895defa2b45da84476bb97be3eda63143", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "997875b0-8b68-4eb2-b1d0-a77464f21cf6", "node_type": "1", "metadata": {}, "hash": "267b7cb6d45e87dcc728905071a7e217f7df6decb62cf9868b471d25cae1e221", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in Lustre 2.4If multiple MDTs are going to be present in the system, each MDT should be specified for the anticipated usage and load. For details on how to add additional MDTs to the filesystem, see [the section called \u201cAdding a New MDT to a Lustre File System\u201d](03.03-Lustre%20Maintenance.md#adding-a-new-mdt-to-a-lustre-file-system).\n\nIntroduced in Lustre 2.4\n\n### Warning\n\nMDT0000 contains the root of the Lustre file system. If MDT0000 is unavailable for any reason, the file system cannot be used.\n\n### Note\n\nUsing the DNE feature it is possible to dedicate additional MDTs to sub-directories off the file system root directory stored on MDT0000, or arbitrarily for lower-level subdirectories. using the `lfs mkdir -i mdt_index` command. If an MDT serving a subdirectory becomes unavailable, any subdirectories on that MDT and all directories beneath it will also become inaccessible. This is typically useful for top-level directories to assign different users or projects to separate MDTs, or to distribute other large working sets of files to multiple MDTs.\n\nIntroduced in Lustre 2.8\n\n### Note\n\nStarting in the 2.8 release it is possible to spread a single large directory across multiple MDTs using the DNE striped directory feature by specifying multiple stripes (or shards) at creation time using the `lfs mkdir -c stripe_count` command, where *stripe_count* is often the number of MDTs in the filesystem. Striped directories should typically not be used for all directories in the filesystem, since this incurs extra overhead compared to non-striped directories, but is useful for larger directories (over 50k entries) where many output files are being created at one time.\n\n### OST Storage Hardware Considerations\n\nThe data access pattern for the OSS storage is a streaming I/O pattern that is dependent on the access patterns of applications being used. Each OSS can manage multiple object storage targets (OSTs), one for each volume with I/O traffic load-balanced between servers and targets. An OSS should be configured to have a balance between the network bandwidth and the attached storage bandwidth to prevent bottlenecks in the I/O path. Depending on the server hardware, an OSS typically serves between 2 and 8 targets, with each target between 24-48TB, but may be up to 256 terabytes (TBs) in size.\n\nLustre file system capacity is the sum of the capacities provided by the targets. For example, 64 OSSs, each with two 8 TB OSTs, provide a file system with a capacity of nearly 1 PB. If each OST uses ten 1 TB SATA disks (8 data disks plus 2 parity disks in a RAID-6 configuration), it may be possible to get 50 MB/sec from each drive, providing up to 400 MB/sec of disk bandwidth per OST. If this system is used as storage backend with a system network, such as the InfiniBand network, that provides a similar bandwidth, then each OSS could provide 800 MB/sec of end-to-end I/O throughput. (Although the architectural constraints described here are simple, in practice it takes careful hardware selection, benchmarking and integration to obtain such results.)\n\n## Determining Space Requirements\n\nThe desired performance characteristics of the backing file systems on the MDT and OSTs are independent of one another. The size of the MDT backing file system depends on the number of inodes needed in the total Lustre file system, while the aggregate OST space depends on the total amount of data stored on the file system. If MGS data is to be stored on the MDT device (co-located MGT and MDT), add 100 MB to the required size estimate for the MDT.\n\nEach time a file is created on a Lustre file system, it consumes one inode on the MDT and one OST object over which the file is striped. Normally, each file's stripe count is based on the system-wide default stripe count. However, this can be changed for individual files using the `lfs setstripe` option. For more details, see [*Managing File Layout (Striping) and Free Space*](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md).\n\nIn a Lustre ldiskfs file system, all the MDT inodes and OST objects are allocated when the file system is first formatted.", "mimetype": "text/plain", "start_char_idx": 7202, "end_char_idx": 11356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "997875b0-8b68-4eb2-b1d0-a77464f21cf6": {"__data__": {"id_": "997875b0-8b68-4eb2-b1d0-a77464f21cf6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e5c261f-01e6-4679-a9ba-020d587fe252", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "182c13b154976d0da4e5f28096e0e9120689db430f6762d8637c2e69fec7acbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd95e015-3f83-4b82-8cdf-06160241bc94", "node_type": "1", "metadata": {}, "hash": "475d99184ac602e5419df7ada566d1c3ef1821fd3c67e5863e91d10fecd65839", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If MGS data is to be stored on the MDT device (co-located MGT and MDT), add 100 MB to the required size estimate for the MDT.\n\nEach time a file is created on a Lustre file system, it consumes one inode on the MDT and one OST object over which the file is striped. Normally, each file's stripe count is based on the system-wide default stripe count. However, this can be changed for individual files using the `lfs setstripe` option. For more details, see [*Managing File Layout (Striping) and Free Space*](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md).\n\nIn a Lustre ldiskfs file system, all the MDT inodes and OST objects are allocated when the file system is first formatted. When the file system is in use and a file is created, metadata associated with that file is stored in one of the pre-allocated inodes and does not consume any of the free space used to store file data. The total number of inodes on a formatted ldiskfs MDT or OST cannot be easily changed. Thus, the number of inodes created at format time should be generous enough to anticipate near term expected usage, with some room for growth without the effort of additional storage.\n\nBy default, the ldiskfs file system used by Lustre servers to store user-data objects and system data reserves 5% of space that cannot be used by the Lustre file system. Additionally, an ldiskfs Lustre file system reserves up to 400 MB on each OST, and up to 4GB on each MDT for journal use and a small amount of space outside the journal to store accounting data. This reserved space is unusable for general storage. Thus, at least this much space will be used per OST before any file object data is saved.\n\nIntroduced in Lustre 2.4\n\nWith a ZFS backing filesystem for the MDT or OST, the space allocation for inodes and file data is dynamic, and inodes are allocated as needed. A minimum of 4kB of usable space (before mirroring) is needed for each inode, exclusive of other overhead such as directories, internal log files, extended attributes, ACLs, etc. ZFS also reserves approximately 3% of the total storage space for internal and redundant metadata, which is not usable by Lustre. Since the size of extended attributes and ACLs is highly dependent on kernel versions and site-specific policies, it is best to over-estimate the amount of space needed for the desired number of inodes, and any excess space will be utilized to store more inodes.\n\n### Determining MGT Space Requirements\n\nLess than 100 MB of space is typically required for the MGT. The size is determined by the total number of servers in the Lustre file system cluster(s) that are managed by the MGS.\n\n### Determining MDT Space Requirements\n\nWhen calculating the MDT size, the important factor to consider is the number of files to be stored in the file system, which depends on at least 2 KiB per inode of usable space on the MDT. Since MDTs typically use RAID-1+0 mirroring, the total storage needed will be double this.\n\nPlease note that the actual used space per MDT depends on the number of files per directory, the number of stripes per file, whether files have ACLs or user xattrs, and the number of hard links per file. The storage required for Lustre file system metadata is typically 1-2 percent of the total file system capacity depending upon file size. If the [*Data on MDT (DoM)*](03.09-Data%20on%20MDT%20(DoM).md) feature is in use for Lustre 2.11 or later, MDT space should typically be 5 percent or more of the total space, depending on the distribution of small files within the filesystem and the `lod.*.dom_stripesize` limit on the MDT and file layout used.\n\nFor ZFS-based MDT filesystems, the number of inodes created on the MDT and OST is dynamic, so there is less need to determine the number of inodes in advance, though there still needs to be some thought given to the total MDT space compared to the total filesystem size.", "mimetype": "text/plain", "start_char_idx": 10657, "end_char_idx": 14552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd95e015-3f83-4b82-8cdf-06160241bc94": {"__data__": {"id_": "bd95e015-3f83-4b82-8cdf-06160241bc94", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "997875b0-8b68-4eb2-b1d0-a77464f21cf6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db2c856a28fa9ef0949db98ebf1430e1569fc48239c56baf1fc9a2241a5ff825", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56f9f000-2de4-4757-bbbb-36f169265b26", "node_type": "1", "metadata": {}, "hash": "11c4d99a1c93277a6a5b3ef1a6b96933a3f1df68d6fe631f87f749e145345a27", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The storage required for Lustre file system metadata is typically 1-2 percent of the total file system capacity depending upon file size. If the [*Data on MDT (DoM)*](03.09-Data%20on%20MDT%20(DoM).md) feature is in use for Lustre 2.11 or later, MDT space should typically be 5 percent or more of the total space, depending on the distribution of small files within the filesystem and the `lod.*.dom_stripesize` limit on the MDT and file layout used.\n\nFor ZFS-based MDT filesystems, the number of inodes created on the MDT and OST is dynamic, so there is less need to determine the number of inodes in advance, though there still needs to be some thought given to the total MDT space compared to the total filesystem size.\n\nFor example, if the average file size is 5 MiB and you have 100 TiB of usable OST space, then you can calculate the *minimum*total number of inodes for MDTs and OSTs as follows:\n\n(500 TB * 1000000 MB/TB) / 5 MB/inode = 100M inodes\n\nIt is recommended that the MDT(s) have at least twice the minimum number of inodes to allow for future expansion and allow for an average file size smaller than expected. Thus, the minimum space for ldiskfs MDT(s) should be approximately:\n\n2 KiB/inode x 100 million inodes x 2 = 400 GiB ldiskfs MDT\n\nFor details about formatting options for ldiskfs MDT and OST file systems, see [the section called \u201cSetting Formatting Options for an ldiskfs MDT\u201d](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#setting-formatting-options-for-an-ldiskfs-mdt).\n\n### Note\n\nIf the median file size is very small, 4 KB for example, the MDT would use as much space for each file as the space used on the OST, so the use of Data-on-MDT is strongly recommended in that case. The MDT space per inode should be increased correspondingly to account for the extra data space usage for each inode:\n\n6 KiB/inode x 100 million inodes x 2 = 1200 GiB ldiskfs MDT\n\n### Note\n\nIf the MDT has too few inodes, this can cause the space on the OSTs to be inaccessible since no new files can be created. In this case, the `lfs df -i`and `df -i` commands will limit the number of available inodes reported for the filesystem to match the total number of available objects on the OSTs. Be sure to determine the appropriate MDT size needed to support the filesystem before formatting. It is possible to increase the number of inodes after the file system is formatted, depending on the storage. For ldiskfs MDT filesystems the `resize2fs` tool can be used if the underlying block device is on a LVM logical volume and the underlying logical volume size can be increased. For ZFS new (mirrored) VDEVs can be added to the MDT pool to increase the total space available for inode storage. Inodes will be added approximately in proportion to space added.\n\nIntroduced in Lustre 2.4\n\n### Note\n\nNote that the number of total and free inodes reported by `lfs df -i` for ZFS MDTs and OSTs is estimated based on the current average space used per inode. When a ZFS filesystem is first formatted, this free inode estimate will be very conservative (low) due to the high ratio of directories to regular files created for internal Lustre metadata storage, but this estimate will improve as more files are created by regular users and the average file size will better reflect actual site usage.\n\nIntroduced in Lustre 2.4\n\n### Note\n\nUsing the DNE remote directory feature it is possible to increase the total number of inodes of a Lustre filesystem, as well as increasing the aggregate metadata performance, by configuring additional MDTs into the filesystem, see[the section called \u201cAdding a New MDT to a Lustre File System\u201d](03.03-Lustre%20Maintenance.md#adding-a-new-mdt-to-a-lustre-file-system) for details.\n\n### Determining OST Space Requirements\n\nFor the OST, the amount of space taken by each object depends on the usage pattern of the users/applications running on the system.", "mimetype": "text/plain", "start_char_idx": 13831, "end_char_idx": 17757, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56f9f000-2de4-4757-bbbb-36f169265b26": {"__data__": {"id_": "56f9f000-2de4-4757-bbbb-36f169265b26", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd95e015-3f83-4b82-8cdf-06160241bc94", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a1fbd2de6d6232bbcc554861f85a7dd456c46d0b0346b2083299b5d693f6916b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b805162-992c-4dbb-a3e5-cf7f71c3236e", "node_type": "1", "metadata": {}, "hash": "47bb8de0a9e48e9a7a6622caf83547f02b8d36b1e61570976de0bee7b848f7af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When a ZFS filesystem is first formatted, this free inode estimate will be very conservative (low) due to the high ratio of directories to regular files created for internal Lustre metadata storage, but this estimate will improve as more files are created by regular users and the average file size will better reflect actual site usage.\n\nIntroduced in Lustre 2.4\n\n### Note\n\nUsing the DNE remote directory feature it is possible to increase the total number of inodes of a Lustre filesystem, as well as increasing the aggregate metadata performance, by configuring additional MDTs into the filesystem, see[the section called \u201cAdding a New MDT to a Lustre File System\u201d](03.03-Lustre%20Maintenance.md#adding-a-new-mdt-to-a-lustre-file-system) for details.\n\n### Determining OST Space Requirements\n\nFor the OST, the amount of space taken by each object depends on the usage pattern of the users/applications running on the system. The Lustre software defaults to a conservative estimate for the average object size (between 64 KiB per object for 10 GiB OSTs, and 1 MiB per object for 16 TiB and larger OSTs). If you are confident that the average file size for your applications will be different than this, you can specify a different average file size (number of total inodes for a given OST size) to reduce file system overhead and minimize file system check time. See [the section called \u201cSetting Formatting Options for an ldiskfs OST\u201d](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#setting-formatting-options-for-an-ldiskfs-ost) for more details.\n\n## Setting ldiskfs File System Formatting Options\n\nBy default, the `mkfs.lustre` utility applies these options to the Lustre backing file system used to store data and metadata in order to enhance Lustre file system performance and scalability. These options include:\n\n- `flex_bg` - When the flag is set to enable this flexible-block-groups feature, block and inode bitmaps for multiple groups are aggregated to minimize seeking when bitmaps are read or written and to reduce read/modify/write operations on typical RAID storage (with 1 MiB RAID stripe widths). This flag is enabled on both OST and MDT file systems. On MDT file systems the `flex_bg` factor is left at the default value of 16. On OSTs, the `flex_bg` factor is set to 256 to allow all of the block or inode bitmaps in a single `flex_bg` to be read or written in a single 1MiB I/O typical for RAID storage.\n- `huge_file` - Setting this flag allows files on OSTs to be larger than 2 TiB in size.\n- `lazy_journal_init` - This extended option is enabled to prevent a full overwrite to zero out the large journal that is allocated by default in a Lustre file system (up to 400 MiB for OSTs, up to 4GiB for MDTs), to reduce the formatting time.\n\nTo override the default formatting options, use arguments to`mkfs.lustre` to pass formatting options to the backing file system:\n\n```\n--mkfsoptions='backing fs options'\n```\n\nFor other `mkfs.lustre` options, see the Linux man page for`mke2fs(8)`.\n\n### Setting Formatting Options for an ldiskfs MDT\n\nThe number of inodes on the MDT is determined at format time based on the total size of the file system to be created. The default bytes-per-inode ratio (\"inode ratio\") for an ldiskfs MDT is optimized at one inode for every 2560 bytes of file system space.\n\nThis setting takes into account the space needed for additional ldiskfs filesystem-wide metadata, such as the journal (up to 4 GB), bitmaps, and directories, as well as files that Lustre uses internally to maintain cluster consistency. There is additional per-file metadata such as file layout for files with a large number of stripes, Access Control Lists (ACLs), and user extended attributes.\n\nStarting in Lustre 2.11, the [*Data on MDT (DoM)*](03.09-Data%20on%20MDT%20(DoM).md)  (DoM) feature allows storing small files on the MDT to take advantage of high-performance flash storage, as well as reduce space and network overhead. If you are planning to use the DoM feature with an ldiskfs MDT, it is recommended to increase the bytes-per-inode ratio to have enough space on the MDT for small files, as described below.", "mimetype": "text/plain", "start_char_idx": 16831, "end_char_idx": 21009, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b805162-992c-4dbb-a3e5-cf7f71c3236e": {"__data__": {"id_": "0b805162-992c-4dbb-a3e5-cf7f71c3236e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56f9f000-2de4-4757-bbbb-36f169265b26", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e260762fbc526defcf801c79f1082b2bddb70085e86a665825605a2e10b69d72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa4c1ab0-c77f-4351-a209-42bc80883a18", "node_type": "1", "metadata": {}, "hash": "e61f94321062d5ded8ee6d9dd866285199778a53f31f37b02857255ad63805b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This setting takes into account the space needed for additional ldiskfs filesystem-wide metadata, such as the journal (up to 4 GB), bitmaps, and directories, as well as files that Lustre uses internally to maintain cluster consistency. There is additional per-file metadata such as file layout for files with a large number of stripes, Access Control Lists (ACLs), and user extended attributes.\n\nStarting in Lustre 2.11, the [*Data on MDT (DoM)*](03.09-Data%20on%20MDT%20(DoM).md)  (DoM) feature allows storing small files on the MDT to take advantage of high-performance flash storage, as well as reduce space and network overhead. If you are planning to use the DoM feature with an ldiskfs MDT, it is recommended to increase the bytes-per-inode ratio to have enough space on the MDT for small files, as described below.\n\nIt is possible to change the recommended default of 2560 bytes per inode for an ldiskfs MDT when it is first formatted by adding the `--mkfsoptions=\"-i bytes-per-inode\"` option to `mkfs.lustre`. Decreasing the inode ratio tunable `bytes-per-inode` will create more inodes for a given MDT size, but will leave less space for extra per-file metadata and is not recommended. The inode ratio must always be strictly larger than the MDT inode size, which is 1024 bytes by default. It is recommended to use an inode ratio at least 1536 bytes larger than the inode size to ensure the MDT does not run out of space. Increasing the inode ratio with enough space for the most commonly file size (e.g. 5632 or 66560 bytes if 4KB or 64KB files are widely used) is recommended for DoM.\n\nThe size of the inode may be changed  at format time by adding the `--stripe-count-hint=N` to have `mkfs.lustre` automatically calculate a reasonable inode size based on the default stripe count that will be used by the filesystem, or directly by specifying the `--mkfsoptions=\"-I inode-size\"` option. Increasing the inode size will provide more space in the inode for a larger Lustre file layout, ACLs, user and system extended attributes, SELinux and other security labels, and other internal metadata and DoM data. However, if these features or other in-inode xattrs are not needed, the larger inode size may hurt metadata performance as 2x, 4x, or 8x as much data would be read or written for each MDT inode access.\n\n### Setting Formatting Options for an ldiskfs OST\n\nWhen formatting an OST file system, it can be beneficial to take local file system usage into account, for example by running `df` and `df -i` on a current filesystem to get the used bytes and used inodes respectively, then computing the average bytes-per-inode value. When deciding on the ratio for a new filesystem, try to avoid having too many inodes on each OST, while keeping enough margin to allow for future usage of smaller files. This helps reduce the format and e2fsck time and makes more space available for data.\n\nThe table below shows the default bytes-per-inode ratio (\"inode ratio\") used for OSTs of various sizes when they are formatted.\n\n##### Table 3. Default Inode Ratios Used for Newly Formatted OSTs\n\n| **LUN/OST size** | **Default Inode ratio** | **Total inodes** |\n| ---------------- | ----------------------- | ---------------- |\n| under 10GiB      | 1 inode/16KiB           | 640 - 655k       |\n| 10GiB - 1TiB     | 1 inode/68KiB           | 153k - 15.7M     |\n| 1TiB - 8TiB      | 1 inode/256KiB          | 4.2M - 33.6M     |\n| over 8TiB        | 1 inode/1MiB            | 8.4M - 268M      |\n\nIn environments with few small files, the default inode ratio may result in far too many inodes for the average file size. In this case, performance can be improved by increasing the number ofbytes-per-inode. To set the inode ratio, use the `--mkfsoptions=\"-i *bytes-per-inode*\"` argument to `mkfs.lustre`to specify the expected average (mean) size of OST objects.", "mimetype": "text/plain", "start_char_idx": 20188, "end_char_idx": 24040, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa4c1ab0-c77f-4351-a209-42bc80883a18": {"__data__": {"id_": "aa4c1ab0-c77f-4351-a209-42bc80883a18", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b805162-992c-4dbb-a3e5-cf7f71c3236e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "df2becbe613f2b5641cfdfaad6d74adf506932daaf6ea5848f3e5fe79e28a8c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac8b0e86-593f-454f-ac21-95982f436246", "node_type": "1", "metadata": {}, "hash": "aae2b27e182e575b3d515d81d92bcea0416a47986752560ceda946d8b9c7d0e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case, performance can be improved by increasing the number ofbytes-per-inode. To set the inode ratio, use the `--mkfsoptions=\"-i *bytes-per-inode*\"` argument to `mkfs.lustre`to specify the expected average (mean) size of OST objects. For example, to create an OST with an expected average object size of 8 MiB run:\n\n```\n[oss#] mkfs.lustre --ost --mkfsoptions=\"-i $((8192 * 1024))\" ...\n```\n\n### Note\n\nOSTs formatted with ldiskfs should preferably have fewer than 320 million objects per MDT, and up to a maximum of 4 billion inodes. Specifying a very small bytes-per-inode ratio for a large OST that exceeds this limit can cause either premature out-of-space errors and prevent the full OST space from being used, or will waste space and slow down e2fsck more than necessary. The default inode ratios are chosen to ensure the total number of inodes remain below this limit.\n\n### Note\n\nFile system check time on OSTs is affected by a number of variables in addition to the number of inodes, including the size of the file system, the number of allocated blocks, the distribution of allocated blocks on the disk, disk speed, CPU speed, and the amount of RAM on the server. Reasonable file system check times for valid filesystems are 5-30 minutes per TiB, but may increase significantly if substantial errors are detected and need to be repaired.\n\nFor further details about optimizing MDT and OST file systems, see [the section called \u201c Formatting Options for ldiskfs RAID Devices\u201d](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#formatting-options-for-ldiskfs-raid-devices).\n\n## File and File System Limits\n\n[Table 4, \u201cFile and file system limits\u201d](#table-4-file-and-file-system-limits) describes current known limits of Lustre. These limits may be imposed by either the Lustre architecture or the Linux virtual file system (VFS) and virtual memory subsystems. In a few cases, a limit is defined within the code Lustre based on tested values and could be changed by editing and re-compiling the Lustre software. In these cases, the indicated limit was used for testing of the Lustre software.\n\n##### Table 4. File and file system limits\n\n| **Limit**                                                    | **Value**                                                    | **Description**                                              |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Maximum number of MDTs                                       | 256                                                          | A single MDS can host one or more MDTs, either for separate filesystems, or aggregated into a single namespace. Each filesystem requires a separate MDT for the filesystem root directory. Up to 255 more MDTs can be added to the filesystem and are attached into the filesystem namespace with creation of DNE remote or striped directories. |\n| Maximum number of OSTs                                       | 8150                                                         | The maximum number of OSTs is a constant that can be changed at compile time. Lustre file systems with up to 4000 OSTs have been configured in the past. Multiple OST targets can be configured on a single OSS node. |\n| Maximum OST size                                             | 1024TiB (ldiskfs), 1024TiB (ZFS)                             | This is not a *hard* limit. Larger OSTs are possible, but most production systems do not typically go beyond the stated limit per OST because Lustre can add capacity and performance with additional OSTs, and having more OSTs improves aggregate I/O performance, minimizes contention, and allows parallel recovery (e2fsck for ldiskfs OSTs, scrub for ZFS OSTs).With 32-bit kernels, due to page cache limits, 16TB is the maximum block device size, which in turn applies to the size of OST. It is **strongly** recommended to run Lustre clients and servers with 64-bit kernels. |\n| Maximum number of clients                                    | 131072                                                       | The maximum number of clients is a constant that can be changed at compile time. Up to 30000 clients have been used in production accessing a single filesystem. |\n| Maximum size of a single file system                         | 2EiB or larger                                               | Each OST can have a file system up to the \"Maximum OST size\" limit, and the Maximum number of OSTs can be combined into a single filesystem.", "mimetype": "text/plain", "start_char_idx": 23799, "end_char_idx": 28415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac8b0e86-593f-454f-ac21-95982f436246": {"__data__": {"id_": "ac8b0e86-593f-454f-ac21-95982f436246", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa4c1ab0-c77f-4351-a209-42bc80883a18", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ae01c215f9008a4ce1dc69b813956e536fb3c88852319296326c8106189be26f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c3f0732-6917-44a4-a33e-a6956ed7de57", "node_type": "1", "metadata": {}, "hash": "1d2a0e5cdd631b85af74983c0481af1ed0be90c8126b38b13f77e09b73276715", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is **strongly** recommended to run Lustre clients and servers with 64-bit kernels. |\n| Maximum number of clients                                    | 131072                                                       | The maximum number of clients is a constant that can be changed at compile time. Up to 30000 clients have been used in production accessing a single filesystem. |\n| Maximum size of a single file system                         | 2EiB or larger                                               | Each OST can have a file system up to the \"Maximum OST size\" limit, and the Maximum number of OSTs can be combined into a single filesystem. |\n| Maximum stripe count                                         | 2000                                                         | This limit is imposed by the size of the layout that needs to be stored on disk and sent in RPC requests, but is not a hard limit of the protocol. The number of OSTs in the filesystem can exceed the stripe count, but this is the maximum number of OSTs on which a single file can be striped.<br/>**Note** <br/>Before 2.13, the default for ldiskfs MDTs the maximum stripe count for a single file is limited to 160 OSTs. In order to increase the maximum file stripe count, use `--mkfsoptions=\"- O ea_inode\"` when formatting the MDT, or `use tune2fs -O ea_inode` to enable it after the MDT has been formatted. |\n| Maximum stripe size                                          | < 4 GiB                                                      | The amount of data written to each object before moving on to next object. |\n| Minimum stripe size                                          | 64 KiB                                                       | Due to the use of 64 KiB PAGE_SIZE on some CPU architectures such as ARM and POWER, the minimum stripe size is 64 KiB so that a single page is not split over multiple servers. |\n| Maximum single object size                                   | 16TiB (ldiskfs), 256TiB (ZFS)                                | The amount of data that can be stored in a single object. An object corresponds to a stripe. The ldiskfs limit of 16 TB for a single object applies. For ZFS the limit is the size of the underlying OST. Files can consist of up to 2000 stripes, each stripe can be up to the maximum object size. |\n| Maximum file size                                            | 16 TiB on 32-bit systems 31.25 PiB on 64-bit ldiskfs systems, 8EiB on 64-bit ZFS systems | Individual files have a hard limit of nearly 16 TiB on 32-bit systems imposed by the kernel memory subsystem. On 64-bit systems this limit does not exist. Hence, files can be 2^63 bits (8EiB) in size if the backing filesystem can support large enough objects and/or the files are sparse. A single file can have a maximum of 2000 stripes, which gives an upper single file limit of 31.25 PiB for 64-bit ldiskfs systems. The actual amount of data that can be stored in a file depends upon the amount of free space in each OST on which the file is striped. |\n| Maximum number of files or subdirectories in a single directory | 10 million files (ldiskfs), 2^48 (ZFS)                       | The Lustre software uses the ldiskfs hashed directory code, which has a limit of at least 600 million files, depending on the length of the file name. The limit on subdirectories is the same as the limit on regular files.<br />**Note**<br />Starting in the 2.8 release it is possible to exceed this limit by striping a single directory over multiple MDTs with the `lfs mkdir -c` command, which increases the single directory limit by a factor of the number of directory stripes used.<br />**Note**<br />Starting in the 2.14 release, the `large_dir` feature of ldiskfs is enabled by default to allow directories with more than 10M entries. In the 2.12 release, the `large_dir` feature was present but not enabled by default. |\n| Maximum number of files in the file system                   | 4 billion (ldiskfs), 256 trillion (ZFS) *per MDT*            | The ldiskfs filesystem imposes an upper limit of 4 billion inodes per filesystem. By default, the MDT filesystem is formatted with one inode per 2KB of space, meaning 512 million inodes per TiB of MDT space. This can be increased initially at the time of MDT filesystem creation.", "mimetype": "text/plain", "start_char_idx": 27768, "end_char_idx": 32063, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c3f0732-6917-44a4-a33e-a6956ed7de57": {"__data__": {"id_": "4c3f0732-6917-44a4-a33e-a6956ed7de57", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac8b0e86-593f-454f-ac21-95982f436246", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e4f2808381d5b38b97eeb2f6398762838e1a3d545251b76e8acc6f69f308410d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b09a0ca5-72e1-4e96-851e-d212de48b481", "node_type": "1", "metadata": {}, "hash": "7d009022e9a9ff3a54df45ea18579627d5a11d881703409b2f04d632217400ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the 2.12 release, the `large_dir` feature was present but not enabled by default. |\n| Maximum number of files in the file system                   | 4 billion (ldiskfs), 256 trillion (ZFS) *per MDT*            | The ldiskfs filesystem imposes an upper limit of 4 billion inodes per filesystem. By default, the MDT filesystem is formatted with one inode per 2KB of space, meaning 512 million inodes per TiB of MDT space. This can be increased initially at the time of MDT filesystem creation. For more information, see [*Determining Hardware Configuration Requirements and Formatting Options*](#determining-hardware-configuration-requirements-and-formatting-options).                                                                                            The ZFS filesystem dynamically allocates inodes and does not have a fixed ratio of inodes per unit of MDT space, but consumes approximately 4KiB of mirrored space per inode, depending on the configuration.<br/>Each additional MDT can hold up to the above maximum number of additional files, depending on available space and the distribution directories and files in the filesystem. |\n| Maximum length of a filename                                 | 255 bytes (filename)                                         | This limit is 255 bytes for a single filename, the same as the limit in the underlying filesystems. |\n| Maximum length of a pathname                                 | 4096 bytes (pathname)                                        | The Linux VFS imposes a full pathname length of 4096 bytes.  |\n| Maximum number of open files for a Lustre file system        | No limit                                                     | The Lustre software does not impose a maximum for the number of open files, but the practical limit depends on the amount of RAM on the MDS. No \"tables\" for open files exist on the MDS, as they are only linked in a list to a given client's export. Each client process has a limit of several thousands of open files which depends on its ulimit. |\n\n## Determining Memory Requirements\n\nThis section describes the memory requirements for each Lustre file system component.\n\n### Client Memory Requirements\n\nA minimum of 2 GB RAM is recommended for clients.\n\n### MDS Memory Requirements\n\nMDS memory requirements are determined by the following factors:\n\n- Number of clients\n- Size of the directories\n- Load placed on server\n\nThe amount of memory used by the MDS is a function of how many clients are on the system, and how many files they are using in their working set. This is driven, primarily, by the number of locks a client can hold at one time. The number of locks held by clients varies by load and memory availability on the server. Interactive clients can hold in excess of 10,000 locks at times. On the MDS, memory usage is approximately 2 KB per file, including the Lustre distributed lock manager (LDLM) lock and kernel data structures for the files currently in use. Having file data in cache can improve metadata performance by a factor of 10x or more compared to reading it from storage.\n\nMDS memory requirements include:\n\n- **File system metadata** : A reasonable amount of RAM needs to be available for file system metadata. While no hard limit can be placed on the amount of file system metadata, if more RAM is available, then the disk I/O is needed less often to retrieve the metadata.\n- **Network transport** : If you are using TCP or other network transport that uses system memory for send/ receive buffers, this memory requirement must also be taken into consideration.\n- **Journal size** : By default, the journal size is 4096 MB for each MDT ldiskfs file system. This can pin up to an equal amount of RAM on the MDS node per file system.\n- **Failover configuration** : If the MDS node will be used for failover from another node, then the RAM for each journal should be doubled, so the backup server can handle the additional load if the primary server fails.\n\n#### Calculating MDS Memory Requirements\n\nBy default, 4096 MB are used for the ldiskfs filesystem journal. Additional RAM is used for caching file data for the larger working set, which is not actively in use by clients but should be kept \"hot\" for improved access times. Approximately 1.5 KB per file is needed to keep a file in cache without a lock.", "mimetype": "text/plain", "start_char_idx": 31569, "end_char_idx": 35896, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b09a0ca5-72e1-4e96-851e-d212de48b481": {"__data__": {"id_": "b09a0ca5-72e1-4e96-851e-d212de48b481", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c3f0732-6917-44a4-a33e-a6956ed7de57", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f0782fc30471994d2d4de546129846e1ccfe21d2ea2016615e725210d7a83263", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a18efd4e-6f06-4fd7-b994-d2fb42626c28", "node_type": "1", "metadata": {}, "hash": "b369e8c7caccae44454cba423cd2739ce5a0ef0aa5f7b60f08edbcdaec10e866", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Journal size** : By default, the journal size is 4096 MB for each MDT ldiskfs file system. This can pin up to an equal amount of RAM on the MDS node per file system.\n- **Failover configuration** : If the MDS node will be used for failover from another node, then the RAM for each journal should be doubled, so the backup server can handle the additional load if the primary server fails.\n\n#### Calculating MDS Memory Requirements\n\nBy default, 4096 MB are used for the ldiskfs filesystem journal. Additional RAM is used for caching file data for the larger working set, which is not actively in use by clients but should be kept \"hot\" for improved access times. Approximately 1.5 KB per file is needed to keep a file in cache without a lock. \n\nFor example, for a single MDT on an MDS with 1,024 compute nodes, 12 interactive login nodes, and a 20 million file working set (of which 9 million files are cached on the clients at one time): \n\nOperating system overhead = 4096 MB (RHEL8) \n\nFile system journal = 4096 MB \n\n1024 * 32-core clients * 256 files/core * 2KB = 16384 MB \n\n12 interactive clients * 100,000 files * 2KB = 2400 MB \n\n20 million file working set * 1.5KB/file = 30720 MB\n\nThus, a reasonable MDS configuration for this workload is at least 60 GB of RAM. For active-active DNE MDT failover pairs, each MDS should have at least 96 GB of RAM. The additional memory can be used during normal operation to allow more metadata and locks to be cached and improve performance, depending on the workload. \n\nFor directories containing 1 million or more files, more memory can provide a significant benefit. For example, in an environment where clients randomly a single directory with 10 million files can consume as much as 35GB of RAM on the MDS.\n\n### OSS Memory Requirements\n\nWhen planning the hardware for an OSS node, consider the memory usage of several components in the Lustre file system (i.e., journal, service threads, file system metadata, etc.). Also, consider the effect of the OSS read cache feature, which consumes memory as it caches data on the OSS node.\n\nIn addition to the MDS memory requirements mentioned in above, the OSS requirements also include:\n\n- **Service threads** : The service threads on the OSS node pre-allocate an RPC-sized MB I/O buffer for each `ost_io` service thread, so these large buffers do not need to be allocated and freed for each I/O request.\n- **OSS read cache** :  OSS read cache provides read-only caching of data on an HDD-based OSS, using the regular Linux page cache to store the data. Just like caching from a regular file system in the Linux operating system, OSS read cache uses as much physical memory as is available.\n\nThe same calculation applies to files accessed from the OSS as for the MDS, but the load is typically distributed over more OSS nodes, so the amount of memory required for locks, inode cache, etc. listed for the MDS is spread out over the OSS nodes.\n\nBecause of these memory requirements, the following calculations should be taken as determining the minimum RAM required in an OSS node.\n\n#### Calculating OSS Memory Requirements\n\nThe minimum recommended RAM size for an OSS with eight OSTs, handling objects for 1/4 of the active files for the MDS:\n\nNetwork send/receive buffers (16 MB * 512 threads) = 8192 MB \n\n1024 MB ldiskfs journal size * 8 OST devices = 8192 MB \n\n16 MB read/write buffer per OST IO thread * 512 threads = 8192 MB \n\n2048 MB file system read cache * 8 OSTs = 16384 MB \n\n1024 * 32-core clients * 64 objects/core * 2KB/object = 4096 MB \n\n12 interactive clients * 25,000 objects * 2KB/object = 600 MB \n\n5 million object working set * 1.5KB/object = 7500 MB \n\nFor a non-failover configuration, the minimum RAM would be about 60 GB for an OSS node with eight OSTs. Additional memory on the OSS will improve the performance of reading smaller, frequently-accessed files. \n\nFor a failover configuration, the minimum RAM would be about 90 GB, as some of the memory is pernode. When the OSS is not handling any failed-over OSTs the extra RAM will be used as a read cache.", "mimetype": "text/plain", "start_char_idx": 35152, "end_char_idx": 39220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a18efd4e-6f06-4fd7-b994-d2fb42626c28": {"__data__": {"id_": "a18efd4e-6f06-4fd7-b994-d2fb42626c28", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a09abf-8c64-40ef-b0e4-3f0c680c6528", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3349ae5f53ffb379f33cfc7503d3f7b30044f0f9e4b4f4b2a94b87f58679e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b09a0ca5-72e1-4e96-851e-d212de48b481", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f89632bd884ab163c71e213e2bb9434c345ee2ffc84faa61a54fe4c6943c84b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additional memory on the OSS will improve the performance of reading smaller, frequently-accessed files. \n\nFor a failover configuration, the minimum RAM would be about 90 GB, as some of the memory is pernode. When the OSS is not handling any failed-over OSTs the extra RAM will be used as a read cache. \n\nAs a reasonable rule of thumb, about 24 GB of base memory plus 4 GB per OST can be used. In failover configurations, about 8 GB per primary OST is needed.\n\n## Implementing Networks To Be Used by the Lustre File System\n\nAs a high performance file system, the Lustre file system places heavy loads on networks. Thus, a network interface in each Lustre server and client is commonly dedicated to Lustre file system traffic. This is often a dedicated TCP/IP subnet, although other network hardware can also be used.\n\nA typical Lustre file system implementation may include the following:\n\n- A high-performance backend network for the Lustre servers, typically an InfiniBand (IB) network.\n- A larger client network.\n- Lustre routers to connect the two networks.\n\nLustre networks and routing are configured and managed by specifying parameters to the Lustre Networking (`lnet`) module in`/etc/modprobe.d/lustre.conf`.\n\nTo prepare to configure Lustre networking, complete the following steps:\n\n1. **Identify all machines that will be running Lustre software and the network interfaces they will use to run Lustre file system traffic. These machines will form the Lustre network .**\n\n   A network is a group of nodes that communicate directly with one another. The Lustre software includes Lustre network drivers (LNDs) to support a variety of network types and hardware (see [*Understanding Lustre Networking (LNet)*](02-Introducing%20the%20Lustre%20File%20System.md#understanding-lustre-networking-lnet) for a complete list). The standard rules for specifying networks applies to Lustre networks. For example, two TCP networks on two different subnets (`tcp0` and `tcp1`) are considered to be two different Lustre networks.\n\n2. **If routing is needed, identify the nodes to be used to route traffic between networks.**\n\n   If you are using multiple network types, then you will need a router. Any node with appropriate interfaces can route Lustre networking (LNet) traffic between different network hardware types or topologies --the node may be a server, a client, or a standalone router. LNet can route messages between different network types (such as TCP-to-InfiniBand) or across different topologies (such as bridging two InfiniBand or TCP/IP networks). Routing will be configured in [*Configuring Lustre Networking (LNet)*](02.06-Configuring%20Lustre%20Networking%20(LNet).md).\n\n3. **Identify the network interfaces to include in or exclude from LNet.**\n\n   If not explicitly specified, LNet uses either the first available interface or a pre-defined default for a given network type. Interfaces that LNet should not use (such as an administrative network or IP-over-IB), can be excluded.\n\n   Network interfaces to be used or excluded will be specified using the lnet kernel module parameters `networks` and`ip2nets` as described in [*Configuring Lustre Networking (LNet)*](02.06-Configuring%20Lustre%20Networking%20(LNet).md).\n\n4. **To ease the setup of networks with complex network configurations, determine a cluster-wide module configuration.**\n\n   For large clusters, you can configure the networking setup for all nodes by using a single, unified set of parameters in the `lustre.conf` file on each node. Cluster-wide configuration is described in [*Configuring Lustre Networking (LNet)*](02.06-Configuring%20Lustre%20Networking%20(LNet).md).\n\n### Note\n\nWe recommend that you use 'dotted-quad' notation for IP addresses rather than host names to make it easier to read debug logs and debug configurations with multiple interfaces.", "mimetype": "text/plain", "start_char_idx": 38918, "end_char_idx": 42760, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e105c176-31c7-49fd-ba76-6cbc88c67273": {"__data__": {"id_": "e105c176-31c7-49fd-ba76-6cbc88c67273", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/06-Configuring Storage on a Lustre File System.md", "file_name": "06-Configuring Storage on a Lustre File System.md", "file_type": "text/markdown", "file_size": 10504, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "777b7a2a-a9f3-4979-b600-eec8a49b8e7d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/06-Configuring Storage on a Lustre File System.md", "file_name": "06-Configuring Storage on a Lustre File System.md", "file_type": "text/markdown", "file_size": 10504, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f0c6ec40904b8e04b81a43aceb05c286d74c9588d44143e4db9c4cb930da6298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbac1618-9d37-4eaf-afef-7bad5a39609a", "node_type": "1", "metadata": {}, "hash": "a919451cf48f654f4b2f1317c9a01e8dc159e64c5bb722df51d6f365d735207e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Configuring Storage on a Lustre File System\n\n**Table of Contents**\n\n- [Configuring Storage on a Lustre File System](#configuring-storage-on-a-lustre-file-system)\n    * [Selecting Storage for the MDT and OSTs](#selecting-storage-for-the-mdt-and-osts)\n      * [Metadata Target (MDT)](#metadata-target-mdt)\n      * [Object Storage Server (OST)](#object-storage-server-ost)\n  * [Reliability Best Practices](#reliability-best-practices)\n  * [Performance Tradeoffs](#performance-tradeoffs)\n  * [Formatting Options for ldiskfs RAID Devices](#formatting-options-for-ldiskfs-raid-devices)\n    + [Computing file system parameters for mkfs](#computing-file-system-parameters-for-mkfs)\n    + [Choosing Parameters for an External Journal](#choosing-parameters-for-an-external-journal)\n  * [Connecting a SAN to a Lustre File System](#connecting-a-san-to-a-lustre-file-system)\n\nThis chapter describes best practices for storage selection and file system options to optimize performance on RAID, and includes the following sections:\n\n- [the section called \u201c Selecting Storage for the MDT and OSTs\u201d](#selecting-storage-for-the-mdt-and-osts)\n- [the section called \u201cReliability Best Practices\u201d](#reliability-best-practices)\n- [the section called \u201cPerformance Tradeoffs\u201d](#performance-tradeoffs)\n- [the section called \u201c Formatting Options for ldiskfs RAID Devices\u201d](#formatting-options-for-ldiskfs-raid-devices)\n- [the section called \u201cConnecting a SAN to a Lustre File System\u201d](#connecting-a-san-to-a-lustre-file-system)\n\n### Note\n\n**It is strongly recommended that storage used in a Lustre file system be configured with hardware RAID.** The Lustre software does not support redundancy at the file system level and RAID is required to protect against disk failure.\n\n## Selecting Storage for the MDT and OSTs\n\nThe Lustre architecture allows the use of any kind of block device as backend storage. The characteristics of such devices, particularly in the case of failures, vary significantly and have an impact on configuration choices.\n\nThis section describes issues and recommendations regarding backend storage.\n\n### Metadata Target (MDT)\n\nI/O on the MDT is typically mostly reads and writes of small amounts of data. For this reason, we recommend that you use RAID 1 for MDT storage. If you require more capacity for an MDT than one disk provides, we recommend RAID 1 + 0 or RAID 10.\n\n### Object Storage Server (OST)\n\nA quick calculation makes it clear that without further redundancy, RAID 6 is required for large clusters and RAID 5 is not acceptable:\n\n> For a 2 PB file system (2,000 disks of 1 TB capacity) assume the mean time to failure (MTTF) of a disk is about 1,000 days. This means that the expected failure rate is 2000/1000 = 2 disks per day. Repair time at 10% of disk bandwidth is 1000 GB at 10MB/sec = 100,000 sec, or about 1 day.\n>\n> For a RAID 5 stripe that is 10 disks wide, during 1 day of rebuilding, the chance that a second disk in the same array will fail is about 9/1000 or about 1% per day. After 50 days, you have a 50% chance of a double failure in a RAID 5 array leading to data loss.\n>\n> Therefore, RAID 6 or another double parity algorithm is needed to provide sufficient redundancy for OST storage.\n\nFor better performance, we recommend that you create RAID sets with 4 or 8 data disks plus one or two parity disks. Using larger RAID sets will negatively impact performance compared to having multiple independent RAID sets.\n\nTo maximize performance for small I/O request sizes, storage configured as RAID 1+0 can yield much better results but will increase cost or reduce capacity.\n\n## Reliability Best Practices\n\nRAID monitoring software is recommended to quickly detect faulty disks and allow them to be replaced to avoid double failures and data loss. Hot spare disks are recommended so that rebuilds happen without delays.\n\nBackups of the metadata file systems are recommended. For details, see [*Backing Up and Restoring a File System*](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md).\n\n## Performance Tradeoffs\n\nA writeback cache in a RAID storage controller can dramatically increase write performance on many types of RAID arrays if the writes are not done at full stripe width.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fbac1618-9d37-4eaf-afef-7bad5a39609a": {"__data__": {"id_": "fbac1618-9d37-4eaf-afef-7bad5a39609a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/06-Configuring Storage on a Lustre File System.md", "file_name": "06-Configuring Storage on a Lustre File System.md", "file_type": "text/markdown", "file_size": 10504, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "777b7a2a-a9f3-4979-b600-eec8a49b8e7d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/06-Configuring Storage on a Lustre File System.md", "file_name": "06-Configuring Storage on a Lustre File System.md", "file_type": "text/markdown", "file_size": 10504, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f0c6ec40904b8e04b81a43aceb05c286d74c9588d44143e4db9c4cb930da6298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e105c176-31c7-49fd-ba76-6cbc88c67273", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/06-Configuring Storage on a Lustre File System.md", "file_name": "06-Configuring Storage on a Lustre File System.md", "file_type": "text/markdown", "file_size": 10504, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f3c34fa1c9ed921fb90d2b7ab03d2a209678d80fc3ad23fffac76144ae10c310", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad928673-1a9a-4b70-835b-9bd579452e5c", "node_type": "1", "metadata": {}, "hash": "ca6188006e3dce0cdd8a432fe30a107ac2f5915e4ba9c1f1073afe12b1c7cfec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using larger RAID sets will negatively impact performance compared to having multiple independent RAID sets.\n\nTo maximize performance for small I/O request sizes, storage configured as RAID 1+0 can yield much better results but will increase cost or reduce capacity.\n\n## Reliability Best Practices\n\nRAID monitoring software is recommended to quickly detect faulty disks and allow them to be replaced to avoid double failures and data loss. Hot spare disks are recommended so that rebuilds happen without delays.\n\nBackups of the metadata file systems are recommended. For details, see [*Backing Up and Restoring a File System*](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md).\n\n## Performance Tradeoffs\n\nA writeback cache in a RAID storage controller can dramatically increase write performance on many types of RAID arrays if the writes are not done at full stripe width. Unfortunately, unless the RAID array has battery-backed cache (a feature only found in some higher-priced hardware RAID arrays), interrupting the power to the array may result in out-of-sequence or lost writes, and corruption of RAID parity and/ or filesystem metadata, resulting in data loss.\n\nHaving a read or writeback cache onboard a PCI adapter card installed in an MDS or OSS is NOT SAFE in a high-availability (HA) failover configuration, as this will result in inconsistencies between nodes and immediate or eventual filesystem corruption. Such devices should not be used, or should have the onboard cache disabled.\n\nIf writeback cache is enabled, a file system check is required after the array loses power. Data may also be lost because of this.\n\nTherefore, we recommend against the use of writeback cache when data integrity is critical. You should carefully consider whether the benefits of using writeback cache outweigh the risks.\n\n## Formatting Options for ldiskfs RAID Devices\n\nWhen formatting an ldiskfs file system on a RAID device, it can be beneficial to ensure that I/O requests are aligned with the underlying RAID geometry. This ensures that Lustre RPCs do not generate unnecessary disk operations which may reduce performance dramatically. Use the `--mkfsoptions` parameter to specify additional parameters when formatting the OST or MDT.\n\nFor RAID 5, RAID 6, or RAID 1+0 storage, specifying the following option to the `--mkfsoptions` parameter option improves the layout of the file system metadata, ensuring that no single disk contains all of the allocation bitmaps:\n\n```\n-E stride = chunk_blocks \n```\n\nThe `*chunk_blocks*` variable is in units of 4096-byte blocks and represents the amount of contiguous data written to a single disk before moving to the next disk. This is alternately referred to as the RAID stripe size. This is applicable to both MDT and OST file systems.\n\nFor more information on how to override the defaults while formatting MDT or OST file systems, see [the section called \u201c Setting ldiskfs File System Formatting Options \u201d](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#setting-ldiskfs-file-system-formatting-options).\n\n### Computing file system parameters for mkfs\n\nFor best results, use RAID 5 with 5 or 9 disks or RAID 6 with 6 or 10 disks, each on a different controller. The stripe width is the optimal minimum I/O size. Ideally, the RAID configuration should allow 1 MB Lustre RPCs to fit evenly on a single RAID stripe without an expensive read-modify-write cycle. Use this formula to determine the `*stripe_width*`, where`*number_of_data_disks*` does *not* include the RAID parity disks (1 for RAID 5 and 2 for RAID 6):\n\n```\nstripe_width_blocks = chunk_blocks * number_of_data_disks = 1 MB \n```\n\nIf the RAID configuration does not allow `*chunk_blocks*` to fit evenly into 1 MB, select `*stripe_width_blocks*`, such that is close to 1 MB, but not larger.\n\nThe `*stripe_width_blocks*` value must equal `*chunk_blocks* * *number_of_data_disks*`. Specifying the `*stripe_width_blocks*` parameter is only relevant for RAID 5 or RAID 6, and is not needed for RAID 1 plus 0.\n\nRun `--reformat` on the file system device (`/dev/sdc`), specifying the RAID geometry to the underlying ldiskfs file system, where:\n\n```\n--mkfsoptions \"other_options -E stride=chunk_blocks, stripe_width=stripe_width_blocks\"\n```\n\nA RAID 6 configuration with 6 disks has 4 data and 2 parity disks.", "mimetype": "text/plain", "start_char_idx": 3332, "end_char_idx": 7686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad928673-1a9a-4b70-835b-9bd579452e5c": {"__data__": {"id_": "ad928673-1a9a-4b70-835b-9bd579452e5c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/06-Configuring Storage on a Lustre File System.md", "file_name": "06-Configuring Storage on a Lustre File System.md", "file_type": "text/markdown", "file_size": 10504, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "777b7a2a-a9f3-4979-b600-eec8a49b8e7d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/06-Configuring Storage on a Lustre File System.md", "file_name": "06-Configuring Storage on a Lustre File System.md", "file_type": "text/markdown", "file_size": 10504, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f0c6ec40904b8e04b81a43aceb05c286d74c9588d44143e4db9c4cb930da6298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbac1618-9d37-4eaf-afef-7bad5a39609a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/06-Configuring Storage on a Lustre File System.md", "file_name": "06-Configuring Storage on a Lustre File System.md", "file_type": "text/markdown", "file_size": 10504, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "71037da59c0bd26ca04959e9d947bded4adfb0138dd93b7caf428c905424e740", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The `*stripe_width_blocks*` value must equal `*chunk_blocks* * *number_of_data_disks*`. Specifying the `*stripe_width_blocks*` parameter is only relevant for RAID 5 or RAID 6, and is not needed for RAID 1 plus 0.\n\nRun `--reformat` on the file system device (`/dev/sdc`), specifying the RAID geometry to the underlying ldiskfs file system, where:\n\n```\n--mkfsoptions \"other_options -E stride=chunk_blocks, stripe_width=stripe_width_blocks\"\n```\n\nA RAID 6 configuration with 6 disks has 4 data and 2 parity disks. The`*chunk_blocks*` <= 1024KB/4 = 256KB.\n\nBecause the number of data disks is equal to the power of 2, the stripe width is equal to 1 MB.\n\n```\n--mkfsoptions \"other_options -E stride=chunk_blocks, stripe_width=stripe_width_blocks\"...\n```\n\n### Choosing Parameters for an External Journal\n\nIf you have configured a RAID array and use it directly as an OST, it contains both data and metadata. For better performance, we recommend putting the OST journal on a separate device, by creating a small RAID 1 array and using it as an external journal for the OST.\n\nIn a typical Lustre file system, the default OST journal size is up to 1GB, and the default MDT journal size is up to 4GB, in order to handle a high transaction rate without blocking on journal flushes. Additionally, a copy of the journal is kept in RAM. Therefore, make sure you have enough RAM on the servers to hold copies of all journals.\n\nThe file system journal options are specified to `mkfs.lustre` using the `--mkfsoptions` parameter. For example:\n\n```\n--mkfsoptions \"other_options -j -J device=/dev/mdJ\" \n```\n\nTo create an external journal, perform these steps for each OST on the OSS:\n\n1. Create a 400 MB (or larger) journal partition (RAID 1 is recommended).\n\n   In this example, `/dev/sdb` is a RAID 1 device.\n\n2. Create a journal device on the partition. Run:\n\n   ```\n   oss# mke2fs -b 4096 -O journal_dev /dev/sdb journal_size\n   ```\n\n   The value of `*journal_size*` is specified in units of 4096-byte blocks. For example, 262144 for a 1 GB journal size.\n\n3. Create the OST.\n\n   In this example, `/dev/sdc` is the RAID 6 device to be used as the OST, run:\n\n   ```\n   [oss#] mkfs.lustre --ost ... \\\n   --mkfsoptions=\"-J device=/dev/sdb1\" /dev/sdc\n   ```\n\n4. Mount the OST as usual.\n\n## Connecting a SAN to a Lustre File System\n\nDepending on your cluster size and workload, you may want to connect a SAN to a Lustre file system. Before making this connection, consider the following:\n\n- In many SAN file systems, clients allocate and lock blocks or inodes individually as they are updated. The design of the Lustre file system avoids the high contention that some of these blocks and inodes may have.\n- The Lustre file system is highly scalable and can have a very large number of clients. SAN switches do not scale to a large number of nodes, and the cost per port of a SAN is generally higher than other networking.\n- File systems that allow direct-to-SAN access from the clients have a security risk because clients can potentially read any data on the SAN disks, and misbehaving clients can corrupt the file system for many reasons like improper file system, network, or other kernel software, bad cabling, bad memory, and so on. The risk increases with increase in the number of clients directly accessing the storage.", "mimetype": "text/plain", "start_char_idx": 7177, "end_char_idx": 10480, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0aee2b2-4853-43f7-b674-3ac18a01e8d9": {"__data__": {"id_": "d0aee2b2-4853-43f7-b674-3ac18a01e8d9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67237423-af56-4120-aa24-70eb93f29297", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e2ffe3a3bbc461a0aa775e064e6564a509ede829e4c844f0ee87ad712b8b2dee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27217ae4-0599-4cbb-8c2d-d39754903de9", "node_type": "1", "metadata": {}, "hash": "bb7a8b67e03248d0ca0f96b1d35b392937f2c4203c8c69673340dde0396aba38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Setting Up Network Interface Bonding\n\n**Table of Contents**\n\n- [Setting Up Network Interface Bonding](#setting-up-network-interface-bonding)\n    * [Network Interface Bonding Overview](#network-interface-bonding-overview)\n  * [Requirements](#requirements)\n  * [Bonding Module Parameters](#bonding-module-parameters)\n  * [Setting Up Bonding](#setting-up-bonding)\n    + [Examples](#examples)\n  * [Configuring a Lustre File System with Bonding](#configuring-a-lustre-file-system-with-bonding)\n  * [Bonding References](#bonding-references)\n\nThis chapter describes how to use multiple network interfaces in parallel to increase bandwidth and/or redundancy. Topics include:\n\n- [the section called \u201cNetwork Interface Bonding Overview\u201d](#network-interface-bonding-overview)\n- [the section called \u201cRequirements\u201d](#requirements)\n- [the section called \u201cBonding Module Parameters\u201d](#bonding-module-parameters)\n- [the section called \u201cSetting Up Bonding\u201d](#setting-up-bonding)\n- [the section called \u201cConfiguring a Lustre File System with Bonding\u201d](#configuring-a-lustre-file-system-with-bonding)\n- [the section called \u201cBonding References\u201d](#bonding-references)\n\n### Note\n\nUsing network interface bonding is optional.\n\n## Network Interface Bonding Overview\n\nBonding, also known as link aggregation, trunking and port trunking, is a method of aggregating multiple physical network links into a single logical link for increased bandwidth.\n\nSeveral different types of bonding are available in the Linux distribution. All these types are referred to as 'modes', and use the bonding kernel module.\n\nModes 0 to 3 allow load balancing and fault tolerance by using multiple interfaces. Mode 4 aggregates a group of interfaces into a single virtual interface where all members of the group share the same speed and duplex settings. This mode is described under IEEE spec 802.3ad, and it is referred to as either 'mode 4' or '802.3ad.'\n\n## Requirements\n\nThe most basic requirement for successful bonding is that both endpoints of the connection must be capable of bonding. In a normal case, the non-server endpoint is a switch. (Two systems connected via crossover cables can also use bonding.) Any switch used must explicitly handle 802.3ad Dynamic Link Aggregation.\n\nThe kernel must also be configured with bonding. All supported Lustre kernels have bonding functionality. The network driver for the interfaces to be bonded must have the ethtool functionality to determine slave speed and duplex settings. All recent network drivers implement it.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2525, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27217ae4-0599-4cbb-8c2d-d39754903de9": {"__data__": {"id_": "27217ae4-0599-4cbb-8c2d-d39754903de9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67237423-af56-4120-aa24-70eb93f29297", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e2ffe3a3bbc461a0aa775e064e6564a509ede829e4c844f0ee87ad712b8b2dee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0aee2b2-4853-43f7-b674-3ac18a01e8d9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "33bae556b8f698593b723d173da13f52c18921965634bdb35d995c7cef62da11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a15ccb5-78ba-499f-a01a-3b43a9295015", "node_type": "1", "metadata": {}, "hash": "77dca1741e0595211ac927d84cf8d40c7ebee421f847bac8842d8e3aca7c7c2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Modes 0 to 3 allow load balancing and fault tolerance by using multiple interfaces. Mode 4 aggregates a group of interfaces into a single virtual interface where all members of the group share the same speed and duplex settings. This mode is described under IEEE spec 802.3ad, and it is referred to as either 'mode 4' or '802.3ad.'\n\n## Requirements\n\nThe most basic requirement for successful bonding is that both endpoints of the connection must be capable of bonding. In a normal case, the non-server endpoint is a switch. (Two systems connected via crossover cables can also use bonding.) Any switch used must explicitly handle 802.3ad Dynamic Link Aggregation.\n\nThe kernel must also be configured with bonding. All supported Lustre kernels have bonding functionality. The network driver for the interfaces to be bonded must have the ethtool functionality to determine slave speed and duplex settings. All recent network drivers implement it.\n\nTo verify that your interface works with ethtool, run:\n\n```\n# which ethtool\n/sbin/ethtool\n \n# ethtool eth0\nSettings for eth0:\n           Supported ports: [ TP MII ]\n           Supported link modes:   10baseT/Half 10baseT/Full\n                                   100baseT/Half 100baseT/Full\n           Supports auto-negotiation: Yes\n           Advertised link modes:  10baseT/Half 10baseT/Full\n                                   100baseT/Half 100baseT/Full\n           Advertised auto-negotiation: Yes\n           Speed: 100Mb/s\n           Duplex: Full\n           Port: MII\n           PHYAD: 1\n           Transceiver: internal\n           Auto-negotiation: on\n           Supports Wake-on: pumbg\n           Wake-on: d\n           Current message level: 0x00000001 (1)\n           Link detected: yes\n \n# ethtool eth1\n \nSettings for eth1:\n   Supported ports: [ TP MII ]\n   Supported link modes:   10baseT/Half 10baseT/Full\n                           100baseT/Half 100baseT/Full\n   Supports auto-negotiation: Yes\n   Advertised link modes:  10baseT/Half 10baseT/Full\n   100baseT/Half 100baseT/Full\n   Advertised auto-negotiation: Yes\n   Speed: 100Mb/s\n   Duplex: Full\n   Port: MII\n   PHYAD: 32\n   Transceiver: internal\n   Auto-negotiation: on\n   Supports Wake-on: pumbg\n   Wake-on: d\n   Current message level: 0x00000007 (7)\n   Link detected: yes\n   To quickly check whether your kernel supports bonding, run:     \n   # grep ifenslave /sbin/ifup\n   # which ifenslave\n   /sbin/ifenslave\n```\n\n## Bonding Module Parameters\n\nBonding module parameters control various aspects of bonding.\n\nOutgoing traffic is mapped across the slave interfaces according to the transmit hash policy. We recommend that you set the `xmit_hash_policy` option to the layer3+4 option for bonding. This policy uses upper layer protocol information if available to generate the hash. This allows traffic to a particular network peer to span multiple slaves, although a single connection does not span multiple slaves.\n\n```\n$ xmit_hash_policy=layer3+4\n```\n\nThe `miimon` option enables users to monitor the link status. (The parameter is a time interval in milliseconds.) It makes an interface failure transparent to avoid serious network degradation during link failures. A reasonable default setting is 100 milliseconds; run:\n\n```\n$ miimon=100\n```\n\nFor a busy network, increase the timeout.\n\n## Setting Up Bonding\n\nTo set up bonding:\n\n1. Create a virtual 'bond' interface by creating a configuration file:\n\n   ```\n   # vi /etc/sysconfig/network-scripts/ifcfg-bond0\n   ```\n\n2. Append the following lines to the file.\n\n   ```\n   DEVICE=bond0\n   IPADDR=192.168.10.79 # Use the free IP Address of your network\n   NETWORK=192.168.10.0\n   NETMASK=255.255.255.0\n   USERCTL=no\n   BOOTPROTO=none\n   ONBOOT=yes\n   ```\n\n3. Attach one or more slave interfaces to the bond interface. Modify the eth0 and eth1 configuration files (using a VI text editor).\n\n   1.", "mimetype": "text/plain", "start_char_idx": 1581, "end_char_idx": 5434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a15ccb5-78ba-499f-a01a-3b43a9295015": {"__data__": {"id_": "6a15ccb5-78ba-499f-a01a-3b43a9295015", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67237423-af56-4120-aa24-70eb93f29297", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e2ffe3a3bbc461a0aa775e064e6564a509ede829e4c844f0ee87ad712b8b2dee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27217ae4-0599-4cbb-8c2d-d39754903de9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "70458207f3eda1c6c792bd9a3b4adc8db6cf4064e6540fa7800c61586235a273", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ba2013c-de54-4185-9c98-baee14e2b2ed", "node_type": "1", "metadata": {}, "hash": "f12cd69e82ed40112846b713a4ca8351f6d8738da2c267af2ba230da9447f97b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A reasonable default setting is 100 milliseconds; run:\n\n```\n$ miimon=100\n```\n\nFor a busy network, increase the timeout.\n\n## Setting Up Bonding\n\nTo set up bonding:\n\n1. Create a virtual 'bond' interface by creating a configuration file:\n\n   ```\n   # vi /etc/sysconfig/network-scripts/ifcfg-bond0\n   ```\n\n2. Append the following lines to the file.\n\n   ```\n   DEVICE=bond0\n   IPADDR=192.168.10.79 # Use the free IP Address of your network\n   NETWORK=192.168.10.0\n   NETMASK=255.255.255.0\n   USERCTL=no\n   BOOTPROTO=none\n   ONBOOT=yes\n   ```\n\n3. Attach one or more slave interfaces to the bond interface. Modify the eth0 and eth1 configuration files (using a VI text editor).\n\n   1. Use the VI text editor to open the eth0 configuration file.\n\n      ```\n      # vi /etc/sysconfig/network-scripts/ifcfg-eth0\n      ```\n\n   2. Modify/append the eth0 file as follows:\n\n      ```\n      DEVICE=eth0\n      USERCTL=no\n      ONBOOT=yes\n      MASTER=bond0\n      SLAVE=yes\n      BOOTPROTO=none\n      ```\n\n   3. Use the VI text editor to open the eth1 configuration file.\n\n      ```\n      # vi /etc/sysconfig/network-scripts/ifcfg-eth1\n      ```\n\n   4. Modify/append the eth1 file as follows:\n\n      ```\n      DEVICE=eth1\n      USERCTL=no\n      ONBOOT=yes\n      MASTER=bond0\n      SLAVE=yes\n      BOOTPROTO=none\n      ```\n\n4. Set up the bond interface and its options in `/etc/modprobe.d/bond.conf`. Start the slave interfaces by your normal network method.\n\n   ```\n   # vi /etc/modprobe.d/bond.conf\n   ```\n\n   1. Append the following lines to the file.\n\n      ```\n      alias bond0 bonding\n      options bond0 mode=balance-alb miimon=100\n      ```\n\n   2. Load the bonding module.\n\n      ```\n      # modprobe bonding\n      # ifconfig bond0 up\n      # ifenslave bond0 eth0 eth1\n      ```\n\n5. Start/restart the slave interfaces (using your normal network method).\n\n   ### Note\n\n   You must `modprobe` the bonding module for each bonded interface. If you wish to create bond0 and bond1, two entries in `bond.conf` file are required.\n\n   The examples below are from systems running Red Hat Enterprise Linux. For setup use: `/etc/sysconfig/networking-scripts/ifcfg-*` The website referenced below includes detailed instructions for other configuration methods, instructions to use DHCP with bonding, and other setup details. We strongly recommend you use this website.\n\n   <http://www.linuxfoundation.org/collaborate/workgroups/networking/bonding>\n\n6. Check /proc/net/bonding to determine status on bonding. There should be a file there for each bond interface.\n\n   ```\n   # cat /proc/net/bonding/bond0\n   Ethernet Channel Bonding Driver: v3.0.3 (March 23, 2006)\n    \n   Bonding Mode: load balancing (round-robin)\n   MII Status: up\n   MII Polling Interval (ms): 0\n   Up Delay (ms): 0\n   Down Delay (ms): 0\n    \n   Slave Interface: eth0\n   MII Status: up\n   Link Failure Count: 0\n   Permanent HW addr: 4c:00:10:ac:61:e0\n    \n   Slave Interface: eth1\n   MII Status: up\n   Link Failure Count: 0\n   Permanent HW addr: 00:14:2a:7c:40:1d\n   ```\n\n7. Use ethtool or ifconfig to check the interface state. ifconfig lists the first bonded interface as 'bond0.'", "mimetype": "text/plain", "start_char_idx": 4757, "end_char_idx": 7885, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ba2013c-de54-4185-9c98-baee14e2b2ed": {"__data__": {"id_": "9ba2013c-de54-4185-9c98-baee14e2b2ed", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67237423-af56-4120-aa24-70eb93f29297", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e2ffe3a3bbc461a0aa775e064e6564a509ede829e4c844f0ee87ad712b8b2dee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a15ccb5-78ba-499f-a01a-3b43a9295015", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cdbe1025acb6447f0e7cbb2ec4159d2244947be9f080c66b770d197dd98f3f6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81966ca0-1c09-44d1-a78e-3cba76b65ef3", "node_type": "1", "metadata": {}, "hash": "f71e78095b5227d711eda1c56da5eabd15aaf7d19390fadcc106b7e721fa8829", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Use ethtool or ifconfig to check the interface state. ifconfig lists the first bonded interface as 'bond0.'\n\n   ```\n   ifconfig\n   bond0      Link encap:Ethernet  HWaddr 4C:00:10:AC:61:E0\n      inet addr:192.168.10.79  Bcast:192.168.10.255 \\     Mask:255.255.255.0\n      inet6 addr: fe80::4e00:10ff:feac:61e0/64 Scope:Link\n      UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500 Metric:1\n      RX packets:3091 errors:0 dropped:0 overruns:0 frame:0\n      TX packets:880 errors:0 dropped:0 overruns:0 carrier:0\n      collisions:0 txqueuelen:0\n      RX bytes:314203 (306.8 KiB)  TX bytes:129834 (126.7 KiB)\n    \n   eth0       Link encap:Ethernet  HWaddr 4C:00:10:AC:61:E0\n      inet6 addr: fe80::4e00:10ff:feac:61e0/64 Scope:Link\n      UP BROADCAST RUNNING SLAVE MULTICAST  MTU:1500 Metric:1\n      RX packets:1581 errors:0 dropped:0 overruns:0 frame:0\n      TX packets:448 errors:0 dropped:0 overruns:0 carrier:0\n      collisions:0 txqueuelen:1000\n      RX bytes:162084 (158.2 KiB)  TX bytes:67245 (65.6 KiB)\n      Interrupt:193 Base address:0x8c00\n    \n   eth1       Link encap:Ethernet  HWaddr 4C:00:10:AC:61:E0\n      inet6 addr: fe80::4e00:10ff:feac:61e0/64 Scope:Link\n      UP BROADCAST RUNNING SLAVE MULTICAST  MTU:1500 Metric:1\n      RX packets:1513 errors:0 dropped:0 overruns:0 frame:0\n      TX packets:444 errors:0 dropped:0 overruns:0 carrier:0\n      collisions:0 txqueuelen:1000\n      RX bytes:152299 (148.7 KiB)  TX bytes:64517 (63.0 KiB)\n      Interrupt:185 Base address:0x6000\n   ```\n\n### Examples\n\nThis is an example showing `bond.conf` entries for bonding Ethernet interfaces `eth1`and `eth2` to `bond0`:\n\n```\n# cat /etc/modprobe.d/bond.conf\nalias eth0 8139too\nalias eth1 via-rhine\nalias bond0 bonding\noptions bond0 mode=balance-alb miimon=100\n \n# cat /etc/sysconfig/network-scripts/ifcfg-bond0\nDEVICE=bond0\nBOOTPROTO=none\nNETMASK=255.255.255.0\nIPADDR=192.168.10.79 # (Assign here the IP of the bonded interface.)\nONBOOT=yes\nUSERCTL=no\n \nifcfg-ethx \n# cat /etc/sysconfig/network-scripts/ifcfg-eth0\nTYPE=Ethernet\nDEVICE=eth0\nHWADDR=4c:00:10:ac:61:e0\nBOOTPROTO=none\nONBOOT=yes\nUSERCTL=no\nIPV6INIT=no\nPEERDNS=yes\nMASTER=bond0\nSLAVE=yes\n```\n\nIn the following example, the `bond0` interface is the master (MASTER) while `eth0`and `eth1` are slaves (SLAVE).\n\n### Note\n\nAll slaves of `bond0` have the same MAC address (Hwaddr) - `bond0`. All modes, except TLB and ALB, have this MAC address. TLB and ALB require a unique MAC address for each slave.", "mimetype": "text/plain", "start_char_idx": 7778, "end_char_idx": 10230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81966ca0-1c09-44d1-a78e-3cba76b65ef3": {"__data__": {"id_": "81966ca0-1c09-44d1-a78e-3cba76b65ef3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67237423-af56-4120-aa24-70eb93f29297", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e2ffe3a3bbc461a0aa775e064e6564a509ede829e4c844f0ee87ad712b8b2dee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ba2013c-de54-4185-9c98-baee14e2b2ed", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "473bd34bc09ffd66f7030f1261dcf64f3d52ee1c8256ee335b0c0a664922ec11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ONBOOT=yes\nUSERCTL=no\n \nifcfg-ethx \n# cat /etc/sysconfig/network-scripts/ifcfg-eth0\nTYPE=Ethernet\nDEVICE=eth0\nHWADDR=4c:00:10:ac:61:e0\nBOOTPROTO=none\nONBOOT=yes\nUSERCTL=no\nIPV6INIT=no\nPEERDNS=yes\nMASTER=bond0\nSLAVE=yes\n```\n\nIn the following example, the `bond0` interface is the master (MASTER) while `eth0`and `eth1` are slaves (SLAVE).\n\n### Note\n\nAll slaves of `bond0` have the same MAC address (Hwaddr) - `bond0`. All modes, except TLB and ALB, have this MAC address. TLB and ALB require a unique MAC address for each slave.\n\n```\n$ /sbin/ifconfig\n \nbond0Link encap:EthernetHwaddr 00:C0:F0:1F:37:B4\ninet addr:XXX.XXX.XXX.YYY Bcast:XXX.XXX.XXX.255 Mask:255.255.252.0\nUP BROADCAST RUNNING MASTER MULTICAST MTU:1500  Metric:1\nRX packets:7224794 errors:0 dropped:0 overruns:0 frame:0\nTX packets:3286647 errors:1 dropped:0 overruns:1 carrier:0\ncollisions:0 txqueuelen:0\n \neth0Link encap:EthernetHwaddr 00:C0:F0:1F:37:B4\ninet addr:XXX.XXX.XXX.YYY Bcast:XXX.XXX.XXX.255 Mask:255.255.252.0\nUP BROADCAST RUNNING SLAVE MULTICAST MTU:1500  Metric:1\nRX packets:3573025 errors:0 dropped:0 overruns:0 frame:0\nTX packets:1643167 errors:1 dropped:0 overruns:1 carrier:0\ncollisions:0 txqueuelen:100\nInterrupt:10 Base address:0x1080\n \neth1Link encap:EthernetHwaddr 00:C0:F0:1F:37:B4\ninet addr:XXX.XXX.XXX.YYY Bcast:XXX.XXX.XXX.255 Mask:255.255.252.0\nUP BROADCAST RUNNING SLAVE MULTICAST MTU:1500  Metric:1\nRX packets:3651769 errors:0 dropped:0 overruns:0 frame:0\nTX packets:1643480 errors:0 dropped:0 overruns:0 carrier:0\ncollisions:0 txqueuelen:100\nInterrupt:9 Base address:0x1400\n```\n\n## Configuring a Lustre File System with Bonding\n\nThe Lustre software uses the IP address of the bonded interfaces and requires no special configuration. The bonded interface is treated as a regular TCP/IP interface. If needed, specify `bond0` using the Lustre `networks` parameter in `/etc/modprobe`.\n\n```\noptions lnet networks=tcp(bond0)\n```\n\n## Bonding References\n\nWe recommend the following bonding references:\n\n- In the Linux kernel source tree, see`documentation/networking/bonding.txt`\n- <http://linux-ip.net/html/ether-bonding.html>.\n- <http://www.sourceforge.net/projects/bonding>.\n- Linux Foundation bonding website: <http://www.linuxfoundation.org/collaborate/workgroups/networking/bonding>. This is the most extensive reference and we highly recommend it. This website includes explanations of more complicated setups, including the use of DHCP with bonding.", "mimetype": "text/plain", "start_char_idx": 9703, "end_char_idx": 12144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb24aeb7-99f2-404e-b3c1-47f2bc628112": {"__data__": {"id_": "bb24aeb7-99f2-404e-b3c1-47f2bc628112", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1fc8ea6f4b26b94e01c4665ea641603c25ea2c18272ca4d6c7b8b617cb6ed9d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d9b8bbd-faf9-4797-9672-b1d414b96f9b", "node_type": "1", "metadata": {}, "hash": "e4d3502a54d8689867c071b1e9228768cee49aec3c6b0d03610e44a38eb9db54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Installing the Lustre Software\n\n**Table of Contents**\n\n- [Installing the Lustre Software](#installing-the-lustre-software)\n  \n  * [Preparing to Install the Lustre Software](#preparing-to-install-the-lustre-software)\n    + [Software Requirements](#software-requirements)\n    + [Environmental Requirements](#environmental-requirements)\n  * [Lustre Software Installation Procedure](#lustre-software-installation-procedure)\n  \n  \n\nThis chapter describes how to install the Lustre software from RPM packages. It includes:\n\n- [the section called \u201c Preparing to Install the Lustre Software\u201d](#preparing-to-install-the-lustre-software)\n- [the section called \u201cLustre Software Installation Procedure\u201d](#lustre-software-installation-procedure)\n\nFor hardware and system requirements and hardware configuration information, see[*Determining Hardware Configuration Requirements and Formatting Options*](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md).\n\n## Preparing to Install the Lustre Software\n\n- [Software Requirements](section_rqs_tjw_3k.html)\n- [Environmental Requirements](section_rh2_d4w_gk.html)\n\nYou can install the Lustre software from downloaded packages (RPMs) or directly from the source code. This chapter describes how to install the Lustre RPM packages. Instructions to install from source code are beyond the scope of this document, and can be found elsewhere online.\n\nThe Lustre RPM packages are tested on current versions of Linux enterprise distributions at the time they are created. See the release notes for each version for specific details.\n\n### Software Requirements\n\nTo install the Lustre software from RPMs, the following are required:\n\n- **Lustre server packages** . The required packages for Lustre 2.9 EL7 servers are listed in the table below, where *ver* refers to the Lustre release and kernel version (e.g., 2.9.0-1.el7) and *arch* refers to the processor architecture (e.g., x86_64). These packages are available in the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases) repository, and may differ depending on your distro and version.\n\n  \n\n  ##### Table 5. Packages Installed on Lustre Servers\n\n  | Package Name                            | Description                                                  |\n| --------------------------------------- | ------------------------------------------------------------ |\n  | `kernel-*ver*_lustre.*arch*`            | Linux kernel with Lustre software patches (often referred to as \"patched kernel\") |\n  | `lustre-*ver*.*arch*`                   | Lustre software command line tools                           |\n  | `kmod-lustre-*ver*.*arch*`              | Lustre-patched kernel modules                                |\n  | `kmod-lustre-osd-ldiskfs-*ver*.*arch*`  | Lustre back-end file system tools for ldiskfs-based servers. |\n  | `lustre-osd-ldiskfs-mount-*ver*.*arch*` | Helper library for `mount.lustre` and `mkfs.lustre` for ldiskfs-based servers. |\n  | `kmod-lustre-osd-zfs-*ver*.*arch*`      | Lustre back-end file system tools for ZFS. This is an alternative to `lustre-osd-ldiskfs`(kmod-spl and kmod-zfs available separately). |\n  | `lustre-osd-zfs-mount-*ver*.*arch*`     | Helper library for `mount.lustre` and `mkfs.lustre` for ZFS-based servers (zfs utilities available separately). |\n  | `e2fsprogs`                             | Utilities to maintain Lustre ldiskfs back-end file system(s) |\n  | `lustre-tests-*ver*_lustre.*arch*`      | Scripts and programs used for running regression tests for Lustre, but likely only of interest to Lustre developers or testers. |\n  \n- **Lustre client packages** . The required packages for Lustre 2.9 EL7 clients are listed in the table below, where *ver* refers to the Linux distribution (e.g., 3.6.18-348.1.1.el5). These packages are available in the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases)repository.\n\n  ##### Table 6. Packages Installed on Lustre Clients\n\n  | Package Name                      | Description                                                  |\n| --------------------------------- | ------------------------------------------------------------ |\n  | `kmod-lustre-client-*ver*.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d9b8bbd-faf9-4797-9672-b1d414b96f9b": {"__data__": {"id_": "6d9b8bbd-faf9-4797-9672-b1d414b96f9b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1fc8ea6f4b26b94e01c4665ea641603c25ea2c18272ca4d6c7b8b617cb6ed9d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb24aeb7-99f2-404e-b3c1-47f2bc628112", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1413ccec31f10afa3b9b4e200651d8332c4aecf9f7d3d7fb10e1084616b65f47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d8ab82b-4a62-4e06-9299-c7661a4834a2", "node_type": "1", "metadata": {}, "hash": "c7b338f743b93c00130c5dabf801ac2917f164f6eb70651e79589fd22c756832", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*arch*`      | Scripts and programs used for running regression tests for Lustre, but likely only of interest to Lustre developers or testers. |\n  \n- **Lustre client packages** . The required packages for Lustre 2.9 EL7 clients are listed in the table below, where *ver* refers to the Linux distribution (e.g., 3.6.18-348.1.1.el5). These packages are available in the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases)repository.\n\n  ##### Table 6. Packages Installed on Lustre Clients\n\n  | Package Name                      | Description                                                  |\n| --------------------------------- | ------------------------------------------------------------ |\n  | `kmod-lustre-client-*ver*.*arch*` | Patchless kernel modules for client                          |\n| `lustre-client-*ver*.*arch*`      | Client command line tools                                    |\n  | `lustre-client-dkms-*ver*.*arch*` | Alternate client RPM to kmod-lustre-client with Dynamic Kernel Module Support (DKMS) installation. This avoids the need to install a new RPM for each kernel update, but requires a full build environment on the client. |\n  \n  ### Note\n  \n  The version of the kernel running on a Lustre client must be the same as the version of the `kmod-lustre-client-*ver*` package being installed, unless the DKMS package is installed. If the kernel running on the client is not compatible, a kernel that is compatible must be installed on the client before the Lustre file system software is used.\n\n- **Lustre LNet network driver (LND)** . The Lustre LNDs provided with the Lustre software are listed in the table below. For more information about Lustre LNet, see [*Understanding Lustre Networking (LNet)*](02-Introducing%20the%20Lustre%20File%20System.md#understanding-lustre-networking-lnet).\n\n  \n\n  ##### Table 7. Network Types Supported by Lustre LNDs\n\n  | Supported Network Types | Notes                                                        |\n  | ----------------------- | ------------------------------------------------------------ |\n  | TCP                     | Any network carrying TCP traffic, including GigE, 10GigE, and IPoIB |\n  | InfiniBand network      | OpenFabrics OFED (o2ib)                                      |\n  | gni                     | Gemini (Cray)                                                |\n\n   \n\n   \n\n### Note\n\nThe InfiniBand and TCP Lustre LNDs are routinely tested during release cycles. The other LNDs are maintained by their respective owners\n\n- **High availability software** . If needed, install third party high-availability software. For more information, see [the section called \u201cPreparing a Lustre File System for Failover\u201d](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#preparing-a-lustre-file-system-for-failover).\n- **Optional packages.** Optional packages provided in the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases)repository may include the following (depending on the operating system and platform):\n  - `kernel-debuginfo`, `kernel-debuginfo-common`, `lustre-debuginfo`,`lustre-osd-ldiskfs-debuginfo`- Versions of required packages with debugging symbols and other debugging options enabled for use in troubleshooting.\n  - `kernel-devel`, - Portions of the kernel tree needed to compile third party modules, such as network drivers.\n  - `kernel-firmware`- Standard Red Hat Enterprise Linux distribution that has been recompiled to work with the Lustre kernel.\n  - `kernel-headers`- Header files installed under /user/include and used when compiling user-space, kernel-related code.\n  - `lustre-source`- Lustre software source code.\n  - (Recommended) `perf`, `perf-debuginfo`, `python-perf`, `python-perf-debuginfo`- Linux performance analysis tools that have been compiled to match the Lustre kernel version.\n\n### Environmental Requirements\n\nBefore installing the Lustre software, make sure the following environmental requirements are met.\n\n- (Required) **Use the same user IDs (UID) and group IDs (GID) on all clients.** If use of supplemental groups is required, see [the section called \u201cUser/Group Upcall\u201d](06.04-Programming%20Interfaces.md#usergroup-upcall) for information about supplementary user and group cache upcall (`identity_upcall`).\n- (Recommended) **Provide remote shell access to clients.", "mimetype": "text/plain", "start_char_idx": 3472, "end_char_idx": 7826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d8ab82b-4a62-4e06-9299-c7661a4834a2": {"__data__": {"id_": "3d8ab82b-4a62-4e06-9299-c7661a4834a2", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1fc8ea6f4b26b94e01c4665ea641603c25ea2c18272ca4d6c7b8b617cb6ed9d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d9b8bbd-faf9-4797-9672-b1d414b96f9b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "067e985a823684feec09179e238040e62d0b5f0f213f52a6e4636566176de31e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f01f2a99-fd5b-4648-aa8e-4f0c579664d9", "node_type": "1", "metadata": {}, "hash": "ba3b3e9f6241e1efcf7c444f5fd3e933358361ab2aea4f1df7b616249fdb6635", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `kernel-headers`- Header files installed under /user/include and used when compiling user-space, kernel-related code.\n  - `lustre-source`- Lustre software source code.\n  - (Recommended) `perf`, `perf-debuginfo`, `python-perf`, `python-perf-debuginfo`- Linux performance analysis tools that have been compiled to match the Lustre kernel version.\n\n### Environmental Requirements\n\nBefore installing the Lustre software, make sure the following environmental requirements are met.\n\n- (Required) **Use the same user IDs (UID) and group IDs (GID) on all clients.** If use of supplemental groups is required, see [the section called \u201cUser/Group Upcall\u201d](06.04-Programming%20Interfaces.md#usergroup-upcall) for information about supplementary user and group cache upcall (`identity_upcall`).\n- (Recommended) **Provide remote shell access to clients.** It is recommended that all cluster nodes have remote shell client access to facilitate the use of Lustre configuration and monitoring scripts. Parallel Distributed SHell (pdsh) is preferable, although Secure SHell (SSH) is acceptable.\n- (Recommended) **Ensure client clocks are synchronized.** The Lustre file system uses client clocks for timestamps. If clocks are out of sync between clients, files will appear with different time stamps when accessed by different clients. Drifting clocks can also cause problems by, for example, making it difficult to debug multi-node issues or correlate logs, which depend on timestamps. We recommend that you use Network Time Protocol (NTP) to keep client and server clocks in sync with each other. For more information about NTP, see: [https://www.ntp.org](https://www.ntp.org/).\n- (Recommended) **Make sure security extensions** (such as the Novell AppArmor *security system) and **network packet filtering tools** (such as iptables) do not interfere with the Lustre software.\n\n## Lustre Software Installation Procedure\n\n### Caution\n\nBefore installing the Lustre software, back up ALL data. The Lustre software contains kernel modifications that interact with storage devices and may introduce security issues and data loss if not installed, configured, or administered properly.\n\nTo install the Lustre software from RPMs, complete the steps below.\n\n1. Verify that all Lustre installation requirements have been met.\n\n   - For hardware requirements, see [*Determining Hardware Configuration Requirements and Formatting Options*](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md).\n   - For software and environmental requirements, see the section [the section called \u201c Preparing to Install the Lustre Software\u201d](#installing-the-lustre-software)above.\n\n2. Download the `e2fsprogs` RPMs for your platform from the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases)repository.\n\n3. Download the Lustre server RPMs for your platform from the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases)repository. See [Table 5, \u201cPackages Installed on Lustre Servers\u201d](#table-5-packages-installed-on-lustre-servers)for a list of required packages.\n\n4. Install the Lustre server and `e2fsprogs` packages on all Lustre servers (MGS, MDSs, and OSSs).\n\n   1. Log onto a Lustre server as the `root` user\n\n   2. Use the `yum` command to install the packages:\n\n      \n\n      ```\n      # yum --nogpgcheck install pkg1.rpm pkg2.rpm ...\n      ```\n\n      \n\n   3. Verify the packages are installed correctly:\n\n      \n\n      ```\n      rpm -qa|egrep \"lustre|wc\"|sort\n      ```\n\n      \n\n   4. Reboot the server.\n\n   5. Repeat these steps on each Lustre server.\n\n5. Download the Lustre client RPMs for your platform from the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases)repository. See [Table 6, \u201cPackages Installed on Lustre Clients\u201d](#table-6-packages-installed-on-lustre-clients)for a list of required packages.\n\n6. Install the Lustre client packages on all Lustre clients.\n\n   ### Note\n\n   The version of the kernel running on a Lustre client must be the same as the version of the `lustre-client-modules-` *ver*package being installed. If not, a compatible kernel must be installed on the client before the Lustre client packages are installed.", "mimetype": "text/plain", "start_char_idx": 6983, "end_char_idx": 11204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f01f2a99-fd5b-4648-aa8e-4f0c579664d9": {"__data__": {"id_": "f01f2a99-fd5b-4648-aa8e-4f0c579664d9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1fc8ea6f4b26b94e01c4665ea641603c25ea2c18272ca4d6c7b8b617cb6ed9d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d8ab82b-4a62-4e06-9299-c7661a4834a2", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "71d026d26710a55140af7ae66a5b30efdebcd933fc164fd98366487952293961", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Reboot the server.\n\n   5. Repeat these steps on each Lustre server.\n\n5. Download the Lustre client RPMs for your platform from the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases)repository. See [Table 6, \u201cPackages Installed on Lustre Clients\u201d](#table-6-packages-installed-on-lustre-clients)for a list of required packages.\n\n6. Install the Lustre client packages on all Lustre clients.\n\n   ### Note\n\n   The version of the kernel running on a Lustre client must be the same as the version of the `lustre-client-modules-` *ver*package being installed. If not, a compatible kernel must be installed on the client before the Lustre client packages are installed.\n\n   1. Log onto a Lustre client as the root user.\n\n   2. Use the `yum` command to install the packages:\n\n      \n\n      ```\n      # yum --nogpgcheck install pkg1.rpm pkg2.rpm ...\n      ```\n\n      \n\n   3. Verify the packages were installed correctly:\n\n      \n\n      ```\n      # rpm -qa|egrep \"lustre|kernel\"|sort\n      ```\n\n      \n\n   4. Reboot the client.\n\n   5. Repeat these steps on each Lustre client.\n\nTo configure LNet, go to [*Configuring Lustre Networking (LNet)*](02.06-Configuring%20Lustre%20Networking%20(LNet).md). If default settings will be used for LNet, go to [*Configuring a Lustre File System*](02.07-Configuring%20a%20Lustre%20File%20System.md).", "mimetype": "text/plain", "start_char_idx": 10521, "end_char_idx": 11867, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ff5ddcc-8cb9-4cc3-bb88-ae1986126506": {"__data__": {"id_": "3ff5ddcc-8cb9-4cc3-bb88-ae1986126506", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8466c53-c24f-4e28-a96d-352cdaa4595e", "node_type": "1", "metadata": {}, "hash": "d70bae4e2f9167a19db125daca4d6612a8399eb12775a5e6c485ada3dacdea2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Configuring Lustre Networking (LNet)\n\n**Table of Contents**\n\n- [Configuring LNet via `lnetctl`](#configuring-lnet-via-lnetctl)L 2.7\n  * [Configuring LNet](#configuring-lnet)\n  * [Displaying Global Settings](#displaying-global-settings)\n  * [Adding, Deleting and Showing Networks](#adding-deleting-and-showing-networks)\n  * [Manual Adding, Deleting and Showing Peers](#manual-adding-deleting-and-showing-peers)L 2.10\n  * [Dynamic Peer Discovery](#dynamic-peer-discovery)L 2.11\n    + [Overview](#overview)\n    + [Protocol](#protocol)\n    + [Dynamic Discovery and User-space Configuration](#dynamic-discovery-and-user-space-configuration)\n    + [Configuration](#configuration)\n    + [Initiating Dynamic Discovery on Demand](#initiating-dynamic-discovery-on-demand)\n  * [Adding, Deleting and Showing routes](#adding-deleting-and-showing-routes)\n  * [Enabling and Disabling Routing](#enabling-and-disabling-routing)\n  * [Showing routing information](#showing-routing-information)\n  * [Configuring Routing Buffers](#configuring-routing-buffers)\n  * [Asymmetrical Routes](#asymmetrical-routes)L 2.13\n    + [Overview](#overview-1)\n    + [Configuration](#configuration-1)\n  * [Importing YAML Configuration File](#importing-yaml-configuration-file)\n  * [Exporting Configuration in YAML format](#exporting-configuration-in-yaml-format)\n  * [Showing LNet Traffic Statistics](#showing-lnet-traffic-statistics)\n  * [YAML Syntax](#yaml-syntax)\n    + [Network Configuration](#network-configuration)\n    + [Enable Routing and Adjust Router Buffer Configuration](#enable-routing-and-adjust-router-buffer-configuration)\n    + [Show Statistics](#show-statistics)\n    + [Route Configuration](#route-configuration)\n- [Overview of LNet Module Parameters](#overview-of-lnet-module-parameters)\n  * [Using a Lustre Network Identifier (NID) to Identify a Node](#using-a-lustre-network-identifier-nid-to-identify-a-node)\n- [Setting the LNet Module networks Parameter](#setting-the-lnet-module-networks-parameter)\n  * [Multihome Server Example](#multihome-server-example)\n- [Setting the LNet Module ip2nets Parameter](#setting-the-lnet-module-ip2nets-parameter)\n- [Setting the LNet Module routes Parameter](#setting-the-lnet-module-routes-parameter)\n  * [Routing Example](#routing-example)\n- [Testing the LNet Configuration](#testing-the-lnet-configuration)\n- [Configuring the Router Checker](#configuring-the-router-checker)\n- [Best Practices for LNet Options](#best-practices-for-lnet-options)\n  * [Escaping commas with quotes](#escaping-commas-with-quotes)\n  * [Including comments](#including-comments)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d8466c53-c24f-4e28-a96d-352cdaa4595e": {"__data__": {"id_": "d8466c53-c24f-4e28-a96d-352cdaa4595e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ff5ddcc-8cb9-4cc3-bb88-ae1986126506", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "609b079ca18d31bb4b3c935c0bd59303648db51218f6c2e4faf74daf15f01ea2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efcb45fd-4838-4ab9-925e-deb45eeec8ca", "node_type": "1", "metadata": {}, "hash": "4898133b1b08826420f191a5128fe438887489f6674f984db3df1f44edab0393", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This chapter describes how to configure Lustre Networking (LNet). It includes the following sections:\n\n- [the section called \u201cConfiguring LNet via `lnetctl`\u201d](#configuring-lnet-via-lnetctl)\n- [the section called \u201c Overview of LNet Module Parameters\u201d](#overview-of-lnet-module-parameters)\n- [the section called \u201cSetting the LNet Module networks Parameter\u201d](#setting-the-lnet-module-networks-parameter)\n- [the section called \u201cSetting the LNet Module ip2nets Parameter\u201d](#setting-the-lnet-module-ip2nets-parameter)\n- [the section called \u201cSetting the LNet Module routes Parameter\u201d](#setting-the-lnet-module-routes-parameter)\n- [the section called \u201cTesting the LNet Configuration\u201d](#testing-the-lnet-configuration)\n- [the section called \u201cConfiguring the Router Checker\u201d](#configuring-the-router-checker)\n- [the section called \u201cBest Practices for LNet Options\u201d](#best-practices-for-lnet-options)\n\n### Note\n\nConfiguring LNet is optional.\n\nLNet will use the first TCP/IP interface it discovers on a system (`eth0`) if it's loaded using the `lctl network up`. If this network configuration is sufficient, you do not need to configure LNet. LNet configuration is required if you are using Infiniband or multiple Ethernet interfaces.\n\nIntroduced in Lustre 2.7The `lnetctl` utility can be used to initialize LNet without bringing up any network interfaces. Network interfaces can be added after configuring LNet via `lnetctl`. `lnetctl` can also be used to manage an operational LNet. However, if it wasn't initialized by `lnetctl` then `lnetctl lnet configure` must be invoked before `lnetctl` can be used to manage LNet.\n\nIntroduced in Lustre 2.7DLC also introduces a C-API to enable configuring LNet programatically. See [*LNet Configuration C-API*](06.08-LNet%20Configuration%20C-API.md)\n\n\n\nIntroduced in Lustre 2.7\n\n## Configuring LNet via `lnetctl`\n\nThe `lnetctl` utility can be used to initialize and configure the LNet kernel module after it has been loaded via `modprobe`. In general the lnetctl format is as follows:\n\n```\nlnetctl cmd subcmd [options]\n```\n\nThe following configuration items are managed by the tool:\n\n- Configuring/unconfiguring LNet\n- Adding/removing/showing Networks\n- Adding/removing/showing Routes\n- Enabling/Disabling routing\n- Configuring Router Buffer Pools\n\n### Configuring LNet\n\nAfter LNet has been loaded via `modprobe`, `lnetctl` utility can be used to configure LNet without bringing up networks which are specified in the module parameters. It can also be used to configure network interfaces specified in the module prameters by providing the `--all` option.\n\n```\nlnetctl lnet configure [--all]\n# --all: load NI configuration from module parameters\n```\n\nThe `lnetctl` utility can also be used to unconfigure LNet.\n\n```\nlnetctl lnet unconfigure\n```\n\n### Displaying Global Settings\n\nThe active LNet global settings can be displayed using the `lnetctl` command shown below:\n\n```\nlnetctl global show\n```\n\nFor example:\n\n```\n# lnetctl global show\n        global:\n        numa_range: 0\n        max_intf: 200\n        discovery: 1\n        drop_asym_route: 0\n```\n\n### Adding, Deleting and Showing Networks\n\nNetworks can be added, deleted, or shown after the LNet kernel module is loaded.", "mimetype": "text/plain", "start_char_idx": 2583, "end_char_idx": 5787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "efcb45fd-4838-4ab9-925e-deb45eeec8ca": {"__data__": {"id_": "efcb45fd-4838-4ab9-925e-deb45eeec8ca", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8466c53-c24f-4e28-a96d-352cdaa4595e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b568da7a64f835b2e25a661209383c178d39485a3260ef54124814a774ac1ffe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cdbcef2-c1ef-4d54-be5b-075258d9e220", "node_type": "1", "metadata": {}, "hash": "d9aa727f18a334398024c2deab743dca670665d5a19dc453095274fa43312c57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It can also be used to configure network interfaces specified in the module prameters by providing the `--all` option.\n\n```\nlnetctl lnet configure [--all]\n# --all: load NI configuration from module parameters\n```\n\nThe `lnetctl` utility can also be used to unconfigure LNet.\n\n```\nlnetctl lnet unconfigure\n```\n\n### Displaying Global Settings\n\nThe active LNet global settings can be displayed using the `lnetctl` command shown below:\n\n```\nlnetctl global show\n```\n\nFor example:\n\n```\n# lnetctl global show\n        global:\n        numa_range: 0\n        max_intf: 200\n        discovery: 1\n        drop_asym_route: 0\n```\n\n### Adding, Deleting and Showing Networks\n\nNetworks can be added, deleted, or shown after the LNet kernel module is loaded.\n\nThe **lnetctl net add** command is used to add networks:\n\n```\nlnetctl net add: add a network\n        --net: net name (ex tcp0)\n        --if: physical interface (ex eth0)\n        --peer_timeout: time to wait before declaring a peer dead\n        --peer_credits: defines the max number of inflight messages\n        --peer_buffer_credits: the number of buffer credits per peer\n        --credits: Network Interface credits\n        --cpts: CPU Partitions configured net uses\n        --help: display this help text\n\nExample:\nlnetctl net add --net tcp2 --if eth0\n                --peer_timeout 180 --peer_credits 8\n```\n\nIntroduced in Lustre 2.10NoteWith the addition of Software based Multi-Rail in Lustre 2.10, the following should be noted:--net: no longer needs to be unique since multiple interfaces can be added to the same network.--if: The same interface per network can be added only once, however, more than one interface can now be specified (separated by a comma) for a node. For example: eth0,eth1,eth2.For examples on adding multiple interfaces via `lnetctl net add`and/or YAML, please see [the section called \u201cConfiguring Multi-Rail\u201d](03.05-LNet%20Software%20Multi-Rail%202.10.md)\n\nNetworks can be deleted with the **lnetctl net del** command:\n\n```\nnet del: delete a network\n        --net: net name (ex tcp0)\n        --if:  physical inerface (e.g. eth0)\n\nExample:\nlnetctl net del --net tcp2\n```\n\nIntroduced in Lustre 2.10NoteIn a Software Multi-Rail configuration, specifying only the `--net`argument will delete the entire network and all interfaces under it. The new `--if` switch should also be used in conjunction with `--net` to specify deletion of a specific interface.\n\nAll or a subset of the configured networks can be shown with the **lnetctl net show**command. The output can be non-verbose or verbose.\n\n```\nnet show: show networks\n        --net: net name (ex tcp0) to filter on\n        --verbose: display detailed output per network\n\nExamples:\nlnetctl net show\nlnetctl net show --verbose\nlnetctl net show --net tcp2 --verbose\n```\n\nBelow are examples of non-detailed and detailed network configuration show.\n\n```\n# non-detailed show\n> lnetctl net show --net tcp2\nnet:\n    - nid: 192.168.205.130@tcp2\n      status: up\n      interfaces:\n          0: eth3\n\n# detailed show\n> lnetctl net show --net tcp2 --verbose\nnet:\n    - nid: 192.168.205.130@tcp2\n      status: up\n      interfaces:\n          0: eth3\n      tunables:\n          peer_timeout: 180\n          peer_credits: 8\n          peer_buffer_credits: 0\n          credits: 256\n```\n\nIntroduced in Lustre 2.10\n\n### Manual Adding, Deleting and Showing Peers\n\nThe **lnetctl peer add** command is used to manually add a remote peer to a software multi-rail configuration. For the dynamic peer discovery capability introduced in Lustre Release 2.11.0, please see [the section called \u201cDynamic Peer Discovery\u201d](02.06-Configuring%20Lustre%20Networking%20(LNet).md#dynamic-peer-discovery).", "mimetype": "text/plain", "start_char_idx": 5050, "end_char_idx": 8733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0cdbcef2-c1ef-4d54-be5b-075258d9e220": {"__data__": {"id_": "0cdbcef2-c1ef-4d54-be5b-075258d9e220", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efcb45fd-4838-4ab9-925e-deb45eeec8ca", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47c375b2cab0db09ebcf4a8d61d6afa3461ed27f6a7c168851fb007105d5d266", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd807025-95a4-4292-8c9f-949027f31ad9", "node_type": "1", "metadata": {}, "hash": "6d434325f497533851ff03eb750cda8571ff68a12919d9df9f3655a9c354cf8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the dynamic peer discovery capability introduced in Lustre Release 2.11.0, please see [the section called \u201cDynamic Peer Discovery\u201d](02.06-Configuring%20Lustre%20Networking%20(LNet).md#dynamic-peer-discovery).\n\nWhen configuring peers, use the `\u2013-prim_nid` option to specify the key or primary nid of the peer node. Then follow that with the `--nid` option to specify a set of comma separated NIDs.\n\n```\npeer add: add a peer\n            --prim_nid: primary NID of the peer\n            --nid: comma separated list of peer nids (e.g. 10.1.1.2@tcp0)\n            --non_mr: if specified this interface is created as a non mulit-rail\n            capable peer. Only one NID can be specified in this case.\n```\n\nFor example:\n\n```\n            lnetctl peer add --prim_nid 10.10.10.2@tcp --nid 10.10.3.3@tcp1,10.4.4.5@tcp2\n        \n```\n\nThe `--prim-nid` (primary nid for the peer node) can go unspecified. In this case, the first listed NID in the `--nid` option becomes the primary nid of the peer. For example:\n\n```\n            lnetctl peer_add --nid 10.10.10.2@tcp,10.10.3.3@tcp1,10.4.4.5@tcp2\n```\n\nYAML can also be used to configure peers:\n\n```\npeer:\n            - primary nid: <key or primary nid>\n            Multi-Rail: True\n            peer ni:\n            - nid: <nid 1>\n            - nid: <nid 2>\n            - nid: <nid n>\n```\n\nAs with all other commands, the result of the `lnetctl peer show` command can be used to gather information to aid in configuring or deleting a peer:\n\n```\nlnetctl peer show -v\n```\n\nExample output from the `lnetctl peer show` command:\n\n```\npeer:\n            - primary nid: 192.168.122.218@tcp\n            Multi-Rail: True\n            peer ni:\n            - nid: 192.168.122.218@tcp\n            state: NA\n            max_ni_tx_credits: 8\n            available_tx_credits: 8\n            available_rtr_credits: 8\n            min_rtr_credits: -1\n            tx_q_num_of_buf: 0\n            send_count: 6819\n            recv_count: 6264\n            drop_count: 0\n            refcount: 1\n            - nid: 192.168.122.78@tcp\n            state: NA\n            max_ni_tx_credits: 8\n            available_tx_credits: 8\n            available_rtr_credits: 8\n            min_rtr_credits: -1\n            tx_q_num_of_buf: 0\n            send_count: 7061\n            recv_count: 6273\n            drop_count: 0\n            refcount: 1\n            - nid: 192.168.122.96@tcp\n            state: NA\n            max_ni_tx_credits: 8\n            available_tx_credits: 8\n            available_rtr_credits: 8\n            min_rtr_credits: -1\n            tx_q_num_of_buf: 0\n            send_count: 6939\n            recv_count: 6286\n            drop_count: 0\n            refcount: 1\n```\n\nUse the following `lnetctl` command to delete a peer:\n\n```\npeer del: delete a peer\n            --prim_nid: Primary NID of the peer\n            --nid: comma separated list of peer nids (e.g. 10.1.1.2@tcp0)\n```\n\n`prim_nid` should always be specified. The `prim_nid` identifies the peer. If the`prim_nid` is the only one specified, then the entire peer is deleted.", "mimetype": "text/plain", "start_char_idx": 8521, "end_char_idx": 11570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd807025-95a4-4292-8c9f-949027f31ad9": {"__data__": {"id_": "fd807025-95a4-4292-8c9f-949027f31ad9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cdbcef2-c1ef-4d54-be5b-075258d9e220", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ac1efbb986d0c9cc7e70182a3d6d80e7e54b53475a5cfd6f4ea2f85331fd39f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f286cdc6-54f9-458d-857e-071b1c754f2f", "node_type": "1", "metadata": {}, "hash": "e3c8803fe08fa323fae34426678a113aef4fc554ecab1c288d1bb7e2168a0fc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.1.1.2@tcp0)\n```\n\n`prim_nid` should always be specified. The `prim_nid` identifies the peer. If the`prim_nid` is the only one specified, then the entire peer is deleted.\n\nExample of deleting a single nid of a peer (10.10.10.3@tcp):\n\n```\nlnetctl peer del --prim_nid 10.10.10.2@tcp --nid 10.10.10.3@tcp\n```\n\nExample of deleting the entire peer:\n\n```\nlnetctl peer del --prim_nid 10.10.10.2@tcp\n```\n\nIntroduced in Lustre 2.11\n\n### Dynamic Peer Discovery\n\n#### Overview\n\nDynamic Discovery (DD) is a feature that allows nodes to dynamically discover a peer's interfaces without having to explicitly configure them. This is very useful for Multi-Rail (MR) configurations. In large clusters, there could be hundreds of nodes and having to configure MR peers on each node becomes error prone. Dynamic Discovery is enabled by default and uses a new protocol based on LNet pings to discover the interfaces of the remote peers on first message.\n\n#### Protocol\n\nWhen LNet on a node is requested to send a message to a peer it first attempts to ping the peer. The reply to the ping contains the peer's NIDs as well as a feature bit outlining what the peer supports. Dynamic Discovery adds a Multi-Rail feature bit. If the peer is Multi-Rail capable, it sets the MR bit in the ping reply. When the node receives the reply it checks the MR bit, and if it is set it then pushes its own list of NIDs to the peer using a new PUT message, referred to as a \"push ping\". After this brief protocol, both the peer and the node will have each other's list of interfaces. The MR algorithm can then proceed to use the list of interfaces of the corresponding peer.\n\nIf the peer is not MR capable, it will not set the MR feature bit in the ping reply. The node will understand that the peer is not MR capable and will only use the interface provided by upper layers for sending messages.\n\n#### Dynamic Discovery and User-space Configuration\n\nIt is possible to configure the peer manually while Dynamic Discovery is running. Manual peer configuration always takes precedence over Dynamic Discovery. If there is a discrepancy between the manual configuration and the dynamically discovered information, a warning is printed.\n\n#### Configuration\n\nDynamic Discovery is very light on the configuration side. It can only be turned on or turned off. To turn the feature on or off, the following command is used:\n\n```\nlnetctl set discovery [0 | 1]\n```\n\nTo check the current `discovery` setting, the `lnetctl global show` command can be used as shown in [the section called \u201cDisplaying Global Settings\u201d](02.06-Configuring%20Lustre%20Networking%20(LNet).md#displaying-global-settings).\n\n#### Initiating Dynamic Discovery on Demand\n\nIt is possible to initiate the Dynamic Discovery protocol on demand without having to wait for a message to be sent to the peer. This can be done with the following command:\n\n```\nlnetctl discover <peer_nid> [<peer_nid> ...]\n```\n\n### Adding, Deleting and Showing routes\n\nA set of routes can be added to identify how LNet messages are to be routed.\n\n```\nlnetctl route add: add a route\n        --net: net name (ex tcp0) LNet message is destined to.\n               The can not be a local network.\n        --gateway: gateway node nid (ex 10.1.1.2@tcp) to route\n                   all LNet messaged destined for the identified\n                   network\n        --hop: number of hops to final destination\n               (1 < hops < 255)\n        --priority: priority of route (0 - highest prio)\n\nExample:\nlnetctl route add --net tcp2 --gateway 192.168.205.130@tcp1 --hop 2 --prio 1\n```\n\nRoutes can be deleted via the following `lnetctl` command.\n\n```\nlnetctl route del: delete a route\n        --net: net name (ex tcp0)\n        --gateway: gateway nid (ex 10.1.1.2@tcp)\n\nExample:\nlnetctl route del --net tcp2 --gateway 192.168.205.130@tcp1\n```\n\nConfigured routes can be shown via the following `lnetctl` command.", "mimetype": "text/plain", "start_char_idx": 11399, "end_char_idx": 15297, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f286cdc6-54f9-458d-857e-071b1c754f2f": {"__data__": {"id_": "f286cdc6-54f9-458d-857e-071b1c754f2f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd807025-95a4-4292-8c9f-949027f31ad9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3c24e42ccd2f04cf660c5b69412ddcc344665f7839f28c8474a35a12ed9b80ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edb03e85-e1e6-4883-84c5-7f04c1f7d52e", "node_type": "1", "metadata": {}, "hash": "d57a4e47bb64119d57f6f3995fd6de5e048e705537cc6b226df3cccade5b18e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\nlnetctl route del: delete a route\n        --net: net name (ex tcp0)\n        --gateway: gateway nid (ex 10.1.1.2@tcp)\n\nExample:\nlnetctl route del --net tcp2 --gateway 192.168.205.130@tcp1\n```\n\nConfigured routes can be shown via the following `lnetctl` command.\n\n```\nlnetctl route show: show routes\n        --net: net name (ex tcp0) to filter on\n        --gateway: gateway nid (ex 10.1.1.2@tcp) to filter on\n        --hop: number of hops to final destination\n               (1 < hops < 255) to filter on\n        --priority: priority of route (0 - highest prio)\n                    to filter on\n        --verbose: display detailed output per route\n\nExamples:\n# non-detailed show\nlnetctl route show\n\n# detailed show\nlnetctl route show --verbose\n```\n\nWhen showing routes the `--verbose` option outputs more detailed information. All show and error output are in YAML format. Below are examples of both non-detailed and detailed route show output.\n\n```\n#Non-detailed output\n> lnetctl route show\nroute:\n    - net: tcp2\n      gateway: 192.168.205.130@tcp1\n\n#detailed output\n> lnetctl route show --verbose\nroute:\n    - net: tcp2\n      gateway: 192.168.205.130@tcp1\n      hop: 2\n      priority: 1\n      state: down\n```\n\n### Enabling and Disabling Routing\n\nWhen an LNet node is configured as a router it will route LNet messages not destined to itself. This feature can be enabled or disabled as follows.\n\n```\nlnetctl set routing [0 | 1]\n# 0 - disable routing feature\n# 1 - enable routing feature\n```\n\n### Showing routing information\n\nWhen routing is enabled on a node, the tiny, small and large routing buffers are allocated. See [the section called \u201c Tuning LNet Parameters\u201d](04.03-Tuning%20a%20Lustre%20File%20System.md#tuning-lnet-parameters) for more details on router buffers. This information can be shown as follows:\n\n```\nlnetctl routing show: show routing information\n\nExample:\nlnetctl routing show\n```\n\nAn example of the show output:\n\n```\n> lnetctl routing show\nrouting:\n    - cpt[0]:\n          tiny:\n              npages: 0\n              nbuffers: 2048\n              credits: 2048\n              mincredits: 2048\n          small:\n              npages: 1\n              nbuffers: 16384\n              credits: 16384\n              mincredits: 16384\n          large:\n              npages: 256\n              nbuffers: 1024\n              credits: 1024\n              mincredits: 1024\n    - enable: 1\n```\n\n### Configuring Routing Buffers\n\nThe routing buffers values configured specify the number of buffers in each of the tiny, small and large groups.\n\nIt is often desirable to configure the tiny, small and large routing buffers to some values other than the default. These values are global values, when set they are used by all configured CPU partitions. If routing is enabled then the values set take effect immediately. If a larger number of buffers is specified, then buffers are allocated to satisfy the configuration change. If fewer buffers are configured then the excess buffers are freed as they become unused. If routing is not set the values are not changed. The buffer values are reset to default if routing is turned off and on.\n\nThe `lnetctl` 'set' command can be used to set these buffer values. A VALUE greater than 0 will set the number of buffers accordingly. A VALUE of 0 will reset the number of buffers to system defaults.", "mimetype": "text/plain", "start_char_idx": 15034, "end_char_idx": 18373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "edb03e85-e1e6-4883-84c5-7f04c1f7d52e": {"__data__": {"id_": "edb03e85-e1e6-4883-84c5-7f04c1f7d52e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f286cdc6-54f9-458d-857e-071b1c754f2f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1fed6f58655be4327c95b5924921243d6695d8ef33eacaf61d50a98ec219c71c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f58d8f3c-6a5c-4be9-a51b-579b5193fbbd", "node_type": "1", "metadata": {}, "hash": "5366c235d899b8965aafa63279b07a45a9fa8da13beaea54291f58e17a972a1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is often desirable to configure the tiny, small and large routing buffers to some values other than the default. These values are global values, when set they are used by all configured CPU partitions. If routing is enabled then the values set take effect immediately. If a larger number of buffers is specified, then buffers are allocated to satisfy the configuration change. If fewer buffers are configured then the excess buffers are freed as they become unused. If routing is not set the values are not changed. The buffer values are reset to default if routing is turned off and on.\n\nThe `lnetctl` 'set' command can be used to set these buffer values. A VALUE greater than 0 will set the number of buffers accordingly. A VALUE of 0 will reset the number of buffers to system defaults.\n\n```\nset tiny_buffers:\n      set tiny routing buffers\n               VALUE must be greater than or equal to 0\n\nset small_buffers: set small routing buffers\n        VALUE must be greater than or equal to 0\n\nset large_buffers: set large routing buffers\n        VALUE must be greater than or equal to 0\n```\n\nUsage examples:\n\n```\n> lnetctl set tiny_buffers 4096\n> lnetctl set small_buffers 8192\n> lnetctl set large_buffers 2048\n```\n\nThe buffers can be set back to the default values as follows:\n\n```\n> lnetctl set tiny_buffers 0\n> lnetctl set small_buffers 0\n> lnetctl set large_buffers 0\n```\n\nIntroduced in Lustre 2.13\n\n### Asymmetrical Routes\n\n#### Overview\n\nAn asymmetrical route is when a message from a remote peer is coming through a router that is not known by this node to reach the remote peer.\n\nAsymmetrical routes can be an issue when debugging network, and allowing them also opens the door to attacks where hostile clients inject data to the servers.\n\nSo it is possible to activate a check in LNet, that will detect any asymmetrical route message and drop it.\n\n#### Configuration\n\nIn order to switch asymmetric route detection on or off, the following command is used:\n\n```\nlnetctl set drop_asym_route [0 | 1]\n```\n\nThis command works on a per-node basis. This means each node in a Lustre cluster can decide whether it accepts asymmetrical route messages.\n\nTo check the current `drop_asym_route` setting, the `lnetctl global show` command can be used as shown in [the section called \u201cDisplaying Global Settings\u201d](#displaying-global-settings).\n\nBy default, asymmetric route detection is off.\n\n### Importing YAML Configuration File\n\nConfiguration can be described in YAML format and can be fed into the `lnetctl`utility. The `lnetctl` utility parses the YAML file and performs the specified operation on all entities described there in. If no operation is defined in the command as shown below, the default operation is 'add'. The YAML syntax is described in a later section.\n\n```\nlnetctl import FILE.yaml\nlnetctl import < FILE.yaml\n```\n\nThe '`lnetctl` import' command provides three optional parameters to define the operation to be performed on the configuration items described in the YAML file.\n\n```\n# if no options are given to the command the \"add\" command is assumed\n              # by default.\nlnetctl import --add FILE.yaml\nlnetctl import --add < FILE.yaml\n\n# to delete all items described in the YAML file\nlnetctl import --del FILE.yaml\nlnetctl import --del < FILE.yaml\n\n# to show all items described in the YAML file\nlnetctl import --show FILE.yaml\nlnetctl import --show < FILE.yaml\n```\n\n### Exporting Configuration in YAML format\n\n`lnetctl` utility provides the 'export' command to dump current LNet configuration in YAML format\n\n```\nlnetctl export FILE.yaml\nlnetctl export > FILE.yaml\n```\n\n### Showing LNet Traffic Statistics\n\n`lnetctl` utility can dump the LNet traffic statistiscs as follows\n\n```\nlnetctl stats show\n```\n\n### YAML Syntax\n\nThe `lnetctl` utility can take in a YAML file describing the configuration items that need to be operated on and perform one of the following operations: add, delete or show on the items described there in.\n\nNet, routing and route YAML blocks are all defined as a YAML sequence, as shown in the following sections. The stats YAML block is a YAML object. Each sequence item can take a seq_no field. This seq_no field is returned in the error block. This allows the caller to associate the error with the item that caused the error.", "mimetype": "text/plain", "start_char_idx": 17581, "end_char_idx": 21863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f58d8f3c-6a5c-4be9-a51b-579b5193fbbd": {"__data__": {"id_": "f58d8f3c-6a5c-4be9-a51b-579b5193fbbd", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edb03e85-e1e6-4883-84c5-7f04c1f7d52e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e38d7a4d987c062bbeabac264bd6d81652366bec2b4151986b0f13f969aee8ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d2226d4-a3c0-4b0f-b6a8-80d6c7e5b9a7", "node_type": "1", "metadata": {}, "hash": "e8a5476c8ed5e280ea9e97d266f5ffc22d07c41aa1f7de8603988ae0255b633e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Net, routing and route YAML blocks are all defined as a YAML sequence, as shown in the following sections. The stats YAML block is a YAML object. Each sequence item can take a seq_no field. This seq_no field is returned in the error block. This allows the caller to associate the error with the item that caused the error. The `lnetctl`utilty does a best effort at configuring items defined in the YAML file. It does not stop processing the file at the first error.\n\nBelow is the YAML syntax describing the various configuration elements which can be operated on via DLC. Not all YAML elements are required for all operations (add/delete/show). The system ignores elements which are not pertinent to the requested operation.\n\n#### Network Configuration\n\n```\nnet:\n   - net: <network.  Ex: tcp or o2ib>\n     interfaces:\n         0: <physical interface>\n     detail: <This is only applicable for show command.  1 - output detailed info.  0 - basic output>\n     tunables:\n        peer_timeout: <Integer. Timeout before consider a peer dead>\n        peer_credits: <Integer. Transmit credits for a peer>\n        peer_buffer_credits: <Integer. Credits available for receiving messages>\n        credits: <Integer.  Network Interface credits>\n\tSMP: <An array of integers of the form: \"[x,y,...]\", where each\n\tinteger represents the CPT to associate the network interface\n\twith> seq_no: <integer.  Optional.  User generated, and is\n\tpassed back in the YAML error block>\n```\n\nBoth seq_no and detail fields do not appear in the show output.\n\n#### Enable Routing and Adjust Router Buffer Configuration\n\n```\nrouting:\n    - tiny: <Integer. Tiny buffers>\n      small: <Integer. Small buffers>\n      large: <Integer. Large buffers>\n      enable: <0 - disable routing.  1 - enable routing>\n      seq_no: <Integer.  Optional.  User generated, and is passed back in the YAML error block>\n```\n\nThe seq_no field does not appear in the show output\n\n#### Show Statistics\n\n```\nstatistics:\n    seq_no: <Integer. Optional.  User generated, and is passed back in the YAML error block>\n```\n\nThe seq_no field does not appear in the show output\n\n#### Route Configuration\n\n```\nroute:\n  - net: <network. Ex: tcp or o2ib>\n    gateway: <nid of the gateway in the form <ip>@<net>: Ex: 192.168.29.1@tcp>\n    hop: <an integer between 1 and 255. Optional>\n    detail: <This is only applicable for show commands.  1 - output detailed info.  0. basic output>\n    seq_no: <integer. Optional. User generated, and is passed back in the YAML error block>\n```\n\nBoth seq_no and detail fields do not appear in the show output.\n\n## Overview of LNet Module Parameters\n\nLNet kernel module (lnet) parameters specify how LNet is to be configured to work with Lustre, including which NICs will be configured to work with Lustre and the routing to be used with Lustre.\n\nParameters for LNet can be specified in the `/etc/modprobe.d/lustre.conf` file. In some cases the parameters may have been stored in `/etc/modprobe.conf`, but this has been deprecated since before RHEL5 and SLES10, and having a separate`/etc/modprobe.d/lustre.conf` file simplifies administration and distribution of the Lustre networking configuration. This file contains one or more entries with the syntax:\n\n```\noptions lnet parameter=value\n```\n\nTo specify the network interfaces that are to be used for Lustre, set either the `networks` parameter or the `ip2nets` parameter (only one of these parameters can be used at a time):\n\n- `networks` - Specifies the networks to be used.\n- `ip2nets` - Lists globally-available networks, each with a range of IP addresses. LNet then identifies locally-available networks through address list-matching lookup.\n\nSee [the section called \u201cSetting the LNet Module networks Parameter\u201d](#setting-the-lnet-module-networks-parameter) and [the section called \u201cSetting the LNet Module ip2nets Parameter\u201d](#setting-the-lnet-module-ip2nets-parameter) for more details.\n\nTo set up routing between networks, use:\n\n- `routes` - Lists networks and the NIDs of routers that forward to them.\n\nSee [the section called \u201cSetting the LNet Module routes Parameter\u201d](#setting-the-lnet-module-routes-parameter) for more details.", "mimetype": "text/plain", "start_char_idx": 21541, "end_char_idx": 25702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d2226d4-a3c0-4b0f-b6a8-80d6c7e5b9a7": {"__data__": {"id_": "1d2226d4-a3c0-4b0f-b6a8-80d6c7e5b9a7", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f58d8f3c-6a5c-4be9-a51b-579b5193fbbd", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "605265cd098747993fccb9a67bf262db8a49c354271e72816e6cb2742eb2e6e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e52c40f6-7d2f-4ae2-b851-16552b84da0b", "node_type": "1", "metadata": {}, "hash": "9aedaccd85bed020b0c074841e9ccb9c23bf6f759f47187c36b160cd99e289a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `ip2nets` - Lists globally-available networks, each with a range of IP addresses. LNet then identifies locally-available networks through address list-matching lookup.\n\nSee [the section called \u201cSetting the LNet Module networks Parameter\u201d](#setting-the-lnet-module-networks-parameter) and [the section called \u201cSetting the LNet Module ip2nets Parameter\u201d](#setting-the-lnet-module-ip2nets-parameter) for more details.\n\nTo set up routing between networks, use:\n\n- `routes` - Lists networks and the NIDs of routers that forward to them.\n\nSee [the section called \u201cSetting the LNet Module routes Parameter\u201d](#setting-the-lnet-module-routes-parameter) for more details.\n\nA `router` checker can be configured to enable Lustre nodes to detect router health status, avoid routers that appear dead, and reuse those that restore service after failures. See [the section called \u201cConfiguring the Router Checker\u201d](#configuring-the-router-checker) for more details.\n\nFor a complete reference to the LNet module parameters, see *Configuration Files and Module ParametersLNet Options*.\n\n### Note\n\nWe recommend that you use 'dotted-quad' notation for IP addresses rather than host names to make it easier to read debug logs and debug configurations with multiple interfaces.\n\n### Using a Lustre Network Identifier (NID) to Identify a Node\n\nA Lustre network identifier (NID) is used to uniquely identify a Lustre network endpoint by node ID and network type. The format of the NID is:\n\n```\nnetwork_id@network_type\n```\n\nExamples are:\n\n```\n10.67.73.200@tcp0\n10.67.75.100@o2ib\n```\n\nThe first entry above identifies a TCP/IP node, while the second entry identifies an InfiniBand node.\n\nWhen a mount command is run on a client, the client uses the NID of the MDS to retrieve configuration information. If an MDS has more than one NID, the client should use the appropriate NID for its local network.\n\nTo determine the appropriate NID to specify in the mount command, use the `lctl`command. To display MDS NIDs, run on the MDS :\n\n```\nlctl list_nids\n```\n\nTo determine if a client can reach the MDS using a particular NID, run on the client:\n\n```\nlctl which_nid MDS_NID\n```\n\n## Setting the LNet Module networks Parameter\n\nIf a node has more than one network interface, you'll typically want to dedicate a specific interface to Lustre. You can do this by including an entry in the `lustre.conf`file on the node that sets the LNet module `networks` parameter:\n\n```\noptions lnet networks=comma-separated list of\n    networks\n```\n\nThis example specifies that a Lustre node will use a TCP/IP interface and an InfiniBand interface:\n\n```\noptions lnet networks=tcp0(eth0),o2ib(ib0)\n```\n\nThis example specifies that the Lustre node will use the TCP/IP interface `eth1`:\n\n```\noptions lnet networks=tcp0(eth1)\n```\n\nDepending on the network design, it may be necessary to specify explicit interfaces. To explicitly specify that interface `eth2` be used for network `tcp0` and `eth3` be used for `tcp1` , use this entry:\n\n```\noptions lnet networks=tcp0(eth2),tcp1(eth3)\n```\n\nWhen more than one interface is available during the network setup, Lustre chooses the best route based on the hop count. Once the network connection is established, Lustre expects the network to stay connected. In a Lustre network, connections do not fail over to another interface, even if multiple interfaces are available on the same node.\n\n### Note\n\nLNet lines in `lustre.conf` are only used by the local node to determine what to call its interfaces. They are not used for routing decisions.\n\n### Multihome Server Example\n\nIf a server with multiple IP addresses (multihome server) is connected to a Lustre network, certain configuration setting are required. An example illustrating these setting consists of a network with the following nodes:\n\n- Server svr1 with three TCP NICs (`eth0`, `eth1`, and `eth2`) and an InfiniBand NIC.\n- Server svr2 with three TCP NICs (`eth0`, `eth1`, and `eth2`) and an InfiniBand NIC. Interface eth2 will not be used for Lustre networking.\n- TCP clients, each with a single TCP interface.", "mimetype": "text/plain", "start_char_idx": 25039, "end_char_idx": 29101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e52c40f6-7d2f-4ae2-b851-16552b84da0b": {"__data__": {"id_": "e52c40f6-7d2f-4ae2-b851-16552b84da0b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d2226d4-a3c0-4b0f-b6a8-80d6c7e5b9a7", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "656f855b399c0124b54bb3b111970b674a88220e299e4384731fb02a6a83ff7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47085eab-6f62-411e-b472-740c6000cb8f", "node_type": "1", "metadata": {}, "hash": "ae7c06d22d53c25ad9e7d76b8837a4174511982764f15426835192cf770a13eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In a Lustre network, connections do not fail over to another interface, even if multiple interfaces are available on the same node.\n\n### Note\n\nLNet lines in `lustre.conf` are only used by the local node to determine what to call its interfaces. They are not used for routing decisions.\n\n### Multihome Server Example\n\nIf a server with multiple IP addresses (multihome server) is connected to a Lustre network, certain configuration setting are required. An example illustrating these setting consists of a network with the following nodes:\n\n- Server svr1 with three TCP NICs (`eth0`, `eth1`, and `eth2`) and an InfiniBand NIC.\n- Server svr2 with three TCP NICs (`eth0`, `eth1`, and `eth2`) and an InfiniBand NIC. Interface eth2 will not be used for Lustre networking.\n- TCP clients, each with a single TCP interface.\n- InfiniBand clients, each with a single Infiniband interface and a TCP/IP interface for administration.\n\nTo set the `networks` option for this example:\n\n- On each server, `svr1` and `svr2`, include the following line in the `lustre.conf`file:\n\n```\noptions lnet networks=tcp0(eth0),tcp1(eth1),o2ib\n```\n\n- For TCP-only clients, the first available non-loopback IP interface is used for `tcp0`. Thus, TCP clients with only one interface do not need to have options defined in the `lustre.conf` file.\n- On the InfiniBand clients, include the following line in the `lustre.conf` file:\n\n```\noptions lnet networks=o2ib\n```\n\n### Note\n\nBy default, Lustre ignores the loopback (`lo0`) interface. Lustre does not ignore IP addresses aliased to the loopback. If you alias IP addresses to the loopback interface, you must specify all Lustre networks using the LNet networks parameter.\n\n### Note\n\nIf the server has multiple interfaces on the same subnet, the Linux kernel will send all traffic using the first configured interface. This is a limitation of Linux, not Lustre. In this case, network interface bonding should be used. For more information about network interface bonding, see [*Setting Up Network Interface Bonding*](02.04-Setting%20Up%20Network%20Interface%20Bonding.md).\n\n## Setting the LNet Module ip2nets Parameter\n\nThe `ip2nets` option is typically used when a single, universal `lustre.conf` file is run on all servers and clients. Each node identifies the locally available networks based on the listed IP address patterns that match the node's local IP addresses.\n\nNote that the IP address patterns listed in the `ip2nets` option are *only* used to identify the networks that an individual node should instantiate. They are *not* used by LNet for any other communications purpose.\n\nFor the example below, the nodes in the network have these IP addresses:\n\n- Server svr1: `eth0` IP address `192.168.0.2`, IP over Infiniband (`o2ib`) address`132.6.1.2`.\n- Server svr2: `eth0` IP address `192.168.0.4`, IP over Infiniband (`o2ib`) address`132.6.1.4`.\n- TCP clients have IP addresses `192.168.0.5-255.`\n- Infiniband clients have IP over Infiniband (`o2ib`) addresses `132.6.[2-3].2, .4, .6, .8`.\n\nThe following entry is placed in the `lustre.conf` file on each server and client:\n\n```\noptions lnet 'ip2nets=\"tcp0(eth0) 192.168.0.[2,4]; \\\ntcp0 192.168.0.*; o2ib0 132.6.[1-3].[2-8/2]\"'\n```\n\nEach entry in `ip2nets` is referred to as a 'rule'.\n\nThe order of LNet entries is important when configuring servers. If a server node can be reached using more than one network, the first network specified in `lustre.conf` will be used.\n\nBecause `svr1` and `svr2` match the first rule, LNet uses `eth0` for `tcp0` on those machines. (Although `svr1` and `svr2` also match the second rule, the first matching rule for a particular network is used).", "mimetype": "text/plain", "start_char_idx": 28286, "end_char_idx": 31943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "47085eab-6f62-411e-b472-740c6000cb8f": {"__data__": {"id_": "47085eab-6f62-411e-b472-740c6000cb8f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e52c40f6-7d2f-4ae2-b851-16552b84da0b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c71045ed056787deef6a747e67c69710ebe83bb84d1698db457203d254e9c4d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45c925c1-251d-4c47-a62e-234c7405e77a", "node_type": "1", "metadata": {}, "hash": "448c5456f5e3ef020e39820c6ce331168cb36d62c696fb7dea5775e786c9d5f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2,4]; \\\ntcp0 192.168.0.*; o2ib0 132.6.[1-3].[2-8/2]\"'\n```\n\nEach entry in `ip2nets` is referred to as a 'rule'.\n\nThe order of LNet entries is important when configuring servers. If a server node can be reached using more than one network, the first network specified in `lustre.conf` will be used.\n\nBecause `svr1` and `svr2` match the first rule, LNet uses `eth0` for `tcp0` on those machines. (Although `svr1` and `svr2` also match the second rule, the first matching rule for a particular network is used).\n\nThe `[2-8/2]` format indicates a range of 2-8 stepped by 2; that is 2,4,6,8. Thus, the clients at `132.6.3.5` will not find a matching o2ib network.\n\nIntroduced in Lustre 2.10NoteMulti-rail deprecates the kernel parsing of ip2nets. ip2nets patterns are matched in user space and translated into Network interfaces to be added into the system.The first interface that matches the IP pattern will be used when adding a network interface.If an interface is explicitly specified as well as a pattern, the interface matched using the IP pattern will be sanitized against the explicitly-defined interface.For example, `tcp(eth0) 192.168.*.3` and there exists in the system `eth0 == 192.158.19.3` and `eth1 == 192.168.3.3`, then the configuration will fail, because the pattern contradicts the interface specified.A clear warning will be displayed if inconsistent configuration is encountered.You could use the following command to configure ip2nets:`lnetctl import < ip2nets.yaml`For example:`ip2nets:   - net-spec: tcp1     interfaces:          0: eth0          1: eth1     ip-range:          0: 192.168.*.19          1: 192.168.100.105   - net-spec: tcp2     interfaces:          0: eth2     ip-range:          0: 192.168.*.*`\n\n## Setting the LNet Module routes Parameter\n\nThe LNet module routes parameter is used to identify routers in a Lustre configuration. These parameters are set in `modprobe.conf` on each Lustre node.\n\nRoutes are typically set to connect to segregated subnetworks or to cross connect two different types of networks such as tcp and o2ib\n\nThe LNet routes parameter specifies a colon-separated list of router definitions. Each route is defined as a network number, followed by a list of routers:\n\n```\nroutes=net_type router_NID(s)\n```\n\nThis example specifies bi-directional routing in which TCP clients can reach Lustre resources on the IB networks and IB servers can access the TCP networks:\n\n```\noptions lnet 'ip2nets=\"tcp0 192.168.0.*; \\\n  o2ib0(ib0) 132.6.1.[1-128]\"' 'routes=\"tcp0   132.6.1.[1-8]@o2ib0; \\\n  o2ib0 192.16.8.0.[1-8]@tcp0\"'\n```\n\nAll LNet routers that bridge two networks are equivalent. They are not configured as primary or secondary, and the load is balanced across all available routers.\n\nThe number of LNet routers is not limited. Enough routers should be used to handle the required file serving bandwidth plus a 25 percent margin for headroom.\n\n### Routing Example\n\nOn the clients, place the following entry in the `lustre.conf` file\n\n```\nlnet networks=\"tcp\" routes=\"o2ib0 192.168.0.[1-8]@tcp0\"\n```\n\nOn the router nodes, use:\n\n```\nlnet networks=\"tcp o2ib\" forwarding=enabled \n```\n\nOn the MDS, use the reverse as shown below:\n\n```\nlnet networks=\"o2ib0\" routes=\"tcp0 132.6.1.[1-8]@o2ib0\" \n```\n\nTo start the routers, run:\n\n```\nmodprobe lnet\nlctl network configure\n```\n\n## Testing the LNet Configuration\n\nAfter configuring Lustre Networking, it is highly recommended that you test your LNet configuration using the LNet Self-Test provided with the Lustre software.", "mimetype": "text/plain", "start_char_idx": 31435, "end_char_idx": 34949, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45c925c1-251d-4c47-a62e-234c7405e77a": {"__data__": {"id_": "45c925c1-251d-4c47-a62e-234c7405e77a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47085eab-6f62-411e-b472-740c6000cb8f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "00d5832c1c657f3cf91fdd20cfa52e641daeb4ed49034107c18ca58254560823", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8007cc36-c2f8-4633-a86c-143f8511294c", "node_type": "1", "metadata": {}, "hash": "41be54da549683573ed96a41daa1dcf79d36dbbd0d260c5accd9118d547d3815", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Routing Example\n\nOn the clients, place the following entry in the `lustre.conf` file\n\n```\nlnet networks=\"tcp\" routes=\"o2ib0 192.168.0.[1-8]@tcp0\"\n```\n\nOn the router nodes, use:\n\n```\nlnet networks=\"tcp o2ib\" forwarding=enabled \n```\n\nOn the MDS, use the reverse as shown below:\n\n```\nlnet networks=\"o2ib0\" routes=\"tcp0 132.6.1.[1-8]@o2ib0\" \n```\n\nTo start the routers, run:\n\n```\nmodprobe lnet\nlctl network configure\n```\n\n## Testing the LNet Configuration\n\nAfter configuring Lustre Networking, it is highly recommended that you test your LNet configuration using the LNet Self-Test provided with the Lustre software. For more information about using LNet Self-Test, see [*Testing Lustre Network Performance (LNet Self-Test)*](04.01-Testing%20Lustre%20Network%20Performance%20(LNet%20Self-Test).md).\n\n## Configuring the Router Checker\n\nIn a Lustre configuration in which different types of networks, such as a TCP/IP network and an Infiniband network, are connected by routers, a router checker can be run on the clients and servers in the routed configuration to monitor the status of the routers. In a multi-hop routing configuration, router checkers can be configured on routers to monitor the health of their next-hop routers.\n\nA router checker is configured by setting LNet parameters in `lustre.conf` by including an entry in this form:\n\n```\noptions lnet\n    router_checker_parameter=value\n```\n\nThe router checker parameters are:\n\n- `live_router_check_interval` - Specifies a time interval in seconds after which the router checker will ping the live routers. The default value is 0, meaning no checking is done. To set the value to 60, enter:\n\n  ```\n  options lnet live_router_check_interval=60\n  ```\n\n- `dead_router_check_interval` - Specifies a time interval in seconds after which the router checker will check for dead routers. The default value is 0, meaning no checking is done. To set the value to 60, enter:\n\n  ```\n  options lnet dead_router_check_interval=60\n  ```\n\n- auto_down - Enables/disables (1/0) the automatic marking of router state as up or down. The default value is 1. To disable router marking, enter:\n\n  ```\n  options lnet auto_down=0\n  ```\n\n- `router_ping_timeout` - Specifies a timeout for the router checker when it checks live or dead routers. The router checker sends a ping message to each dead or live router once every dead_router_check_interval or live_router_check_interval respectively. The default value is 50. To set the value to 60, enter:\n\n  ```\n  options lnet router_ping_timeout=60\n  ```\n\n  ### Note\n\n  The `router_ping_timeout` is consistent with the default LND timeouts. You may have to increase it on very large clusters if the LND timeout is also increased. For larger clusters, we suggest increasing the check interval.\n\n- `check_routers_before_use` - Specifies that routers are to be checked before use. Set to off by default. If this parameter is set to on, the dead_router_check_interval parameter must be given a positive integer value.\n\n  ```\n  options lnet check_routers_before_use=on\n  ```\n\nThe router checker obtains the following information from each router:\n\n- Time the router was disabled\n- Elapsed disable time\n\nIf the router checker does not get a reply message from the router within router_ping_timeout seconds, it considers the router to be down.\n\nIf a router is marked 'up' and responds to a ping, the timeout is reset.\n\nIf 100 packets have been sent successfully through a router, the sent-packets counter for that router will have a value of 100.\n\n## Best Practices for LNet Options\n\nFor the `networks`, `ip2nets`, and `routes` options, follow these best practices to avoid configuration errors.\n\n### Escaping commas with quotes\n\nDepending on the Linux distribution, commas may need to be escaped using single or double quotes. In the extreme case, the `options` entry would look like this:\n\n```\noptions\n      lnet'networks=\"tcp0,elan0\"'\n      'routes=\"tcp [2,10]@elan0\"'\n```\n\nAdded quotes may confuse some distributions.", "mimetype": "text/plain", "start_char_idx": 34334, "end_char_idx": 38325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8007cc36-c2f8-4633-a86c-143f8511294c": {"__data__": {"id_": "8007cc36-c2f8-4633-a86c-143f8511294c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f911bd0-c44f-44b5-b226-560182781bd4", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4707ebe30269f83473dc992032d61a56ad3f6f2dd0d6d250e91c562409fd32e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45c925c1-251d-4c47-a62e-234c7405e77a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "54f648e0086adb8a0ca6616cb8313dc3bbb43cf98d7d1e524b374b752a8622eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If a router is marked 'up' and responds to a ping, the timeout is reset.\n\nIf 100 packets have been sent successfully through a router, the sent-packets counter for that router will have a value of 100.\n\n## Best Practices for LNet Options\n\nFor the `networks`, `ip2nets`, and `routes` options, follow these best practices to avoid configuration errors.\n\n### Escaping commas with quotes\n\nDepending on the Linux distribution, commas may need to be escaped using single or double quotes. In the extreme case, the `options` entry would look like this:\n\n```\noptions\n      lnet'networks=\"tcp0,elan0\"'\n      'routes=\"tcp [2,10]@elan0\"'\n```\n\nAdded quotes may confuse some distributions. Messages such as the following may indicate an issue related to added quotes:\n\n```\nlnet: Unknown parameter 'networks'\n```\n\nA `'Refusing connection - no matching NID'` message generally points to an error in the LNet module configuration.\n\n### Including comments\n\n*Place the semicolon terminating a comment immediately after the comment.* LNet silently ignores everything between the `#` character at the beginning of the comment and the next semicolon.\n\nIn this *incorrect* example, LNet silently ignores `pt11 192.168.0.[92,96]`, resulting in these nodes not being properly initialized. No error message is generated.\n\n```\noptions lnet ip2nets=\"pt10 192.168.0.[89,93]; # comment\n      with semicolon BEFORE comment \\ pt11 192.168.0.[92,96];\n```\n\nThis correct example shows the required syntax:\n\n```\noptions lnet ip2nets=\"pt10 192.168.0.[89,93] \\\n# comment with semicolon AFTER comment; \\\npt11 192.168.0.[92,96] # comment\n```\n\nDo not add an excessive number of comments. The Linux kernel limits the length of character strings used in module options (usually to 1KB, but this may differ between vendor kernels). If you exceed this limit, errors result and the specified configuration may not be processed correctly.", "mimetype": "text/plain", "start_char_idx": 37649, "end_char_idx": 39541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45c0fee2-cc78-40e7-af23-53ec99edf098": {"__data__": {"id_": "45c0fee2-cc78-40e7-af23-53ec99edf098", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7d3a5b59a7eaa2e9f311ca24344bc5f1aa2b36ca75c1e1f2b30b9e13313230d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34e65364-5276-4335-ba80-7937a322d9e9", "node_type": "1", "metadata": {}, "hash": "36ef18bed3037b5e81b9737e74bb55af2527d7abfb73df90d34ab2cc60efff94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Configuring a Lustre File System\n\n**Table of Contents**\n\n- [Configuring a Lustre File System](#configuring-a-lustre-file-system)\n  * [Configuring a Simple Lustre File System](#configuring-a-simple-lustre-file-system)\n    + [Simple Lustre Configuration Example](#simple-lustre-configuration-example)\n  * [Additional Configuration Options](#additional-configuration-options)\n    + [Scaling the Lustre File System](#scaling-the-lustre-file-system)\n    + [Changing Striping Defaults](#changing-striping-defaults)\n    + [Using the Lustre Configuration Utilities](#using-the-lustre-configuration-utilities)\n\nThis chapter shows how to configure a simple Lustre file system comprised of a combined MGS/MDT, an OST and a client. It includes:\n\n- [the section called \u201c Configuring a Simple Lustre File System\u201d](#configuring-a-simple-lustre-file-system)\n- [the section called \u201c Additional Configuration Options\u201d](#additional-configuration-options)\n\n## Configuring a Simple Lustre File System\n\nA Lustre file system can be set up in a variety of configurations by using the administrative utilities provided with the Lustre software. The procedure below shows how to configure a simple Lustre file system consisting of a combined MGS/MDS, one OSS with two OSTs, and a client. For an overview of the entire Lustre installation procedure, see [*Installation Overview*](02.01-Installation%20Overview.md).\n\nThis configuration procedure assumes you have completed the following:\n\n- **\\*Set up and configured your hardware*** . For more information about hardware requirements, see [*Determining Hardware Configuration Requirements and Formatting Options*](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md).\n- **Downloaded and installed the Lustre software.**For more information about preparing for and installing the Lustre software, see [*Installing the Lustre Software*](02.05-Installing%20the%20Lustre%20Software.md).\n\nThe following optional steps should also be completed, if needed, before the Lustre software is configured:\n\n- *Set up a hardware or software RAID on block devices to be used as OSTs or MDTs.*For information about setting up RAID, see the documentation for your RAID controller or [*Configuring Storage on a Lustre File System*](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md).\n- *Set up network interface bonding on Ethernet interfaces.*For information about setting up network interface bonding, see [*Setting Up Network Interface Bonding*](02.04-Setting%20Up%20Network%20Interface%20Bonding.md).\n- *Set* lnet *module parameters to specify how Lustre Networking (LNet) is to be configured to work with a Lustre file system and test the LNet configuration.*LNet will, by default, use the first TCP/IP interface it discovers on a system. If this network configuration is sufficient, you do not need to configure LNet. LNet configuration is required if you are using InfiniBand or multiple Ethernet interfaces.\n\nFor information about configuring LNet, see [*Configuring Lustre Networking (LNet)*](02.06-Configuring%20Lustre%20Networking%20(LNet).md). For information about testing LNet, see [*Testing Lustre Network Performance (LNet Self-Test)*](04.01-Testing%20Lustre%20Network%20Performance%20(LNet%20Self-Test).md).\n\n- *Run the benchmark script sgpdd-survey to determine baseline performance of your hardware.*Benchmarking your hardware will simplify debugging performance issues that are unrelated to the Lustre software and ensure you are getting the best possible performance with your installation. For information about running `sgpdd-survey`, see [*Benchmarking Lustre File System Performance (Lustre I/O Kit)*](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md).\n\n### Note\n\nThe `sgpdd-survey` script overwrites the device being tested so it must be run before the OSTs are configured.\n\nTo configure a simple Lustre file system, complete these steps:\n\n1. Create a combined MGS/MDT file system on a block device.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4020, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34e65364-5276-4335-ba80-7937a322d9e9": {"__data__": {"id_": "34e65364-5276-4335-ba80-7937a322d9e9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7d3a5b59a7eaa2e9f311ca24344bc5f1aa2b36ca75c1e1f2b30b9e13313230d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45c0fee2-cc78-40e7-af23-53ec99edf098", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9376ae0bb3de5b9d93c4b06aa0169e24a589298c73225181cbee2c59a6055201", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "688b7b6e-0c00-4fa6-88d6-dcf3fbf89bb1", "node_type": "1", "metadata": {}, "hash": "2bd5b6dc570b5a4c628e81bd9d526b5fe0b531b1f87ca4f2dba9b2a76c8bed56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- *Run the benchmark script sgpdd-survey to determine baseline performance of your hardware.*Benchmarking your hardware will simplify debugging performance issues that are unrelated to the Lustre software and ensure you are getting the best possible performance with your installation. For information about running `sgpdd-survey`, see [*Benchmarking Lustre File System Performance (Lustre I/O Kit)*](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md).\n\n### Note\n\nThe `sgpdd-survey` script overwrites the device being tested so it must be run before the OSTs are configured.\n\nTo configure a simple Lustre file system, complete these steps:\n\n1. Create a combined MGS/MDT file system on a block device. On the MDS node, run:\n\n   ```\n   mkfs.lustre --fsname=\n   fsname --mgs --mdt --index=0 \n   /dev/block_device\n   ```\n\n   The default file system name ( `fsname`) is `lustre`.\n\n   ### Note\n\n   If you plan to create multiple file systems, the MGS should be created separately on its own dedicated block device, by running:\n\n   ```\n   mkfs.lustre --fsname=\n   fsname --mgs \n   /dev/block_device\n   ```\n\n   See [the section called \u201c Running Multiple Lustre File Systems\u201d](03.02-Lustre%20Operations.md#running-multiple-lustre-file-systems)for more details.\n\n2. Optionally add in additional MDTs.\n\n   ```\n   mkfs.lustre --fsname=\n   fsname --mgsnode=\n   nid --mdt --index=1 \n   /dev/block_device\n   ```\n\n   ### Note\n\n   Up to 4095 additional MDTs can be added.\n\n3. Mount the combined MGS/MDT file system on the block device. On the MDS node, run:\n\n   ```\n   mount -t lustre \n   /dev/block_device \n   /mount_point\n   ```\n\n   ### Note\n\n   If you have created an MGS and an MDT on separate block devices, mount them both.\n\n4. Create the OST. On the OSS node, run:\n\n   ```\n   mkfs.lustre --fsname=\n   fsname --mgsnode=\n   MGS_NID --ost --index=\n   OST_index \n   /dev/block_device\n   ```\n\n   When you create an OST, you are formatting a `ldiskfs` or `ZFS` file system on a block storage device like you would with any local file system.\n\n   You can have as many OSTs per OSS as the hardware or drivers allow. For more information about storage and memory requirements for a Lustre file system, see [*Determining Hardware Configuration Requirements and Formatting Options*](c02-02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md).\n\n   You can only configure one OST per block device. You should create an OST that uses the raw block device and does not use partitioning.\n\n   You should specify the OST index number at format time in order to simplify translating the OST number in error messages or file striping to the OSS node and block device later on.\n\n   If you are using block devices that are accessible from multiple OSS nodes, ensure that you mount the OSTs from only one OSS node at at time. It is strongly recommended that multiple-mount protection be enabled for such devices to prevent serious data corruption. For more information about multiple-mount protection, see [*Lustre File System Failover and Multiple-Mount Protection*](03.13-Lustre%20File%20System%20Failover%20and%20Multiple-Mount%20Protection.md).\n\n   ### Note\n\n   The Lustre software currently supports block devices up to 128 TB on Red Hat Enterprise Linux 5 and 6 (up to 8 TB on other distributions). If the device size is only slightly larger that 16 TB, it is recommended that you limit the file system size to 16 TB at format time. We recommend that you not place DOS partitions on top of RAID 5/6 block devices due to negative impacts on performance, but instead format the whole disk for the file system.\n\n5. Mount the OST.", "mimetype": "text/plain", "start_char_idx": 3287, "end_char_idx": 6954, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "688b7b6e-0c00-4fa6-88d6-dcf3fbf89bb1": {"__data__": {"id_": "688b7b6e-0c00-4fa6-88d6-dcf3fbf89bb1", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7d3a5b59a7eaa2e9f311ca24344bc5f1aa2b36ca75c1e1f2b30b9e13313230d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34e65364-5276-4335-ba80-7937a322d9e9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2cdbc7189c6b2331ed0eda1a39cc8e0cb2ddb6779ff251d282ac3ee026894586", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3de7168f-247a-4265-9390-1eaf79e5baf3", "node_type": "1", "metadata": {}, "hash": "733f330e4320fbaba56fb70bf5d324f3bc0989bbd393245baed5ef0c981c5ae9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is strongly recommended that multiple-mount protection be enabled for such devices to prevent serious data corruption. For more information about multiple-mount protection, see [*Lustre File System Failover and Multiple-Mount Protection*](03.13-Lustre%20File%20System%20Failover%20and%20Multiple-Mount%20Protection.md).\n\n   ### Note\n\n   The Lustre software currently supports block devices up to 128 TB on Red Hat Enterprise Linux 5 and 6 (up to 8 TB on other distributions). If the device size is only slightly larger that 16 TB, it is recommended that you limit the file system size to 16 TB at format time. We recommend that you not place DOS partitions on top of RAID 5/6 block devices due to negative impacts on performance, but instead format the whole disk for the file system.\n\n5. Mount the OST. On the OSS node where the OST was created, run:\n\n   ```\n   mount -t lustre \n   /dev/block_device \n   /mount_point\n   ```\n\n   ### Note\n\n   To create additional OSTs, repeat Step [4](#configuring-a-simple-lustre-file-system)and Step [5](#simple-lustre-configuration-example), specifying the next higher OST index number.\n\n6. Mount the Lustre file system on the client. On the client node, run:\n\n   ```\n   mount -t lustre \n   MGS_node:/\n   fsname \n   /mount_point \n   ```\n\n   ### Note\n\n   To mount the filesystem on additional clients, repeat Step [6](#configuring-a-simple-lustre-file-system).\n\n   ### Note\n\n   If you have a problem mounting the file system, check the syslogs on the client and all the servers for errors and also check the network settings. A common issue with newly-installed systems is that `hosts.deny` or firewall rules may prevent connections on port 988.\n\n7. Verify that the file system started and is working correctly. Do this by running`lfs df`, `dd` and `ls` commands on the client node.\n\n8. *(Optional)*Run benchmarking tools to validate the performance of hardware and software layers in the cluster. Available tools include:\n\n   - `obdfilter-survey`- Characterizes the storage performance of a Lustre file system. For details, see [the section called \u201cTesting OST Performance (`obdfilter-survey`)\u201d](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-ost-performance-obdfilter-survey).\n   - `ost-survey`- Performs I/O against OSTs to detect anomalies between otherwise identical disk subsystems. For details, see [the section called \u201cTesting OST I/O Performance (`ost-survey`)\u201d](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-ost-io-performance-ost-survey).\n\n### Simple Lustre Configuration Example\n\nTo see the steps to complete for a simple Lustre file system configuration, follow this example in which a combined MGS/MDT and two OSTs are created to form a file system called `temp`. Three block devices are used, one for the combined MGS/MDS node and one for each OSS node. Common parameters used in the example are listed below, along with individual node parameters.", "mimetype": "text/plain", "start_char_idx": 6148, "end_char_idx": 9138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3de7168f-247a-4265-9390-1eaf79e5baf3": {"__data__": {"id_": "3de7168f-247a-4265-9390-1eaf79e5baf3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7d3a5b59a7eaa2e9f311ca24344bc5f1aa2b36ca75c1e1f2b30b9e13313230d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "688b7b6e-0c00-4fa6-88d6-dcf3fbf89bb1", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7fca1cef82704c1fb55b473660d62a4d2399b8d611998f5e275f902db877dd20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e069d5ff-744a-41ba-b927-8ffb9781fa71", "node_type": "1", "metadata": {}, "hash": "7293a24ccfcf673c5bf0cb8d684fcaee693824a60d41d968e46b4eb69969bf36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `ost-survey`- Performs I/O against OSTs to detect anomalies between otherwise identical disk subsystems. For details, see [the section called \u201cTesting OST I/O Performance (`ost-survey`)\u201d](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-ost-io-performance-ost-survey).\n\n### Simple Lustre Configuration Example\n\nTo see the steps to complete for a simple Lustre file system configuration, follow this example in which a combined MGS/MDT and two OSTs are created to form a file system called `temp`. Three block devices are used, one for the combined MGS/MDS node and one for each OSS node. Common parameters used in the example are listed below, along with individual node parameters.\n\n| **Common Parameters** | **Value**        | **Description** |                                                |\n| --------------------- | ---------------- | --------------- | ---------------------------------------------- |\n|                       | **MGS node**     | `10.2.0.1@tcp0` | Node for the combined MGS/MDS                  |\n|                       | **file system**  | `temp`          | Name of the Lustre file system                 |\n|                       | **network type** | `TCP/IP`        | Network type used for Lustre file system`temp` |\n\n| **Node Parameters** | **Value**        | **Description** |                                                              |\n| ------------------- | ---------------- | --------------- | ------------------------------------------------------------ |\n| MGS/MDS node        |                  |                 |                                                              |\n|                     | **MGS/MDS node** | `mdt0`          | MDS in Lustre file system `temp`                             |\n|                     | **block device** | `/dev/sdb`      | Block device for the combined MGS/MDS node                   |\n|                     | **mount point**  | `/mnt/mdt`      | Mount point for the `mdt0` block device ( `/dev/sdb`) on the MGS/MDS node |\n| First OSS node      |                  |                 |                                                              |\n|                     | **OSS node**     | `oss0`          | First OSS node in Lustre file system `temp`                  |\n|                     | **OST**          | `ost0`          | First OST in Lustre file system `temp`                       |\n|                     | **block device** | `/dev/sdc`      | Block device for the first OSS node ( `oss0`)                |\n|                     | **mount point**  | `/mnt/ost0`     | Mount point for the `ost0` block device ( `/dev/sdc`) on the `oss1` node |\n| Second OSS node     |                  |                 |                                                              |\n|                     | **OSS node**     | `oss1`          | Second OSS node in Lustre file system `temp`                 |\n|                     | **OST**          | `ost1`          | Second OST in Lustre file system `temp`                      |\n|                     | **block device** | `/dev/sdd`      | Block device for the second OSS node (oss1)                  |\n|                     | **mount point**  | `/mnt/ost1`     | Mount point for the `ost1` block device ( `/dev/sdd`) on the `oss1` node |\n| Client node         |                  |                 |                                                              |\n|                     | **client node**  | `client1`       | Client in Lustre file system `temp`                          |\n|                     | **mount point**  | `/lustre`       | Mount point for Lustre file system `temp` on the`client1` node |\n\n### Note\n\nWe recommend that you use 'dotted-quad' notation for IP addresses rather than host names to make it easier to read debug logs and debug configurations with multiple interfaces.\n\nFor this example, complete the steps below:\n\n1. Create a combined MGS/MDT file system on the block device.", "mimetype": "text/plain", "start_char_idx": 8412, "end_char_idx": 12391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e069d5ff-744a-41ba-b927-8ffb9781fa71": {"__data__": {"id_": "e069d5ff-744a-41ba-b927-8ffb9781fa71", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7d3a5b59a7eaa2e9f311ca24344bc5f1aa2b36ca75c1e1f2b30b9e13313230d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3de7168f-247a-4265-9390-1eaf79e5baf3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "03d63e843fb13876b151466f75a8b20a23729cf0ca9ee95819e621ed5a2c1407", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3492345-b073-4336-838a-371332162bbf", "node_type": "1", "metadata": {}, "hash": "c0cfcab212ca5d8439f236da8f74595ec0e2289f7880202803bf4df64516a719", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this example, complete the steps below:\n\n1. Create a combined MGS/MDT file system on the block device. On the MDS node, run:\n\n   ```\n   [root@mds /]# mkfs.lustre --fsname=temp --mgs --mdt --index=0 /dev/sdb\n   ```\n\n   This command generates this output:\n\n   ```\n       Permanent disk data:\n   Target:            temp-MDT0000\n   Index:             0\n   Lustre FS: temp\n   Mount type:        ldiskfs\n   Flags:             0x75\n      (MDT MGS first_time update )\n   Persistent mount opts: errors=remount-ro,iopen_nopriv,user_xattr\n   Parameters: mdt.identity_upcall=/usr/sbin/l_getidentity\n    \n   checking for existing Lustre data: not found\n   device size = 16MB\n   2 6 18\n   formatting backing filesystem ldiskfs on /dev/sdb\n      target name             temp-MDTffff\n      4k blocks               0\n      options                 -i 4096 -I 512 -q -O dir_index,uninit_groups -F\n   mkfs_cmd = mkfs.ext2 -j -b 4096 -L temp-MDTffff  -i 4096 -I 512 -q -O \n   dir_index,uninit_groups -F /dev/sdb\n   Writing CONFIGS/mountdata \n   ```\n\n2. Mount the combined MGS/MDT file system on the block device. On the MDS node, run:\n\n   ```\n   [root@mds /]# mount -t lustre /dev/sdb /mnt/mdt\n   ```\n\n   This command generates this output:\n\n   ```\n   Lustre: temp-MDT0000: new disk, initializing \n   Lustre: 3009:0:(lproc_mds.c:262:lprocfs_wr_identity_upcall()) temp-MDT0000:\n   group upcall set to /usr/sbin/l_getidentity\n   Lustre: temp-MDT0000.mdt: set parameter identity_upcall=/usr/sbin/l_getidentity\n   Lustre: Server temp-MDT0000 on device /dev/sdb has started \n   ```\n\n3. Create and mount `ost0`.\n\n   In this example, the OSTs ( `ost0` and `ost1`) are being created on different OSS nodes ( `oss0` and `oss1` respectively).\n\n   1. Create `ost0`. On `oss0` node, run:\n\n      ```\n      [root@oss0 /]# mkfs.lustre --fsname=temp --mgsnode=10.2.0.1@tcp0 --ost\n      --index=0 /dev/sdc\n      ```\n\n      The command generates this output:\n\n      ```\n          Permanent disk data:\n      Target:            temp-OST0000\n      Index:             0\n      Lustre FS: temp\n      Mount type:        ldiskfs\n      Flags:             0x72\n      (OST first_time update)\n      Persistent mount opts: errors=remount-ro,extents,mballoc\n      Parameters: mgsnode=10.2.0.1@tcp\n       \n      checking for existing Lustre data: not found\n      device size = 16MB\n      2 6 18\n      formatting backing filesystem ldiskfs on /dev/sdc\n         target name             temp-OST0000\n         4k blocks               0\n         options                 -I 256 -q -O dir_index,uninit_groups -F\n      mkfs_cmd = mkfs.ext2 -j -b 4096 -L temp-OST0000  -I 256 -q -O\n      dir_index,uninit_groups -F /dev/sdc\n      Writing CONFIGS/mountdata \n      ```\n\n   2. Mount ost0 on the OSS on which it was created.", "mimetype": "text/plain", "start_char_idx": 12285, "end_char_idx": 15046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3492345-b073-4336-838a-371332162bbf": {"__data__": {"id_": "f3492345-b073-4336-838a-371332162bbf", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7d3a5b59a7eaa2e9f311ca24344bc5f1aa2b36ca75c1e1f2b30b9e13313230d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e069d5ff-744a-41ba-b927-8ffb9781fa71", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "218230fef6bd531f028d6b9092102d309637e7d326721207232d48370d0cd64f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "580f4f20-b533-451e-9f4d-41040256ad76", "node_type": "1", "metadata": {}, "hash": "e19ce860adb2e27d41bef232e2af6d6bb6b531875b578e1c5f7a9528ef390118", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mount ost0 on the OSS on which it was created. On `oss0` node, run:\n\n      ```\n      root@oss0 /] mount -t lustre /dev/sdc /mnt/ost0\n      ```\n\n      The command generates this output:\n\n      ```\n      LDISKFS-fs: file extents enabled \n      LDISKFS-fs: mballoc enabled\n      Lustre: temp-OST0000: new disk, initializing\n      Lustre: Server temp-OST0000 on device /dev/sdb has started\n      ```\n\n      Shortly afterwards, this output appears:\n\n      ```\n      Lustre: temp-OST0000: received MDS connection from 10.2.0.1@tcp0\n      Lustre: MDS temp-MDT0000: temp-OST0000_UUID now active, resetting orphans \n      ```\n\n4. Create and mount `ost1`.\n\n   1. Create ost1. On `oss1` node, run:\n\n      ```\n      [root@oss1 /]# mkfs.lustre --fsname=temp --mgsnode=10.2.0.1@tcp0 \\\n                 --ost --index=1 /dev/sdd\n      ```\n\n      The command generates this output:\n\n      ```\n          Permanent disk data:\n      Target:            temp-OST0001\n      Index:             1\n      Lustre FS: temp\n      Mount type:        ldiskfs\n      Flags:             0x72\n      (OST first_time update)\n      Persistent mount opts: errors=remount-ro,extents,mballoc\n      Parameters: mgsnode=10.2.0.1@tcp\n       \n      checking for existing Lustre data: not found\n      device size = 16MB\n      2 6 18\n      formatting backing filesystem ldiskfs on /dev/sdd\n         target name             temp-OST0001\n         4k blocks               0\n         options                 -I 256 -q -O dir_index,uninit_groups -F\n      mkfs_cmd = mkfs.ext2 -j -b 4096 -L temp-OST0001  -I 256 -q -O\n      dir_index,uninit_groups -F /dev/sdc\n      Writing CONFIGS/mountdata \n      ```\n\n   2. Mount ost1 on the OSS on which it was created. On `oss1` node, run:\n\n      ```\n      root@oss1 /] mount -t lustre /dev/sdd /mnt/ost1 \n      ```\n\n      The command generates this output:\n\n      ```\n      LDISKFS-fs: file extents enabled \n      LDISKFS-fs: mballoc enabled\n      Lustre: temp-OST0001: new disk, initializing\n      Lustre: Server temp-OST0001 on device /dev/sdb has started\n      ```\n\n      Shortly afterwards, this output appears:\n\n      ```\n      Lustre: temp-OST0001: received MDS connection from 10.2.0.1@tcp0\n      Lustre: MDS temp-MDT0000: temp-OST0001_UUID now active, resetting orphans \n      ```\n\n5. Mount the Lustre file system on the client. On the client node, run:\n\n   ```\n   root@client1 /] mount -t lustre 10.2.0.1@tcp0:/temp /lustre \n   ```\n\n   This command generates this output:\n\n   ```\n   Lustre: Client temp-client has started\n   ```\n\n6. Verify that the file system started and is working by running the `df`, `dd` and `ls`commands on the client node.\n\n   1. Run the `lfs df -h` command:\n\n      ```\n      [root@client1 /] lfs df -h \n      ```\n\n      The `lfs df -h` command lists space usage per OST and the MDT in human-readable format.", "mimetype": "text/plain", "start_char_idx": 15000, "end_char_idx": 17826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "580f4f20-b533-451e-9f4d-41040256ad76": {"__data__": {"id_": "580f4f20-b533-451e-9f4d-41040256ad76", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7d3a5b59a7eaa2e9f311ca24344bc5f1aa2b36ca75c1e1f2b30b9e13313230d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3492345-b073-4336-838a-371332162bbf", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d4351e4743c7e933f2597b2c8494c40a8078428975429d970f6d460c3bb58d92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3066eb5-9faf-476c-a0f6-d17bcd552755", "node_type": "1", "metadata": {}, "hash": "08d6040ddb2070ad5c4abd6184bc00e6524a192b77e5baf45f8c2e2254529686", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mount the Lustre file system on the client. On the client node, run:\n\n   ```\n   root@client1 /] mount -t lustre 10.2.0.1@tcp0:/temp /lustre \n   ```\n\n   This command generates this output:\n\n   ```\n   Lustre: Client temp-client has started\n   ```\n\n6. Verify that the file system started and is working by running the `df`, `dd` and `ls`commands on the client node.\n\n   1. Run the `lfs df -h` command:\n\n      ```\n      [root@client1 /] lfs df -h \n      ```\n\n      The `lfs df -h` command lists space usage per OST and the MDT in human-readable format. This command generates output similar to this:\n\n      ```\n      UUID               bytes      Used      Available   Use%    Mounted on\n      temp-MDT0000_UUID  8.0G      400.0M       7.6G        0%      /lustre[MDT:0]\n      temp-OST0000_UUID  800.0G    400.0M     799.6G        0%      /lustre[OST:0]\n      temp-OST0001_UUID  800.0G    400.0M     799.6G        0%      /lustre[OST:1]\n      filesystem summary:  1.6T    800.0M       1.6T        0%      /lustre\n      ```\n\n   2. Run the `lfs df -ih` command.\n\n      ```\n      [root@client1 /] lfs df -ih\n      ```\n\n      The `lfs df -ih` command lists inode usage per OST and the MDT. This command generates output similar to this:\n\n      ```\n      UUID              Inodes      IUsed       IFree   IUse%     Mounted on\n      temp-MDT0000_UUID   2.5M        32         2.5M      0%       /lustre[MDT:0]\n      temp-OST0000_UUID   5.5M        54         5.5M      0%       /lustre[OST:0]\n      temp-OST0001_UUID   5.5M        54         5.5M      0%       /lustre[OST:1]\n      filesystem summary: 2.5M        32         2.5M      0%       /lustre\n      ```\n\n   3. Run the `dd` command:\n\n      ```\n      [root@client1 /] cd /lustre\n      [root@client1 /lustre] dd if=/dev/zero of=/lustre/zero.dat bs=4M count=2\n      ```\n\n      The `dd` command verifies write functionality by creating a file containing all zeros ( `0`s). In this command, an 8 MB file is created. This command generates output similar to this:\n\n      ```\n      2+0 records in\n      2+0 records out\n      8388608 bytes (8.4 MB) copied, 0.159628 seconds, 52.6 MB/s\n      ```\n\n   4. Run the `ls` command:\n\n      ```\n      [root@client1 /lustre] ls -lsah\n      ```\n\n      The `ls -lsah` command lists files and directories in the current working directory. This command generates output similar to this:\n\n      ```\n      total 8.0M\n      4.0K drwxr-xr-x  2 root root 4.0K Oct 16 15:27 .\n      8.0K drwxr-xr-x 25 root root 4.0K Oct 16 15:27 ..\n      8.0M -rw-r--r--  1 root root 8.0M Oct 16 15:27 zero.dat \n       \n      ```\n\nOnce the Lustre file system is configured, it is ready for use.\n\n## Additional Configuration Options\n\nThis section describes how to scale the Lustre file system or make configuration changes using the Lustre configuration utilities.\n\n### Scaling the Lustre File System\n\nA Lustre file system can be scaled by adding OSTs or clients. For instructions on creating additional OSTs repeat Step and Step above. For mounting additional clients, repeat Step for each client.", "mimetype": "text/plain", "start_char_idx": 17278, "end_char_idx": 20327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3066eb5-9faf-476c-a0f6-d17bcd552755": {"__data__": {"id_": "b3066eb5-9faf-476c-a0f6-d17bcd552755", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9366dfb-6705-4f16-8ac5-76e1945f8dc8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7d3a5b59a7eaa2e9f311ca24344bc5f1aa2b36ca75c1e1f2b30b9e13313230d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "580f4f20-b533-451e-9f4d-41040256ad76", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0e092b1c2d0ebc1032ba2a50ae8e08726d2a0e5e5d582148936b92d7e7b6944b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.0K drwxr-xr-x 25 root root 4.0K Oct 16 15:27 ..\n      8.0M -rw-r--r--  1 root root 8.0M Oct 16 15:27 zero.dat \n       \n      ```\n\nOnce the Lustre file system is configured, it is ready for use.\n\n## Additional Configuration Options\n\nThis section describes how to scale the Lustre file system or make configuration changes using the Lustre configuration utilities.\n\n### Scaling the Lustre File System\n\nA Lustre file system can be scaled by adding OSTs or clients. For instructions on creating additional OSTs repeat Step and Step above. For mounting additional clients, repeat Step for each client.\n\n### Changing Striping Defaults\n\nThe default settings for the file layout stripe pattern are shown in [Table 8, \u201cDefault stripe pattern\u201d](#table-8-default-stripe-pattern).\n\n**Table 8. Default stripe pattern**\n\n| **File Layout Parameter** | **Default** | **Description**                                              |\n| ------------------------- | ----------- | ------------------------------------------------------------ |\n| `stripe_size`             | 1 MB        | Amount of data to write to one OST before moving to the next OST. |\n| `stripe_count`            | 1           | The number of OSTs to use for a single file.                 |\n| `start_ost`               | -1          | The first OST where objects are created for each file. The default -1 allows the MDS to choose the starting index based on available space and load balancing. *It's strongly recommended not to change the default for this parameter to a value other than -1.* |\n\n\n\nUse the `lfs setstripe` command described in [*Managing File Layout (Striping) and Free Space*](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md)to change the file layout configuration.\n\n### Using the Lustre Configuration Utilities\n\nIf additional configuration is necessary, several configuration utilities are available:\n\n- `mkfs.lustre`- Use to format a disk for a Lustre service.\n- `tunefs.lustre`- Use to modify configuration information on a Lustre target disk.\n- `lctl`- Use to directly control Lustre features via an `ioctl` interface, allowing various configuration, maintenance and debugging features to be accessed.\n- `mount.lustre`- Use to start a Lustre client or target service.\n\nFor examples using these utilities, see the topic [*System Configuration Utilities*](06.07-System%20Configuration%20Utilities.md)\n\nThe `lfs` utility is useful for configuring and querying a variety of options related to files. For more information, see [*User Utilities*](06.03-User%20Utilities.md).\n\n### Note\n\nSome sample scripts are included in the directory where the Lustre software is installed. If you have installed the Lustre source code, the scripts are located in the `lustre/tests` sub-directory. These scripts enable quick setup of some simple standard Lustre configurations.", "mimetype": "text/plain", "start_char_idx": 19729, "end_char_idx": 22577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af8294bb-953e-4ba7-a128-25c0962b4deb": {"__data__": {"id_": "af8294bb-953e-4ba7-a128-25c0962b4deb", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/11-Configuring Failover in a Lustre File System.md", "file_name": "11-Configuring Failover in a Lustre File System.md", "file_type": "text/markdown", "file_size": 7933, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0bbbec1f-efdf-4525-ba8a-93c7d136290f", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/11-Configuring Failover in a Lustre File System.md", "file_name": "11-Configuring Failover in a Lustre File System.md", "file_type": "text/markdown", "file_size": 7933, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4cc77dcbda1a28ee15cc302eb43d77beeae6496c4b2211187cc14ab88355a01f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61e1d57a-0e7f-44eb-ac51-dd2150203ec5", "node_type": "1", "metadata": {}, "hash": "e499d640d51db71b53cab1d6fee85439811e5a48ce3fa9a59952347cda210bde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Configuring Failover in a Lustre File System\n\n**Table of Contents**\n\n- [Configuring Failover in a Lustre File System](#configuring-failover-in-a-lustre-file-system)\n  * [Setting Up a Failover Environment](#setting-up-a-failover-environment)\n    + [Selecting Power Equipment](#selecting-power-equipment)\n    + [Selecting Power Management Software](#selecting-power-management-software)\n    + [Selecting High-Availability (HA) Software](#selecting-high-availability-ha-software)\n  * [Preparing a Lustre File System for Failover](#preparing-a-lustre-file-system-for-failover)\n  * [Administering Failover in a Lustre File System](#administering-failover-in-a-lustre-file-system)\n\nThis chapter describes how to configure failover in a Lustre file system. It includes:\n\n- [the section called \u201cSetting Up a Failover Environment\u201d](#setting-up-a-failover-environment)\n- [the section called \u201cPreparing a Lustre File System for Failover\u201d](#preparing-a-lustre-file-system-for-failover)\n- [the section called \u201cAdministering Failover in a Lustre File System\u201d](#administering-failover-in-a-lustre-file-system)\n\nFor an overview of failover functionality in a Lustre file system, see [*Understanding Failover in a Lustre File System*](02-Introducing%20the%20Lustre%20File%20System.md#understanding-failover-in-a-lustre-file-system).\n\n## Setting Up a Failover Environment\n\nThe Lustre software provides failover mechanisms only at the layer of the Lustre file system. No failover functionality is provided for system-level components such as failing hardware or applications, or even for the entire failure of a node, as would typically be provided in a complete failover solution. Failover functionality such as node monitoring, failure detection, and resource fencing must be provided by external HA software, such as PowerMan or the open source Corosync and Pacemaker packages provided by Linux operating system vendors. Corosync provides support for detecting failures, and Pacemaker provides the actions to take once a failure has been detected.\n\n### Selecting Power Equipment\n\nFailover in a Lustre file system requires the use of a remote power control (RPC) mechanism, which comes in different configurations. For example, Lustre server nodes may be equipped with IPMI/BMC devices that allow remote power control. In the past, software or even \u201csneakerware\u201d has been used, but these are not recommended. For recommended devices, refer to the list of supported RPC devices on the website for the PowerMan cluster power management utility:\n\n<http://code.google.com/p/powerman/wiki/SupportedDevs>\n\n### Selecting Power Management Software\n\nLustre failover requires RPC and management capability to verify that a failed node is shut down before I/O is directed to the failover node. This avoids double-mounting the two nodes and the risk of unrecoverable data corruption. A variety of power management tools will work. Two packages that have been commonly used with the Lustre software are PowerMan and Linux-HA (aka. STONITH ).\n\nThe PowerMan cluster power management utility is used to control RPC devices from a central location. PowerMan provides native support for several RPC varieties and Expect-like configuration simplifies the addition of new devices. The latest versions of PowerMan are available at:\n\n<http://code.google.com/p/powerman/>\n\nSTONITH, or \u201cShoot The Other Node In The Head\u201d, is a set of power management tools provided with the Linux-HA package prior to Red Hat Enterprise Linux 6. Linux-HA has native support for many power control devices, is extensible (uses Expect scripts to automate control), and provides the software to detect and respond to failures. With Red Hat Enterprise Linux 6, Linux-HA is being replaced in the open source community by the combination of Corosync and Pacemaker. For Red Hat Enterprise Linux subscribers, cluster management using CMAN is available from Red Hat.\n\n### Selecting High-Availability (HA) Software\n\nThe Lustre file system must be set up with high-availability (HA) software to enable a complete Lustre failover solution. Except for PowerMan, the HA software packages mentioned above provide both power management and cluster management.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4187, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61e1d57a-0e7f-44eb-ac51-dd2150203ec5": {"__data__": {"id_": "61e1d57a-0e7f-44eb-ac51-dd2150203ec5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/11-Configuring Failover in a Lustre File System.md", "file_name": "11-Configuring Failover in a Lustre File System.md", "file_type": "text/markdown", "file_size": 7933, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0bbbec1f-efdf-4525-ba8a-93c7d136290f", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/11-Configuring Failover in a Lustre File System.md", "file_name": "11-Configuring Failover in a Lustre File System.md", "file_type": "text/markdown", "file_size": 7933, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4cc77dcbda1a28ee15cc302eb43d77beeae6496c4b2211187cc14ab88355a01f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af8294bb-953e-4ba7-a128-25c0962b4deb", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/11-Configuring Failover in a Lustre File System.md", "file_name": "11-Configuring Failover in a Lustre File System.md", "file_type": "text/markdown", "file_size": 7933, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "601fa443748c5173431367612f206e6f09b7201e5cd9bfac41d40dc8d9b76b12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8df0a542-59a5-4039-b9e7-70c5ed180bb3", "node_type": "1", "metadata": {}, "hash": "a210b70d714dc9dae170d42cba74778460c4718163aeb8998e0d0f6200304ab7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Linux-HA has native support for many power control devices, is extensible (uses Expect scripts to automate control), and provides the software to detect and respond to failures. With Red Hat Enterprise Linux 6, Linux-HA is being replaced in the open source community by the combination of Corosync and Pacemaker. For Red Hat Enterprise Linux subscribers, cluster management using CMAN is available from Red Hat.\n\n### Selecting High-Availability (HA) Software\n\nThe Lustre file system must be set up with high-availability (HA) software to enable a complete Lustre failover solution. Except for PowerMan, the HA software packages mentioned above provide both power management and cluster management. For information about setting up failover with Pacemaker, see:\n\n- Pacemaker Project website: <http://clusterlabs.org/>\n- Article Using Pacemaker with a Lustre File System:<https://wiki.whamcloud.com/display/PUB/Using+Pacemaker+with+a+Lustre+File+System>\n\n## Preparing a Lustre File System for Failover\n\nTo prepare a Lustre file system to be configured and managed as an HA system by a third-party HA application, each storage target (MGT, MGS, OST) must be associated with a second node to create a failover pair. This configuration information is then communicated by the MGS to a client when the client mounts the file system.\n\nThe per-target configuration is relayed to the MGS at mount time. Some rules related to this are:\n\n- When a target is initially mounted, the MGS reads the configuration information from the target (such as mgt vs. ost, failnode, fsname) to configure the target into a Lustre file system. If the MGS is reading the initial mount configuration, the mounting node becomes that target's \u201cprimary\u201d node.\n- When a target is subsequently mounted, the MGS reads the current configuration from the target and, as needed, will reconfigure the MGS database target information\n\nWhen the target is formatted using the `mkfs.lustre` command, the failover service node(s) for the target are designated using the `--servicenode` option. In the example below, an OST with index `0` in the file system `testfs` is formatted with two service nodes designated to serve as a failover pair:\n\n```\nmkfs.lustre --reformat --ost --fsname testfs --mgsnode=192.168.10.1@o3ib \\  \n              --index=0 --servicenode=192.168.10.7@o2ib \\\n              --servicenode=192.168.10.8@o2ib \\  \n              /dev/sdb\n```\n\nMore than two potential service nodes can be designated for a target. The target can then be mounted on any of the designated service nodes.\n\nWhen HA is configured on a storage target, the Lustre software enables multi-mount protection (MMP) on that storage target. MMP prevents multiple nodes from simultaneously mounting and thus corrupting the data on the target. For more about MMP, see [*Lustre File System Failover and Multiple-Mount Protection*](03.13-Lustre%20File%20System%20Failover%20and%20Multiple-Mount%20Protection.md).\n\nIf the MGT has been formatted with multiple service nodes designated, this information must be conveyed to the Lustre client in the mount command used to mount the file system. In the example below, NIDs for two MGSs that have been designated as service nodes for the MGT are specified in the mount command executed on the client:\n\n```\nmount -t lustre 10.10.120.1@tcp1:10.10.120.2@tcp1:/testfs /lustre/testfs\n```\n\nWhen a client mounts the file system, the MGS provides configuration information to the client for the MDT(s) and OST(s) in the file system along with the NIDs for all service nodes associated with each target and the service node on which the target is mounted. Later, when the client attempts to access data on a target, it will try the NID for each specified service node until it connects to the target.", "mimetype": "text/plain", "start_char_idx": 3490, "end_char_idx": 7261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8df0a542-59a5-4039-b9e7-70c5ed180bb3": {"__data__": {"id_": "8df0a542-59a5-4039-b9e7-70c5ed180bb3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/11-Configuring Failover in a Lustre File System.md", "file_name": "11-Configuring Failover in a Lustre File System.md", "file_type": "text/markdown", "file_size": 7933, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0bbbec1f-efdf-4525-ba8a-93c7d136290f", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/11-Configuring Failover in a Lustre File System.md", "file_name": "11-Configuring Failover in a Lustre File System.md", "file_type": "text/markdown", "file_size": 7933, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4cc77dcbda1a28ee15cc302eb43d77beeae6496c4b2211187cc14ab88355a01f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61e1d57a-0e7f-44eb-ac51-dd2150203ec5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/11-Configuring Failover in a Lustre File System.md", "file_name": "11-Configuring Failover in a Lustre File System.md", "file_type": "text/markdown", "file_size": 7933, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d922a49554476f330e70fbcf0fd2ab2b184dd3e65a54149c7859f140bcab0216", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the MGT has been formatted with multiple service nodes designated, this information must be conveyed to the Lustre client in the mount command used to mount the file system. In the example below, NIDs for two MGSs that have been designated as service nodes for the MGT are specified in the mount command executed on the client:\n\n```\nmount -t lustre 10.10.120.1@tcp1:10.10.120.2@tcp1:/testfs /lustre/testfs\n```\n\nWhen a client mounts the file system, the MGS provides configuration information to the client for the MDT(s) and OST(s) in the file system along with the NIDs for all service nodes associated with each target and the service node on which the target is mounted. Later, when the client attempts to access data on a target, it will try the NID for each specified service node until it connects to the target.\n\n## Administering Failover in a Lustre File System\n\nFor additional information about administering failover features in a Lustre file system, see:\n\n- [the section called \u201c Specifying Failout/Failover Mode for OSTs\u201d](03.02-Lustre%20Operations.md#specifying-failoutfailover-mode-for-osts)\n- [the section called \u201c Specifying NIDs and Failover\u201d](03.02-Lustre%20Operations.md#specifying-nids-and-failover)\n- [the section called \u201c Changing the Address of a Failover Node\u201d](03.03-Lustre%20Maintenance.md#changing-the-address-of-a-failover-node)\n- [the section called \u201c mkfs.lustre\u201d](06.07-System%20Configuration%20Utilities.md#mkfslustre)", "mimetype": "text/plain", "start_char_idx": 6440, "end_char_idx": 7893, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c225c7ad-7af7-47bb-b644-a6ff481ae9fd": {"__data__": {"id_": "c225c7ad-7af7-47bb-b644-a6ff481ae9fd", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfe753d7-fc48-4b93-a301-6f81e7047022", "node_type": "1", "metadata": {}, "hash": "5b8ad7b568f003595896277d04e0d3a3160d8dbdbab0bdd02062548c0e1e966a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Monitoring a Lustre File System\n\nThis chapter provides information on monitoring a Lustre file system and includes the following sections:\n\n- [Monitoring a Lustre File System](#monitoring-a-lustre-file-system)\n  * [Lustre Changelogs](#lustre-changelogs)\n    + [Working with Changelogs](#working-with-changelogs)\n      - [`lctl changelog_register`](#lctl-changelog_register)\n      - [`lfs changelog`](#lfs-changelog)\n      - [`lfs changelog_clear`](#lfs-changelog_clear)\n      - [`lctl changelog_deregister`](#lctl-changelog_deregister)\n    + [Changelog Examples](#changelog-examples)\n      - [Registering a Changelog User](#registering-a-changelog-user)\n      - [Displaying Changelog Records](#displaying-changelog-records)\n      - [Clearing Changelog Records](#clearing-changelog-records)\n      - [Deregistering a Changelog User](#deregistering-a-changelog-user)\n      - [Displaying the Changelog Index and Registered Users](#displaying-the-changelog-index-and-registered-users)\n      - [Displaying the Changelog Mask](#displaying-the-changelog-mask)\n      - [Setting the Changelog Mask](#setting-the-changelog-mask)\n    + [Audit with Changelogs](#audit-with-changelogs)\n      - [Enabling Audit](#enabling-audit)\n      - [Audit examples](#audit-examples)\n        * [`OPEN`](#open)\n        * [`GETXATTR`](#getxattr)\n        * [`SETXATTR`](#setxattr)\n        * [`DENIED OPEN`](#denied-open)\n  * [Lustre Jobstats](#lustre-jobstats)\n    + [How Jobstats Works](#how-jobstats-works)\n    + [Enable/Disable Jobstats](#enabledisable-jobstats)\n    + [Check Job Stats](#check-job-stats)\n    + [Clear Job Stats](#clear-job-stats)\n    + [Configure Auto-cleanup Interval](#configure-auto-cleanup-interval)\n  * [Lustre Monitoring Tool (LMT)](#lustre-monitoring-tool-lmt)\n  * [`CollectL`](#collectl)\n  * [Other Monitoring Options](#other-monitoring-options)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1844, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfe753d7-fc48-4b93-a301-6f81e7047022": {"__data__": {"id_": "cfe753d7-fc48-4b93-a301-6f81e7047022", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c225c7ad-7af7-47bb-b644-a6ff481ae9fd", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "21c88168501f07d988b9b304ff38442a7679d1069b323ea446d3c17a53fb6822", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87ba4671-b829-4c5d-8cfb-a3f6e74c1006", "node_type": "1", "metadata": {}, "hash": "967e957dae89f6af551800eda0bbc8b86e969ef1cb4f05440d562acd157d3d97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Lustre Changelogs\n\nThe changelogs feature records events that change the file system namespace or file metadata. Changes such as file creation, deletion, renaming, attribute changes, etc. are recorded with the target and parent file identifiers (FIDs), the name of the target, a timestamp, and user information. These records can be used for a variety of purposes:\n\n- Capture recent changes to feed into an archiving system.\n- Use changelog entries to exactly replicate changes in a file system mirror.\n- Set up \"watch scripts\" that take action on certain events or directories.\n- Audit activity on Lustre, thanks to user information associated to file/directory changes with timestamps.\n\nChangelogs record types are:\n\n| **Value** | **Description**                                |\n| --------- | ---------------------------------------------- |\n| MARK      | Internal recordkeeping                         |\n| CREAT     | Regular file creation                          |\n| MKDIR     | Directory creation                             |\n| HLINK     | Hard link                                      |\n| SLINK     | Soft link                                      |\n| MKNOD     | Other file creation                            |\n| UNLNK     | Regular file removal                           |\n| RMDIR     | Directory removal                              |\n| RENME     | Rename, original                               |\n| RNMTO     | Rename, final                                  |\n| OPEN *    | Open                                           |\n| CLOSE     | Close                                          |\n| LYOUT     | Layout change                                  |\n| TRUNC     | Regular file truncated                         |\n| SATTR     | Attribute change                               |\n| XATTR     | Extended attribute change (setxattr)           |\n| HSM       | HSM specific event                             |\n| MTIME     | MTIME change                                   |\n| CTIME     | CTIME change                                   |\n| ATIME *   | ATIME change                                   |\n| MIGRT     | Migration event                                |\n| FLRW      | File Level Replication: file initially written |\n| RESYNC    | File Level Replication: file re-synced         |\n| GXATR *   | Extended attribute access (getxattr)           |\n| NOPEN *   | Denied open                                    |\n\n**Note**\n\nEvent types marked with * are not recorded by default. Refer to *the section called \u201cSetting the Changelog Mask\u201d* for instructions on modifying the Changelogs mask.\n\nFID-to-full-pathname and pathname-to-FID functions are also included to map target and parent FIDs into the file system namespace.\n\n\n\n### Working with Changelogs\n\nSeveral commands are available to work with changelogs.\n\n#### `lctl changelog_register`\n\nBecause changelog records take up space on the MDT, the system administration must register changelog users. As soon as a changelog user is registered, the Changelogs feature is enabled. The registrants specify which records they are \"done with\", and the system purges up to the greatest common record.\n\nTo register a new changelog user, run:\n\n```\nmds# lctl --device fsname-MDTnumber changelog_register\n```\n\nChangelog entries are not purged beyond a registered user's set point (see `lfs changelog_clear`).", "mimetype": "text/plain", "start_char_idx": 1847, "end_char_idx": 5205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87ba4671-b829-4c5d-8cfb-a3f6e74c1006": {"__data__": {"id_": "87ba4671-b829-4c5d-8cfb-a3f6e74c1006", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfe753d7-fc48-4b93-a301-6f81e7047022", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "933a216411f8fa5d376832c66d03dbb89bfd9762d6bc29784cfcd08bb8d3afd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b765fd0-82a0-44ad-980d-aec425c5d8a7", "node_type": "1", "metadata": {}, "hash": "80aded31f4c11d71336a6317963fef5c7befd8b55be0a5d4542ae0821a63877b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Working with Changelogs\n\nSeveral commands are available to work with changelogs.\n\n#### `lctl changelog_register`\n\nBecause changelog records take up space on the MDT, the system administration must register changelog users. As soon as a changelog user is registered, the Changelogs feature is enabled. The registrants specify which records they are \"done with\", and the system purges up to the greatest common record.\n\nTo register a new changelog user, run:\n\n```\nmds# lctl --device fsname-MDTnumber changelog_register\n```\n\nChangelog entries are not purged beyond a registered user's set point (see `lfs changelog_clear`).\n\n\n\n#### `lfs changelog`\n\nTo display the metadata changes on an MDT (the changelog records), run:\n\n```\nlfs changelog fsname-MDTnumber [startrec [endrec]] \n```\n\nIt is optional whether to specify the start and end records.\n\nThese are sample changelog records:\n\n```\n1 02MKDIR 15:15:21.977666834 2018.01.09 0x0 t=[0x200000402:0x1:0x0] j=mkdir.500 ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000007:0x1:0x0] pics\n2 01CREAT 15:15:36.687592024 2018.01.09 0x0 t=[0x200000402:0x2:0x0] j=cp.500 ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000402:0x1:0x0] chloe.jpg\n3 06UNLNK 15:15:41.305116815 2018.01.09 0x1 t=[0x200000402:0x2:0x0] j=rm.500 ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000402:0x1:0x0] chloe.jpg\n4 07RMDIR 15:15:46.468790091 2018.01.09 0x1 t=[0x200000402:0x1:0x0] j=rmdir.500 ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000007:0x1:0x0] pics \n```\n\n\n\n#### `lfs changelog_clear`\n\nTo clear old changelog records for a specific user (records that the user no longer needs), run:\n\n```\nlfs changelog_clear mdt_name userid endrec\n```\n\nThe `changelog_clear` command indicates that changelog records previous to *endrec* are no longer of interest to a particular user *userid*, potentially allowing the MDT to free up disk space. An `*endrec*` value of 0 indicates the current last record. To run`changelog_clear`, the changelog user must be registered on the MDT node using `lctl`.\n\nWhen all changelog users are done with records < X, the records are deleted.\n\n\n\n#### `lctl changelog_deregister`\n\nTo deregister (unregister) a changelog user, run:\n\n```\nmds# lctl --device mdt_device changelog_deregister userid       \n```\n\n`changelog_deregister cl1` effectively does a `lfs changelog_clear cl1 0` as it deregisters.\n\n\n\n### Changelog Examples\n\nThis section provides examples of different changelog commands.", "mimetype": "text/plain", "start_char_idx": 4581, "end_char_idx": 7027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b765fd0-82a0-44ad-980d-aec425c5d8a7": {"__data__": {"id_": "2b765fd0-82a0-44ad-980d-aec425c5d8a7", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87ba4671-b829-4c5d-8cfb-a3f6e74c1006", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "197399b6377b123e06584d144982286b582f70f8e93f09954043bc0927bb199a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e5f49d9-4a1e-4cf2-9044-bd2b7c53a4e1", "node_type": "1", "metadata": {}, "hash": "3a9033b65cc8e49fd8138c3d7a9ac83a6bd36d5ca09d05e88f9f49c93aa9265e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### `lctl changelog_deregister`\n\nTo deregister (unregister) a changelog user, run:\n\n```\nmds# lctl --device mdt_device changelog_deregister userid       \n```\n\n`changelog_deregister cl1` effectively does a `lfs changelog_clear cl1 0` as it deregisters.\n\n\n\n### Changelog Examples\n\nThis section provides examples of different changelog commands.\n\n#### Registering a Changelog User\n\nTo register a new changelog user for a device (`lustre-MDT0000`):\n\n```\nmds# lctl --device lustre-MDT0000 changelog_register\nlustre-MDT0000: Registered changelog userid 'cl1'\n```\n\n#### Displaying Changelog Records\n\nTo display changelog records on an MDT (`lustre-MDT0000`):\n\n```\n$ lfs changelog lustre-MDT0000\n1 02MKDIR 15:15:21.977666834 2018.01.09 0x0 t=[0x200000402:0x1:0x0] ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000007:0x1:0x0] pics\n2 01CREAT 15:15:36.687592024 2018.01.09 0x0 t=[0x200000402:0x2:0x0] ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000402:0x1:0x0] chloe.jpg\n3 06UNLNK 15:15:41.305116815 2018.01.09 0x1 t=[0x200000402:0x2:0x0] ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000402:0x1:0x0] chloe.jpg\n4 07RMDIR 15:15:46.468790091 2018.01.09 0x1 t=[0x200000402:0x1:0x0] ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000007:0x1:0x0] pics\n```\n\nChangelog records include this information:\n\n```\nrec# \noperation_type(numerical/text) \ntimestamp \ndatestamp \nflags \nt=target_FID \nef=extended_flags\nu=uid:gid\nnid=client_NID\np=parent_FID \ntarget_name\n```\n\nDisplayed in this format:\n\n```\nrec# operation_type(numerical/text) timestamp datestamp flags t=target_FID \\\nef=extended_flags u=uid:gid nid=client_NID p=parent_FID target_name\n```\n\nFor example:\n\n```\n2 01CREAT 15:15:36.687592024 2018.01.09 0x0 t=[0x200000402:0x2:0x0] ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000402:0x1:0x0] chloe.jpg\n```\n\n#### Clearing Changelog Records\n\nTo notify a device that a specific user (`cl1`) no longer needs records (up to and including 3):\n\n```\n$ lfs changelog_clear  lustre-MDT0000 cl1 3\n```\n\nTo confirm that the `changelog_clear` operation was successful, run `lfs changelog`; only records after id-3 are listed:\n\n```\n$ lfs changelog lustre-MDT0000\n4 07RMDIR 15:15:46.468790091 2018.01.09 0x1 t=[0x200000402:0x1:0x0] ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000007:0x1:0x0] pics\n```\n\n#### Deregistering a Changelog User\n\nTo deregister a changelog user (`cl1`) for a specific device (`lustre-MDT0000`):\n\n```\nmds# lctl --device lustre-MDT0000 changelog_deregister cl1\nlustre-MDT0000: Deregistered changelog user 'cl1'\n```\n\nThe deregistration operation clears all changelog records for the specified user (`cl1`).", "mimetype": "text/plain", "start_char_idx": 6685, "end_char_idx": 9301, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e5f49d9-4a1e-4cf2-9044-bd2b7c53a4e1": {"__data__": {"id_": "6e5f49d9-4a1e-4cf2-9044-bd2b7c53a4e1", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b765fd0-82a0-44ad-980d-aec425c5d8a7", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "beba06047ab508b2d918326526b996c76dcee70207a202fc95b0ef5814668c12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97ceb643-957e-4ba3-8375-750a353ea863", "node_type": "1", "metadata": {}, "hash": "760fbffc5eecd9d291f5dcc47d18bb71578997a51450445bf35806cc01f4c776", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\n$ lfs changelog lustre-MDT0000\n5 00MARK  15:56:39.603643887 2018.01.09 0x0 t=[0x20001:0x0:0x0] ef=0xf \\\nu=500:500 nid=0@<0:0> p=[0:0x50:0xb] mdd_obd-lustre-MDT0000-0\n```\n\n** Note **\n\nMARK records typically indicate changelog recording status changes.\n\n#### Displaying the Changelog Index and Registered Users\n\nTo display the current, maximum changelog index and registered changelog users for a specific device (`lustre-MDT0000`):\n\n```\nmds# lctl get_param  mdd.lustre-MDT0000.changelog_users \nmdd.lustre-MDT0000.changelog_users=current index: 8 \nID    index (idle seconds)\ncl2   8 (180)\n```\n\n#### Displaying the Changelog Mask\n\nTo show the current changelog mask on a specific device (`lustre-MDT0000`):\n\n```\nmds# lctl get_param  mdd.lustre-MDT0000.changelog_mask \n\nmdd.lustre-MDT0000.changelog_mask= \nMARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO CLOSE LYOUT \\\nTRUNC SATTR XATTR HSM MTIME CTIME MIGRT\n```\n\n#### Setting the Changelog Mask\n\nTo set the current changelog mask on a specific device (`lustre-MDT0000`):\n\n```\nmds# lctl set_param mdd.lustre-MDT0000.changelog_mask=HLINK \nmdd.lustre-MDT0000.changelog_mask=HLINK \n$ lfs changelog_clear lustre-MDT0000 cl1 0 \n$ mkdir /mnt/lustre/mydir/foo\n$ cp /etc/hosts /mnt/lustre/mydir/foo/file\n$ ln /mnt/lustre/mydir/foo/file /mnt/lustre/mydir/myhardlink\n```\n\nOnly item types that are in the mask show up in the changelog.\n\n```\n$ lfs changelog lustre-MDT0000\n9 03HLINK 16:06:35.291636498 2018.01.09 0x0 t=[0x200000402:0x4:0x0] ef=0xf \\\nu=500:500 nid=10.128.11.159@tcp p=[0x200000007:0x3:0x0] myhardlink\n```\n\n### Audit with Changelogs\n\nA specific use case for Lustre Changelogs is audit. According to a definition found on [Wikipedia](https://en.wikipedia.org/wiki/Information_technology_audit), information technology audits are used to evaluate the organization's ability to protect its information assets and to properly dispense information to authorized parties. Basically, audit consists in controlling that all data accesses made were done according to the access control policy in place. And usually, this is done by analyzing access logs.\n\nAudit can be used as a proof of security in place. But Audit can also be a requirement to comply with regulations.\n\nLustre Changelogs are a good mechanism for audit, because this is a centralized facility, and it is designed to be transactional. Changelog records contain all information necessary for auditing purposes:\n\n- ability to identify object of action thanks to file identifiers (FIDs) and name of targets\n- ability to identify subject of action thanks to UID/GID and NID information\n- ability to identify time of action thanks to timestamp\n\n#### Enabling Audit\n\nTo have a fully functional Changelogs-based audit facility, some additional Changelog record types must be enabled, to be able to record events such as OPEN, ATIME, GETXATTR and DENIED OPEN. Please note that enabling these record types may have some performance impact. For instance, recording OPEN and GETXATTR events generate writes in the Changelog records for a read operation from a file-system standpoint.\n\nBeing able to record events such as OPEN or DENIED OPEN is important from an audit perspective. For instance, if Lustre file system is used to store medical records on a system dedicated to Life Sciences, data privacy is crucial. Administrators may need to know which doctors accessed, or tried to access, a given medical record and when. And conversely, they might need to know which medical records a given doctor accessed.", "mimetype": "text/plain", "start_char_idx": 9303, "end_char_idx": 12823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97ceb643-957e-4ba3-8375-750a353ea863": {"__data__": {"id_": "97ceb643-957e-4ba3-8375-750a353ea863", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e5f49d9-4a1e-4cf2-9044-bd2b7c53a4e1", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b40fbe1cf4f5e0365efe2760a9cda77d935ce6ca724e03e55a0f95f5c3cc2d3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f3efa13-85c7-423c-b003-cc10f0e145fd", "node_type": "1", "metadata": {}, "hash": "2c94a8ac564378d5ca48f69559362c06c7c00ffc58a2be8882c1c5311631bf7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Please note that enabling these record types may have some performance impact. For instance, recording OPEN and GETXATTR events generate writes in the Changelog records for a read operation from a file-system standpoint.\n\nBeing able to record events such as OPEN or DENIED OPEN is important from an audit perspective. For instance, if Lustre file system is used to store medical records on a system dedicated to Life Sciences, data privacy is crucial. Administrators may need to know which doctors accessed, or tried to access, a given medical record and when. And conversely, they might need to know which medical records a given doctor accessed.\n\nTo enable all changelog entry types, do:\n\n```\nmds# lctl set_param mdd.lustre-MDT0000.changelog_mask=ALL\nmdd.seb-MDT0000.changelog_mask=ALL\n```\n\nOnce all required record types have been enabled, just register a Changelogs user and the audit facility is operational.\n\nNote that, however, it is possible to control which Lustre client nodes can trigger the recording of file system access events to the Changelogs, thanks to the `audit_mode` flag on nodemap entries. The reason to disable audit on a per-nodemap basis is to prevent some nodes (e.g. backup, HSM agent nodes) from flooding the audit logs. When `audit_mode` flag is set to 1 on a nodemap entry, a client pertaining to this nodemap will be able to record file system access events to the Changelogs, if Changelogs are otherwise activated. When set to 0, events are not logged into the Changelogs, no matter if Changelogs are activated or not. By default, `audit_mode` flag is set to 1 in newly created nodemap entries. And it is also set to 1 in 'default' nodemap.\n\nTo prevent nodes pertaining to a nodemap to generate Changelog entries, do:\n\n```\nmgs# lctl nodemap_modify --name nm1 --property audit_mode --value 0\n```\n\n#### Audit examples\n\n##### `OPEN`\n\nAn OPEN changelog entry is in the form:\n\n```\n7 10OPEN  13:38:51.510728296 2017.07.25 0x242 t=[0x200000401:0x2:0x0] \\\nef=0x7 u=500:500 nid=10.128.11.159@tcp m=-w-\n```\n\nIt includes information about the open mode, in the form m=rwx.\n\nOPEN entries are recorded only once per UID/GID, for a given open mode, as long as the file is not closed by this UID/GID. It avoids flooding the Changelogs for instance if there is an MPI job opening the same file thousands of times from different threads. It reduces the ChangeLog load significantly, without significantly affecting the audit information. Similarly, only the last CLOSE per UID/GID is recorded.\n\n##### `GETXATTR`\n\nA GETXATTR changelog entry is in the form:\n\n```\n8 23GXATR 09:22:55.886793012 2017.07.27 0x0 t=[0x200000402:0x1:0x0] \\\nef=0xf u=500:500 nid=10.128.11.159@tcp x=user.name0\n```\n\nIt includes information about the name of the extended attribute being accessed, in the form `x=<xattr name>`.\n\n##### `SETXATTR`\n\nA SETXATTR changelog entry is in the form:\n\n```\n4 15XATTR 09:41:36.157333594 2018.01.10 0x0 t=[0x200000402:0x1:0x0] \\\nef=0xf u=500:500 nid=10.128.11.159@tcp x=user.name0\n```\n\nIt includes information about the name of the extended attribute being modified, in the form `x=<xattr name>`.\n\n##### `DENIED OPEN`\n\nA DENIED OPEN changelog entry is in the form:\n\n```\n4 24NOPEN 15:45:44.947406626 2017.08.31 0x2 t=[0x200000402:0x1:0x0] \\\nef=0xf u=500:500 nid=10.128.11.158@tcp m=-w-\n```\n\nIt has the same information as a regular OPEN entry.", "mimetype": "text/plain", "start_char_idx": 12176, "end_char_idx": 15541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f3efa13-85c7-423c-b003-cc10f0e145fd": {"__data__": {"id_": "2f3efa13-85c7-423c-b003-cc10f0e145fd", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97ceb643-957e-4ba3-8375-750a353ea863", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b5f68faa6c3acdad46e2a1e8b31b3af459e62ae0239e3171e25fac87be810a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41c9842f-3d0a-4a2c-b5bb-9df0586049a8", "node_type": "1", "metadata": {}, "hash": "dd6bffa0f0203d692196f59586e9ac751289a6461ee8d283c9aa77c93707b3f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "##### `DENIED OPEN`\n\nA DENIED OPEN changelog entry is in the form:\n\n```\n4 24NOPEN 15:45:44.947406626 2017.08.31 0x2 t=[0x200000402:0x1:0x0] \\\nef=0xf u=500:500 nid=10.128.11.158@tcp m=-w-\n```\n\nIt has the same information as a regular OPEN entry. In order to avoid flooding the Changelogs, DENIED OPEN entries are rate limited: no more than one entry per user per file per time interval, this time interval (in seconds) being configurable via `mdd.<mdtname>.changelog_deniednext` (default value is 60 seconds).\n\n```\nmds# lctl set_param mdd.lustre-MDT0000.changelog_deniednext=120\nmdd.seb-MDT0000.changelog_deniednext=120\nmds# lctl get_param mdd.lustre-MDT0000.changelog_deniednext\nmdd.seb-MDT0000.changelog_deniednext=120\n```\n\n## Lustre Jobstats\n\nThe Lustre jobstats feature collects file system operation statistics for user processes running on Lustre clients, and exposes on the server using the unique Job Identifier (JobID) provided by the job scheduler for each job. Job schedulers known to be able to work with jobstats include: SLURM, SGE, LSF, Loadleveler, PBS and Maui/MOAB.\n\nSince jobstats is implemented in a scheduler-agnostic manner, it is likely that it will be able to work with other schedulers also, and also in environments that do not use a job scheduler, by storing custom format strings in the `jobid_name`.\n\n### How Jobstats Works\n\nThe Lustre jobstats code on the client extracts the unique JobID from an environment variable within the user process, and sends this JobID to the server with the I/O operation. The server tracks statistics for operations whose JobID is given, indexed by that ID.\n\nA Lustre setting on the client, `jobid_var`, specifies which environment variable to holds the JobID for that process Any environment variable can be specified. For example, SLURM sets the `SLURM_JOB_ID` environment variable with the unique job ID on each client when the job is first launched on a node, and the `SLURM_JOB_ID` will be inherited by all child processes started below that process.\n\nLustre can be configured to generate a synthetic JobID from the client's process name and numeric UID, by setting `jobid_var=procname_uid`. This will generate a uniform JobID when running the same binary across multiple client nodes, but cannot distinguish whether the binary is part of a single distributed process or multiple independent processes.\n\nIn Lustre 2.8 and later it is possible to set `jobid_var=nodelocal` and then also set `jobid_name=name`, which all processes on that client node will use. This is useful if only a single job is run on a client at one time, but if multiple jobs are run on a client concurrently, the per-session JobID should be used.\n\nIn Lustre 2.12 and later, it is possible to specify more complex JobID values for `jobid_name` by using a string that contains format codes that are evaluated for each process, in order to generate a site- or nodespecific JobID string.\n\n\u2022 %e print executable name \n\n\u2022 %g print group ID number \n\n\u2022 %h print fully-qualified hostname \n\n\u2022 %H print short hostname \n\n\u2022 %j print JobID from process environment variable named by the jobid_var parameter \n\n\u2022 %p print numeric process ID \n\n\u2022 %u print user ID number\n\nIn Lustre 2.13 and later, it is possible to set a per-session JobID by setting the `jobid_this_session` parameter. This will be inherited by all processes that are started in this login session, but there can be a different JobID for each login session.\n\nThe setting of `jobid_var` need not be the same on all clients. For example, one could use `SLURM_JOB_ID` on all clients managed by SLURM, and use `procname_uid` on clients not managed by SLURM, such as interactive login nodes.\n\nIt is not possible to have different `jobid_var` settings on a single node, since it is unlikely that multiple job schedulers are active on one client.", "mimetype": "text/plain", "start_char_idx": 15297, "end_char_idx": 19123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41c9842f-3d0a-4a2c-b5bb-9df0586049a8": {"__data__": {"id_": "41c9842f-3d0a-4a2c-b5bb-9df0586049a8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f3efa13-85c7-423c-b003-cc10f0e145fd", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "49025f6bf9b854e713f67cfa1004ac2f95fd12e45c5ab3b5511cbb5c2be4a732", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a382b835-c6ef-4ac1-89e1-dcf054ed463d", "node_type": "1", "metadata": {}, "hash": "498c0f03e49db61902e95c57e98262aaeb77e7b7401e128cf5d62c6f88a141d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This will be inherited by all processes that are started in this login session, but there can be a different JobID for each login session.\n\nThe setting of `jobid_var` need not be the same on all clients. For example, one could use `SLURM_JOB_ID` on all clients managed by SLURM, and use `procname_uid` on clients not managed by SLURM, such as interactive login nodes.\n\nIt is not possible to have different `jobid_var` settings on a single node, since it is unlikely that multiple job schedulers are active on one client. However, the actual JobID value is local to each process environment and it is possible for multiple jobs with different JobIDs to be active on a single client at one time\n\n### Enable/Disable Jobstats\n\nJobstats are disabled by default. The current state of jobstats can be verified by checking `lctl get_param jobid_var` on a client:\n\n```\n$ lctl get_param jobid_var\njobid_var=disable\n      \n```\n\nTo enable jobstats on the `testfs` file system with SLURM:\n\n```\n# lctl conf_param testfs.sys.jobid_var=SLURM_JOB_ID\n```\n\nThe `lctl conf_param` command to enable or disable jobstats should be run on the MGS as root. The change is persistent, and will be propagated to the MDS, OSS, and client nodes automatically when it is set on the MGS and for each new client mount.\n\nTo temporarily enable jobstats on a client, or to use a different jobid_var on a subset of nodes, such as nodes in a remote cluster that use a different job scheduler, or interactive login nodes that do not use a job scheduler at all, run the `lctl set_param`command directly on the client node(s) after the filesystem is mounted. For example, to enable the `procname_uid` synthetic JobID on a login node run:\n\n```\n# lctl set_param jobid_var=procname_uid\n```\n\nThe `lctl set_param` setting is not persistent, and will be reset if the global `jobid_var` is set on the MGS or if the filesystem is unmounted.\n\nThe following table shows the environment variables which are set by various job schedulers. Set `jobid_var` to the value for your job scheduler to collect statistics on a per job basis.\n\n| **Job Scheduler**                                    | **Environment Variable** |\n| ---------------------------------------------------- | ------------------------ |\n| Simple Linux Utility for Resource Management (SLURM) | SLURM_JOB_ID             |\n| Sun Grid Engine (SGE)                                | JOB_ID                   |\n| Load Sharing Facility (LSF)                          | LSB_JOBID                |\n| Loadleveler                                          | LOADL_STEP_ID            |\n| Portable Batch Scheduler (PBS)/MAUI                  | PBS_JOBID                |\n| Cray Application Level Placement Scheduler (ALPS)    | ALPS_APP_ID              |\n\nThere are two special values for `jobid_var`: `disable` and `procname_uid`. To disable jobstats, specify `jobid_var` as `disable`:\n\n```\n# lctl conf_param testfs.sys.jobid_var=disable\n```\n\nTo track job stats per process name and user ID (for debugging, or if no job scheduler is in use on some nodes such as login nodes), specify `jobid_var` as `procname_uid`:\n\n```\n# lctl conf_param testfs.sys.jobid_var=procname_uid\n```\n\n### Check Job Stats\n\nMetadata operation statistics are collected on MDTs. These statistics can be accessed for all file systems and all jobs on the MDT via the `lctl get_param mdt.*.job_stats`. For example, clients running with `jobid_var=procname_uid`:\n\n```\n# lctl get_param mdt.", "mimetype": "text/plain", "start_char_idx": 18603, "end_char_idx": 22059, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a382b835-c6ef-4ac1-89e1-dcf054ed463d": {"__data__": {"id_": "a382b835-c6ef-4ac1-89e1-dcf054ed463d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41c9842f-3d0a-4a2c-b5bb-9df0586049a8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "70403d6a51808cba8664a3993eee3a67c0733a1154ac5fc95a62b21aa87de221", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6a1833c-0d62-4f9c-ab24-55d14624bdd9", "node_type": "1", "metadata": {}, "hash": "219135c0374a693a49b599bb634d90b11d61c9873720be12f30c41f67701b1fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To disable jobstats, specify `jobid_var` as `disable`:\n\n```\n# lctl conf_param testfs.sys.jobid_var=disable\n```\n\nTo track job stats per process name and user ID (for debugging, or if no job scheduler is in use on some nodes such as login nodes), specify `jobid_var` as `procname_uid`:\n\n```\n# lctl conf_param testfs.sys.jobid_var=procname_uid\n```\n\n### Check Job Stats\n\nMetadata operation statistics are collected on MDTs. These statistics can be accessed for all file systems and all jobs on the MDT via the `lctl get_param mdt.*.job_stats`. For example, clients running with `jobid_var=procname_uid`:\n\n```\n# lctl get_param mdt.*.job_stats\njob_stats:\n- job_id:          bash.0\n  snapshot_time:   1352084992\n  open:            { samples:     2, unit:  reqs }\n  close:           { samples:     2, unit:  reqs }\n  mknod:           { samples:     0, unit:  reqs }\n  link:            { samples:     0, unit:  reqs }\n  unlink:          { samples:     0, unit:  reqs }\n  mkdir:           { samples:     0, unit:  reqs }\n  rmdir:           { samples:     0, unit:  reqs }\n  rename:          { samples:     0, unit:  reqs }\n  getattr:         { samples:     3, unit:  reqs }\n  setattr:         { samples:     0, unit:  reqs }\n  getxattr:        { samples:     0, unit:  reqs }\n  setxattr:        { samples:     0, unit:  reqs }\n  statfs:          { samples:     0, unit:  reqs }\n  sync:            { samples:     0, unit:  reqs }\n  samedir_rename:  { samples:     0, unit:  reqs }\n  crossdir_rename: { samples:     0, unit:  reqs }\n- job_id:          mythbackend.0\n  snapshot_time:   1352084996\n  open:            { samples:    72, unit:  reqs }\n  close:           { samples:    73, unit:  reqs }\n  mknod:           { samples:     0, unit:  reqs }\n  link:            { samples:     0, unit:  reqs }\n  unlink:          { samples:    22, unit:  reqs }\n  mkdir:           { samples:     0, unit:  reqs }\n  rmdir:           { samples:     0, unit:  reqs }\n  rename:          { samples:     0, unit:  reqs }\n  getattr:         { samples:   778, unit:  reqs }\n  setattr:         { samples:    22, unit:  reqs }\n  getxattr:        { samples:     0, unit:  reqs }\n  setxattr:        { samples:     0, unit:  reqs }\n  statfs:          { samples: 19840, unit:  reqs }\n  sync:            { samples: 33190, unit:  reqs }\n  samedir_rename:  { samples:     0, unit:  reqs }\n  crossdir_rename: { samples:     0, unit:  reqs }\n    \n```\n\nData operation statistics are collected on OSTs. Data operations statistics can be accessed via `lctl get_param obdfilter.*.job_stats`, for example:\n\n```\n$ lctl get_param obdfilter.", "mimetype": "text/plain", "start_char_idx": 21433, "end_char_idx": 24024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6a1833c-0d62-4f9c-ab24-55d14624bdd9": {"__data__": {"id_": "a6a1833c-0d62-4f9c-ab24-55d14624bdd9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a382b835-c6ef-4ac1-89e1-dcf054ed463d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2ef3401b84977e84bd4511115188ca54f05837775d1d2c16608707c55999afb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0676830f-743b-40b4-af9d-0948b4818846", "node_type": "1", "metadata": {}, "hash": "c5973ef3c669c0cacebade1d2f16b7544ae2d3aad971d80d164b5d07ce551f4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data operations statistics can be accessed via `lctl get_param obdfilter.*.job_stats`, for example:\n\n```\n$ lctl get_param obdfilter.*.job_stats\nobdfilter.myth-OST0000.job_stats=\njob_stats:\n- job_id:          mythcommflag.0\n  snapshot_time:   1429714922\n  read:    { samples: 974, unit: bytes, min: 4096, max: 1048576, sum: 91530035 }\n  write:   { samples:   0, unit: bytes, min:    0, max:       0, sum:        0 }\n  setattr: { samples:   0, unit:  reqs }\n  punch:   { samples:   0, unit:  reqs }\n  sync:    { samples:   0, unit:  reqs }\nobdfilter.myth-OST0001.job_stats=\njob_stats:\n- job_id:          mythbackend.0\n  snapshot_time:   1429715270\n  read:    { samples:   0, unit: bytes, min:     0, max:      0, sum:        0 }\n  write:   { samples:   1, unit: bytes, min: 96899, max:  96899, sum:    96899 }\n  setattr: { samples:   0, unit:  reqs }\n  punch:   { samples:   1, unit:  reqs }\n  sync:    { samples:   0, unit:  reqs }\nobdfilter.myth-OST0002.job_stats=job_stats:\nobdfilter.myth-OST0003.job_stats=job_stats:\nobdfilter.myth-OST0004.job_stats=\njob_stats:\n- job_id:          mythfrontend.500\n  snapshot_time:   1429692083\n  read:    { samples:   9, unit: bytes, min: 16384, max: 1048576, sum: 4444160 }\n  write:   { samples:   0, unit: bytes, min:     0, max:       0, sum:       0 }\n  setattr: { samples:   0, unit:  reqs }\n  punch:   { samples:   0, unit:  reqs }\n  sync:    { samples:   0, unit:  reqs }\n- job_id:          mythbackend.500\n  snapshot_time:   1429692129\n  read:    { samples:   0, unit: bytes, min:     0, max:       0, sum:       0 }\n  write:   { samples:   1, unit: bytes, min: 56231, max:   56231, sum:   56231 }\n  setattr: { samples:   0, unit:  reqs }\n  punch:   { samples:   1, unit:  reqs }\n  sync:    { samples:   0, unit:  reqs }\n```\n\n### Clear Job Stats\n\nAccumulated job statistics can be reset by writing proc file `job_stats`.\n\nClear statistics for all jobs on the local node:\n\n```\n# lctl set_param obdfilter.*.job_stats=clear\n```\n\nClear statistics only for job 'bash.0' on lustre-MDT0000:\n\n```\n# lctl set_param mdt.lustre-MDT0000.job_stats=bash.0\n```\n\n### Configure Auto-cleanup Interval\n\nBy default, if a job is inactive for 600 seconds (10 minutes) statistics for this job will be dropped. This expiration value can be changed temporarily via:\n\n```\n# lctl set_param *.*.job_cleanup_interval={max_age}\n```\n\nIt can also be changed permanently, for example to 700 seconds via:\n\n```\n# lctl conf_param testfs.mdt.job_cleanup_interval=700\n```\n\nThe `job_cleanup_interval` can be set as 0 to disable the auto-cleanup. Note that if auto-cleanup of Jobstats is disabled, then all statistics will be kept in memory forever, which may eventually consume all memory on the servers. In this case, any monitoring tool should explicitly clear individual job statistics as they are processed, as shown above.\n\n## Lustre Monitoring Tool (LMT)\n\nThe Lustre Monitoring Tool (LMT) is a Python-based, distributed system that provides a top-like display of activity on server-side nodes (MDS, OSS and portals routers) on one or more Lustre file systems.", "mimetype": "text/plain", "start_char_idx": 23892, "end_char_idx": 26962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0676830f-743b-40b4-af9d-0948b4818846": {"__data__": {"id_": "0676830f-743b-40b4-af9d-0948b4818846", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb3d8a82-59bb-4143-bfee-fdc2379b11d6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47e480347fd4014338a1d4ecc44674635ee7ab6595a64cbdc201323ce217a7e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6a1833c-0d62-4f9c-ab24-55d14624bdd9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3ea00e952a4c681a25acba8993ae807552156ba4bcdb09b87fb6ef77392c9350", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This expiration value can be changed temporarily via:\n\n```\n# lctl set_param *.*.job_cleanup_interval={max_age}\n```\n\nIt can also be changed permanently, for example to 700 seconds via:\n\n```\n# lctl conf_param testfs.mdt.job_cleanup_interval=700\n```\n\nThe `job_cleanup_interval` can be set as 0 to disable the auto-cleanup. Note that if auto-cleanup of Jobstats is disabled, then all statistics will be kept in memory forever, which may eventually consume all memory on the servers. In this case, any monitoring tool should explicitly clear individual job statistics as they are processed, as shown above.\n\n## Lustre Monitoring Tool (LMT)\n\nThe Lustre Monitoring Tool (LMT) is a Python-based, distributed system that provides a top-like display of activity on server-side nodes (MDS, OSS and portals routers) on one or more Lustre file systems. It does not provide support for monitoring clients. For more information on LMT, including the setup procedure, see:\n\n[https://github.com/chaos/lmt/wiki](https://github.com/chaos/lmt/wiki)\n\n## `CollectL`\n\n`CollectL` is another tool that can be used to monitor a Lustre file system. You can run `CollectL` on a Lustre system that has any combination of MDSs, OSTs and clients. The collected data can be written to a file for continuous logging and played back at a later time. It can also be converted to a format suitable for plotting.\n\nFor more information about `CollectL`, see:\n\n<http://collectl.sourceforge.net>\n\nLustre-specific documentation is also available. See:\n\n<http://collectl.sourceforge.net/Tutorial-Lustre.html>\n\n## Other Monitoring Options\n\nA variety of standard tools are available publicly including the following:\n\n- `lltop` - Lustre load monitor with batch scheduler integration. <https://github.com/jhammond/lltop>\n- `tacc_stats` - A job-oriented system monitor, analyzation, and visualization tool that probes Lustre interfaces and collects statistics. <https://github.com/jhammond/tacc_stats>\n- `xltop` - A continuous Lustre monitor with batch scheduler integration. <https://github.com/jhammond/xltop>\n\nAnother option is to script a simple monitoring solution that looks at various reports from `ipconfig`, as well as the `procfs` files generated by the Lustre software.", "mimetype": "text/plain", "start_char_idx": 26123, "end_char_idx": 28357, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "850eb63d-3f73-47be-a4f8-194429b25a53": {"__data__": {"id_": "850eb63d-3f73-47be-a4f8-194429b25a53", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fff02b91-315c-4bda-8221-b5ee521682f3", "node_type": "1", "metadata": {}, "hash": "deca5e5eef6b0ea2608fae7674ec9cc985f18542c6e448b9deea74bdbe02133e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Lustre Operations\n\nOnce you have the Lustre file system up and running, you can use the procedures in this section to perform these basic Lustre administration tasks.\n\n- [Lustre Operations](#lustre-operations)\n  * [Mounting by Label](#mounting-by-label)\n  * [Starting Lustre](#starting-lustre)\n  * [Mounting a Server](#mounting-a-server)\n  * [Stopping the Filesystem](#stopping-the-filesystem)\n  * [Unmounting a Specific Target on a Server](#unmounting-a-specific-target-on-a-server)\n  * [Specifying Failout/Failover Mode for OSTs](#specifying-failoutfailover-mode-for-osts)\n  * [Handling Degraded OST RAID Arrays](#handling-degraded-ost-raid-arrays)\n  * [Running Multiple Lustre File Systems](#running-multiple-lustre-file-systems)\n  * [Creating a sub-directory on a specific MDT](#creating-a-sub-directory-on-a-specific-mdt)L2.4\n  * [Creating a directory striped across multiple MDTs](#creating-a-directory-striped-across-multiple-mdts)L2.8\n  * [Setting and Retrieving Lustre Parameters](#setting-and-retrieving-lustre-parameters)\n    + [Setting Tunable Parameters with `mkfs.lustre`](#setting-tunable-parameters-with-mkfslustre)\n    + [Setting Parameters with `tunefs.lustre`](#setting-parameters-with-tunefslustre)\n    + [Setting Parameters with `lctl`](#setting-parameters-with-lctl)\n      - [Setting Temporary Parameters](#setting-temporary-parameters)\n      - [Setting Permanent Parameters](#setting-permanent-parameters)\n      - [Setting Permanent Parameters with lctl set_param -P](#setting-permanent-parameters-with-lctl-set_param--p)L2.5\n      - [Listing Parameters](#listing-parameters)\n      - [Reporting Current Parameter Values](#reporting-current-parameter-values)\n  * [Specifying NIDs and Failover](#specifying-nids-and-failover)\n  * [Erasing a File System](#erasing-a-file-system)\n  * [Reclaiming Reserved Disk Space](#reclaiming-reserved-disk-space)\n  * [Replacing an Existing OST or MDT](#replacing-an-existing-ost-or-mdt)\n  * [Identifying To Which Lustre File an OST Object Belongs](#identifying-to-which-lustre-file-an-ost-object-belongs)\n\n## Mounting by Label\n\nThe file system name is limited to 8 characters. We have encoded the file system and target information in the disk label, so you can mount by label. This allows system administrators to move disks around without worrying about issues such as SCSI disk reordering or getting the `/dev/device` wrong for a shared target. Soon, file system naming will be made as fail-safe as possible. Currently, Linux disk labels are limited to 16 characters. To identify the target within the file system, 8 characters are reserved, leaving 8 characters for the file system name:\n\n```\nfsname-MDT0000 or \nfsname-OST0a19\n```\n\nTo mount by label, use this command:\n\n```\nmount -t lustre -L \nfile_system_label \n/mount_point\n```\n\nThis is an example of mount-by-label:\n\n```\nmds# mount -t lustre -L testfs-MDT0000 /mnt/mdt\n```\n\n**Caution**\n\nMount-by-label should NOT be used in a multi-path environment or when snapshots are being created of the device, since multiple block devices will have the same label.\n\nAlthough the file system name is internally limited to 8 characters, you can mount the clients at any mount point, so file system users are not subjected to short names. Here is an example:\n\n```\nclient# mount -t lustre mds0@tcp0:/short \n/dev/long_mountpoint_name\n```\n\n## Starting Lustre\n\nOn the first start of a Lustre file system, the components must be started in the following order:\n\n1. Mount the MGT.\n\n   ### Note\n\n   If a combined MGT/MDT is present, Lustre will correctly mount the MGT and MDT automatically.\n\n2. Mount the MDT.\n\n   ### Note\n\n   Introduced in Lustre 2.4Mount all MDTs if multiple MDTs are present.\n\n3. Mount the OST(s).\n\n4. Mount the client(s).\n\n## Mounting a Server\n\nStarting a Lustre server is straightforward and only involves the mount command.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fff02b91-315c-4bda-8221-b5ee521682f3": {"__data__": {"id_": "fff02b91-315c-4bda-8221-b5ee521682f3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "850eb63d-3f73-47be-a4f8-194429b25a53", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4876c416cc3d0156debf7a45190b86c4c34a6043a4e7ee2a5e53bba4d7e093b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a480064-3ecd-4233-8b6a-e9cd659bd8a1", "node_type": "1", "metadata": {}, "hash": "6a41565e96174b745e198d5734417a2b11f91a4ec621a37c2667ac96815dea14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here is an example:\n\n```\nclient# mount -t lustre mds0@tcp0:/short \n/dev/long_mountpoint_name\n```\n\n## Starting Lustre\n\nOn the first start of a Lustre file system, the components must be started in the following order:\n\n1. Mount the MGT.\n\n   ### Note\n\n   If a combined MGT/MDT is present, Lustre will correctly mount the MGT and MDT automatically.\n\n2. Mount the MDT.\n\n   ### Note\n\n   Introduced in Lustre 2.4Mount all MDTs if multiple MDTs are present.\n\n3. Mount the OST(s).\n\n4. Mount the client(s).\n\n## Mounting a Server\n\nStarting a Lustre server is straightforward and only involves the mount command. Lustre servers can be added to `/etc/fstab`:\n\n```\nmount -t lustre\n```\n\nThe mount command generates output similar to this:\n\n```\n/dev/sda1 on /mnt/test/mdt type lustre (rw)\n/dev/sda2 on /mnt/test/ost0 type lustre (rw)\n192.168.0.21@tcp:/testfs on /mnt/testfs type lustre (rw)\n```\n\nIn this example, the MDT, an OST (ost0) and file system (testfs) are mounted.\n\n```\nLABEL=testfs-MDT0000 /mnt/test/mdt lustre defaults,_netdev,noauto 0 0\nLABEL=testfs-OST0000 /mnt/test/ost0 lustre defaults,_netdev,noauto 0 0\n```\n\nIn general, it is wise to specify noauto and let your high-availability (HA) package manage when to mount the device. If you are not using failover, make sure that networking has been started before mounting a Lustre server. If you are running Red Hat Enterprise Linux, SUSE Linux Enterprise Server, Debian operating system (and perhaps others), use the `_netdev` flag to ensure that these disks are mounted after the network is up.\n\nWe are mounting by disk label here. The label of a device can be read with `e2label`. The label of a newly-formatted Lustre server may end in `FFFF` if the `--index` option is not specified to `mkfs.lustre`, meaning that it has yet to be assigned. The assignment takes place when the server is first started, and the disk label is updated. It is recommended that the `--index` option always be used, which will also ensure that the label is set at format time.\n\n**Caution**\n\nDo not do this when the client and OSS are on the same node, as memory pressure between the client and OSS can lead to deadlocks.\n\n**Caution**\n\nMount-by-label should NOT be used in a multi-path environment.\n\n\n\n## Stopping the Filesystem\n\nA complete Lustre filesystem shutdown occurs by unmounting all clients and servers in the order shown below. Please note that unmounting a block device causes the Lustre software to be shut down on that node.\n\n**Note**\n\nPlease note that the `-a -t lustre` in the commands below is not the name of a filesystem, but rather is specifying to unmount all entries in /etc/mtab that are of type `lustre`\n\n1. Unmount the clients\n\n   On each client node, unmount the filesystem on that client using the `umount` command:\n\n   `umount -a -t lustre`\n\n   The example below shows the unmount of the `testfs` filesystem on a client node:\n\n   ```\n   [root@client1 ~]# mount |grep testfs\n   XXX.XXX.0.11@tcp:/testfs on /mnt/testfs type lustre (rw,lazystatfs)\n   \n   [root@client1 ~]# umount -a -t lustre\n   [154523.177714] Lustre: Unmounted testfs-client\n   ```\n\n2.", "mimetype": "text/plain", "start_char_idx": 3241, "end_char_idx": 6347, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a480064-3ecd-4233-8b6a-e9cd659bd8a1": {"__data__": {"id_": "5a480064-3ecd-4233-8b6a-e9cd659bd8a1", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fff02b91-315c-4bda-8221-b5ee521682f3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2f0e6c9183d76f0c8fde4b393281b2c591377207dfc224096095525ef97a5fd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ae89cdf-3a48-417e-be0d-ae50a557964b", "node_type": "1", "metadata": {}, "hash": "f549d5dae450d59d1e57079d47cfb024b1c565dedae34862b9be8621af830ec4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Note**\n\nPlease note that the `-a -t lustre` in the commands below is not the name of a filesystem, but rather is specifying to unmount all entries in /etc/mtab that are of type `lustre`\n\n1. Unmount the clients\n\n   On each client node, unmount the filesystem on that client using the `umount` command:\n\n   `umount -a -t lustre`\n\n   The example below shows the unmount of the `testfs` filesystem on a client node:\n\n   ```\n   [root@client1 ~]# mount |grep testfs\n   XXX.XXX.0.11@tcp:/testfs on /mnt/testfs type lustre (rw,lazystatfs)\n   \n   [root@client1 ~]# umount -a -t lustre\n   [154523.177714] Lustre: Unmounted testfs-client\n   ```\n\n2. Unmount the MDT and MGT\n\n   On the MGS and MDS node(s), run the `umount` command:\n\n   `umount -a -t lustre`\n\n   The example below shows the unmount of the MDT and MGT for the `testfs` filesystem on a combined MGS/MDS:\n\n   ```\n   [root@mds1 ~]# mount |grep lustre\n   /dev/sda on /mnt/mgt type lustre (ro)\n   /dev/sdb on /mnt/mdt type lustre (ro)\n   \n   [root@mds1 ~]# umount -a -t lustre\n   [155263.566230] Lustre: Failing over testfs-MDT0000\n   [155263.775355] Lustre: server umount testfs-MDT0000 complete\n   [155269.843862] Lustre: server umount MGS complete\n   ```\n\n   For a seperate MGS and MDS, the same command is used, first on the MDS and then followed by the MGS.\n\n3. Unmount all the OSTs\n\n   On each OSS node, use the `umount` command:\n\n   `umount -a -t lustre`\n\n   The example below shows the unmount of all OSTs for the `testfs` filesystem on server `OSS1`:\n\n   ```\n   [root@oss1 ~]# mount |grep lustre\n   /dev/sda on /mnt/ost0 type lustre (ro)\n   /dev/sdb on /mnt/ost1 type lustre (ro)\n   /dev/sdc on /mnt/ost2 type lustre (ro)\n   \n   [root@oss1 ~]# umount -a -t lustre\n   [155336.491445] Lustre: Failing over testfs-OST0002\n   [155336.556752] Lustre: server umount testfs-OST0002 complete\n   ```\n\nFor unmount command syntax for a single OST, MDT, or MGT target please refer to [*the section called \u201c Unmounting a Specific Target on a Server\u201d*](#unmounting-a-specific-target-on-a-server)\n\n## Unmounting a Specific Target on a Server\n\nTo stop a Lustre OST, MDT, or MGT , use the `umount */mount_point*` command.\n\nThe example below stops an OST, `ost0`, on mount point `/mnt/ost0` for the `testfs` filesystem:\n\n```\n[root@oss1 ~]# umount /mnt/ost0\n[  385.142264] Lustre: Failing over testfs-OST0000\n[  385.210810] Lustre: server umount testfs-OST0000 complete\n```\n\nGracefully stopping a server with the `umount` command preserves the state of the connected clients. The next time the server is started, it waits for clients to reconnect, and then goes through the recovery procedure.\n\nIf the force ( `-f`) flag is used, then the server evicts all clients and stops WITHOUT recovery. Upon restart, the server does not wait for recovery. Any currently connected clients receive I/O errors until they reconnect.\n\n**Note**\n\nIf you are using loopback devices, use the `-d` flag. This flag cleans up loop devices and can always be safely specified.", "mimetype": "text/plain", "start_char_idx": 5708, "end_char_idx": 8700, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ae89cdf-3a48-417e-be0d-ae50a557964b": {"__data__": {"id_": "4ae89cdf-3a48-417e-be0d-ae50a557964b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a480064-3ecd-4233-8b6a-e9cd659bd8a1", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4719e233f43cef9485a57e67c5f124c4fc9f33497c07acdd7840e785476f7e56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c7f22bd-f4e4-48a6-9398-339099bcd19b", "node_type": "1", "metadata": {}, "hash": "3c0dc2d4ae28d8137bd5f3459c24f0a49bb30a42c77f665f6c083eb9dc1ff0ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The next time the server is started, it waits for clients to reconnect, and then goes through the recovery procedure.\n\nIf the force ( `-f`) flag is used, then the server evicts all clients and stops WITHOUT recovery. Upon restart, the server does not wait for recovery. Any currently connected clients receive I/O errors until they reconnect.\n\n**Note**\n\nIf you are using loopback devices, use the `-d` flag. This flag cleans up loop devices and can always be safely specified.\n\n## Specifying Failout/Failover Mode for OSTs\n\nIn a Lustre file system, an OST that has become unreachable because it fails, is taken off the network, or is unmounted can be handled in one of two ways:\n\n- In `failout` mode, Lustre clients immediately receive errors (EIOs) after a timeout, instead of waiting for the OST to recover.\n- In `failover` mode, Lustre clients wait for the OST to recover.\n\nBy default, the Lustre file system uses `failover` mode for OSTs. To specify `failout` mode instead, use the `--param=\"failover.mode=failout\"` option as shown below (entered on one line):\n\n```\noss# mkfs.lustre --fsname=\nfsname --mgsnode=\nmgs_NID --param=failover.mode=failout \n      --ost --index=\nost_index \n/dev/ost_block_device\n```\n\nIn the example below, `failout` mode is specified for the OSTs on the MGS `mds0` in the file system `testfs`(entered on one line).\n\n```\noss# mkfs.lustre --fsname=testfs --mgsnode=mds0 --param=failover.mode=failout \n      --ost --index=3 /dev/sdb \n```\n\n**Caution**\n\nBefore running this command, unmount all OSTs that will be affected by a change in `failover`/ `failout` mode.\n\n**Note**\n\nAfter initial file system configuration, use the `tunefs.lustre` utility to change the mode. For example, to set the `failout` mode, run:\n\n```\n$ tunefs.lustre --param failover.mode=failout \n/dev/ost_device\n```\n\n## Handling Degraded OST RAID Arrays\n\nLustre includes functionality that notifies Lustre if an external RAID array has degraded performance (resulting in reduced overall file system performance), either because a disk has failed and not been replaced, or because a disk was replaced and is undergoing a rebuild. To avoid a global performance slowdown due to a degraded OST, the MDS can avoid the OST for new object allocation if it is notified of the degraded state.\n\nA parameter for each OST, called `degraded`, specifies whether the OST is running in degraded mode or not.\n\nTo mark the OST as degraded, use:\n\n```\nlctl set_param obdfilter.{OST_name}.degraded=1\n```\n\nTo mark that the OST is back in normal operation, use:\n\n```\nlctl set_param obdfilter.{OST_name}.degraded=0\n```\n\nTo determine if OSTs are currently in degraded mode, use:\n\n```\nlctl get_param obdfilter.*.degraded\n```\n\nIf the OST is remounted due to a reboot or other condition, the flag resets to `0`.\n\nIt is recommended that this be implemented by an automated script that monitors the status of individual RAID devices, such as MD-RAID's `mdadm(8)` command with the `--monitor` option to mark an affected device degraded or restored.\n\n## Running Multiple Lustre File Systems\n\nLustre supports multiple file systems provided the combination of `NID:fsname` is unique. Each file system must be allocated a unique name during creation with the `--fsname` parameter. Unique names for file systems are enforced if a single MGS is present. If multiple MGSs are present (for example if you have an MGS on every MDS) the administrator is responsible for ensuring file system names are unique. A single MGS and unique file system names provides a single point of administration and allows commands to be issued against the file system even if it is not mounted.\n\nLustre supports multiple file systems on a single MGS. With a single MGS fsnames are guaranteed to be unique. Lustre also allows multiple MGSs to co-exist. For example, multiple MGSs will be necessary if multiple file systems on different Lustre software versions are to be concurrently available. With multiple MGSs additional care must be taken to ensure file system names are unique.", "mimetype": "text/plain", "start_char_idx": 8224, "end_char_idx": 12241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c7f22bd-f4e4-48a6-9398-339099bcd19b": {"__data__": {"id_": "1c7f22bd-f4e4-48a6-9398-339099bcd19b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ae89cdf-3a48-417e-be0d-ae50a557964b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b38d154c95d3fe3bb5a108ea0a32972ed6c8106f7d0f785134929d6cbbdcd637", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89a49ef1-3931-4d51-a21a-0c4aa7385ee8", "node_type": "1", "metadata": {}, "hash": "4fb60e2364a9b2664cba58c455f02ff8218c509a4f0d4ac8da0a881ed4116e92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each file system must be allocated a unique name during creation with the `--fsname` parameter. Unique names for file systems are enforced if a single MGS is present. If multiple MGSs are present (for example if you have an MGS on every MDS) the administrator is responsible for ensuring file system names are unique. A single MGS and unique file system names provides a single point of administration and allows commands to be issued against the file system even if it is not mounted.\n\nLustre supports multiple file systems on a single MGS. With a single MGS fsnames are guaranteed to be unique. Lustre also allows multiple MGSs to co-exist. For example, multiple MGSs will be necessary if multiple file systems on different Lustre software versions are to be concurrently available. With multiple MGSs additional care must be taken to ensure file system names are unique. Each file system should have a unique fsname among all systems that may interoperate in the future.\n\nBy default, the `mkfs.lustre` command creates a file system named `lustre`. To specify a different file system name (limited to 8 characters) at format time, use the `--fsname` option:\n\n```\nmkfs.lustre --fsname=\nfile_system_name\n```\n\n**Note**\n\nThe MDT, OSTs and clients in the new file system must use the same file system name (prepended to the device name). For example, for a new file system named `foo`, the MDT and two OSTs would be named `foo-MDT0000`, `foo-OST0000`, and `foo-OST0001`.\n\nTo mount a client on the file system, run:\n\n```\nclient# mount -t lustre \nmgsnode:\n/new_fsname \n/mount_point\n```\n\nFor example, to mount a client on file system foo at mount point /mnt/foo, run:\n\n```\nclient# mount -t lustre mgsnode:/foo /mnt/foo\n```\n\n**Note**\n\nIf a client(s) will be mounted on several file systems, add the following line to `/etc/xattr.conf` file to avoid problems when files are moved between the file systems: `lustre.* skip`\n\n**Note**\n\nTo ensure that a new MDT is added to an existing MGS create the MDT by specifying: `--mdt --mgsnode= *mgs_NID*`.\n\nA Lustre installation with two file systems ( `foo` and `bar`) could look like this, where the MGS node is `mgsnode@tcp0` and the mount points are `/mnt/foo` and `/mnt/bar`.\n\n```\nmgsnode# mkfs.lustre --mgs /dev/sda\nmdtfoonode# mkfs.lustre --fsname=foo --mgsnode=mgsnode@tcp0 --mdt --index=0\n/dev/sdb\nossfoonode# mkfs.lustre --fsname=foo --mgsnode=mgsnode@tcp0 --ost --index=0\n/dev/sda\nossfoonode# mkfs.lustre --fsname=foo --mgsnode=mgsnode@tcp0 --ost --index=1\n/dev/sdb\nmdtbarnode# mkfs.lustre --fsname=bar --mgsnode=mgsnode@tcp0 --mdt --index=0\n/dev/sda\nossbarnode# mkfs.lustre --fsname=bar --mgsnode=mgsnode@tcp0 --ost --index=0\n/dev/sdc\nossbarnode# mkfs.lustre --fsname=bar --mgsnode=mgsnode@tcp0 --ost --index=1\n/dev/sdd\n```\n\nTo mount a client on file system foo at mount point `/mnt/foo`, run:\n\n```\nclient# mount -t lustre mgsnode@tcp0:/foo /mnt/foo\n```\n\nTo mount a client on file system bar at mount point `/mnt/bar`, run:\n\n```\nclient# mount -t lustre mgsnode@tcp0:/bar /mnt/bar\n```\n\n \n\nIntroduced in Lustre 2.4\n\n## Creating a sub-directory on a specific MDT\n\nIt is possible to create individual directories, along with its files and sub-directories, to be stored on specific MDTs. To create a sub-directory on a given MDT use the command:\n\n```\nclient# lfs mkdir \u2013i\nmdt_index\n/mount_point/remote_dir\n```\n\nThis command will allocate the sub-directory\u00a0`remote_dir`\u00a0onto the MDT of index\u00a0`mdt_index`.", "mimetype": "text/plain", "start_char_idx": 11368, "end_char_idx": 14810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89a49ef1-3931-4d51-a21a-0c4aa7385ee8": {"__data__": {"id_": "89a49ef1-3931-4d51-a21a-0c4aa7385ee8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c7f22bd-f4e4-48a6-9398-339099bcd19b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0a6a014c8ca36f5690f60ea4ec1c4e57afbaa5f9ba10f6e0c0173dbb8be7131c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00576cf0-08da-4d13-a604-ced094435a92", "node_type": "1", "metadata": {}, "hash": "f1b86bbdb8b1e23c464021534964c16dd9a262bc80dfe96d8dc8df67fd0e396c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To create a sub-directory on a given MDT use the command:\n\n```\nclient# lfs mkdir \u2013i\nmdt_index\n/mount_point/remote_dir\n```\n\nThis command will allocate the sub-directory\u00a0`remote_dir`\u00a0onto the MDT of index\u00a0`mdt_index`. For more information on adding additional MDTs and\u00a0`mdt_index`\u00a0see\u00a0[2](02.07-Configuring%20a%20Lustre%20File%20System.md##configuring-a-simple-lustre-file-system)\n\n**Warning**\n\nAn administrator can allocate remote sub-directories to separate MDTs. Creating remote sub-directories in parent directories not hosted on MDT0000 is not recommended. This is because the failure of the parent MDT will leave the namespace below it inaccessible. For this reason, by default it is only possible to create remote sub-directories off MDT0000. To relax this restriction and enable remote sub-directories off any MDT, an administrator must issue the following command on the MGS:\n\n```\nmgs# lctl conf_param fsname.mdt.enable_remote_dir=1\n```\n\nFor Lustre filesystem 'scratch', the command executed is:\n\n```\nmgs# lctl conf_param scratch.mdt.enable_remote_dir=1\n```\n\nTo verify the configuration setting execute the following command on any MDS:\n\n```\nmds# lctl get_param mdt.*.enable_remote_dir\n```\n\nIntroduced in Lustre 2.8\n\nWith Lustre software version 2.8, a new tunable is available to allow users with a specific group ID to create and delete remote and striped directories. This tunable is `enable_remote_dir_gid`. For example, setting this parameter to the 'wheel' or 'admin' group ID allows users with that GID to create and delete remote and striped directories. Setting this parameter to `-1` on MDT0000 to permanently allow any non-root users create and delete remote and striped directories. On the MGS execute the following command:\n\n`mgs# lctl conf_param *fsname*.mdt.enable_remote_dir_gid=-1`\n\nFor the Lustre filesystem 'scratch', the commands expands to:\n\n`mgs# lctl conf_param scratch.mdt.enable_remote_dir_gid=-1`. \n\nThe change can be verified by executing the following command on every MDS:\n\n`mds# lctl get_param mdt.***.enable_remote_dir_gid`\n\n Introduced in Lustre 2.8\n\n## Creating a directory striped across multiple MDTs\n\n\n\nThe Lustre 2.8 DNE feature enables individual files in a given directory to store their metadata on separate MDTs (a *striped directory*) once additional MDTs have been added to the filesystem, see *the section called \u201cAdding a New MDT to a Lustre File System\u201d*. The result of this is that metadata requests for files in a striped directory are serviced by multiple MDTs and metadata service load is distributed over all the MDTs that service a given directory. By distributing metadata service load over multiple MDTs, performance can be improved beyond the limit of single MDT performance. Prior to the development of this feature all files in a directory must record their metadata on a single MDT.\n\nThis command to stripe a directory over *mdt_count* MDTs is:\n\n```\nclient# lfs mkdir -c\nmdt_count\n/mount_point/new_directory\n```\n\nThe striped directory feature is most useful for distributing single large directories (50k entries or more) across multiple MDTs, since it incurs more overhead than non-striped directories.\n\n###  Directory creation by space/inode usage\n\nIf the starting MDT is not specified when creating a new directory, this directory and its stripes will be distributed on MDTs by space usage. For example the following will create a directory and its stripes on MDTs with balanced space usage:\n\n```\nlfs mkdir -c 2 <dir1>\n```\n\nAlternatively, if a default directory stripe is set on a directory, the subsequent syscall `mkdir` under will `<dir1>` have the same effect:\n\n```\nlfs setdirstripe -D -c 2 <dir1>\n```\n\nThe policy is: \n\n\u2022 If free inodes/blocks on all MDT are almost the same, i.e. `max_inodes_avail * 84% < min_inodes_avail `and `max_blocks_avail * 84% < min_blocks_avail`, then choose MDT roundrobin. \n\n\u2022 Otherwise, create more subdirectories on MDTs with more free inodes/blocks.", "mimetype": "text/plain", "start_char_idx": 14595, "end_char_idx": 18548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00576cf0-08da-4d13-a604-ced094435a92": {"__data__": {"id_": "00576cf0-08da-4d13-a604-ced094435a92", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89a49ef1-3931-4d51-a21a-0c4aa7385ee8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0bb956953bb772ceb3a40d117db983ddc62e838c76be9e1ec2204b1a728d4eb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "169f02aa-7d6b-4ae2-bd3e-6a78d9255178", "node_type": "1", "metadata": {}, "hash": "b63ac37514d84850f3fa8f7d28a20597dfedcddf7393e42bc01b23ac3d1ca183", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example the following will create a directory and its stripes on MDTs with balanced space usage:\n\n```\nlfs mkdir -c 2 <dir1>\n```\n\nAlternatively, if a default directory stripe is set on a directory, the subsequent syscall `mkdir` under will `<dir1>` have the same effect:\n\n```\nlfs setdirstripe -D -c 2 <dir1>\n```\n\nThe policy is: \n\n\u2022 If free inodes/blocks on all MDT are almost the same, i.e. `max_inodes_avail * 84% < min_inodes_avail `and `max_blocks_avail * 84% < min_blocks_avail`, then choose MDT roundrobin. \n\n\u2022 Otherwise, create more subdirectories on MDTs with more free inodes/blocks.\n\n## Setting and Retrieving Lustre Parameters\n\nSeveral options are available for setting parameters in Lustre:\n\n- When creating a file system, use mkfs.lustre. See *the section called \u201cSetting Tunable Parameters with `mkfs.lustre`\u201d*below.\n- When a server is stopped, use tunefs.lustre. See *the section called \u201cSetting Parameters with `tunefs.lustre`\u201d* below.\n- When the file system is running, use lctl to set or retrieve Lustre parameters. See *the section called \u201cSetting Parameters with `lctl`\u201d* and *the section called \u201cReporting Current Parameter Values\u201d* below.\n\n### Setting Tunable Parameters with `mkfs.lustre`\n\nWhen the file system is first formatted, parameters can simply be added as a `--param` option to the `mkfs.lustre` command. For example:\n\n```\nmds# mkfs.lustre --mdt --param=\"sys.timeout=50\" /dev/sda\n```\n\nFor more details about creating a file system,see *Configuring a Lustre File System*. For more details about `mkfs.lustre`, see *System Configuration Utilities*.\n\n### Setting Parameters with `tunefs.lustre`\n\nIf a server (OSS or MDS) is stopped, parameters can be added to an existing file system using the `--param` option to the `tunefs.lustre` command. For example:\n\n```\noss# tunefs.lustre --param=failover.node=192.168.0.13@tcp0 /dev/sda\n```\n\nWith `tunefs.lustre`, parameters are *additive*-- new parameters are specified in addition to old parameters, they do not replace them. To erase all old `tunefs.lustre` parameters and just use newly-specified parameters, run:\n\n```\nmds# tunefs.lustre --erase-params --param=\nnew_parameters \n```\n\nThe tunefs.lustre command can be used to set any parameter settable via `lctl conf_param` and that has its own OBD device, so it can be specified as `*obdname|fsname*. *obdtype*. *proc_file_name*= *value*`. For example:\n\n```\nmds# tunefs.lustre --param mdt.identity_upcall=NONE /dev/sda1\n```\n\nFor more details about `tunefs.lustre`, see *System Configuration Utilities*.\n\n### Setting Parameters with `lctl`\n\nWhen the file system is running, the `lctl` command can be used to set parameters (temporary or permanent) and report current parameter values. Temporary parameters are active as long as the server or client is not shut down. Permanent parameters live through server and client reboots.\n\n**Note**\n\nThe `lctl list_param` command enables users to list all parameters that can be set. See [the section called \u201cListing Parameters\u201d](#listing-parameters).\n\nFor more details about the `lctl` command, see the examples in the sections below and [*System Configuration Utilities*](06.07-System%20Configuration%20Utilities.md).\n\n#### Setting Temporary Parameters\n\nUse `lctl set_param` to set temporary parameters on the node where it is run. These parameters map to items in `/proc/{fs,sys}/{lnet,lustre}`. The `lctl set_param` command uses this syntax:\n\n```\nlctl set_param [-n] [-P]\nobdtype.\nobdname.\nproc_file_name=\nvalue\n```\n\nFor example:\n\n```\n# lctl set_param osc.", "mimetype": "text/plain", "start_char_idx": 17954, "end_char_idx": 21479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "169f02aa-7d6b-4ae2-bd3e-6a78d9255178": {"__data__": {"id_": "169f02aa-7d6b-4ae2-bd3e-6a78d9255178", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00576cf0-08da-4d13-a604-ced094435a92", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e11fd7fc5bdad382c9e25deff713dcf498ebd73da618e7dae8d0c7123176ef4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55670111-0771-4ded-b490-742332a58dee", "node_type": "1", "metadata": {}, "hash": "3638af1f68b7494ed50b17e8f90e20dcda89c26544bac38a727ba512f0f7c5d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Permanent parameters live through server and client reboots.\n\n**Note**\n\nThe `lctl list_param` command enables users to list all parameters that can be set. See [the section called \u201cListing Parameters\u201d](#listing-parameters).\n\nFor more details about the `lctl` command, see the examples in the sections below and [*System Configuration Utilities*](06.07-System%20Configuration%20Utilities.md).\n\n#### Setting Temporary Parameters\n\nUse `lctl set_param` to set temporary parameters on the node where it is run. These parameters map to items in `/proc/{fs,sys}/{lnet,lustre}`. The `lctl set_param` command uses this syntax:\n\n```\nlctl set_param [-n] [-P]\nobdtype.\nobdname.\nproc_file_name=\nvalue\n```\n\nFor example:\n\n```\n# lctl set_param osc.*.max_dirty_mb=1024\nosc.myth-OST0000-osc.max_dirty_mb=32\nosc.myth-OST0001-osc.max_dirty_mb=32\nosc.myth-OST0002-osc.max_dirty_mb=32\nosc.myth-OST0003-osc.max_dirty_mb=32\nosc.myth-OST0004-osc.max_dirty_mb=32\n```\n\n#### Setting Permanent Parameters\n\nUse `lctl set_param -P` or `lctl conf_param` command to set permanent parameters. In general, the `lctl conf_param` command can be used to specify any parameter settable in a `/proc/fs/lustre` file, with its own OBD device. The `lctl conf_param`command uses this syntax (same as the `mkfs.lustre` and `tunefs.lustre` commands):\n\n```\nobdname|fsname.\nobdtype.\nproc_file_name=\nvalue) \n```\n\nHere are a few examples of `lctl conf_param` commands:\n\n```\nmgs# lctl conf_param testfs-MDT0000.sys.timeout=40\n$ lctl conf_param testfs-MDT0000.mdt.identity_upcall=NONE\n$ lctl conf_param testfs.llite.max_read_ahead_mb=16\n$ lctl conf_param testfs-MDT0000.lov.stripesize=2M\n$ lctl conf_param testfs-OST0000.osc.max_dirty_mb=29.15\n$ lctl conf_param testfs-OST0000.ost.client_cache_seconds=15\n$ lctl conf_param testfs.sys.timeout=40 \n```\n\n**Caution**\n\nParameters specified with the `lctl conf_param` command are set permanently in the file system's configuration file on the MGS.\n\nIntroduced in Lustre 2.5\n\n#### Setting Permanent Parameters with lctl set_param -P\n\nThe `lctl set_param -P` command can also set parameters permanently. This command must be issued on the MGS. The given parameter is set on every host using `lctl` upcall. Parameters map to items in `/proc/{fs,sys}/{lnet,lustre}`. The `lctl set_param` command uses this syntax:\n\n```\nlctl set_param -P \nobdtype.\nobdname.\nproc_file_name=\nvalue\n```\n\nFor example:\n\n```\n# lctl set_param -P osc.*.max_dirty_mb=1024\nosc.myth-OST0000-osc.max_dirty_mb=32\nosc.myth-OST0001-osc.max_dirty_mb=32\nosc.myth-OST0002-osc.max_dirty_mb=32\nosc.myth-OST0003-osc.max_dirty_mb=32\nosc.myth-OST0004-osc.max_dirty_mb=32 \n```\n\nUse `-d`(only with -P) option to delete permanent parameter. Syntax:\n\n```\nlctl set_param -P -d\nobdtype.\nobdname.\nproc_file_name\n```\n\nFor example:\n\n```\n# lctl set_param -P -d osc.*.max_dirty_mb \n```\n\n#### Listing Parameters\n\nTo list Lustre or LNet parameters that are available to set, use the `lctl list_param` command. For example:\n\n```\nlctl list_param [-FR] \nobdtype.\nobdname\n```\n\nThe following arguments are available for the `lctl list_param` command.", "mimetype": "text/plain", "start_char_idx": 20747, "end_char_idx": 23826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55670111-0771-4ded-b490-742332a58dee": {"__data__": {"id_": "55670111-0771-4ded-b490-742332a58dee", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "169f02aa-7d6b-4ae2-bd3e-6a78d9255178", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6d11b0a7f0d45a96702a02ea0277c5130a958888755304beeddbdbfb6791258c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85f1a93c-3f49-4333-88f6-0812e8f8df1d", "node_type": "1", "metadata": {}, "hash": "27584fae918771b78bd05def43cd576d597fe108be96d385c497248e4d3ba537", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Syntax:\n\n```\nlctl set_param -P -d\nobdtype.\nobdname.\nproc_file_name\n```\n\nFor example:\n\n```\n# lctl set_param -P -d osc.*.max_dirty_mb \n```\n\n#### Listing Parameters\n\nTo list Lustre or LNet parameters that are available to set, use the `lctl list_param` command. For example:\n\n```\nlctl list_param [-FR] \nobdtype.\nobdname\n```\n\nThe following arguments are available for the `lctl list_param` command.\n\n`-F` Add ' `/`', ' `@`' or ' `=`' for directories, symlinks and writeable files, respectively\n\n`-R` Recursively lists all parameters under the specified path\n\nFor example:\n\n```\noss# lctl list_param obdfilter.lustre-OST0000 \n```\n\n#### Reporting Current Parameter Values\n\nTo report current Lustre parameter values, use the `lctl get_param` command with this syntax:\n\n```\nlctl get_param [-n] \nobdtype.\nobdname.\nproc_file_name\n```\n\nThis example reports data on RPC service times.\n\n```\noss# lctl get_param -n ost.*.ost_io.timeouts\nservice : cur 1 worst 30 (at 1257150393, 85d23h58m54s ago) 1 1 1 1 \n```\n\nThis example reports the amount of space this client has reserved for writeback cache with each OST:\n\n```\nclient# lctl get_param osc.*.cur_grant_bytes\nosc.myth-OST0000-osc-ffff8800376bdc00.cur_grant_bytes=2097152\nosc.myth-OST0001-osc-ffff8800376bdc00.cur_grant_bytes=33890304\nosc.myth-OST0002-osc-ffff8800376bdc00.cur_grant_bytes=35418112\nosc.myth-OST0003-osc-ffff8800376bdc00.cur_grant_bytes=2097152\nosc.myth-OST0004-osc-ffff8800376bdc00.cur_grant_bytes=33808384\n```\n\n## Specifying NIDs and Failover\n\nIf a node has multiple network interfaces, it may have multiple NIDs, which must all be identified so other nodes can choose the NID that is appropriate for their network interfaces. Typically, NIDs are specified in a list delimited by commas ( `,`). However, when failover nodes are specified, the NIDs are delimited by a colon ( `:`) or by repeating a keyword such as `--mgsnode=` or `--servicenode=`).\n\nTo display the NIDs of all servers in networks configured to work with the Lustre file system, run (while LNet is running):\n\n```\nlctl list_nids\n```\n\nIn the example below, `mds0` and `mds1` are configured as a combined MGS/MDT failover pair and `oss0` and `oss1` are configured as an OST failover pair. The Ethernet address for `mds0` is 192.168.10.1, and for `mds1` is 192.168.10.2. The Ethernet addresses for`oss0` and `oss1` are 192.168.10.20 and 192.168.10.21 respectively.", "mimetype": "text/plain", "start_char_idx": 23432, "end_char_idx": 25811, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85f1a93c-3f49-4333-88f6-0812e8f8df1d": {"__data__": {"id_": "85f1a93c-3f49-4333-88f6-0812e8f8df1d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55670111-0771-4ded-b490-742332a58dee", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5880931b3cd4692e2f2bdd78faf7efe9dd114185b5be22c83f40746670f3794d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d094437-13ef-4076-bf8c-37dda3427653", "node_type": "1", "metadata": {}, "hash": "a49ff817c1d02463f3053a6e8ad659abb6b174f8a979d127ea4ab4b17cfe068a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, when failover nodes are specified, the NIDs are delimited by a colon ( `:`) or by repeating a keyword such as `--mgsnode=` or `--servicenode=`).\n\nTo display the NIDs of all servers in networks configured to work with the Lustre file system, run (while LNet is running):\n\n```\nlctl list_nids\n```\n\nIn the example below, `mds0` and `mds1` are configured as a combined MGS/MDT failover pair and `oss0` and `oss1` are configured as an OST failover pair. The Ethernet address for `mds0` is 192.168.10.1, and for `mds1` is 192.168.10.2. The Ethernet addresses for`oss0` and `oss1` are 192.168.10.20 and 192.168.10.21 respectively.\n\n```\nmds0# mkfs.lustre --fsname=testfs --mdt --mgs \\\n        --servicenode=192.168.10.2@tcp0 \\\n        -\u2013servicenode=192.168.10.1@tcp0 /dev/sda1\nmds0# mount -t lustre /dev/sda1 /mnt/test/mdt\noss0# mkfs.lustre --fsname=testfs --servicenode=192.168.10.20@tcp0 \\\n        --servicenode=192.168.10.21 --ost --index=0 \\\n        --mgsnode=192.168.10.1@tcp0 --mgsnode=192.168.10.2@tcp0 \\\n        /dev/sdb\noss0# mount -t lustre /dev/sdb /mnt/test/ost0\nclient# mount -t lustre 192.168.10.1@tcp0:192.168.10.2@tcp0:/testfs \\\n        /mnt/testfs\nmds0# umount /mnt/mdt\nmds1# mount -t lustre /dev/sda1 /mnt/test/mdt\nmds1# lctl get_param mdt.testfs-MDT0000.recovery_status\n```\n\nWhere multiple NIDs are specified separated by commas (for example, `10.67.73.200@tcp,192.168.10.1@tcp`), the two NIDs refer to the same host, and the Lustre software chooses the *best* one for communication. When a pair of NIDs is separated by a colon (for example, `10.67.73.200@tcp:10.67.73.201@tcp`), the two NIDs refer to two different hosts and are treated as a failover pair (the Lustre software tries the first one, and if that fails, it tries the second one.)\n\nTwo options to `mkfs.lustre` can be used to specify failover nodes. The `--servicenode` option is used to specify all service NIDs, including those for primary nodes and failover nodes. When the `--servicenode`option is used, the first service node to load the target device becomes the primary service node, while nodes corresponding to the other specified NIDs become failover locations for the target device. An older option, `--failnode`, specifies just the NIDS of failover nodes. For more information about the `--servicenode` and `--failnode` options, see *Configuring Failover in a Lustre File System*.\n\n## Erasing a File System\n\nIf you want to erase a file system and permanently delete all the data in the file system, run this command on your targets:\n\n```\n$ \"mkfs.lustre --reformat\"\n```\n\nIf you are using a separate MGS and want to keep other file systems defined on that MGS, then set the `writeconf` flag on the MDT for that file system. The `writeconf` flag causes the configuration logs to be erased; they are regenerated the next time the servers start.\n\nTo set the `writeconf` flag on the MDT:\n\n1. Unmount all clients/servers using this file system, run:\n\n   ```\n   $ umount /mnt/lustre\n   ```\n\n2. Permanently erase the file system and, presumably, replace it with another file system, run:\n\n   ```\n   $ mkfs.lustre --reformat --fsname spfs --mgs --mdt --index=0 /dev/\n   {mdsdev}\n   ```\n\n3.", "mimetype": "text/plain", "start_char_idx": 25180, "end_char_idx": 28353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d094437-13ef-4076-bf8c-37dda3427653": {"__data__": {"id_": "8d094437-13ef-4076-bf8c-37dda3427653", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85f1a93c-3f49-4333-88f6-0812e8f8df1d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7e72ebf3100ce9ca59f89868f4ce790c07ff175b0f40f6227b8e9532e2dc84d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8fda46a-e6bb-44fe-b59c-cc89ba5e41a0", "node_type": "1", "metadata": {}, "hash": "fda88143049c8089ffedb08e05016b238d5da6f6d576c5769bec7a6277d4e442", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The `writeconf` flag causes the configuration logs to be erased; they are regenerated the next time the servers start.\n\nTo set the `writeconf` flag on the MDT:\n\n1. Unmount all clients/servers using this file system, run:\n\n   ```\n   $ umount /mnt/lustre\n   ```\n\n2. Permanently erase the file system and, presumably, replace it with another file system, run:\n\n   ```\n   $ mkfs.lustre --reformat --fsname spfs --mgs --mdt --index=0 /dev/\n   {mdsdev}\n   ```\n\n3. If you have a separate MGS (that you do not want to reformat), then add the `--writeconf` flag to `mkfs.lustre` on the MDT, run:\n\n   ```\n   $ mkfs.lustre --reformat --writeconf --fsname spfs --mgsnode=\n   mgs_nid --mdt --index=0 \n   /dev/mds_device\n   ```\n\n**Note**\n\nIf you have a combined MGS/MDT, reformatting the MDT reformats the MGS as well, causing all configuration information to be lost; you can start building your new file system. Nothing needs to be done with old disks that will not be part of the new file system, just do not mount them.\n\n## Reclaiming Reserved Disk Space\n\nAll current Lustre installations run the ldiskfs file system internally on service nodes. By default, ldiskfs reserves 5% of the disk space to avoid file system fragmentation. In order to reclaim this space, run the following command on your OSS for each OST in the file system:\n\n```\ntune2fs [-m reserved_blocks_percent] /dev/\n{ostdev}\n```\n\nYou do not need to shut down Lustre before running this command or restart it afterwards.\n\n**Warning**\n\nReducing the space reservation can cause severe performance degradation as the OST file system becomes more than 95% full, due to difficulty in locating large areas of contiguous free space. This performance degradation may persist even if the space usage drops below 95% again. It is recommended NOT to reduce the reserved disk space below 5%.\n\n## Replacing an Existing OST or MDT\n\nTo copy the contents of an existing OST to a new OST (or an old MDT to a new MDT), follow the process for either OST/MDT backups in *the section called \u201c Backing Up and Restoring an MDT or OST (ldiskfs Device Level)\u201d*or *the section called \u201c Backing Up an OST or MDT (Backend File System Level)\u201d*. For more information on removing a MDT, see *the section called \u201cRemoving an MDT from the File System\u201d*.\n\n## Identifying To Which Lustre File an OST Object Belongs\n\nUse this procedure to identify the file containing a given object on a given OST.\n\n1. On the OST (as root), run `debugfs` to display the file identifier ( `FID`) of the file associated with the object.", "mimetype": "text/plain", "start_char_idx": 27896, "end_char_idx": 30434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8fda46a-e6bb-44fe-b59c-cc89ba5e41a0": {"__data__": {"id_": "e8fda46a-e6bb-44fe-b59c-cc89ba5e41a0", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d094437-13ef-4076-bf8c-37dda3427653", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "eae3cb0a65ade51833b40c8abb31732d091a660379cec6e9585ff6cc7bc26630", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d45ccdc5-24bc-4705-aa6e-c099b00aac72", "node_type": "1", "metadata": {}, "hash": "f27b72b9cef5f559eef2e530969f69843ecfa03dcf74f4453800c0a24ccc5631", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is recommended NOT to reduce the reserved disk space below 5%.\n\n## Replacing an Existing OST or MDT\n\nTo copy the contents of an existing OST to a new OST (or an old MDT to a new MDT), follow the process for either OST/MDT backups in *the section called \u201c Backing Up and Restoring an MDT or OST (ldiskfs Device Level)\u201d*or *the section called \u201c Backing Up an OST or MDT (Backend File System Level)\u201d*. For more information on removing a MDT, see *the section called \u201cRemoving an MDT from the File System\u201d*.\n\n## Identifying To Which Lustre File an OST Object Belongs\n\nUse this procedure to identify the file containing a given object on a given OST.\n\n1. On the OST (as root), run `debugfs` to display the file identifier ( `FID`) of the file associated with the object.\n\n   For example, if the object is `34976` on `/dev/lustre/ost_test2`, the debug command is:\n\n   ```\n   # debugfs -c -R \"stat /O/0/d$((34976 % 32))/34976\" /dev/lustre/ost_test2 \n   ```\n\n   The command output is:\n\n   ```\n   debugfs 1.45.6.wc1 (20-Mar-2020)\n   /dev/lustre/ost_test2: catastrophic mode - not reading inode or group bitmaps\n   Inode: 352365   Type: regular    Mode:  0666   Flags: 0x80000\n   Generation: 2393149953    Version: 0x0000002a:00005f81\n   User:  1000   Group:  1000   Size: 260096\n   File ACL: 0    Directory ACL: 0\n   Links: 1   Blockcount: 512\n   Fragment:  Address: 0    Number: 0    Size: 0\n   ctime: 0x4a216b48:00000000 -- Sat May 30 13:22:16 2009\n   atime: 0x4a216b48:00000000 -- Sat May 30 13:22:16 2009\n   mtime: 0x4a216b48:00000000 -- Sat May 30 13:22:16 2009\n   crtime: 0x4a216b3c:975870dc -- Sat May 30 13:22:04 2009\n   Size of extra inode fields: 24\n   Extended attributes stored in inode body:\n     fid = \"b9 da 24 00 00 00 00 00 6a fa 0d 3f 01 00 00 00 eb 5b 0b 00 00 00 0000\n   00 00 00 00 00 00 00 00 \" (32)\n     fid: objid=34976 seq=0 parent=[0x200000400:0x122:0x0] stripe=1\n   EXTENTS:\n   (0-64):4620544-4620607\n   ```\n\n2. The parent FID will be of the form [0x200000400:0x122:0x0] and can be resolved directly using the `lfs fid2path [0x200000404:0x122:0x0] /mnt/lustre` command on any Lustre client, and the process is complete.\n\n3. In cases of an upgraded 1.x inode (if the first part of the FID is below 0x200000400), the MDT inode number is `0x24dab9` and generation `0x3f0dfa6a` and the pathname can also be resolved using `debugfs`.\n\n4.", "mimetype": "text/plain", "start_char_idx": 29666, "end_char_idx": 32019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d45ccdc5-24bc-4705-aa6e-c099b00aac72": {"__data__": {"id_": "d45ccdc5-24bc-4705-aa6e-c099b00aac72", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c068f47-9062-4cc9-803d-e6acfb6c66a0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8847a3a0dcb4f2239a5bb07baddab076a5292e708c2478b0467d2543e5cbf9c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8fda46a-e6bb-44fe-b59c-cc89ba5e41a0", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "971ede68734cf161fc28f1b524a61fe2480706bb8f83ee72cae3db7b7fd112cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The parent FID will be of the form [0x200000400:0x122:0x0] and can be resolved directly using the `lfs fid2path [0x200000404:0x122:0x0] /mnt/lustre` command on any Lustre client, and the process is complete.\n\n3. In cases of an upgraded 1.x inode (if the first part of the FID is below 0x200000400), the MDT inode number is `0x24dab9` and generation `0x3f0dfa6a` and the pathname can also be resolved using `debugfs`.\n\n4. On the MDS (as root), use `debugfs` to find the file associated with the inode:\n\n   ```\n   # debugfs -c -R \"ncheck 0x24dab9\" /dev/lustre/mdt_test \n   ```\n\n   Here is the command output:\n\n   ```\n   debugfs 1.42.3.wc3 (15-Aug-2012)\n   /dev/lustre/mdt_test: catastrophic mode - not reading inode or group bitmap\\\n   s\n   Inode      Pathname\n   2415289    /ROOT/brian-laptop-guest/clients/client11/~dmtmp/PWRPNT/ZD16.BMP\n   ```\n\nThe command lists the inode and pathname associated with the object.\n\n**Note**\n\n`Debugfs`' ''ncheck'' is a brute-force search that may take a long time to complete.\n\n**Note**\n\nTo find the Lustre file from a disk LBA, follow the steps listed in the document at this URL: <http://smartmontools.sourceforge.net/badblockhowto.html>. Then, follow the steps above to resolve the Lustre filename.", "mimetype": "text/plain", "start_char_idx": 31599, "end_char_idx": 32834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78efda29-5e61-436b-aad0-505d1d6cd97d": {"__data__": {"id_": "78efda29-5e61-436b-aad0-505d1d6cd97d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ae5ac95-2db8-4620-aa1f-092ac89f40ac", "node_type": "1", "metadata": {}, "hash": "4f39f834cefa77ca2b5e4b2fe2e0c7be5370d4f3a0bfe1c6c70404aa095cc85d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Lustre Maintenance\n\n- [Lustre Maintenance](#lustre-maintenance)\n  * [Working with Inactive OSTs](#working-with-inactive-osts)\n  * [Finding Nodes in the Lustre File System](#finding-nodes-in-the-lustre-file-system)\n  * [Mounting a Server Without Lustre Service](#mounting-a-server-without-lustre-service)\n  * [Regenerating Lustre Configuration Logs](#regenerating-lustre-configuration-logs)\n  * [Changing a Server NID](#changing-a-server-nid)\n  * [Clearing configuration](#clearing-configuration)L 2.11\n  * [Adding a New MDT to a Lustre File System](#adding-a-new-mdt-to-a-lustre-file-system)L 2.4\n  * [Adding a New OST to a Lustre File System](#adding-a-new-ost-to-a-lustre-file-system)\n  * [Removing and Restoring MDTs and OSTs](#removing-and-restoring-mdts-and-osts)\n    + [Removing an MDT from the File System](#removing-an-mdt-from-the-file-system)L 2.4\n    + [Working with Inactive MDTs](#working-with-inactive-mdts)L 2.4\n    + [Removing an OST from the File System](#removing-an-ost-from-the-file-system)\n    + [Backing Up OST Configuration Files](#backing-up-ost-configuration-files)\n    + [Restoring OST Configuration Files](#restoring-ost-configuration-files)\n    + [Returning a Deactivated OST to Service](#returning-a-deactivated-ost-to-service)\n  * [Aborting Recovery](#aborting-recovery)\n  * [Determining Which Machine is Serving an OST](#determining-which-machine-is-serving-an-ost)\n  * [Changing the Address of a Failover Node](#changing-the-address-of-a-failover-node)\n  * [Separate a combined MGS/MDT](#separate-a-combined-mgsmdt)\n\nOnce you have the Lustre file system up and running, you can use the procedures in this section to perform these basic Lustre maintenance tasks:\n\n- [the section called \u201c Working with Inactive OSTs\u201d](#working-with-inactive-osts)\n- [the section called \u201c Finding Nodes in the Lustre File System\u201d](#finding-nodes-in-the-lustre-file-system)\n- [the section called \u201c Mounting a Server Without Lustre Service\u201d](#mounting-a-server-without-lustre-service)\n- [the section called \u201c Regenerating Lustre Configuration Logs\u201d](#regenerating-lustre-configuration-logs)\n- [the section called \u201c Changing a Server NID\u201d](#regenerating-lustre-configuration-logs)\n- [the section called \u201c Adding a New MDT to a Lustre File System\u201d](#adding-a-new-mdt-to-a-lustre-file-system)\n- [the section called \u201c Adding a New OST to a Lustre File System\u201d](#adding-a-new-ost-to-a-lustre-file-system)\n- [the section called \u201c Removing and Restoring MDTs and OSTs\u201d](#removing-and-restoring-mdts-and-osts)\n- [the section called \u201c Removing an MDT from the File System\u201d](#removing-an-mdt-from-the-file-system)\n- [the section called \u201c Working with Inactive MDTs\u201d](#working-with-inactive-mdts)\n- [the section called \u201c Removing an OST from the File System\u201d](#removing-an-ost-from-the-file-system)\n- [the section called \u201c Backing Up OST Configuration Files\u201d](#backing-up-ost-configuration-files)\n- [the section called \u201c Restoring OST Configuration Files\u201d](#restoring-ost-configuration-files)\n- [the section called \u201c Returning a Deactivated OST to Service\u201d](#returning-a-deactivated-ost-to-service)\n- [the section called \u201c Aborting Recovery\u201d](#aborting-recovery)\n- [the section called \u201c Determining Which Machine is Serving an OST \u201d](#determining-which-machine-is-serving-an-ost)\n- [the section called \u201c Changing the Address of a Failover Node\u201d](#changing-the-address-of-a-failover-node)\n- [the section called \u201c Separate a combined MGS/MDT\u201d](#separate-a-combined-mgsmdt)\n- [the section called \u201c Set an MDT to read-only\u201d](#Set an MDT to read-only)\n- [the section called \u201c Tune Fallocate for ldiskfs\u201d](#Tune Fallocate for ldiskfs)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ae5ac95-2db8-4620-aa1f-092ac89f40ac": {"__data__": {"id_": "2ae5ac95-2db8-4620-aa1f-092ac89f40ac", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78efda29-5e61-436b-aad0-505d1d6cd97d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "006c56a70e6b74d667f0866cda6ce34b866b07f48226e085662199bd7c5aa10e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23c3446b-a8c9-4e32-9c40-27dd5bcffeab", "node_type": "1", "metadata": {}, "hash": "a10eaa90f13f5e1b2a06c19223e3c71f7f9084d9239c54fb136c823ded9a9c21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Working with Inactive OSTs\n\nTo mount a client or an MDT with one or more inactive OSTs, run commands similar to this:\n\n```\nclient# mount -o exclude=testfs-OST0000 -t lustre \\\n           uml1:/testfs /mnt/testfs\n            client# lctl get_param lov.testfs-clilov-*.target_obd\n```\n\nTo activate an inactive OST on a live client or MDT, use the `lctl activate` command on the OSC device. For example:\n\n```\nlctl --device 7 activate\n```\n\n**Note**\n\nA colon-separated list can also be specified. For example, `exclude=testfs-OST0000:testfs-OST0001`.\n\n## Finding Nodes in the Lustre File System\n\nThere may be situations in which you need to find all nodes in your Lustre file system or get the names of all OSTs.\n\nTo get a list of all Lustre nodes, run this command on the MGS:\n\n```\n# lctl get_param mgs.MGS.live.*\n```\n\n**Note**\n\nThis command must be run on the MGS.\n\nIn this example, file system `testfs` has three nodes, `testfs-MDT0000`, `testfs-OST0000`, and `testfs-OST0001`.\n\n```\nmgs:/root# lctl get_param mgs.MGS.live.* \n                fsname: testfs \n                flags: 0x0     gen: 26 \n                testfs-MDT0000 \n                testfs-OST0000 \n                testfs-OST0001 \n```\n\nTo get the names of all OSTs, run this command on the MDS:\n\n```\nmds:/root# lctl get_param lov.*-mdtlov.target_obd \n```\n\n**Note**\n\nThis command must be run on the MDS.\n\nIn this example, there are two OSTs, testfs-OST0000 and testfs-OST0001, which are both active.\n\n```\nmgs:/root# lctl get_param lov.testfs-mdtlov.target_obd \n0: testfs-OST0000_UUID ACTIVE \n1: testfs-OST0001_UUID ACTIVE \n```\n\n## Mounting a Server Without Lustre Service\n\nIf you are using a combined MGS/MDT, but you only want to start the MGS and not the MDT, run this command:\n\n```\nmount -t lustre /dev/mdt_partition -o nosvc /mount_point\n```\n\nThe `*mdt_partition*` variable is the combined MGS/MDT block device.\n\nIn this example, the combined MGS/MDT is `testfs-MDT0000` and the mount point is `/mnt/test/mdt`.\n\n```\n$ mount -t lustre -L testfs-MDT0000 -o nosvc /mnt/test/mdt\n```\n\n## Regenerating Lustre Configuration Logs\n\nIf the Lustre file system configuration logs are in a state where the file system cannot be started, use the `tunefs.lustre --writeconf` command to regenerate them. After the `writeconf` command is run and the servers restart, the configuration logs are re-generated and stored on the MGS (as with a new file system).\n\nYou should only use the `writeconf` command if:\n\n- The configuration logs are in a state where the file system cannot start\n- A server NID is being changed\n\nThe `writeconf` command is destructive to some configuration items (e.g. OST pools information and tunables set via `conf_param`), and should be used with caution.\n\n**Caution**\n\nThe OST pools feature enables a group of OSTs to be named for file striping purposes. If you use OST pools, be aware that running the `writeconf` command erases **all** pools information (as well as any other parameters set via `lctl conf_param`). We recommend that the pools definitions (and `conf_param` settings) be executed via a script, so they can be regenerated easily after `writeconf` is performed. However, tunables saved with `lctl set_param -P` are *not* erased in this case.", "mimetype": "text/plain", "start_char_idx": 3632, "end_char_idx": 6860, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23c3446b-a8c9-4e32-9c40-27dd5bcffeab": {"__data__": {"id_": "23c3446b-a8c9-4e32-9c40-27dd5bcffeab", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ae5ac95-2db8-4620-aa1f-092ac89f40ac", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "235bd5605c7d3c5f251ce3f55fa7afd60101cf8bc58f859c25ce169a0f38d5b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "324c4d1a-38ad-4a4a-995e-8879cae54099", "node_type": "1", "metadata": {}, "hash": "b3bbc2cc5aadfa5d71cf91c34be68c46f126796790c585d4d3ec2bbe4a3893c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You should only use the `writeconf` command if:\n\n- The configuration logs are in a state where the file system cannot start\n- A server NID is being changed\n\nThe `writeconf` command is destructive to some configuration items (e.g. OST pools information and tunables set via `conf_param`), and should be used with caution.\n\n**Caution**\n\nThe OST pools feature enables a group of OSTs to be named for file striping purposes. If you use OST pools, be aware that running the `writeconf` command erases **all** pools information (as well as any other parameters set via `lctl conf_param`). We recommend that the pools definitions (and `conf_param` settings) be executed via a script, so they can be regenerated easily after `writeconf` is performed. However, tunables saved with `lctl set_param -P` are *not* erased in this case.\n\n**Note**\n\nIf the MGS still holds any configuration logs, it may be possible to dump these logs to save any parameters stored with `lctl conf_param` by dumping the config logs on the MGS and saving the output:\n\n```\nmgs# lctl --device MGS llog_print fsname-client\nmgs# lctl --device MGS llog_print fsname-MDT0000\nmgs# lctl --device MGS llog_print fsname-OST0000\n```\n\nTo regenerate Lustre file system configuration logs:\n\n1. Stop the file system services in the following order before running the `tunefs.lustre --writeconf` command:\n\n   a. Unmount the clients.\n   b. Unmount the MDT(s).\n   c. Unmount the OST(s).\n   d. If the MGS is separate from the MDT it can remain mounted during this process.\n\n2. Make sure the MDT and OST devices are available.\n\n3. Run the `tunefs.lustre --writeconf` command on all target devices.\n\n   Run writeconf on the MDT(s) first, and then the OST(s).\n\n   a. On each MDS, for each MDT run:\n\n      ```\n      mds# tunefs.lustre --writeconf /dev/mdt_device\n      ```\n\n   b. On each OSS, for each OST run:\n\n      ```\n      oss# tunefs.lustre --writeconf /dev/ost_device\n      ```\n\n4. Restart the file system in the following order:\n\n   1. Mount the separate MGT, if it is not already mounted.\n   2. Mount the MDT(s) in order, starting with MDT0000.\n   3. Mount the OSTs in order, starting with OST0000.\n   4. Mount the clients.\n\nAfter the `tunefs.lustre --writeconf` command is run, the configuration logs are re-generated as servers connect to the MGS.\n\n## Changing a Server NID\n\nIn order to totally rewrite the Lustre configuration, the `tunefs.lustre --writeconf` command is used to rewrite all of the configuration files.\n\nIf you need to change only the NID of the MDT or OST, the `replace_nids` command can simplify this process. The `replace_nids` command differs from `tunefs.lustre --writeconf` in that it does not erase the entire configuration log, precluding the need the need to execute the `writeconf` command on all servers and re-specify all permanent parameter settings. However, the `writeconf` command can still be used if desired.\n\nChange a server NID in these situations:\n\n- New server hardware is added to the file system, and the MDS or an OSS is being moved to the new machine.\n- New network card is installed in the server.\n- You want to reassign IP addresses.\n\nTo change a server NID:\n\n1. Update the LNet configuration in the `/etc/modprobe.conf` file so the list of server NIDs is correct. Use `lctl list_nids` to view the list of server NIDS.\n\n   The `lctl list_nids` command indicates which network(s) are configured to work with the Lustre file system.\n\n2. Shut down the file system in this order:\n\n   a. Unmount the clients.\n   b. Unmount the MDT.\n   c. Unmount all OSTs.\n\n3. If the MGS and MDS share a partition, start the MGS only:\n\n   ```\n   mount -t lustre MDT partition -o nosvc mount_point\n   ```\n\n4.", "mimetype": "text/plain", "start_char_idx": 6038, "end_char_idx": 9722, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "324c4d1a-38ad-4a4a-995e-8879cae54099": {"__data__": {"id_": "324c4d1a-38ad-4a4a-995e-8879cae54099", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23c3446b-a8c9-4e32-9c40-27dd5bcffeab", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2328a7f8fc8543f95417eac5f751a401b1e19f4d25802a31e11bd1c6710b32da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36e2b789-7df7-4f6c-b28d-de01b984a2ae", "node_type": "1", "metadata": {}, "hash": "6853ee02440c2e8427ec82577779c885fa544205d28717408735e72391185b62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- New network card is installed in the server.\n- You want to reassign IP addresses.\n\nTo change a server NID:\n\n1. Update the LNet configuration in the `/etc/modprobe.conf` file so the list of server NIDs is correct. Use `lctl list_nids` to view the list of server NIDS.\n\n   The `lctl list_nids` command indicates which network(s) are configured to work with the Lustre file system.\n\n2. Shut down the file system in this order:\n\n   a. Unmount the clients.\n   b. Unmount the MDT.\n   c. Unmount all OSTs.\n\n3. If the MGS and MDS share a partition, start the MGS only:\n\n   ```\n   mount -t lustre MDT partition -o nosvc mount_point\n   ```\n\n4. Run the `replace_nids` command on the MGS:\n\n   ```\n   lctl replace_nids devicename nid1[,nid2,nid3 ...]\n   ```\n\n   where *devicename* is the Lustre target name, e.g. `testfs-OST0013`\n\n5. If the MGS and MDS share a partition, stop the MGS:\n\n   ```\n   umount mount_point\n   ```\n\n**Note**\n\nThe `replace_nids` command also cleans all old, invalidated records out of the configuration log, while preserving all other current settings.\n\n**Note**\n\nThe previous configuration log is backed up on the MGS disk with the suffix `'.bak'`.\n\n\n\nIntroduced in Lustre 2.11\n\n## Clearing configuration\n\nThis command runs on MGS node having the MGS device mounted with `-o nosvc.` It cleans up configuration files stored in the CONFIGS/ directory of any records marked SKIP. If the device name is given, then the specific logs for that filesystem (e.g. testfs-MDT0000) are processed. Otherwise, if a filesystem name is given then all configuration files are cleared. The previous configuration log is backed up on the MGS disk with the suffix 'config.timestamp.bak'. Eg: Lustre-MDT0000-1476454535.bak.\n\nTo clear a configuration:\n\n1. Shut down the file system in this order:\n\n   a. Unmount the clients.\n   b. Unmount the MDT.\n   c. Unmount all OSTs.\n\n2. If the MGS and MDS share a partition, start the MGS only using \"nosvc\" option.\n\n   ```\n   mount -t lustre MDT partition -o nosvc mount_point\n   ```\n\n3. Run the `clear_conf` command on the MGS:\n\n   ```\n   lctl clear_conf config\n   ```\n\n   Example: To clear the configuration for `MDT0000` on a filesystem named `testfs`\n\n   ```\n   mgs# lctl clear_conf testfs-MDT0000\n   ```\n\n\n\nIntroduced in Lustre 2.4\n\n## Adding a New MDT to a Lustre File System\n\nAdditional MDTs can be added using the DNE feature to serve one or more remote sub-directories within a filesystem, in order to increase the total number of files that can be created in the filesystem, to increase aggregate metadata performance, or to isolate user or application workloads from other users of the filesystem. It is possible to have multiple remote sub-directories reference the same MDT. However, the root directory will always be located on MDT0000. To add a new MDT into the file system:\n\n1. Discover the maximum MDT index. Each MDT must have unique index.\n\n   ```\n   client$ lctl dl | grep mdc\n   36 UP mdc testfs-MDT0000-mdc-ffff88004edf3c00 4c8be054-144f-9359-b063-8477566eb84e 5\n   37 UP mdc testfs-MDT0001-mdc-ffff88004edf3c00 4c8be054-144f-9359-b063-8477566eb84e 5\n   38 UP mdc testfs-MDT0002-mdc-ffff88004edf3c00 4c8be054-144f-9359-b063-8477566eb84e 5\n   39 UP mdc testfs-MDT0003-mdc-ffff88004edf3c00 4c8be054-144f-9359-b063-8477566eb84e 5\n   ```\n\n2. Add the new block device as a new MDT at the next available index.", "mimetype": "text/plain", "start_char_idx": 9087, "end_char_idx": 12446, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36e2b789-7df7-4f6c-b28d-de01b984a2ae": {"__data__": {"id_": "36e2b789-7df7-4f6c-b28d-de01b984a2ae", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "324c4d1a-38ad-4a4a-995e-8879cae54099", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "60310b1fd0973e42394d31c84b9c88699475e20706f60c6ca2b8647de086258d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e96e415-7fc6-4a95-ba0c-f59072f0ffdc", "node_type": "1", "metadata": {}, "hash": "03514f6afa99ee3501a5e3085193c902a3a7539de0bfffbc029ea921e79845dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Add the new block device as a new MDT at the next available index. In this example, the next available index is 4.\n\n   ```\n   mds# mkfs.lustre --reformat --fsname=testfs --mdt --mgsnode=mgsnode --index 4 /dev/mdt4_device\n   ```\n\n3. Mount the MDTs.\n\n   ```\n   mds# mount \u2013t lustre /dev/mdt4_blockdevice /mnt/mdt4\n   ```\n\n4. In order to start creating new files and directories on the new MDT(s) they need to be attached into the namespace at one or more subdirectories using the `lfs mkdir` command. All files and directories below those created with `lfs mkdir` will also be created on the same MDT unless otherwise specified.\n\n   ```\n   client# lfs mkdir -i 3 /mnt/testfs/new_dir_on_mdt3\n   client# lfs mkdir -i 4 /mnt/testfs/new_dir_on_mdt4\n   client# lfs mkdir -c 4 /mnt/testfs/new_directory_striped_across_4_mdts\n   ```\n\n## Adding a New OST to a Lustre File System\n\nA new OST can be added to existing Lustre file system on either an existing OSS node or on a new OSS node. In order to keep client IO load balanced across OSS nodes for maximum aggregate performance, it is not recommended to configure different numbers of OSTs to each OSS node.\n\n1. Add a new OST by using `mkfs.lustre` as when the filesystem was first formatted, see [4](02.07-Configuring%20a%20Lustre%20File%20System.md#configuring-a-simple-lustre-file-system) for details. Each new OST must have a unique index number, use `lctl dl` to see a list of all OSTs. For example, to add a new OST at index 12 to the `testfs`filesystem run following commands should be run on the OSS:\n\n   ```\n   oss# mkfs.lustre --fsname=testfs --mgsnode=mds16@tcp0 --ost --index=12 /dev/sda\n   oss# mkdir -p /mnt/testfs/ost12\n   oss# mount -t lustre /dev/sda /mnt/testfs/ost12\n   ```\n\n2. Balance OST space usage (possibly).\n\n   The file system can be quite unbalanced when new empty OSTs are added to a relatively full filesystem. New file creations are automatically balanced to favour the new OSTs. If this is a scratch file system or files are pruned at regular intervals, then no further work may be needed to balance the OST space usage as new files being created will preferentially be placed on the less full OST(s). As old files are deleted, they will release space on the old OST(s).\n\n   Files existing prior to the expansion can optionally be rebalanced using the `lfs_migrate` utility. This redistributes file data over the entire set of OSTs.\n\n   For example, to rebalance all files within the directory `/mnt/lustre/dir`, enter:\n\n   ```\n   client# lfs_migrate /mnt/lustre/dir\n   ```\n\n   To migrate files within the `/test` file system on `OST0004` that are larger than 4GB in size to other OSTs, enter:\n\n   ```\n   client# lfs find /test --ost test-OST0004 -size +4G | lfs_migrate -y\n   ```\n\n   See [*the section called \u201c `lfs_migrate` \u201d*](06.03-User%20Utilities.md#lfs_migrate) for details.\n\n## Removing and Restoring MDTs and OSTs\n\nOSTs and DNE MDTs can be removed from and restored to a Lustre filesystem. Deactivating an OST means that it is temporarily or permanently marked unavailable. Deactivating an OST on the MDS means it will not try to allocate new objects there or perform OST recovery, while deactivating an OST the client means it will not wait for OST recovery if it cannot contact the OST and will instead return an IO error to the application immediately if files on the OST are accessed. An OST may be permanently deactivated from the file system, depending on the situation and commands used.\n\n**Note**\n\nA permanently deactivated MDT or OST still appears in the filesystem configuration until the configuration is regenerated with `writeconf` or it is replaced with a new MDT or OST at the same index and permanently reactivated. A deactivated OST will not be listed by `lfs df`.", "mimetype": "text/plain", "start_char_idx": 12380, "end_char_idx": 16142, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e96e415-7fc6-4a95-ba0c-f59072f0ffdc": {"__data__": {"id_": "6e96e415-7fc6-4a95-ba0c-f59072f0ffdc", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36e2b789-7df7-4f6c-b28d-de01b984a2ae", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b16e0aeb4606ea9acabb1a5e5ce8075d3a78efe2bf9761f3ee58e27bbb1a29ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a33c3387-2dd9-4966-ac12-ed75207f98a5", "node_type": "1", "metadata": {}, "hash": "4f0f6e8fdee158cf56fc06fb17f1c2792518e85be175d8d96830b11b5f3171e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Removing and Restoring MDTs and OSTs\n\nOSTs and DNE MDTs can be removed from and restored to a Lustre filesystem. Deactivating an OST means that it is temporarily or permanently marked unavailable. Deactivating an OST on the MDS means it will not try to allocate new objects there or perform OST recovery, while deactivating an OST the client means it will not wait for OST recovery if it cannot contact the OST and will instead return an IO error to the application immediately if files on the OST are accessed. An OST may be permanently deactivated from the file system, depending on the situation and commands used.\n\n**Note**\n\nA permanently deactivated MDT or OST still appears in the filesystem configuration until the configuration is regenerated with `writeconf` or it is replaced with a new MDT or OST at the same index and permanently reactivated. A deactivated OST will not be listed by `lfs df`.\n\nYou may want to temporarily deactivate an OST on the MDS to prevent new files from being written to it in several situations:\n\n- A hard drive has failed and a RAID resync/rebuild is underway, though the OST can also be marked *degraded* by the RAID system to avoid allocating new files on the slow OST which can reduce performance, see [*the section called \u201c Handling Degraded OST RAID Arrays\u201d*](03.02-Lustre%20Operations.md#handling-degraded-ost-raid-arrays) for more details.\n- OST is nearing its space capacity, though the MDS will already try to avoid allocating new files on overly-full OSTs if possible, see [*the section called \u201cAllocating Free Space on OSTs\u201d*](06.02-Lustre%20Parameters.md#allocating-free-space-on-osts) for details.\n- MDT/OST storage or MDS/OSS node has failed, and will not be available for some time (or forever), but there is still a desire to continue using the filesystem before it is repaired.\n\n\n\nIntroduced in Lustre 2.4\n\n### Removing an MDT from the File System\n\nIf the MDT is permanently inaccessible, `lfs rm_entry {directory}` can be used to delete the directory entry for the unavailable MDT. Using `rmdir` would otherwise report an IO error due to the remote MDT being inactive. Please note that if the MDT *is*available, standard `rm -r` should be used to delete the remote directory. After the remote directory has been removed, the administrator should mark the MDT as permanently inactive with:\n\n```\nlctl conf_param {MDT name}.mdc.active=0\n```\n\nA user can identify which MDT holds a remote sub-directory using the `lfs` utility. For example:\n\n```\nclient$ lfs getstripe --mdt-index /mnt/lustre/remote_dir1\n1\nclient$ mkdir /mnt/lustre/local_dir0\nclient$ lfs getstripe --mdt-index /mnt/lustre/local_dir0\n0\n```\n\nThe `lfs getstripe --mdt-index` command returns the index of the MDT that is serving the given directory.\n\n\n\nIntroduced in Lustre 2.4\n\n### Working with Inactive MDTs\n\nFiles located on or below an inactive MDT are inaccessible until the MDT is activated again. Clients accessing an inactive MDT will receive an EIO error.\n\n### Removing an OST from the File System     \n\nWhen deactivating an OST, note that the client and MDS each have an OSC device that handles communication with the corresponding OST.  To remove an OST from the file system:      \n\n1. If the OST is functional, and there are files located on the OST that need to be migrated off of the OST, the file creation for that OST should be temporarily deactivated on the MDS (each MDS if running with multiple MDS nodes in DNE mode).           \n\n   1. Introduced in Lustre 2.9\n\n      With Lustre 2.9 and later, the MDS should be set to only disable file creation on that OST by setting   `max_create_count` to zero:\n\n       `mds# lctl set_param osp.*osc_name*.max_create_count=0` \n\n      This ensures that files deleted or migrated off of the OST will have their corresponding OST objects destroyed, and the space will be freed.  For example, to disable `OST0000`               in the filesystem `testfs`, run: \n\n       `mds# lctl set_param osp.testfs-OST0000-osc-MDT*.max_create_count=0`               on each MDS in the `testfs` filesystem.\n\n   2.", "mimetype": "text/plain", "start_char_idx": 15235, "end_char_idx": 19304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a33c3387-2dd9-4966-ac12-ed75207f98a5": {"__data__": {"id_": "a33c3387-2dd9-4966-ac12-ed75207f98a5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e96e415-7fc6-4a95-ba0c-f59072f0ffdc", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8d32d7d44f73f35139c934730dad4b3be385df0f4abc7ecd382b00150e91a620", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9911285a-b813-4946-9702-07251119218c", "node_type": "1", "metadata": {}, "hash": "675ced22c70708c407a2246278d0d1fdd8633e6bbf922c1b8d2b0f7f14610957", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Introduced in Lustre 2.9\n\n      With Lustre 2.9 and later, the MDS should be set to only disable file creation on that OST by setting   `max_create_count` to zero:\n\n       `mds# lctl set_param osp.*osc_name*.max_create_count=0` \n\n      This ensures that files deleted or migrated off of the OST will have their corresponding OST objects destroyed, and the space will be freed.  For example, to disable `OST0000`               in the filesystem `testfs`, run: \n\n       `mds# lctl set_param osp.testfs-OST0000-osc-MDT*.max_create_count=0`               on each MDS in the `testfs` filesystem.\n\n   2. With older versions of Lustre, to deactivate the OSC on the MDS node(s) use:               \n\n      ```\n      mds# lctl set_param osp.osc_name.active=0\n      ```\n\n      This will prevent the MDS from attempting any communication with that OST, including destroying objects located thereon.  This is fine if the OST will be removed permanently, if the OST is not stable in operation, or if it is in a read-only state.  Otherwise,  the free space and objects on the OST will not decrease when files are deleted, and object destruction will be deferred until the MDS reconnects to the OST.\n\n      For example, to deactivate `OST0000` in the filesystem `testfs`, run:               \n\n      ```\n      mds# lctl set_param osp.testfs-OST0000-osc-MDT*.active=0\n      ```\n\n      Deactivating the OST on the *MDS* does not prevent use of existing objects for read/write by a client.\n\n      ### Note\n\n      If migrating files from a working OST, do not deactivate the OST on clients. This causes IO errors when accessing files located there, and migrating files on the OST would fail.\n\n      ### Caution\n\n      Do not use `lctl conf_param` to deactivate the OST if it is still working, as this immediately and permanently deactivates it in the file system configuration on both the MDS and all clients.\n\n2. Discover all files that have objects residing on the deactivated OST. Depending on whether the deactivated OST is available or not, the data from that OST may be migrated to           other OSTs, or may need to be restored from backup.\n\n   1. If the OST is still online and available, find all files with objects on the deactivated OST, and copy them to other OSTs in the file system to: \n\n      ```\n      client# lfs find --ost ost_name /mount/point | lfs_migrate -y\n      ```\n\n      Note that if multiple OSTs are being deactivated at one time, the `lfs find` command can take multiple  `--ost` arguments, and will return files that are located on *any* of the specified OSTs. \t      \n\n   2. If the OST is no longer available, delete the files on that OST and restore them from backup:                 \n\n      ```\n      client# lfs find --ost ost_uuid -print0 /mount/point |\n              tee /tmp/files_to_restore | xargs -0 -n 1 unlink\n      ```\n\n      The list of files that need to be restored from backup is stored in `/tmp/files_to_restore`. Restoring these files is beyond the scope of this document.\n\n3. Deactivate the OST.\n\n   1. If there is expected to be a replacement OST in some short time (a few days), the OST can temporarily be deactivated on the clients using:                 \n\n      ```\n      client# lctl set_param osc.fsname-OSTnumber-*.active=0                 \n      ```\n\n      ### Note\n\n      This setting is only temporary and will be reset if the clients are remounted or rebooted. It needs to be run on all clients.\n\n   2. If there is not expected to be a replacement for this OST in the near future, permanently deactivate it on all clients and the MDS by running the following command on the MGS:               \n\n      ```\n      mgs# lctl conf_param ost_name.osc.active=0\n      ```\n\n      ### Note\n\n      A deactivated OST still appears in the file system configuration, though a replacement OST can be created that re-uses the same OST index with the `mkfs.lustre --replace` option, see [the section called \u201c Restoring OST Configuration Files\u201d](#restoring-ost-configuration-files).", "mimetype": "text/plain", "start_char_idx": 18704, "end_char_idx": 22713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9911285a-b813-4946-9702-07251119218c": {"__data__": {"id_": "9911285a-b813-4946-9702-07251119218c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a33c3387-2dd9-4966-ac12-ed75207f98a5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "49d7f4ffa43c6be43c79daae1718628da34c6afb0ec84d14e793336237957c54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c52e911-1f68-4bb8-b478-64ad1e88846e", "node_type": "1", "metadata": {}, "hash": "279a0017c4850954a908f4373456195d13e6d11b0a3262485c90ece6e2ca9435", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It needs to be run on all clients.\n\n   2. If there is not expected to be a replacement for this OST in the near future, permanently deactivate it on all clients and the MDS by running the following command on the MGS:               \n\n      ```\n      mgs# lctl conf_param ost_name.osc.active=0\n      ```\n\n      ### Note\n\n      A deactivated OST still appears in the file system configuration, though a replacement OST can be created that re-uses the same OST index with the `mkfs.lustre --replace` option, see [the section called \u201c Restoring OST Configuration Files\u201d](#restoring-ost-configuration-files). \n\nTo totally remove the OST from the filesystem configuration, the OST configuration records should be found in the startup logs by running the command \"`lctl --device MGS llog_print fsname-client`\" on the MGS (and also \"... `$fsname-MDTxxxx`\" for all the MDTs) to list all attach, setup, add_osc, add_pool, and other records related to the removed OST(s). Once the index value is known for each configuration record, the command \"`lctl --device MGS llog_cancel llog_name -i index` \" will drop that record from the configuration log llog_name for each of the `fsname-client` and `fsname-MDTxxxx` configuration logs so that new mounts will no longer process it. If a whole OSS is being removed, the`add_uuid` records for the OSS should similarly be canceled.\n\n```\nmgs# lctl --device MGS llog_print testfs-client | egrep \"192.168.10.99@tcp|OST0003\"\n- { index: 135, event: add_uuid, nid: 192.168.10.99@tcp(0x20000c0a80a63), node: 192.168.10.99@tcp }\n- { index: 136, event: attach, device: testfs-OST0003-osc, type: osc, UUID: testfs-clilov_UUID }\n- { index: 137, event: setup, device: testfs-OST0003-osc, UUID: testfs-OST0003_UUID, node: 192.168.10.99@tcp }\n- { index: 138, event: add_osc, device: testfs-clilov, ost: testfs-OST0003_UUID, index: 3, gen: 1 }\nmgs# lctl --device MGS llog_cancel testfs-client -i 138\nmgs# lctl --device MGS llog_cancel testfs-client -i 137\nmgs# lctl --device MGS llog_cancel testfs-client -i 136\n```\n\n\n\n### Backing Up OST Configuration Files\n\nIf the OST device is still accessible, then the Lustre configuration files on the OST should be backed up and saved for future use in order to avoid difficulties when a replacement OST is returned to service. These files rarely change, so they can and should be backed up while the OST is functional and accessible. If the deactivated OST is still available to mount (i.e. has not permanently failed or is unmountable due to severe corruption), an effort should be made to preserve these files.\n\n1. Mount the OST file system.\n\n   ```\n   oss# mkdir -p /mnt/ost\n   oss# mount -t ldiskfs /dev/ost_device /mnt/ost\n   ```\n\n   \n\n2. Back up the OST configuration files.\n\n   ```\n   oss# tar cvf ost_name.tar -C /mnt/ost last_rcvd \\\n              CONFIGS/ O/0/LAST_ID\n   ```\n\n   \n\n3. Unmount the OST file system.", "mimetype": "text/plain", "start_char_idx": 22110, "end_char_idx": 24987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c52e911-1f68-4bb8-b478-64ad1e88846e": {"__data__": {"id_": "3c52e911-1f68-4bb8-b478-64ad1e88846e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9911285a-b813-4946-9702-07251119218c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3384fc4953292a95958fd563abb47755d67778af4fb1b9c548227854d8585c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57ee31a2-4529-40bc-8a6c-de04c79d481c", "node_type": "1", "metadata": {}, "hash": "3586cb6b56c028afc97c8af62c4d380eb2afceac450febe168f86cbdd987898d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These files rarely change, so they can and should be backed up while the OST is functional and accessible. If the deactivated OST is still available to mount (i.e. has not permanently failed or is unmountable due to severe corruption), an effort should be made to preserve these files.\n\n1. Mount the OST file system.\n\n   ```\n   oss# mkdir -p /mnt/ost\n   oss# mount -t ldiskfs /dev/ost_device /mnt/ost\n   ```\n\n   \n\n2. Back up the OST configuration files.\n\n   ```\n   oss# tar cvf ost_name.tar -C /mnt/ost last_rcvd \\\n              CONFIGS/ O/0/LAST_ID\n   ```\n\n   \n\n3. Unmount the OST file system.\n\n   ```\n   oss# umount /mnt/ost\n   ```\n\n   \n\n### Restoring OST Configuration Files\n\nIf the original OST is still available, it is best to follow the OST backup and restore procedure given in either [*the section called \u201c Backing Up and Restoring an MDT or OST (ldiskfs Device Level)\u201d*](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-and-restoring-an-mdt-or-ost-ldiskfs-device-level), or [*the section called \u201c Backing Up an OST or MDT (Backend File System Level)\u201d*](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-an-ost-or-mdt-backend-file-system-level) and [*the section called \u201c Restoring a File-Level Backup\u201d*](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#restoring-a-file-level-backup).\n\nTo replace an OST that was removed from service due to corruption or hardware failure, the replacement OST needs to be formatted using `mkfs.lustre`, and the Lustre file system configuration should be restored, if available. Any objects stored on the OST will be permanently lost, and files using the OST should be deleted and/or restored from backup.\n\nIntroduced in Lustre 2.5\n\nWith Lustre 2.5 and later, it is possible to replace an OST to the same index without restoring the configuration files, using the `--replace` option at format time.\n\n```\noss# mkfs.lustre --ost --reformat --replace --index=old_ost_index \\\n        other_options /dev/new_ost_dev\n```\n\nThe MDS and OSS will negotiate the `LAST_ID` value for the replacement OST.\n\nIf the OST configuration files were not backed up, due to the OST file system being completely inaccessible, it is still possible to replace the failed OST with a new one at the same OST index.\n\n1. For older versions, format the OST file system without the `--replace` option and restore the saved configuration:\n\n   ```\n   oss# mkfs.lustre --ost --reformat --index=old_ost_index \\\n              other_options /dev/new_ost_dev\n   ```\n\n2. Mount the OST file system.\n\n   ```\n   oss# mkdir /mnt/ost\n   oss# mount -t ldiskfs /dev/new_ost_dev /mnt/ost\n   ```\n\n3. Restore the OST configuration files, if available.\n\n   ```\n   oss# tar xvf ost_name.tar -C /mnt/ost\n   ```\n\n4. Recreate the OST configuration files, if unavailable.\n\n   Follow the procedure in [*the section called \u201cFixing a Bad LAST_ID on an OST\u201d*](05.01-Lustre%20File%20System%20Troubleshooting.md#fixing-a-bad-last-id-on-an-ost) to recreate the LAST_ID file for this OST index. The `last_rcvd` file will be recreated when the OST is first mounted using the default parameters, which are normally correct for all file systems. The `CONFIGS/mountdata` file is created by `mkfs.lustre` at format time, but has flags set that request it to register itself with the MGS. It is possible to copy the flags from another working OST (which should be the same):\n\n   ```\n   oss1# debugfs -c -R \"dump CONFIGS/mountdata /tmp\" /dev/other_osdev\n   oss1# scp /tmp/mountdata oss0:/tmp/mountdata\n   oss0# dd if=/tmp/mountdata of=/mnt/ost/CONFIGS/mountdata bs=4 count=1 seek=5 skip=5 conv=notrunc\n   ```\n\n5. Unmount the OST file system.", "mimetype": "text/plain", "start_char_idx": 24393, "end_char_idx": 28050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57ee31a2-4529-40bc-8a6c-de04c79d481c": {"__data__": {"id_": "57ee31a2-4529-40bc-8a6c-de04c79d481c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c52e911-1f68-4bb8-b478-64ad1e88846e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5d1b7f16f6b53daee2d1658fec7e134a5ff42c37afc13516b1d7422c091eac0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88f18a19-e019-4711-bafe-3477f3895402", "node_type": "1", "metadata": {}, "hash": "b98ed8b7264a6d895ac9628247d8133dc3bc0adf668c50de106701817b77dc47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The `last_rcvd` file will be recreated when the OST is first mounted using the default parameters, which are normally correct for all file systems. The `CONFIGS/mountdata` file is created by `mkfs.lustre` at format time, but has flags set that request it to register itself with the MGS. It is possible to copy the flags from another working OST (which should be the same):\n\n   ```\n   oss1# debugfs -c -R \"dump CONFIGS/mountdata /tmp\" /dev/other_osdev\n   oss1# scp /tmp/mountdata oss0:/tmp/mountdata\n   oss0# dd if=/tmp/mountdata of=/mnt/ost/CONFIGS/mountdata bs=4 count=1 seek=5 skip=5 conv=notrunc\n   ```\n\n5. Unmount the OST file system.\n\n   ```\n   oss# umount /mnt/ost\n   ```\n\n### Returning a Deactivated OST to Service\n\nIf the OST was permanently deactivated, it needs to be reactivated in the MGS configuration.\n\n```\nmgs# lctl conf_param ost_name.osc.active=1\n```\n\nIf the OST was temporarily deactivated, it needs to be reactivated on the MDS and clients.\n\n```\nmds# lctl set_param osp.fsname-OSTnumber-*.active=1\nclient# lctl set_param osc.fsname-OSTnumber-*.active=1\n```\n## Aborting Recovery\n\nYou can abort recovery with either the `lctl` utility or by mounting the target with the `abort_recov` option (`mount -o abort_recov`). When starting a target, run:\n\n```\nmds# mount -t lustre -L mdt_name -o abort_recov /mount_point\n```\n\n**Note**\n\nThe recovery process is blocked until all OSTs are available.\n\n## Determining Which Machine is Serving an OST\n\nIn the course of administering a Lustre file system, you may need to determine which machine is serving a specific OST. It is not as simple as identifying the machine\u2019s IP address, as IP is only one of several networking protocols that the Lustre software uses and, as such, LNet does not use IP addresses as node identifiers, but NIDs instead. To identify the NID that is serving a specific OST, run one of the following commands on a client (you do not need to be a root user):\n\n```\nclient$ lctl get_param osc.fsname-OSTnumber*.ost_conn_uuid\n```\n\nFor example:\n\n```\nclient$ lctl get_param osc.*-OST0000*.ost_conn_uuid \nosc.testfs-OST0000-osc-f1579000.ost_conn_uuid=192.168.20.1@tcp\n```\n\n\\- OR -\n\n```\nclient$ lctl get_param osc.*.ost_conn_uuid \nosc.testfs-OST0000-osc-f1579000.ost_conn_uuid=192.168.20.1@tcp\nosc.testfs-OST0001-osc-f1579000.ost_conn_uuid=192.168.20.1@tcp\nosc.testfs-OST0002-osc-f1579000.ost_conn_uuid=192.168.20.1@tcp\nosc.testfs-OST0003-osc-f1579000.ost_conn_uuid=192.168.20.1@tcp\nosc.testfs-OST0004-osc-f1579000.ost_conn_uuid=192.168.20.1@tcp\n```\n\n## Changing the Address of a Failover Node\n\nTo change the address of a failover node (e.g, to use node X instead of node Y), run this command on the OSS/OST partition (depending on which option was used to originally identify the NID):\n\n```\noss# tunefs.lustre --erase-params --servicenode=NID /dev/ost_device\n```\n\nor\n\n```\noss# tunefs.lustre --erase-params --failnode=NID /dev/ost_device\n```\n\nFor more information about the `--servicenode` and `--failnode` options, see [*Configuring Failover in a Lustre File System*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md).\n\n## Separate a combined MGS/MDT\n\nThese instructions assume the MGS node will be the same as the MDS node. For instructions on how to move MGS to a different node, see [*the section called \u201c Changing a Server NID\u201d*](#changing-a-server-nid).\n\nThese instructions are for doing the split without shutting down other servers and clients.", "mimetype": "text/plain", "start_char_idx": 27411, "end_char_idx": 30850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88f18a19-e019-4711-bafe-3477f3895402": {"__data__": {"id_": "88f18a19-e019-4711-bafe-3477f3895402", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "645b2691-3c1f-4a87-b41a-d620be00ec85", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b6889fd21ce34052b665fc58d6feff3e7e6ec3f32c1a1331e7d7d0ebeb85c1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57ee31a2-4529-40bc-8a6c-de04c79d481c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4a3cb06536ad68270b8a17264643fd470a72fb7b4fd1497358ef999e699a96f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Separate a combined MGS/MDT\n\nThese instructions assume the MGS node will be the same as the MDS node. For instructions on how to move MGS to a different node, see [*the section called \u201c Changing a Server NID\u201d*](#changing-a-server-nid).\n\nThese instructions are for doing the split without shutting down other servers and clients.\n\n1. Stop the MDS.\n\n   Unmount the MDT\n\n   ```\n   umount -f /dev/mdt_device \n   ```\n\n2. Create the MGS.\n\n   ```\n   mds# mkfs.lustre --mgs --device-size=size /dev/mgs_device\n   ```\n\n3. Copy the configuration data from MDT disk to the new MGS disk.\n\n   ```\n   mds# mount -t ldiskfs -o ro /dev/mdt_device /mdt_mount_point\n   ```\n\n   ```\n   mds# mount -t ldiskfs -o rw /dev/mgs_device /mgs_mount_point \n   ```\n\n   ```\n   mds# cp -r /mdt_mount_point/CONFIGS/filesystem_name-* /mgs_mount_point/CONFIGS/. \n   ```\n\n   ```\n   mds# umount /mgs_mount_point\n   ```\n\n   ```\n   mds# umount /mdt_mount_point\n   ```\n\n   See [*the section called \u201c Regenerating Lustre Configuration Logs\u201d*](#regenerating-lustre-configuration-logs) for alternative method.\n\n4. Start the MGS.\n\n   ```\n   mgs# mount -t lustre /dev/mgs_device /mgs_mount_point\n   ```\n\n   Check to make sure it knows about all your file system\n\n   ```\n   mgs:/root# lctl get_param mgs.MGS.filesystems\n   ```\n\n5. Remove the MGS option from the MDT, and set the new MGS nid.\n\n   ```\n   mds# tunefs.lustre --nomgs --mgsnode=new_mgs_nid /dev/mdt-device\n   ```\n\n6. Start the MDT.\n\n   ```\n   mds# mount -t lustre /dev/mdt_device /mdt_mount_point\n   ```\n\n   Check to make sure the MGS configuration looks right:\n\n   ```\n   mgs# lctl get_param mgs.MGS.live.filesystem_name\n   ```\n\n## Set an MDT to read-only\n\nIt is sometimes desirable to be able to mark the filesystem read-only directly on the server, rather than remounting the clients and setting the option there. This can be useful if there is a rogue client that is deleting files, or when decommissioning a system to prevent already-mounted clients from modifying it anymore. \n\nSet the `mdt.*.readonly` parameter to 1 to immediately set the MDT to read-only. All future MDT access will immediately return a \"Read-only file system\" error (EROFS) until the parameter is set to `0` again. \n\nExample of setting the `readonly` parameter to `1`, verifying the current setting, accessing from a client, and setting the parameter back to `0`:\n\n```\nmds# lctl set_param mdt.fs-MDT0000.readonly=1\nmdt.fs-MDT0000.readonly=1\n\nmds# lctl get_param mdt.fs-MDT0000.readonly\nmdt.fs-MDT0000.readonly=1\n\nclient$ touch test_file\ntouch: cannot touch \u2018test_file\u2019: Read-only file system\n\nmds# lctl set_param mdt.fs-MDT0000.readonly=0\nmdt.fs-MDT0000.readonly=0\n```\n\n## Tune Fallocate for ldiskfs\n\nThis section shows how to tune/enable/disable fallocate for ldiskfs OSTs. \n\nThe default `mode=0` is the standard \"allocate unwritten extents\" behavior used by ext4. This is by far the fastest for space allocation, but requires the unwritten extents to be split and/or zeroed when they are overwritten. \n\nThe OST fallocate `mode=1` can also be set to use \"zeroed extents\", which may be handled by \"WRITE SAME\", \"TRIM zeroes data\", or other low-level functionality in the underlying block device. \n\n`mode=-1` completely disables fallocate.\n\nExample: To completely disable fallocate \n\n```\nlctl set_param osd-ldiskfs.*.fallocate_zero_blocks=-1\n```\n\n Example: To enable fallocate to use 'zeroed extents' \n\n```\nlctl set_param osd-ldiskfs.*.fallocate_zero_blocks=1\n```", "mimetype": "text/plain", "start_char_idx": 30519, "end_char_idx": 33976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c7ef801-325d-4e42-990e-9ee4c2ba11b3": {"__data__": {"id_": "0c7ef801-325d-4e42-990e-9ee4c2ba11b3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2c541e46aafde89add9a6c9ed87c866a12f338b8ddb04609cb3928b949e5f4aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc4bb9cf-43f3-4229-8486-2eb33d659acc", "node_type": "1", "metadata": {}, "hash": "3f3bbfed091b4e9037f9514f18653d1d3405b21fc25e808c535048b0ebab82f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Managing Lustre Networking (LNet)\n\n- [Managing Lustre Networking (LNet)](#managing-lustre-networking-lnet)\n  * [Updating the Health Status of a Peer or Router](#updating-the-health-status-of-a-peer-or-router)\n  * [Starting and Stopping LNet](#starting-and-stopping-lnet)\n    + [Starting LNet](#starting-lnet)\n      - [Starting Clients](#starting-clients)\n    + [Stopping LNet](#stopping-lnet)\n  * [Hardware Based Multi-Rail Configurations with LNet](#hardware-based-multi-rail-configurations-with-lnet)\n  * [Load Balancing with an InfiniBand* Network](#load-balancing-with-an-infiniband-network)\n    + [Setting Up `lustre.conf` for Load Balancing](#setting-up-lustreconf-for-load-balancing)\n  * [Dynamically Configuring LNet Routes](#dynamically-configuring-lnet-routes)L 2.4\n    + [`lustre_routes_config`](#lustre_routes_config)\n    + [`lustre_routes_conversion`](#lustre_routes_conversion)\n    + [`Route Configuration Examples`](#route-configuration-examples)\n\nThis chapter describes some tools for managing Lustre networking (LNet) and includes the following sections:\n\n* [the section called \u201cUpdating the Health Status of a Peer or Router\u201d](#updating-the-health-status-of-a-peer-or-router)\n\n* [the section called \u201cStarting and Stopping LNet\u201d](#starting-and-stopping-lnet)\n\n* [the section called \u201cHardware Based Multi-Rail Configurations with LNet\u201d](#hardware-based-multi-rail-configurations-with-lnet)\n\n* [the section called \u201cLoad Balancing with an InfiniBand* Network\u201d](#load-balancing-with-an-infiniband-network)\n\n* [the section called \u201cDynamically Configuring LNet Routes\u201d](#dynamically-configuring-lnet-routes)\n\n\n\n## Updating the Health Status of a Peer or Router\n\nThere are two mechanisms to update the health status of a peer or a router:\n\n- LNet can actively check health status of all routers and mark them as dead or alive automatically. By default, this is off. To enable it set `auto_down` and if desired `check_routers_before_use`. This initial check may cause a pause equal to `router_ping_timeout` at system startup, if there are dead routers in the system.\n- When there is a communication error, all LNDs notify LNet that the peer (not necessarily a router) is down. This mechanism is always on, and there is no parameter to turn it off. However, if you set the LNet module parameter `auto_down` to `0`, LNet ignores all such peer-down notifications.\n\nSeveral key differences in both mechanisms:\n\n- The router pinger only checks routers for their health, while LNDs notices all dead peers, regardless of whether they are a router or not.\n- The router pinger actively checks the router health by sending pings, but LNDs only notice a dead peer when there is network traffic going on.\n- The router pinger can bring a router from alive to dead or vice versa, but LNDs can only bring a peer down.\n\n \n\n## Starting and Stopping LNet\n\nThe Lustre software automatically starts and stops LNet, but it can also be manually started in a standalone manner. This is particularly useful to verify that your networking setup is working correctly before you attempt to start the Lustre file system.\n\n### Starting LNet\n\nTo start LNet, run:\n\n```\n$ modprobe lnet\n$ lctl network up\n```\n\nTo see the list of local NIDs, run:\n\n```\n$ lctl list_nids\n```\n\nThis command tells you the network(s) configured to work with the Lustre file system.\n\nIf the networks are not correctly setup, see the `modules.conf` \"`networks=`\" line and make sure the network layer modules are correctly installed and configured.\n\nTo get the best remote NID, run:\n\n```\n$ lctl which_nid NIDs\n```\n\nwhere `*NIDs*` is the list of available NIDs.\n\nThis command takes the \"best\" NID from a list of the NIDs of a remote host. The \"best\" NID is the one that the local node uses when trying to communicate with the remote node.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc4bb9cf-43f3-4229-8486-2eb33d659acc": {"__data__": {"id_": "fc4bb9cf-43f3-4229-8486-2eb33d659acc", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2c541e46aafde89add9a6c9ed87c866a12f338b8ddb04609cb3928b949e5f4aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c7ef801-325d-4e42-990e-9ee4c2ba11b3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b1aa6ef36eb6e9abe52755f9748bbff30a8c733d7b7eb458f19807254bca3bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c235149f-381c-49c5-8309-e774a4457fb0", "node_type": "1", "metadata": {}, "hash": "056b78a320991264ba3e7e56cf1dd8f7fb65b023e80feb84376c6e04d97c441a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Starting LNet\n\nTo start LNet, run:\n\n```\n$ modprobe lnet\n$ lctl network up\n```\n\nTo see the list of local NIDs, run:\n\n```\n$ lctl list_nids\n```\n\nThis command tells you the network(s) configured to work with the Lustre file system.\n\nIf the networks are not correctly setup, see the `modules.conf` \"`networks=`\" line and make sure the network layer modules are correctly installed and configured.\n\nTo get the best remote NID, run:\n\n```\n$ lctl which_nid NIDs\n```\n\nwhere `*NIDs*` is the list of available NIDs.\n\nThis command takes the \"best\" NID from a list of the NIDs of a remote host. The \"best\" NID is the one that the local node uses when trying to communicate with the remote node.\n\n#### Starting Clients\n\nTo start a TCP client, run:\n\n```\nmount -t lustre mdsnode:/mdsA/client /mnt/lustre/\n```\n\nTo start an Elan client, run:\n\n```\nmount -t lustre 2@elan0:/mdsA/client /mnt/lustre\n```\n\n### Stopping LNet\n\nBefore the LNet modules can be removed, LNet references must be removed. In general, these references are removed automatically when the Lustre file system is shut down, but for standalone routers, an explicit step is needed to stop LNet. Run:\n\n```\nlctl network unconfigure\n```\n\n**Note**\n\nAttempting to remove Lustre modules prior to stopping the network may result in a crash or an LNet hang. If this occurs, the node must be rebooted (in most cases). Make sure that the Lustre network and Lustre file system are stopped prior to unloading the modules. Be extremely careful using `rmmod -f`.\n\nTo unconfigure the LNet network, run:\n\n```\nmodprobe -r lnd_and_lnet_modules\n```\n\n**Note**\n\nTo remove all Lustre modules, run:\n\n`$ lustre_rmmod`\n\n## Hardware Based Multi-Rail Configurations with LNet\n\nTo aggregate bandwidth across both rails of a dual-rail IB cluster (o2iblnd) [1]) using LNet, consider these points:\n\n- LNet can work with multiple rails, however, it does not load balance across them. The actual rail used for any communication is determined by the peer NID.\n- Hardware multi-rail LNet configurations do not provide an additional level of network fault tolerance. The configurations described below are for bandwidth aggregation only.\n- A Lustre node always uses the same local NID to communicate with a given peer NID. The criteria used to determine the local NID are:\n  - Introduced in Lustre 2.5Lowest route priority number (lower number, higher priority).\n  - Fewest hops (to minimize routing), and\n  - Appears first in the \"`networks`\" or \"`ip2nets`\" LNet configuration strings\n\n----------\n\n[1]Hardware multi-rail configurations are only supported by o2iblnd; other IB LNDs do not support multiple interfaces.\n\n-------------------\n\n## Load Balancing with an InfiniBand* Network\n\nA Lustre file system contains OSSs with two InfiniBand HCAs. Lustre clients have only one InfiniBand HCA using OFED-based Infiniband ''o2ib'' drivers. Load balancing between the HCAs on the OSS is accomplished through LNet.\n\n### Setting Up `lustre.conf` for Load Balancing\n\nTo configure LNet for load balancing on clients and servers:\n\n1. Set the `lustre.conf` options.\n\n   Depending on your configuration, set `lustre.conf` options as follows:\n\n   - Dual HCA OSS server\n\n   ```\n   options lnet networks=\"o2ib0(ib0),o2ib1(ib1)\"\n   ```\n\n   - Client with the odd IP address\n\n   ```\n   options lnet ip2nets=\"o2ib0(ib0) 192.168.10.[103-253/2]\"\n   ```\n\n   - Client with the even IP address\n\n   ```\n   options lnet ip2nets=\"o2ib1(ib0) 192.168.10.[102-254/2]\"\n   ```\n\n2. Run the modprobe lnet command and create a combined MGS/MDT file system.\n\n   The following commands create an MGS/MDT or OST file system and mount the targets on the servers.", "mimetype": "text/plain", "start_char_idx": 3105, "end_char_idx": 6743, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c235149f-381c-49c5-8309-e774a4457fb0": {"__data__": {"id_": "c235149f-381c-49c5-8309-e774a4457fb0", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2c541e46aafde89add9a6c9ed87c866a12f338b8ddb04609cb3928b949e5f4aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc4bb9cf-43f3-4229-8486-2eb33d659acc", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ed598f5b0a0faab4253945245c84ba25222da53038dd873a09f54900478bc261", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab43e416-226a-481a-bdc2-4b92ede3a636", "node_type": "1", "metadata": {}, "hash": "2f8c2d7b3c78e685c15efb2222726003342b9c31043ae610b0eeda86fbc9b466", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Set the `lustre.conf` options.\n\n   Depending on your configuration, set `lustre.conf` options as follows:\n\n   - Dual HCA OSS server\n\n   ```\n   options lnet networks=\"o2ib0(ib0),o2ib1(ib1)\"\n   ```\n\n   - Client with the odd IP address\n\n   ```\n   options lnet ip2nets=\"o2ib0(ib0) 192.168.10.[103-253/2]\"\n   ```\n\n   - Client with the even IP address\n\n   ```\n   options lnet ip2nets=\"o2ib1(ib0) 192.168.10.[102-254/2]\"\n   ```\n\n2. Run the modprobe lnet command and create a combined MGS/MDT file system.\n\n   The following commands create an MGS/MDT or OST file system and mount the targets on the servers.\n\n   ```\n   modprobe lnet\n   # mkfs.lustre --fsname lustre --mgs --mdt /dev/mdt_device\n   # mkdir -p /mount_point\n   # mount -t lustre /dev/mdt_device /mount_point\n   ```\n\n   For example:\n\n   ```\n   modprobe lnet\n   mds# mkfs.lustre --fsname lustre --mdt --mgs /dev/sda\n   mds# mkdir -p /mnt/test/mdt\n   mds# mount -t lustre /dev/sda /mnt/test/mdt   \n   mds# mount -t lustre mgs@o2ib0:/lustre /mnt/mdt\n   oss# mkfs.lustre --fsname lustre --mgsnode=mds@o2ib0 --ost --index=0 /dev/sda\n   oss# mkdir -p /mnt/test/mdt\n   oss# mount -t lustre /dev/sda /mnt/test/ost   \n   oss# mount -t lustre mgs@o2ib0:/lustre /mnt/ost0\n   ```\n\n3. Mount the clients.\n\n   ```\n   client# mount -t lustre mgs_node:/fsname /mount_point\n   ```\n\n   This example shows an IB client being mounted.\n\n   ```\n   client# mount -t lustre\n   192.168.10.101@o2ib0,192.168.10.102@o2ib1:/mds/client /mnt/lustre\n   ```\n\nAs an example, consider a two-rail IB cluster running the OFED stack with these IPoIB address assignments.\n\n```\n             ib0                             ib1\nServers            192.168.0.*                     192.168.1.*\nClients            192.168.[2-127].*               192.168.[128-253].*\n```\n\nYou could create these configurations:\n\n- A cluster with more clients than servers. The fact that an individual client cannot get two rails of bandwidth is unimportant because the servers are typically the actual bottleneck.\n\n```\nip2nets=\"o2ib0(ib0),    o2ib1(ib1)      192.168.[0-1].*                     \\\n                                            #all servers;\\\n                   o2ib0(ib0)      192.168.[2-253].[0-252/2]       #even cl\\\nients;\\\n                   o2ib1(ib1)      192.168.[2-253].[1-253/2]       #odd cli\\\nents\"\n```\n\nThis configuration gives every server two NIDs, one on each network, and statically load-balances clients between the rails.\n\n- A single client that must get two rails of bandwidth, and it does not matter if the maximum aggregate bandwidth is only (# servers) * (1 rail).\n\n```\nip2nets=\"       o2ib0(ib0)                      192.168.[0-1].[0-252/2]     \\\n                                            #even servers;\\\n           o2ib1(ib1)                      192.168.[0-1].[1-253/2]         \\\n                                        #odd servers;\\\n           o2ib0(ib0),o2ib1(ib1)           192.168.[2-253].*               \\\n                                        #clients\"\n```\n\nThis configuration gives every server a single NID on one rail or the other. Clients have a NID on both rails.\n\n- All clients and all servers must get two rails of bandwidth.\n\n```\nip2nets=\u00e2\u20ac   o2ib0(ib0),o2ib2(ib1)           192.168.[0-1].", "mimetype": "text/plain", "start_char_idx": 6144, "end_char_idx": 9382, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab43e416-226a-481a-bdc2-4b92ede3a636": {"__data__": {"id_": "ab43e416-226a-481a-bdc2-4b92ede3a636", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2c541e46aafde89add9a6c9ed87c866a12f338b8ddb04609cb3928b949e5f4aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c235149f-381c-49c5-8309-e774a4457fb0", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d9f956af8d16c1dba66c99b375fe6b665a15399f23695516935bdac6901b0c55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f5a2669-15f7-4dc7-a7db-3a253df3c03f", "node_type": "1", "metadata": {}, "hash": "a1ab05a4bc37891efa90be38ae547aa50bf5c2d1adca495d88c41f18854c457f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\nip2nets=\"       o2ib0(ib0)                      192.168.[0-1].[0-252/2]     \\\n                                            #even servers;\\\n           o2ib1(ib1)                      192.168.[0-1].[1-253/2]         \\\n                                        #odd servers;\\\n           o2ib0(ib0),o2ib1(ib1)           192.168.[2-253].*               \\\n                                        #clients\"\n```\n\nThis configuration gives every server a single NID on one rail or the other. Clients have a NID on both rails.\n\n- All clients and all servers must get two rails of bandwidth.\n\n```\nip2nets=\u00e2\u20ac   o2ib0(ib0),o2ib2(ib1)           192.168.[0-1].[0-252/2]       \\\n  #even servers;\\\n           o2ib1(ib0),o2ib3(ib1)           192.168.[0-1].[1-253/2]         \\\n#odd servers;\\\n           o2ib0(ib0),o2ib3(ib1)           192.168.[2-253].[0-252/2)       \\\n#even clients;\\\n           o2ib1(ib0),o2ib2(ib1)           192.168.[2-253].[1-253/2)       \\\n#odd clients\"\n```\n\nThis configuration includes two additional proxy o2ib networks to work around the simplistic NID selection algorithm in the Lustre software. It connects \"even\" clients to \"even\" servers with `o2ib0` on `rail0`, and \"odd\" servers with `o2ib3` on `rail1`. Similarly, it connects \"odd\" clients to \"odd\" servers with `o2ib1` on `rail0`, and \"even\" servers with `o2ib2` on `rail1`.", "mimetype": "text/plain", "start_char_idx": 8737, "end_char_idx": 10075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f5a2669-15f7-4dc7-a7db-3a253df3c03f": {"__data__": {"id_": "8f5a2669-15f7-4dc7-a7db-3a253df3c03f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2c541e46aafde89add9a6c9ed87c866a12f338b8ddb04609cb3928b949e5f4aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab43e416-226a-481a-bdc2-4b92ede3a636", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2d895e992c00a1a859c11fcadf9fb6891055f19ae14696577d9347e07a5ca4c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[1-253/2]         \\\n#odd servers;\\\n           o2ib0(ib0),o2ib3(ib1)           192.168.[2-253].[0-252/2)       \\\n#even clients;\\\n           o2ib1(ib0),o2ib2(ib1)           192.168.[2-253].[1-253/2)       \\\n#odd clients\"\n```\n\nThis configuration includes two additional proxy o2ib networks to work around the simplistic NID selection algorithm in the Lustre software. It connects \"even\" clients to \"even\" servers with `o2ib0` on `rail0`, and \"odd\" servers with `o2ib3` on `rail1`. Similarly, it connects \"odd\" clients to \"odd\" servers with `o2ib1` on `rail0`, and \"even\" servers with `o2ib2` on `rail1`.\n\n\n\nIntroduced in Lustre 2.4\n\n## Dynamically Configuring LNet Routes\n\nTwo scripts are provided: `lustre/scripts/lustre_routes_config` and `lustre/scripts/lustre_routes_conversion`.\n\n`lustre_routes_config` sets or cleans up LNet routes from the specified config file. The `/etc/sysconfig/lnet_routes.conf` file can be used to automatically configure routes on LNet startup.\n\n`lustre_routes_conversion` converts a legacy routes configuration file to the new syntax, which is parsed by `lustre_routes_config`\n\n### `lustre_routes_config`\n\n`lustre_routes_config` usage is as follows\n\n```\nlustre_routes_config [--setup|--cleanup|--dry-run|--verbose] config_file\n         --setup: configure routes listed in config_file\n         --cleanup: unconfigure routes listed in config_file\n         --dry-run: echo commands to be run, but do not execute them\n         --verbose: echo commands before they are executed \n```\n\nThe format of the file which is passed into the script is as follows:\n\n`*network*: { gateway: *gateway*@*exit_network* [hop: *hop*] [priority: *priority*] }`\n\nAn LNet router is identified when its local NID appears within the list of routes. However, this can not be achieved by the use of this script, since the script only adds extra routes after the router is identified. To ensure that a router is identified correctly, make sure to add its local NID in the routes parameter in the modprobe lustre configuration file. See [*the section called \u201c Introduction\u201d*](06.06-Configuration%20Files%20and%20Module%20Parameters.md#introduction).\n\n### `lustre_routes_conversion`\n\n`lustre_routes_conversion` usage is as follows:\n\n```\nlustre_routes_conversion legacy_file new_file\n```\n\n`lustre_routes_conversion` takes as a first parameter a file with routes configured as follows:\n\n`*network* [*hop*] *gateway*@*exit network*[:*priority*];`\n\nThe script then converts each routes entry in the provided file to:\n\n`*network*: { gateway: *gateway*@*exit network* [hop: *hop*] [priority: *priority*] }`\n\nand appends each converted entry to the output file passed in as the second parameter to the script.\n\n### `Route Configuration Examples`\n\nBelow is an example of a legacy LNet route configuration. A legacy configuration file can have multiple entries.\n\n```\ntcp1 10.1.1.2@tcp0:1;\ntcp2 10.1.1.3@tcp0:2;\ntcp3 10.1.1.4@tcp0;\n```\n\nBelow is an example of the converted LNet route configuration. The following would be the result of the `lustre_routes_conversion` script, when run on the above legacy entries.\n\n```\ntcp1: { gateway: 10.1.1.2@tcp0 priority: 1 }\ntcp2: { gateway: 10.1.1.2@tcp0 priority: 2 }\ntcp1: { gateway: 10.1.1.4@tcp0 }\n```", "mimetype": "text/plain", "start_char_idx": 9475, "end_char_idx": 12706, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c7f8342-aff1-4a66-ab67-02c98c9aff6f": {"__data__": {"id_": "1c7f8342-aff1-4a66-ab67-02c98c9aff6f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26a1788e-c342-47a4-ae23-22c00611b98c", "node_type": "1", "metadata": {}, "hash": "83ee7f52dd9de7044a0e64f3bedaa3a474126f0711896d2196272b3be6f9975a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in Lustre 2.10\n\n# LNet Software Multi-Rail\n\n- [LNet Software Multi-Rail](#lnet-software-multi-rail)\n  * [Multi-Rail Overview](#multi-rail-overview)\n  * [Configuring Multi-Rail](#configuring-multi-rail)\n    + [Configure Multiple Interfaces on the Local Node](#configure-multiple-interfaces-on-the-local-node)\n    + [Deleting Network Interfaces](#deleting-network-interfaces)\n    + [Adding Remote Peers that are Multi-Rail Capable](#adding-remote-peers-that-are-multi-rail-capable)\n    + [Deleting Remote Peers](#deleting-remote-peers)\n  * [Notes on routing with Multi-Rail](#notes-on-routing-with-multi-rail)\n    + [Multi-Rail Cluster Example](#multi-rail-cluster-example)\n    + [Utilizing Router Resiliency](#utilizing-router-resiliency)\n    + [Mixed Multi-Rail/Non-Multi-Rail Cluster](#mixed-multi-railnon-multi-rail-cluster)\n  * [LNet Health](#lnet-health)L 2.12\n    + [Health Value](#health-value)\n    + [Failure Types and Behavior](#failure-types-and-behavior)\n    + [User Interface](#user-interface)\n    + [Displaying Information](#displaying-information)\n      - [Showing LNet Health Configuration Settings](#showing-lnet-health-configuration-settings)\n      - [Showing LNet Health Statistics](#showing-lnet-health-statistics)\n    + [Initial Settings Recommendations](#initial-settings-recommendations)\n\nThis chapter describes LNet Software Multi-Rail configuration and administration.\n\n* [the section called \u201cMulti-Rail Overview\u201d](#multi-rail-overview)\n\n* [the section called \u201cConfiguring Multi-Rail\u201d)](#multi-rail-overview)\n\n* [the section called \u201cNotes on routing with Multi-Rail\u201d](#notes-on-routing-with-multi-rail)\n\n* [the section called \u201cLNet Health\u201d](#lnet-health)\n\n## Multi-Rail Overview\n\nIn computer networking, multi-rail is an arrangement in which two or more network interfaces to a single network on a computer node are employed, to achieve increased throughput. Multi-rail can also be where a node has one or more interfaces to multiple, even different kinds of networks, such as Ethernet, Infiniband, and Intel\u00ae Omni-Path. For Lustre clients, multi-rail generally presents the combined network capabilities as a single LNet network. Peer nodes that are multi-rail capable are established during configuration, as are user-defined interface-section policies.\n\nThe following link contains a detailed high-level design for the feature: [Multi-Rail High-Level Design](https://wiki.lustre.org/images/b/bb/Multi-Rail_High-Level_Design_20150119.pdf)\n\n## Configuring Multi-Rail\n\nEvery node using multi-rail networking needs to be properly configured. Multi-rail uses `lnetctl` and the LNet Configuration Library for configuration. Configuring multi-rail for a given node involves two tasks:\n\n1. Configuring multiple network interfaces present on the local node.\n2. Adding remote peers that are multi-rail capable (are connected to one or more common networks with at least two interfaces).\n\nThis section is a supplement to [*the section called \u201cAdding, Deleting and Showing Networks\u201d* ](02.06-Configuring%20Lustre%20Networking%20(LNet).md#adding-deleting-and-showing-networks) and contains further examples for Multi-Rail configurations.\n\nFor information on the dynamic peer discovery feature added in Lustre Release 2.11.0, see [*the section called \u201cDynamic Peer Discovery\u201d*](02.06-Configuring%20Lustre%20Networking%20(LNet).md#dynamic-peer-discovery).\n\n### Configure Multiple Interfaces on the Local Node\n\nExample `lnetctl add` command with multiple interfaces in a Multi-Rail configuration:\n\n```\nlnetctl net add --net tcp --if eth0,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26a1788e-c342-47a4-ae23-22c00611b98c": {"__data__": {"id_": "26a1788e-c342-47a4-ae23-22c00611b98c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c7f8342-aff1-4a66-ab67-02c98c9aff6f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04d934efe7cb70434e911100e4508a993400cb2a586e2b9ee6605ce7c5584f0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afcf90c1-6db2-4858-9311-cb671723f0f6", "node_type": "1", "metadata": {}, "hash": "3a64e9c1f345c7b0b38935bcb72361bc5a7dca29676c0f8df06c3dd3ce65abf1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Configuring multiple network interfaces present on the local node.\n2. Adding remote peers that are multi-rail capable (are connected to one or more common networks with at least two interfaces).\n\nThis section is a supplement to [*the section called \u201cAdding, Deleting and Showing Networks\u201d* ](02.06-Configuring%20Lustre%20Networking%20(LNet).md#adding-deleting-and-showing-networks) and contains further examples for Multi-Rail configurations.\n\nFor information on the dynamic peer discovery feature added in Lustre Release 2.11.0, see [*the section called \u201cDynamic Peer Discovery\u201d*](02.06-Configuring%20Lustre%20Networking%20(LNet).md#dynamic-peer-discovery).\n\n### Configure Multiple Interfaces on the Local Node\n\nExample `lnetctl add` command with multiple interfaces in a Multi-Rail configuration:\n\n```\nlnetctl net add --net tcp --if eth0,eth1\n```\n\nExample of YAML net show:\n\n```\nlnetctl net show -v\nnet:\n    - net type: lo\n      local NI(s):\n        - nid: 0@lo\n          status: up\n          statistics:\n              send_count: 0\n              recv_count: 0\n              drop_count: 0\n          tunables:\n              peer_timeout: 0\n              peer_credits: 0\n              peer_buffer_credits: 0\n              credits: 0\n          lnd tunables:\n          tcp bonding: 0\n          dev cpt: 0\n          CPT: \"[0]\"\n    - net type: tcp\n      local NI(s):\n        - nid: 192.168.122.10@tcp\n          status: up\n          interfaces:\n              0: eth0\n          statistics:\n              send_count: 0\n              recv_count: 0\n              drop_count: 0\n          tunables:\n              peer_timeout: 180\n              peer_credits: 8\n              peer_buffer_credits: 0\n              credits: 256\n          lnd tunables:\n          tcp bonding: 0\n          dev cpt: -1\n          CPT: \"[0]\"\n        - nid: 192.168.122.11@tcp\n          status: up\n          interfaces:\n              0: eth1\n          statistics:\n              send_count: 0\n              recv_count: 0\n              drop_count: 0\n          tunables:\n              peer_timeout: 180\n              peer_credits: 8\n              peer_buffer_credits: 0\n              credits: 256\n          lnd tunables:\n          tcp bonding: 0\n          dev cpt: -1\n          CPT: \"[0]\"\n```\n\n### Deleting Network Interfaces\n\nExample delete with `lnetctl net del`:\n\nAssuming the network configuration is as shown above with the `lnetctl net show -v` in the previous section, we can delete a net with following command:\n\n```\nlnetctl net del --net tcp --if eth0\n```\n\nThe resultant net information would look like:\n\n```\nlnetctl net show -v\nnet:\n    - net type: lo\n      local NI(s):\n        - nid: 0@lo\n          status: up\n          statistics:\n              send_count: 0\n              recv_count: 0\n              drop_count: 0\n          tunables:\n              peer_timeout: 0\n              peer_credits: 0\n              peer_buffer_credits: 0\n              credits: 0\n          lnd tunables:\n          tcp bonding: 0\n          dev cpt: 0\n          CPT: \"[0,1,2,3]\"\n```\n\nThe syntax of a YAML file to perform a delete would be:\n\n```\n- net type: tcp\n   local NI(s):\n     - nid: 192.168.122.10@tcp\n       interfaces:\n           0: eth0\n```\n\n### Adding Remote Peers that are Multi-Rail Capable\n\nThe following example `lnetctl peer add` command adds a peer with 2 nids, with `192.168.122.30@tcp` being the primary nid:\n\n```\nlnetctl peer add --prim_nid 192.168.122.30@tcp --nid 192.168.122.30@tcp,192.168.122.31@tcp\n      \n```\n\nThe resulting `lnetctl peer show` would be:\n\n```\nlnetctl peer show -v\npeer:\n    - primary nid: 192.168.122.30@tcp\n      Multi-Rail: True\n      peer ni:\n        - nid: 192.168.", "mimetype": "text/plain", "start_char_idx": 2719, "end_char_idx": 6377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afcf90c1-6db2-4858-9311-cb671723f0f6": {"__data__": {"id_": "afcf90c1-6db2-4858-9311-cb671723f0f6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26a1788e-c342-47a4-ae23-22c00611b98c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2cfc07232c6026161ac999557b14d5d6142fb5b8f282ff37beec4d278f5b6b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c919a47-5f29-41c5-b7d1-b15d78e8dd8a", "node_type": "1", "metadata": {}, "hash": "3aafdb24f03847bec18b03e4f1e9ed8b4180752a746a9c0e52d258c3ba6cc1f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "168.122.10@tcp\n       interfaces:\n           0: eth0\n```\n\n### Adding Remote Peers that are Multi-Rail Capable\n\nThe following example `lnetctl peer add` command adds a peer with 2 nids, with `192.168.122.30@tcp` being the primary nid:\n\n```\nlnetctl peer add --prim_nid 192.168.122.30@tcp --nid 192.168.122.30@tcp,192.168.122.31@tcp\n      \n```\n\nThe resulting `lnetctl peer show` would be:\n\n```\nlnetctl peer show -v\npeer:\n    - primary nid: 192.168.122.30@tcp\n      Multi-Rail: True\n      peer ni:\n        - nid: 192.168.122.30@tcp\n          state: NA\n          max_ni_tx_credits: 8\n          available_tx_credits: 8\n          min_tx_credits: 7\n          tx_q_num_of_buf: 0\n          available_rtr_credits: 8\n          min_rtr_credits: 8\n          refcount: 1\n          statistics:\n              send_count: 2\n              recv_count: 2\n              drop_count: 0\n        - nid: 192.168.122.31@tcp\n          state: NA\n          max_ni_tx_credits: 8\n          available_tx_credits: 8\n          min_tx_credits: 7\n          tx_q_num_of_buf: 0\n          available_rtr_credits: 8\n          min_rtr_credits: 8\n          refcount: 1\n          statistics:\n              send_count: 1\n              recv_count: 1\n              drop_count: 0\n```\n\nThe following is an example YAML file for adding a peer:\n\n```\naddPeer.yaml\npeer:\n    - primary nid: 192.168.122.30@tcp\n      Multi-Rail: True\n      peer ni:\n        - nid: 192.168.122.31@tcp\n```\n\n### Deleting Remote Peers\n\nExample of deleting a single nid of a peer (192.168.122.31@tcp):\n\n```\nlnetctl peer del --prim_nid 192.168.122.30@tcp --nid 192.168.122.31@tcp\n```\n\nExample of deleting the entire peer:\n\n```\nlnetctl peer del --prim_nid 192.168.122.30@tcp\n```\n\nExample of deleting a peer via YAML:\n\n```\nAssuming the following peer configuration:\npeer:\n    - primary nid: 192.168.122.30@tcp\n      Multi-Rail: True\n      peer ni:\n        - nid: 192.168.122.30@tcp\n          state: NA\n        - nid: 192.168.122.31@tcp\n          state: NA\n        - nid: 192.168.122.32@tcp\n          state: NA\n\nYou can delete 192.168.122.32@tcp as follows:\n\ndelPeer.yaml\npeer:\n    - primary nid: 192.168.122.30@tcp\n      Multi-Rail: True\n      peer ni:\n        - nid: 192.168.122.32@tcp\n    \n% lnetctl import --del < delPeer.yaml\n```\n\n## Notes on routing with Multi-Rail\n\nThis section details how to configure Multi-Rail with the routing feature before the Section , \u201cMulti-Rail Routing with LNet Health\u201d feature landed in Lustre 2.13. Routing code has always monitored the state of the route, in order to avoid using unavailable ones. \n\nThis section describes how you can configure multiple interfaces on the same gateway node but as different routes. This uses the existing route monitoring algorithm to guard against interfaces going down. With the Section \u201cMulti-Rail Routing with LNet Health\u201d feature introduced in Lustre 2.13, the new algorithm uses the Section \u201cLNet Health\u201d feature to monitor the different interfaces of the gateway and always ensures that the healthiest interface is used. Therefore, the configuration described in this section applies to releases prior to Lustre 2.13. It will still work in 2.13 as well, however it is not required due to the reason mentioned above.\n\n### Multi-Rail Cluster Example\n\nThe below example outlines a simple system where all the Lustre nodes are MR capable. Each node in the cluster has two interfaces.\n\n##### Figure 9. Routing Configuration with Multi-Rail\n\n !", "mimetype": "text/plain", "start_char_idx": 5860, "end_char_idx": 9294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c919a47-5f29-41c5-b7d1-b15d78e8dd8a": {"__data__": {"id_": "3c919a47-5f29-41c5-b7d1-b15d78e8dd8a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afcf90c1-6db2-4858-9311-cb671723f0f6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "855af38d4abc81303bc6cd82cc15901b5e5a1de5a9bbca73e143e0435de467f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a0c745a-4ce6-46c7-9097-3ef9d8a4d38b", "node_type": "1", "metadata": {}, "hash": "48a02841f1d2a0b9b52a13b0b9eb8dae4ee84a5a6aced94d5fd78af068a252d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This section describes how you can configure multiple interfaces on the same gateway node but as different routes. This uses the existing route monitoring algorithm to guard against interfaces going down. With the Section \u201cMulti-Rail Routing with LNet Health\u201d feature introduced in Lustre 2.13, the new algorithm uses the Section \u201cLNet Health\u201d feature to monitor the different interfaces of the gateway and always ensures that the healthiest interface is used. Therefore, the configuration described in this section applies to releases prior to Lustre 2.13. It will still work in 2.13 as well, however it is not required due to the reason mentioned above.\n\n### Multi-Rail Cluster Example\n\nThe below example outlines a simple system where all the Lustre nodes are MR capable. Each node in the cluster has two interfaces.\n\n##### Figure 9. Routing Configuration with Multi-Rail\n\n ![Routing Configuration with Multi-Rail](./figures/MR_RoutingConfig.png) \n\nThe routers can aggregate the interfaces on each side of the network by configuring them on the appropriate network.\n\nAn example configuration:\n\n```\nRouters\nlnetctl net add --net o2ib0 --if ib0,ib1\nlnetctl net add --net o2ib1 --if ib2,ib3\nlnetctl peer add --nid <peer1-nidA>@o2ib,<peer1-nidB>@o2ib,...\nlnetctl peer add --nid <peer2-nidA>@o2ib1,<peer2-nidB>>@o2ib1,...\nlnetctl set routing 1\n\nClients\nlnetctl net add --net o2ib0 --if ib0,ib1\nlnetctl route add --net o2ib1 --gateway <rtrX-nidA>@o2ib\nlnetctl peer add --nid <rtrX-nidA>@o2ib,<rtrX-nidB>@o2ib\n        \nServers\nlnetctl net add --net o2ib1 --if ib0,ib1\nlnetctl route add --net o2ib0 --gateway <rtrX-nidA>@o2ib1\nlnetctl peer add --nid <rtrX-nidA>@o2ib1,<rtrX-nidB>@o2ib1\n```\n\nIn the above configuration the clients and the servers are configured with only one route entry per router. This works because the routers are MR capable. By adding the routers as peers with multiple interfaces to the clients and the servers, when sending to the router the MR algorithm will ensure that bot interfaces of the routers are used.\n\nHowever, as of the Lustre 2.10 release LNet Resiliency is still under development and single interface failure will still cause the entire router to go down.\n\n### Utilizing Router Resiliency\n\nCurrently, LNet provides a mechanism to monitor each route entry. LNet pings each gateway identified in the route entry on regular, configurable interval to ensure that it is alive. If sending over a specific route fails or if the router pinger determines that the gateway is down, then the route is marked as down and is not used. It is subsequently pinged on regular, configurable intervals to determine when it becomes alive again.\n\nThis mechanism can be combined with the MR feature in Lustre 2.10 to add this router resiliency feature to the configuration.\n\n```\nRouters\nlnetctl net add --net o2ib0 --if ib0,ib1\nlnetctl net add --net o2ib1 --if ib2,ib3\nlnetctl peer add --nid <peer1-nidA>@o2ib,<peer1-nidB>@o2ib,...\nlnetctl peer add --nid <peer2-nidA>@o2ib1,<peer2-nidB>@o2ib1,...\nlnetctl set routing 1\n\nClients\nlnetctl net add --net o2ib0 --if ib0,ib1\nlnetctl route add --net o2ib1 --gateway <rtrX-nidA>@o2ib\nlnetctl route add --net o2ib1 --gateway <rtrX-nidB>@o2ib\n        \nServers\nlnetctl net add --net o2ib1 --if ib0,ib1\nlnetctl route add --net o2ib0 --gateway <rtrX-nidA>@o2ib1\nlnetctl route add --net o2ib0 --gateway <rtrX-nidB>@o2ib1\n```\n\nThere are a few things to note in the above configuration:\n\n1.", "mimetype": "text/plain", "start_char_idx": 8416, "end_char_idx": 11851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a0c745a-4ce6-46c7-9097-3ef9d8a4d38b": {"__data__": {"id_": "0a0c745a-4ce6-46c7-9097-3ef9d8a4d38b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c919a47-5f29-41c5-b7d1-b15d78e8dd8a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0882326cb098df5a320c4ea46578634793214caf8f99b251e8c4c01abdecfe3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0965d9b9-8345-4ac2-9fce-fc4b9eee8918", "node_type": "1", "metadata": {}, "hash": "a5981aee81c6e5915f53ee3c0e98fee507e21fae1a0f49c82c1e675b927dff89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The clients and the servers are now configured with two routes, each route's gateway is one of the interfaces of the route. The clients and servers will view each interface of the same router as a separate gateway and will monitor them as described above.\n2. The clients and the servers are not configured to view the routers as MR capable. This is important because we want to deal with each interface as a separate peers and not different interfaces of the same peer.\n3. The routers are configured to view the peers as MR capable. This is an oddity in the configuration, but is currently required in order to allow the routers to load balance the traffic load across its interfaces evenly.\n\n### Mixed Multi-Rail/Non-Multi-Rail Cluster\n\nThe above principles can be applied to mixed MR/Non-MR cluster. For example, the same configuration shown above can be applied if the clients and the servers are non-MR while the routers are MR capable. This appears to be a common cluster upgrade scenario.\n\n## Multi-Rail Routing with LNet Health\n\nThis section details how routing and pertinent module parameters can be configured beginning with Lustre 2.13. \n\nMulti-Rail with Dynamic Discovery allows LNet to discover and use all configured interfaces of a node. It references a node via it's primary NID. Multi-Rail routing carries forward this concept to the routing infrastructure. The following changes are brought in with the Lustre 2.13 release: \n\n1. Configuring a different route per gateway interface is no longer needed. One route per gateway should be configured. Gateway interfaces are used according to the Multi-Rail selection criteria. \n2. Routing now relies on Section \\ \u201cLNet Health\u201d to keep track of the route aliveness. \n3. Router interfaces are monitored via LNet Health. If an interface fails other interfaces will be used. \n4. Routing uses LNet discovery to discover gateways on regular intervals. \n5. A gateway pushes its list of interfaces upon the discovery of any changes in its interfaces' state.\n\n### Configuration\n\n#### Configuring Routes\n\nA gateway can have multiple interfaces on the same or different networks. The peers using the gateway can reach it on one or more of its interfaces. Multi-Rail routing takes care of managing which interface to use.\n\n```\nlnetctl route add --net <remote network> --gateway <NID for the gateway>\n --hops <number of hops> --priority <route priority>\n```\n\n#### Configuring Module Parameters\n\n**Table 16.1. Configuring Module Parameters**\n\n| **Module Parameter**            | Usage                                                        |\n| ------------------------------- | ------------------------------------------------------------ |\n| `check_routers_before_use`      | Defaults to `0`. If set to `1` all routers must be up before the system can proceed. |\n| `avoid_asym_router_failure`     | Defaults to `1`. If set to `1` a route will be considered up if and only if there exists at least one healthy interface on the local and remote interfaces of the gateway |\n| `avoid_asym_router_failure`     | Defaults to `60` seconds. The gateways will be discovered ever `alive_router_check_interval`. If the gateway can be reached on multiple networks, the interval per network is `alive_router_check_interval` / number of networks. |\n| `router_ping_timeout`           | Defaults to `50` seconds. A gateway sets its interface down if it has not received any traffic for `router_ping_timeout `+ `alive_router_check_interval` |\n| `router_sensitivity_percentage` | Defaults to `100`. This parameter defines how sensitive a gateway interface is to failure. If set to 100 then any gateway interface failure will contribute to all routes using it going down. The lower the value the more tolerant to failures the system becomes. |\n\n### Router Health\n\nThe routing infrastructure now relies on LNet Health to keep track of interface health. Each gateway interface has a health value associated with it. If a send fails to one of these interfaces, then the interface's health value is decremented and placed on a recovery queue. The unhealthy interface is then pinged every `lnet_recovery_interval`. This value defaults to 1 second. \n\nIf the peer receives a message from the gateway, then it immediately assumes that the gateway's interface is up and resets its health value to maximum. This is needed to ensure we start using the gateways immediately instead of holding off until the interface is back to full health.\n\n### Discovery\n\nLNet Discovery is used in place of pinging the peers. This serves two purposes: \n\n1. The discovery communication infrastructure does not need to be duplicated for the routing feature. \n\n2.", "mimetype": "text/plain", "start_char_idx": 11852, "end_char_idx": 16515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0965d9b9-8345-4ac2-9fce-fc4b9eee8918": {"__data__": {"id_": "0965d9b9-8345-4ac2-9fce-fc4b9eee8918", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a0c745a-4ce6-46c7-9097-3ef9d8a4d38b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "41592c4704ff7ef31d7c7b54f5ff63b8fd51473e20d4bbfc65674d37abc6601c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95914297-72b6-4758-a223-358c6e8ebb2d", "node_type": "1", "metadata": {}, "hash": "0233d792e56a59658923160fa6b62dde0c775f42aa449f07e93d75524aa0a2b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The lower the value the more tolerant to failures the system becomes. |\n\n### Router Health\n\nThe routing infrastructure now relies on LNet Health to keep track of interface health. Each gateway interface has a health value associated with it. If a send fails to one of these interfaces, then the interface's health value is decremented and placed on a recovery queue. The unhealthy interface is then pinged every `lnet_recovery_interval`. This value defaults to 1 second. \n\nIf the peer receives a message from the gateway, then it immediately assumes that the gateway's interface is up and resets its health value to maximum. This is needed to ensure we start using the gateways immediately instead of holding off until the interface is back to full health.\n\n### Discovery\n\nLNet Discovery is used in place of pinging the peers. This serves two purposes: \n\n1. The discovery communication infrastructure does not need to be duplicated for the routing feature. \n\n2. It allows propagation of the gateway's interface state changes to the peers using the gateway. \n\nFor (2), if an interface changes state from `UP` to `DOWN` or vice versa, then a discovery `PUSH` is sent to all the peers which can be reached. This allows peers to adapt to changes quicker. \n\nDiscovery is designed to be backwards compatible. The discovery protocol is composed of a `GET` and a `PUT`. The `GET` requests interface information from the peer, this is a basic lnet ping. The peer responds with its interface information and a feature bit. If the peer is multirail capable and discovery is turned on, then the node will `PUSH` its interface information. As a result both peers will be aware of each other's interfaces. \n\nThis information is then used by the peers to decide, based on the interface state provided by the gateway, whether the route is alive or not.\n\n## Route Aliveness Criteria\n\nA route is considered alive if the following conditions hold:\n\n1. The gateway can be reached on the local net via at least one path.\n\n2. If `avoid_asym_router_failure` is enabled then the remote network defined in the route must have at least one healthy interface on the gateway.\n\nIntroduced in Lustre 2.12\n\n## LNet Health\n\nLNet Multi-Rail has\u00a0implemented the ability for multiple interfaces to be used on the same LNet network or across multiple LNet networks. The LNet Health feature adds the ability to maintain a health value for each local and remote interface. This allows the Multi-Rail algorithm to consider the health of the interface before selecting it for sending. The feature also adds the ability to resend messages across different interfaces when interface or network failures are detected. This allows LNet to mitigate communication failures before passing the failures to upper layers for further error handling. To accomplish this, LNet Health monitors the status of the send and receive operations and uses this status to increment the interface's health value in case of success and decrement it in case of failure.\n\n### Health Value\n\nThe initial health value of a local or remote interface is set to `LNET_MAX_HEALTH_VALUE`, currently set to be `1000`. The value itself is arbitrary and is meant to allow for health granularity, as opposed to having a simple boolean state. The granularity allows the Multi-Rail algorithm to select the interface that has the highest likelihood of sending or receiving a message.\n\n### Failure Types and Behavior\n\nLNet health behavior depends on the type of failure detected:\n\n| **Failure Type**  | **Behavior**                                                 |\n| ----------------- | ------------------------------------------------------------ |\n| `localresend`     | A local failure has occurred, such as no route found or an address resolution error. These failures could be temporary, therefore LNet will attempt to resend the message. LNet will decrement the health value of the local interface and will select it less often if there are multiple available interfaces. |\n| `localno-resend`  | A local non-recoverable error occurred in the system, such as out of memory error. In these cases LNet will not attempt to resend the message. LNet will decrement the health value of the local interface and will select it less often if there are multiple available interfaces. |\n| `remoteno-resend` | If LNet successfully sends a message, but the message does not complete or an expected reply is not received, then it is classified as a remote error. LNet will not attempt to resend the message to avoid duplicate messages on the remote end. LNet will decrement the health value of the remote interface and will select it less often if there are multiple available interfaces.", "mimetype": "text/plain", "start_char_idx": 15554, "end_char_idx": 20251, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95914297-72b6-4758-a223-358c6e8ebb2d": {"__data__": {"id_": "95914297-72b6-4758-a223-358c6e8ebb2d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0965d9b9-8345-4ac2-9fce-fc4b9eee8918", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47c57f8157b9828f3cd0bc4c390e3b6d18cd4b76a4dd747749c7dbd4e64dfc76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7a41d50-04ce-4a68-9d7e-b3ec356957ea", "node_type": "1", "metadata": {}, "hash": "3b9ce451176621f2b625aa3c0dc8a728d2a76cef9850d79a5a28643ba30587bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These failures could be temporary, therefore LNet will attempt to resend the message. LNet will decrement the health value of the local interface and will select it less often if there are multiple available interfaces. |\n| `localno-resend`  | A local non-recoverable error occurred in the system, such as out of memory error. In these cases LNet will not attempt to resend the message. LNet will decrement the health value of the local interface and will select it less often if there are multiple available interfaces. |\n| `remoteno-resend` | If LNet successfully sends a message, but the message does not complete or an expected reply is not received, then it is classified as a remote error. LNet will not attempt to resend the message to avoid duplicate messages on the remote end. LNet will decrement the health value of the remote interface and will select it less often if there are multiple available interfaces. |\n| `remoteresend`    | There are a set of failures where we can be reasonably sure that the message was dropped before getting to the remote end. In this case, LNet will attempt to resend the message. LNet will decrement the health value of the remote interface and will select it less often if there are multiple available interfaces. |\n\n### User Interface\n\nLNet Health is turned off by default. There are multiple module parameters available to control the LNet Health feature.\n\nAll the module parameters are implemented in sysfs and are located in /sys/module/lnet/parameters/. They can be set directly by echoing a value into them as well as from lnetctl.\n\n| **Parameter**              | **Description**                                              |\n| -------------------------- | ------------------------------------------------------------ |\n| `lnet_health_sensitivity`  | When LNet detects a failure on a particular interface it will decrement its Health Value by `lnet_health_sensitivity`. The greater the value, the longer it takes for that interface to become healthy again. The default value of `lnet_health_sensitivity` is set to 0, which means the health value will not be decremented. In essense, the health feature is turned off.The sensitivity value can be set greater than 0. A `lnet_health_sensitivity` of 100 would mean that 10 consecutive message failures or a steady-state failure rate over 1% would degrade the interface Health Value until it is disabled, while a lower failure rate would steer traffic away from the interface but it would continue to be available. When a failure occurs on an interface then its Health Value is decremented and the interface is flagged for recovery.                                                                                                                        `lnetctl set health_sensitivity: sensitivity to failure                                                                           0 - turn off health evaluation                                                                 >0 - sensitivity value not more than 1000` |\n| `lnet_recovery_interval`   | When LNet detects a failure on a local or remote interface it will place that interface on a recovery queue. There is a recovery queue for local interfaces and another for remote interfaces. The interfaces on the recovery queues will be LNet PINGed every `lnet_recovery_interval`. This value defaults to `1` second. On every successful PING the health value of the interface pinged will be incremented by `1`.Having this value configurable allows system administrators to control the amount of control traffic on the network.                                                                                                          `lnetctl set recovery_interval: interval to ping unhealthy interfaces                                                                      >0 - timeout in seconds` |\n| `lnet_transaction_timeout` | This timeout is somewhat of an overloaded value. It carries the following functionality:                                                                                                                             - A message is abandoned if it is not sent successfully when the lnet_transaction_timeout expires and the retry_count is not reached.                                                                                                                                                                          - A GET or a PUT which expects an ACK expires if a REPLY or an ACK respectively, is not received within the `lnet_transaction_timeout`.                                                                                                                             This value defaults to 30 seconds.                                                                                                             `lnetctl set transaction_timeout: Message/Response timeout                                                                                                               >0 - timeout in seconds`                                                                                      **Note**                                                                                                                             The LND timeout will now be a fraction of the `lnet_transaction_timeout` as described in the next section.                                                                                                                                                                                                                                                      This means that in networks where very large delays are expected then it will be necessary to increase this value accordingly. |\n| `lnet_retry_count`         | When LNet detects a failure which it deems appropriate for re-sending a message it will check if a message has passed the maximum retry_count specified. After which if a message wasn't sent successfully a failure event will be passed up to the layer which initiated message sending.", "mimetype": "text/plain", "start_char_idx": 19330, "end_char_idx": 25333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7a41d50-04ce-4a68-9d7e-b3ec356957ea": {"__data__": {"id_": "e7a41d50-04ce-4a68-9d7e-b3ec356957ea", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95914297-72b6-4758-a223-358c6e8ebb2d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c71c855de3dbd1e3ed781f56ece02f98872f35c71af16d45f01fe433df540e73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "529c9648-92dd-4acb-8e9e-4abc64947169", "node_type": "1", "metadata": {}, "hash": "27d56a8e750c1733e4a2e63f293f341b2d762a26d1e5bdd773b9d14b0e1a9c09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- A GET or a PUT which expects an ACK expires if a REPLY or an ACK respectively, is not received within the `lnet_transaction_timeout`.                                                                                                                             This value defaults to 30 seconds.                                                                                                             `lnetctl set transaction_timeout: Message/Response timeout                                                                                                               >0 - timeout in seconds`                                                                                      **Note**                                                                                                                             The LND timeout will now be a fraction of the `lnet_transaction_timeout` as described in the next section.                                                                                                                                                                                                                                                      This means that in networks where very large delays are expected then it will be necessary to increase this value accordingly. |\n| `lnet_retry_count`         | When LNet detects a failure which it deems appropriate for re-sending a message it will check if a message has passed the maximum retry_count specified. After which if a message wasn't sent successfully a failure event will be passed up to the layer which initiated message sending.                                                                                        Since the message retry interval (`lnet_lnd_timeout`) is computed from `lnet_transaction_timeout / lnet_retry_count`, the `lnet_retry_count` should be kept low enough that the retry interval is not shorter than the round-trip message delay in the network. A `lnet_retry_count` of 5 is reasonable for the default `lnet_transaction_timeout` of 50 seconds.                                                                           `lnetctl set retry_count: number of retries                                                                                                        0 - turn off retries                                                                             >0 - number of retries, cannot be more than lnet_transaction_timeout` |\n| `lnet_lnd_timeout`         | This is not a configurable parameter. But it is derived from two configurable parameters: `lnet_transaction_timeout` and `retry_count`.                                                                        `lnet_lnd_timeout = lnet_transaction_timeout / retry_count `                                                                                                             As such there is a restriction that `lnet_transaction_timeout >= retry_count`                                                                  The core assumption here is that in a healthy network, sending and receiving LNet messages should not have large delays. There could be large delays with RPC messages and their responses, but that's handled at the PtlRPC layer. |\n\n### Displaying Information\n\n#### Showing LNet Health Configuration Settings\n\n`lnetctl` can be used to show all the LNet health configuration settings using the `lnetctl global show` command.\n\n```\n#> lnetctl global show\n      global:\n      numa_range: 0\n      max_intf: 200\n      discovery: 1\n      retry_count: 3\n      transaction_timeout: 10\n      health_sensitivity: 100\n      recovery_interval: 1\n```\n\n#### Showing LNet Health Statistics\n\nLNet Health statistics are shown under a higher verbosity settings. To show the local interface health statistics:\n\n```\nlnetctl net show -v 3\n```\n\nTo show the remote interface health statistics:\n\n```\nlnetctl peer show -v 3\n```\n\nSample output:\n\n```\n#> lnetctl net show -v 3\n      net:\n      - net type: tcp\n        local NI(s):\n           - nid: 192.168.122.108@tcp\n             status: up\n             interfaces:\n                 0: eth2\n             statistics:\n                 send_count: 304\n                 recv_count: 284\n                 drop_count: 0\n             sent_stats:\n                 put: 176\n                 get: 138\n                 reply: 0\n                 ack: 0\n                 hello: 0\n             received_stats:\n                 put: 145\n                 get: 137\n                 reply: 0\n                 ack: 2\n                 hello: 0\n             dropped_stats:\n                 put: 10\n                 get: 0\n                 reply: 0\n                 ack: 0\n                 hello: 0\n             health stats:\n                 health value: 1000\n                 interrupts: 0\n                 dropped: 10\n                 aborted: 0\n                 no route: 0\n                 timeouts: 0\n                 error: 0\n             tunables:\n                 peer_timeout: 180\n                 peer_credits: 8\n                 peer_buffer_credits: 0\n                 credits: 256\n             dev cpt: -1\n             tcp bonding: 0\n             CPT: \"[0]\"\n      CPT: \"[0]\"\n```\n\nThere is a new YAML block, `health stats`, which displays the health statistics for each local or remote network interface.", "mimetype": "text/plain", "start_char_idx": 23724, "end_char_idx": 29030, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "529c9648-92dd-4acb-8e9e-4abc64947169": {"__data__": {"id_": "529c9648-92dd-4acb-8e9e-4abc64947169", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76c8350f728c1a9c29ac3980d2a3835f738feafff3db6dc6b5704851661d95b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7a41d50-04ce-4a68-9d7e-b3ec356957ea", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7ba017a4196d84d7205da2eee57806405265672750cb45bb7ac955fbbf0a26ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Global statistics also dump the global health statistics as shown below:\n\n```\n#> lnetctl stats show\n        statistics:\n            msgs_alloc: 0\n            msgs_max: 33\n            rst_alloc: 0\n            errors: 0\n            send_count: 901\n            resend_count: 4\n            response_timeout_count: 0\n            local_interrupt_count: 0\n            local_dropped_count: 10\n            local_aborted_count: 0\n            local_no_route_count: 0\n            local_timeout_count: 0\n            local_error_count: 0\n            remote_dropped_count: 0\n            remote_error_count: 0\n            remote_timeout_count: 0\n            network_timeout_count: 0\n            recv_count: 851\n            route_count: 0\n            drop_count: 10\n            send_length: 425791628\n            recv_length: 69852\n            route_length: 0\n            drop_length: 0\n```\n\n### Initial Settings Recommendations\n\nLNet Health is off by default. This means that `lnet_health_sensitivity` and `lnet_retry_count` are set to `0`.\n\nSetting `lnet_health_sensitivity` to `0` will not decrement the health of the interface on failure and will not change the interface selection behavior. Furthermore, the failed interfaces will not be placed on the recovery queues. In essence, turning off the LNet Health feature.\n\nThe LNet Health settings will need to be tuned for each cluster. However, the base configuration would be as follows:\n\n```\n#> lnetctl global show\n    global:\n        numa_range: 0\n        max_intf: 200\n        discovery: 1\n        retry_count: 3\n        transaction_timeout: 10\n        health_sensitivity: 100\n        recovery_interval: 1\n```\n\nThis setting will allow a maximum of two retries for failed messages within the 5 second transaction timeout.\n\nIf there is a failure on the interface the health value will be decremented by 1 and the interface will be LNet PINGed every 1 second.", "mimetype": "text/plain", "start_char_idx": 29032, "end_char_idx": 30928, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ba914da-d7e6-4b83-bc5f-d4afc270dad4": {"__data__": {"id_": "6ba914da-d7e6-4b83-bc5f-d4afc270dad4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0888add2-b651-4598-9893-5922248a97c9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bf0f598e59f15a348f8755c26bf1809118050a124e874498372aa5be0046dc3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "758c62da-76ab-480e-92bd-39b87a48a406", "node_type": "1", "metadata": {}, "hash": "0a74cf9c2bcfc1a4a6cfbf69d5f7584bc9cee1616738e55feb694997eb4c2c88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Upgrading a Lustre File System\n\n- [Upgrading a Lustre File System](#upgrading-a-lustre-file-system)\n  * [Release Interoperability and Upgrade Requirements](#release-interoperability-and-upgrade-requirements)\n  * [Upgrading to Lustre Software Release 2.x (Major Release)](#upgrading-to-lustre-software-release-2x-major-release)\n  * [Upgrading to Lustre Software Release 2.x.y (Minor Release)](#upgrading-to-lustre-software-release-2xy-minor-release)\n\nThis chapter describes interoperability between Lustre software releases. It also provides procedures for upgrading from older Lustre 2.x software releases to a more recent 2.y Lustre release a (major release upgrade), and from a Lustre software release 2.x.y to a more recent Lustre software release 2.x.z (minor release upgrade). It includes the following sections:\n\n* [the section called \u201c Release Interoperability and Upgrade Requirements\u201d](#release-interoperability-and-upgrade-requirements)\n\n* [the section called \u201c Upgrading to Lustre Software Release 2.x (Major Release)\u201d](#upgrading-to-lustre-software-release-2x-major-release)\n\n* [the section called \u201c Upgrading to Lustre Software Release 2.x.y (Minor Release)\u201d](#upgrading-to-lustre-software-release-2xy-minor-release)\n\n## Release Interoperability and Upgrade Requirements\n\n**Lustre software release 2.x (major) upgrade:**\n\n- All servers must be upgraded at the same time, while some or all clients may be upgraded independently of the servers.\n- All servers must be be upgraded to a Linux kernel supported by the Lustre software. See the Lustre Release Notes for your Lustre version for a list of tested Linux distributions.\n- Clients to be upgraded must be running a compatible Linux distribution as described in the Release Notes.\n\n**Lustre software release 2.x.y release (minor) upgrade:**\n\n- All servers must be upgraded at the same time, while some or all clients may be upgraded.\n- Rolling upgrades are supported for minor releases allowing individual servers and clients to be upgraded without stopping the Lustre file system.\n\n## Upgrading to Lustre Software Release 2.x (Major Release)\n\nThe procedure for upgrading from a Lustre software release 2.x to a more recent 2.y major release of the Lustre software is described in this section. To upgrade an existing 2.x installation to a more recent major release, complete the following steps:\n\n1. Create a complete, restorable file system backup.\n\n   ### Caution\n\n   Before installing the Lustre software, back up ALL data. The Lustre software contains kernel modifications that interact with storage devices and may introduce security issues and data loss if not installed, configured, or administered properly. If a full backup of the file system is not practical, a device-level backup of the MDT file system is recommended. See [*Backing Up and Restoring a File System*](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md) for a procedure.\n\n2. Shut down the entire filesystem by following [the section called \u201c Stopping the Filesystem\u201d](03.02-Lustre%20Operations.md#stopping-the-filesystem)\n\n3. Upgrade the Linux operating system on all servers to a compatible (tested) Linux distribution and reboot.\n\n4. Upgrade the Linux operating system on all clients to a compatible (tested) distribution and reboot.\n\n5. Download the Lustre server RPMs for your platform from the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases) repository. See [Table 5, \u201cPackages Installed on Lustre Servers\u201d](02.05-Installing%20the%20Lustre%20Software.md#table-5-packages-installed-on-lustre-servers) for a list of required packages.\n\n6. Install the Lustre server packages on all Lustre servers (MGS, MDSs, and OSSs).\n\n   a. Log onto a Lustre server as the `root` user\n\n   b. Use the `yum` command to install the packages:\n\n   ```\n   # yum --nogpgcheck install pkg1.rpm pkg2.rpm ... \n   ```\n\n   c. Verify the packages are installed correctly:\n\n   ```\n   rpm -qa|egrep \"lustre|wc\"\n   ```\n\n   d. Repeat these steps on each Lustre server.\n\n   ```\n   \n   ```\n\n7.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "758c62da-76ab-480e-92bd-39b87a48a406": {"__data__": {"id_": "758c62da-76ab-480e-92bd-39b87a48a406", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0888add2-b651-4598-9893-5922248a97c9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bf0f598e59f15a348f8755c26bf1809118050a124e874498372aa5be0046dc3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ba914da-d7e6-4b83-bc5f-d4afc270dad4", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b5b307799975aacb7645be5f903d2966befe5124a46812eca1f2716280e8e256", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e271d3c8-a8e4-41c2-a214-c9e3fbeb81ab", "node_type": "1", "metadata": {}, "hash": "6378f85ea8a498b388ab328cc1e290a83e207f4b420b2123cd76e1683f2cc7c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See [Table 5, \u201cPackages Installed on Lustre Servers\u201d](02.05-Installing%20the%20Lustre%20Software.md#table-5-packages-installed-on-lustre-servers) for a list of required packages.\n\n6. Install the Lustre server packages on all Lustre servers (MGS, MDSs, and OSSs).\n\n   a. Log onto a Lustre server as the `root` user\n\n   b. Use the `yum` command to install the packages:\n\n   ```\n   # yum --nogpgcheck install pkg1.rpm pkg2.rpm ... \n   ```\n\n   c. Verify the packages are installed correctly:\n\n   ```\n   rpm -qa|egrep \"lustre|wc\"\n   ```\n\n   d. Repeat these steps on each Lustre server.\n\n   ```\n   \n   ```\n\n7. Download the Lustre client RPMs for your platform from the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases) repository. See [Table 6, \u201cPackages Installed on Lustre Clients\u201d](02.05-Installing%20the%20Lustre%20Software.md#table-6-packages-installed-on-lustre-clients) for a list of required packages.\n\n   ### Note\n\n   The version of the kernel running on a Lustre client must be the same as the version of the `lustre-client-modules-ver`package being installed. If not, a compatible kernel must be installed on the client before the Lustre client packages are installed.\n\n8. Install the Lustre client packages on each of the Lustre clients to be upgraded.\n\n   a. Log onto a Lustre client as the `root` user.\n\n   b. Use the `yum` command to install the packages:\n\n   ```\n   # yum --nogpgcheck install pkg1.rpm pkg2.rpm ... \n   ```\n\n   c. Verify the packages were installed correctly:\n\n   ```\n   # rpm -qa|egrep \"lustre|kernel\"\n   ```\n\n   d. Repeat these steps on each Lustre client.\n\n   \n\n9. The DNE feature allows using multiple MDTs within a single filesystem namespace, and each MDT can each serve one or more remote sub-directories in the file system. The root directory is always located on MDT0.\n\n   Note that clients running a release prior to the Lustre software release 2.4 can only see the namespace hosted by MDT0 and will return an IO error if an attempt is made to access a directory on another MDT.\n\n   (Optional) To format an additional MDT, complete these steps:\n\n   a. Determine the index used for the first MDT (each MDT must have unique index). Enter:\n\n   ```\n   client$ lctl dl | grep mdc\n   36 UP mdc lustre-MDT0000-mdc-ffff88004edf3c00 \n         4c8be054-144f-9359-b063-8477566eb84e 5\n   ```\n\n   In this example, the next available index is 1.\n\n   b. Format the new block device as a new MDT at the next available MDT index by entering (on one line):\n\n   ```\n   mds# mkfs.lustre --reformat --fsname=filesystem_name --mdt \\\n       --mgsnode=mgsnode --index new_mdt_index\n   /dev/mdt1_device\n   ```\n\n10. (Optional) If you are upgrading from Lustre software release 2.10, to enable the project quota feature enter the following on every ldiskfs backend target while unmounted:\n\n   ```\n    tune2fs \u2013O dirdata /dev/mdtdev\n   ```\n\n\u200b\t\t**Note**\n\n\u200b\t\tEnabling the project feature will prevent the filesystem from being used by older versions of ldiskfs, so it should only be enabled if the project quota feature is required and/or after it is known that the upgraded release does not need to be downgraded.\n\n11. When setting up the file system, enter:\n\n    ```\n    conf_param $FSNAME.quota.mdt=$QUOTA_TYPE\n    conf_param $FSNAME.quota.ost=$QUOTA_TYPE\n    ```\n\n12. (Optional) If upgrading an ldiskfs MDT formatted prior to Lustre 2.13, the \"wide striping\" feature that allows files to have more than 160 stripes and store other large xattrs was not enabled by default.", "mimetype": "text/plain", "start_char_idx": 3434, "end_char_idx": 6939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e271d3c8-a8e4-41c2-a214-c9e3fbeb81ab": {"__data__": {"id_": "e271d3c8-a8e4-41c2-a214-c9e3fbeb81ab", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0888add2-b651-4598-9893-5922248a97c9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bf0f598e59f15a348f8755c26bf1809118050a124e874498372aa5be0046dc3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "758c62da-76ab-480e-92bd-39b87a48a406", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29d0dcd74cedafd3be30884e624cb1fd0063dbcaf705ec86b56e41a30e02d6f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04ffd1d6-c418-4587-9df1-d372db06dc6f", "node_type": "1", "metadata": {}, "hash": "c9f898d41a9b9497e7f60d6be8266951cfdf354c1d13882c645a48aa434399c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11. When setting up the file system, enter:\n\n    ```\n    conf_param $FSNAME.quota.mdt=$QUOTA_TYPE\n    conf_param $FSNAME.quota.ost=$QUOTA_TYPE\n    ```\n\n12. (Optional) If upgrading an ldiskfs MDT formatted prior to Lustre 2.13, the \"wide striping\" feature that allows files to have more than 160 stripes and store other large xattrs was not enabled by default. This feature can be enabled on existing MDTs by running the following command on all MDT devices:\n\n    ```\n    mds# tune2fs -O ea_inode /dev/mdtdev\n    ```\n\n    For more information about wide striping, see Section \u201cLustre Striping Internals\u201d.\n\n13. Start the Lustre file system by starting the components in the order shown in the following steps:\n\n    a. Mount the MGT. On the MGS, run\n\n    ```\n    mgs# mount -a -t lustre\n    ```\n\n    b. Mount the MDT(s). On each MDT, run:\n\n    ```\n    mds# mount -a -t lustre\n    ```\n\n    c. Mount all the OSTs. On each OSS node, run:\n\n    ```\n    oss# mount -a -t lustre\n    ```\n\n    ### Note\n\n    This command assumes that all the OSTs are listed in the `/etc/fstab` file. OSTs that are not listed in the `/etc/fstab` file, must be mounted individually by running the mount command:\n\n    ```\n    mount -t lustre /dev/block_device/mount_point\n    ```\n\n    d. Mount the file system on the clients. On each client node, run:\n\n    ```\n    client# mount -a -t lustre\n    ```\n    \n14. (Optional) If you are upgrading from a release before Lustre 2.7, to enable OST FIDs to also store the\n    OST index (to improve reliability of LFSCK and debug messages), after the OSTs are mounted run\n    once on each OSS:\n\n    ```\n    oss# lctl set_param osd-ldiskfs.*.osd_index_in_idif=1\n    ```\n\n    **Note**\n\n    Enabling the `index_in_idif` feature will prevent the OST from being used by older versions of Lustre, so it should only be enabled once it is known there is no need for the OST to be downgraded to an earlier release.\n\n15. If a new MDT was added to the filesystem, the new MDT must be attached into the namespace by creating one or more new DNE subdirectories with the lfs mkdir command that use the new MDT:\n\n    ```\n    client# lfs mkdir -i new_mdt_index /testfs/new_dir\n    ```\n\n    In Lustre 2.8 and later, it is possible to split a new directory across multiple MDTs by creating it with multiple stripes:\n\n    ```\n    client# lfs mkdir -c 2 /testfs/new_striped_dir\n    ```\n\n    In Lustre 2.13 and later, it is possible to set the default striping on existing directories so that new remote subdirectories are created on less-full MDTs:\n\n    ```\n    client# lfs setdirstripe -c 1 -i -1 /testfs/some_dir\n    ```\n\n**Note **\n\nThe mounting order described in the steps above must be followed for the initial mount and registration of a Lustre file system after an upgrade. For a normal start of a Lustre file system, the mounting order is MGT, OSTs, MDT(s), clients.\n\nIf you have a problem upgrading a Lustre file system, see [the section called \u201cReporting a Lustre File System Bug\u201d](05.01-Lustre%20File%20System%20Troubleshooting.md#reporting-a-lustre-file-system-bug) for ways to get help.\n\n## Upgrading to Lustre Software Release 2.x.y (Minor Release)\n\nRolling upgrades are supported for upgrading from any Lustre software release 2.x.y to a more recent Lustre software release 2.X.y. This allows the Lustre file system to continue to run while individual servers (or their failover partners) and clients are upgraded one at a time. The procedure for upgrading a Lustre software release 2.x.y to a more recent minor release is described in this section.\n\nTo upgrade Lustre software release 2.x.y to a more recent minor release, complete these steps:\n\n1. Create a complete, restorable file system backup.", "mimetype": "text/plain", "start_char_idx": 6580, "end_char_idx": 10282, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04ffd1d6-c418-4587-9df1-d372db06dc6f": {"__data__": {"id_": "04ffd1d6-c418-4587-9df1-d372db06dc6f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0888add2-b651-4598-9893-5922248a97c9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bf0f598e59f15a348f8755c26bf1809118050a124e874498372aa5be0046dc3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e271d3c8-a8e4-41c2-a214-c9e3fbeb81ab", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "521eabf9449a9e14164dfb1604039b42edb4ea62c5a054e6d91e5de62a3af7e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Upgrading to Lustre Software Release 2.x.y (Minor Release)\n\nRolling upgrades are supported for upgrading from any Lustre software release 2.x.y to a more recent Lustre software release 2.X.y. This allows the Lustre file system to continue to run while individual servers (or their failover partners) and clients are upgraded one at a time. The procedure for upgrading a Lustre software release 2.x.y to a more recent minor release is described in this section.\n\nTo upgrade Lustre software release 2.x.y to a more recent minor release, complete these steps:\n\n1. Create a complete, restorable file system backup.\n\n   ### Caution\n\n   Before installing the Lustre software, back up ALL data. The Lustre software contains kernel modifications that interact with storage devices and may introduce security issues and data loss if not installed, configured, or administered properly. If a full backup of the file system is not practical, a device-level backup of the MDT file system is recommended. See *Backing Up and Restoring a File System* for a procedure.\n\n2. Download the Lustre server RPMs for your platform from the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases) repository. See [Table 5, \u201cPackages Installed on Lustre Servers\u201d](02.05-Installing%20the%20Lustre%20Software.md#table-5-packages-installed-on-lustre-servers) for a list of required packages.\n\n3. For a rolling upgrade, complete any procedures required to keep the Lustre file system running while the server to be upgraded is offline, such as failing over a primary server to its secondary partner.\n\n4. Unmount the Lustre server to be upgraded (MGS, MDS, or OSS)\n\n5. Install the Lustre server packages on the Lustre server.\n\n   1. Log onto the Lustre server as the `root` user\n\n   2. Use the `yum` command to install the packages:\n\n      \n\n      ```\n      # yum --nogpgcheck install pkg1.rpm pkg2.rpm ... \n      ```\n\n      \n\n   3. Verify the packages are installed correctly:\n\n      \n\n      ```\n      rpm -qa|egrep \"lustre|wc\"\n      ```\n\n      \n\n   4. Mount the Lustre server to restart the Lustre software on the server:\n\n      ```\n      server# mount -a -t lustre\n      ```\n\n   5. Repeat these steps on each Lustre server.\n\n6. Download the Lustre client RPMs for your platform from the [Lustre Releases](https://wiki.whamcloud.com/display/PUB/Lustre+Releases) repository. See [Table 6, \u201cPackages Installed on Lustre Clients\u201d](02.05-Installing%20the%20Lustre%20Software.md#table-6-packages-installed-on-lustre-clients) for a list of required packages.\n\n7. Install the Lustre client packages on each of the Lustre clients to be upgraded.\n\n   1. Log onto a Lustre client as the `root` user.\n\n   2. Use the `yum` command to install the packages:\n\n      ```\n      # yum --nogpgcheck install pkg1.rpm pkg2.rpm ... \n      ```\n\n   3. Verify the packages were installed correctly:\n\n      ```\n      # rpm -qa|egrep \"lustre|kernel\"\n      ```\n\n   4. Mount the Lustre client to restart the Lustre software on the client:\n\n      ```\n      client# mount -a -t lustre\n      ```\n\n   5. Repeat these steps on each Lustre client.\n\nIf you have a problem upgrading a Lustre file system, see [the section called \u201cReporting a Lustre File System Bug\u201d](05.01-Lustre%20File%20System%20Troubleshooting.md#reporting-a-lustre-file-system-bug) for some suggestions for how to get help.", "mimetype": "text/plain", "start_char_idx": 9669, "end_char_idx": 13019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97a87b7f-003a-4b97-92eb-623de84a0ada": {"__data__": {"id_": "97a87b7f-003a-4b97-92eb-623de84a0ada", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78706d88-6079-4cfa-9df0-2cc951081685", "node_type": "1", "metadata": {}, "hash": "cca000737e5691050f612b3e9272244a234c3a0f06910a2e55cdcbafc2fedf5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Backing Up and Restoring a File System\n\n- [Backing Up and Restoring a File System](#backing-up-and-restoring-a-file-system)\n  * [Backing up a File System](#backing-up-a-file-system)\n    + [Lustre_rsync](#lustre_rsync)\n      - [Using Lustre_rsync](#using-lustre_rsync)\n      - [`lustre_rsync` Examples](#lustre_rsync-examples)\n  * [Backing Up and Restoring an MDT or OST (ldiskfs Device Level)](#backing-up-and-restoring-an-mdt-or-ost-ldiskfs-device-level)\n  * [Backing Up an OST or MDT (Backend File System Level)](#backing-up-an-ost-or-mdt-backend-file-system-level)\n    + [Backing Up an OST or MDT (Backend File System Level)](#backing-up-an-ost-or-mdt-backend-file-system-level)L 2.11\n    + [Backing Up an OST or MDT](#backing-up-an-ost-or-mdt)\n  * [Restoring a File-Level Backup](#restoring-a-file-level-backup)\n  * [Using LVM Snapshots with the Lustre File System](#using-lvm-snapshots-with-the-lustre-file-system)\n    + [Creating an LVM-based Backup File System](#creating-an-lvm-based-backup-file-system)\n    + [Backing up New/Changed Files to the Backup File System](#backing-up-newchanged-files-to-the-backup-file-system)\n    + [Creating Snapshot Volumes](#creating-snapshot-volumes)\n    + [Restoring the File System From a Snapshot](#restoring-the-file-system-from-a-snapshot)\n    + [Deleting Old Snapshots](#deleting-old-snapshots)\n    + [Changing Snapshot Volume Size](#changing-snapshot-volume-size)\n  * [Migration Between ZFS and ldiskfs Target Filesystems](#migration-between-zfs-and-ldiskfs-target-filesystems)L 2.11\n    + [Migrate from a ZFS to an ldiskfs based filesystem](#migrate-from-a-zfs-to-an-ldiskfs-based-filesystem)\n    + [Migrate from an ldiskfs to a ZFS based filesystem](#migrate-from-an-ldiskfs-to-a-zfs-based-filesystem)\n\nThis chapter describes how to backup and restore at the file system-level, device-level and file-level in a Lustre file system. Each backup approach is described in the the following sections:\n\n* [the section called \u201c Backing up a File System\u201d](#backing-up-a-file-system)\n* [the section called \u201c Backing Up and Restoring an MDT or OST (ldiskfs Device Level)\u201d](#backing-up-and-restoring-an-mdt-or-ost-ldiskfs-device-level)\n* [the section called \u201c Backing Up an OST or MDT (Backend File System Level)\u201d](#backing-up-an-ost-or-mdt-backend-file-system-level)\n* [the section called \u201c Restoring a File-Level Backup\u201d](#restoring-a-file-level-backup)\n* [the section called \u201c Using LVM Snapshots with the Lustre File System\u201d](#using-lvm-snapshots-with-the-lustre-file-system)\n\nIt is *strongly* recommended that sites perform periodic device-level backup of the MDT(s) ([the section called \u201c Backing Up and Restoring an MDT or OST (ldiskfs Device Level)\u201d](#backing-up-and-restoring-an-mdt-or-ost-ldiskfs-device-level)), for example twice a week with alternate backups going to a separate device, even if there is not enough capacity to do a full backup of all of the filesystem data. Even if there are separate file-level backups of some or all files in the filesystem, having a device-level backup of the MDT can be very useful in case of MDT failure or corruption. Being able to restore a device-level MDT backup can avoid the significantly longer process of restoring the entire filesystem from backup. Since the MDT is required for access to all files, its loss would otherwise force full restore of the filesystem (if that is even possible) even if the OSTs are still OK.\n\nPerforming a periodic device-level MDT backup can be done relatively inexpensively because the storage need only be connected to the primary MDS (it can be manually connected to the backup MDS in the rare case it is needed), and only needs good linear read/write performance. While the device-level MDT backup is not useful for restoring individual files, it is most efficient to handle the case of MDT failure or corruption.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3848, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78706d88-6079-4cfa-9df0-2cc951081685": {"__data__": {"id_": "78706d88-6079-4cfa-9df0-2cc951081685", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97a87b7f-003a-4b97-92eb-623de84a0ada", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3fee73c9c197aaa9f44e13269353599aca0916da5c67d7ded806a42ec0fbc7c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80e00c7f-06a2-48a8-9108-7845ba032f76", "node_type": "1", "metadata": {}, "hash": "390b9a6399692a2e2f7b0c48a5bd0e0b2391a3165255bfc99bd272c73504f22e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even if there are separate file-level backups of some or all files in the filesystem, having a device-level backup of the MDT can be very useful in case of MDT failure or corruption. Being able to restore a device-level MDT backup can avoid the significantly longer process of restoring the entire filesystem from backup. Since the MDT is required for access to all files, its loss would otherwise force full restore of the filesystem (if that is even possible) even if the OSTs are still OK.\n\nPerforming a periodic device-level MDT backup can be done relatively inexpensively because the storage need only be connected to the primary MDS (it can be manually connected to the backup MDS in the rare case it is needed), and only needs good linear read/write performance. While the device-level MDT backup is not useful for restoring individual files, it is most efficient to handle the case of MDT failure or corruption.\n\n## Backing up a File System\n\nBacking up a complete file system gives you full control over the files to back up, and allows restoration of individual files as needed. File system-level backups are also the easiest to integrate into existing backup solutions.\n\nFile system backups are performed from a Lustre client (or many clients working parallel in different directories) rather than on individual server nodes; this is no different than backing up any other file system.\n\nHowever, due to the large size of most Lustre file systems, it is not always possible to get a complete backup. We recommend that you back up subsets of a file system. This includes subdirectories of the entire file system, filesets for a single user, files incremented by date, and so on, so that restores can be done more efficiently.\n\n**Note**\n\nLustre internally uses a 128-bit file identifier (FID) for all files. To interface with user applications, the 64-bit inode numbers are returned by the `stat()`, `fstat()`, and `readdir()` system calls on 64-bit applications, and 32-bit inode numbers to 32-bit applications.\n\nSome 32-bit applications accessing Lustre file systems (on both 32-bit and 64-bit CPUs) may experience problems with the `stat()`, `fstat()` or `readdir()` system calls under certain circumstances, though the Lustre client should return 32-bit inode numbers to these applications.\n\nIn particular, if the Lustre file system is exported from a 64-bit client via NFS to a 32-bit client, the Linux NFS server will export 64-bit inode numbers to applications running on the NFS client. If the 32-bit applications are not compiled with Large File Support (LFS), then they return `EOVERFLOW` errors when accessing the Lustre files. To avoid this problem, Linux NFS clients can use the kernel command-line option \"`nfs.enable_ino64=0`\" in order to force the NFS client to export 32-bit inode numbers to the client.\n\n**Workaround**: We very strongly recommend that backups using `tar(1)` and other utilities that depend on the inode number to uniquely identify an inode to be run on 64-bit clients. The 128-bit Lustre file identifiers cannot be uniquely mapped to a 32-bit inode number, and as a result these utilities may operate incorrectly on 32-bit clients. While there is still a small chance of inode number collisions with 64-bit inodes, the FID allocation pattern is designed to avoid collisions for long periods of usage.\n\n### Lustre_rsync\n\nThe `lustre_rsync` feature keeps the entire file system in sync on a backup by replicating the file system's changes to a second file system (the second file system need not be a Lustre file system, but it must be sufficiently large). `lustre_rsync` uses Lustre changelogs to efficiently synchronize the file systems without having to scan (directory walk) the Lustre file system. This efficiency is critically important for large file systems, and distinguishes the Lustre `lustre_rsync` feature from other replication/backup solutions.\n\n#### Using Lustre_rsync\n\nThe `lustre_rsync` feature works by periodically running `lustre_rsync`, a userspace program used to synchronize changes in the Lustre file system onto the target file system. The `lustre_rsync` utility keeps a status file, which enables it to be safely interrupted and restarted without losing synchronization between the file systems.\n\nThe first time that `lustre_rsync` is run, the user must specify a set of parameters for the program to use. These parameters are described in the following table and in [the section called \u201c lustre_rsync\u201d](06.07-System%20Configuration%20Utilities.md#lustre-rsync).", "mimetype": "text/plain", "start_char_idx": 2929, "end_char_idx": 7458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80e00c7f-06a2-48a8-9108-7845ba032f76": {"__data__": {"id_": "80e00c7f-06a2-48a8-9108-7845ba032f76", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78706d88-6079-4cfa-9df0-2cc951081685", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e0f4851877fedec530159ac3382494edb08777289574ed402c8cd021bcfc8bac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42eceeee-6cc2-4570-8be8-f6e1ff1b2531", "node_type": "1", "metadata": {}, "hash": "2a59e514d49d9974ac5c20be4e342aa74018708cf0945f0cb8caecf95c442b60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This efficiency is critically important for large file systems, and distinguishes the Lustre `lustre_rsync` feature from other replication/backup solutions.\n\n#### Using Lustre_rsync\n\nThe `lustre_rsync` feature works by periodically running `lustre_rsync`, a userspace program used to synchronize changes in the Lustre file system onto the target file system. The `lustre_rsync` utility keeps a status file, which enables it to be safely interrupted and restarted without losing synchronization between the file systems.\n\nThe first time that `lustre_rsync` is run, the user must specify a set of parameters for the program to use. These parameters are described in the following table and in [the section called \u201c lustre_rsync\u201d](06.07-System%20Configuration%20Utilities.md#lustre-rsync). On subsequent runs, these parameters are stored in the the status file, and only the name of the status file needs to be passed to `lustre_rsync`.\n\nBefore using `lustre_rsync`:\n\n- Register the changelog user. For details, see the [*System Configuration Utilities*](06.07-System%20Configuration%20Utilities.md)( `changelog_register`) parameter in the [*System Configuration Utilities*](06.07-System%20Configuration%20Utilities.md)( `lctl`).\n\n\\- AND -\n\n- Verify that the Lustre file system (source) and the replica file system (target) are identical *before* registering the changelog user. If the file systems are discrepant, use a utility, e.g. regular `rsync`(not `lustre_rsync`), to make them identical.\n\nThe `lustre_rsync` utility uses the following parameters:\n\n| **Parameter**       | **Description**                                              |\n| ------------------- | ------------------------------------------------------------ |\n| `--source=*src*`    | The path to the root of the Lustre file system (source) which will be synchronized. This is a mandatory option if a valid status log created during a previous synchronization operation ( `--statuslog`) is not specified. |\n| `--target=*tgt*`    | The path to the root where the source file system will be synchronized (target). This is a mandatory option if the status log created during a previous synchronization operation ( `--statuslog`) is not specified. This option can be repeated if multiple synchronization targets are desired. |\n| `--mdt= *mdt*`      | The metadata device to be synchronized. A changelog user must be registered for this device. This is a mandatory option if a valid status log created during a previous synchronization operation ( `--statuslog`) is not specified. |\n| `--user=*userid*`   | The changelog user ID for the specified MDT. To use `lustre_rsync`, the changelog user must be registered. For details, see the `changelog_register` parameter in [*System Configuration Utilities*](06.07-System%20Configuration%20Utilities.md)( `lctl`). This is a mandatory option if a valid status log created during a previous synchronization operation ( `--statuslog`) is not specified. |\n| `--statuslog=*log*` | A log file to which synchronization status is saved. When the `lustre_rsync` utility starts, if the status log from a previous synchronization operation is specified, then the state is read from the log and otherwise mandatory `--source`, `--target` and `--mdt` options can be skipped. Specifying the `--source`, `--target` and/or `--mdt` options, in addition to the `--statuslog` option, causes the specified parameters in the status log to be overridden. Command line options take precedence over options in the status log. |\n| `--xattr*yes|no*`   | Specifies whether extended attributes ( `xattrs`) are synchronized or not. The default is to synchronize extended attributes.NoteDisabling xattrs causes Lustre striping information not to be synchronized. |\n| `--verbose`         | Produces verbose output.                                     |\n| `--dry-run`         | Shows the output of `lustre_rsync` commands ( `copy`, `mkdir`, etc.) on the target file system without actually executing them. |\n| `--abort-on-err`    | Stops processing the `lustre_rsync` operation if an error occurs. The default is to continue the operation. |\n\n#### `lustre_rsync` Examples\n\nSample `lustre_rsync` commands are listed below.\n\nRegister a changelog user for an MDT (e.g. `testfs-MDT0000`).", "mimetype": "text/plain", "start_char_idx": 6672, "end_char_idx": 10930, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42eceeee-6cc2-4570-8be8-f6e1ff1b2531": {"__data__": {"id_": "42eceeee-6cc2-4570-8be8-f6e1ff1b2531", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80e00c7f-06a2-48a8-9108-7845ba032f76", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "65ac885e3adf6edb0e2528ce48e12eb41ad53ffa77a3b55e9c29293971c0a231", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7f781d4-ccb1-4fdf-a7e8-14684c118690", "node_type": "1", "metadata": {}, "hash": "89ba497ba4973960920fe3c0ccc0825c42b1610ae6c6bcad02512ae4a1fe5adf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `--xattr*yes|no*`   | Specifies whether extended attributes ( `xattrs`) are synchronized or not. The default is to synchronize extended attributes.NoteDisabling xattrs causes Lustre striping information not to be synchronized. |\n| `--verbose`         | Produces verbose output.                                     |\n| `--dry-run`         | Shows the output of `lustre_rsync` commands ( `copy`, `mkdir`, etc.) on the target file system without actually executing them. |\n| `--abort-on-err`    | Stops processing the `lustre_rsync` operation if an error occurs. The default is to continue the operation. |\n\n#### `lustre_rsync` Examples\n\nSample `lustre_rsync` commands are listed below.\n\nRegister a changelog user for an MDT (e.g. `testfs-MDT0000`).\n\n```\n# lctl --device testfs-MDT0000 changelog_register testfs-MDT0000\nRegistered changelog userid 'cl1'\n```\n\nSynchronize a Lustre file system ( `/mnt/lustre`) to a target file system ( `/mnt/target`).\n\n```\n$ lustre_rsync --source=/mnt/lustre --target=/mnt/target \\\n           --mdt=testfs-MDT0000 --user=cl1 --statuslog sync.log  --verbose \nLustre filesystem: testfs \nMDT device: testfs-MDT0000 \nSource: /mnt/lustre \nTarget: /mnt/target \nStatuslog: sync.log \nChangelog registration: cl1 \nStarting changelog record: 0 \nErrors: 0 \nlustre_rsync took 1 seconds \nChangelog records consumed: 22\n```\n\nAfter the file system undergoes changes, synchronize the changes onto the target file system. Only the `statuslog` name needs to be specified, as it has all the parameters passed earlier.\n\n```\n$ lustre_rsync --statuslog sync.log --verbose \nReplicating Lustre filesystem: testfs \nMDT device: testfs-MDT0000 \nSource: /mnt/lustre \nTarget: /mnt/target \nStatuslog: sync.log \nChangelog registration: cl1 \nStarting changelog record: 22 \nErrors: 0 \nlustre_rsync took 2 seconds \nChangelog records consumed: 42\n```\n\nTo synchronize a Lustre file system ( `/mnt/lustre`) to two target file systems ( `/mnt/target1` and `/mnt/target2`).\n\n```\n$ lustre_rsync --source=/mnt/lustre --target=/mnt/target1 \\\n           --target=/mnt/target2 --mdt=testfs-MDT0000 --user=cl1  \\\n           --statuslog sync.log\n```\n\n## Backing Up and Restoring an MDT or OST (ldiskfs Device Level)\n\nIn some cases, it is useful to do a full device-level backup of an individual device (MDT or OST), before replacing hardware, performing maintenance, etc. Doing full device-level backups ensures that all of the data and configuration files is preserved in the original state and is the easiest method of doing a backup. For the MDT file system, it may also be the fastest way to perform the backup and restore, since it can do large streaming read and write operations at the maximum bandwidth of the underlying devices.\n\n**Note**\n\nKeeping an updated full backup of the MDT is especially important because permanent failure or corruption of the MDT file system renders the much larger amount of data in all the OSTs largely inaccessible and unusable. The storage needed for one or two full MDT device backups is much smaller than doing a full filesystem backup, and can use less expensive storage than the actual MDT device(s) since it only needs to have good streaming read/write speed instead of high random IOPS.\n\nIf hardware replacement is the reason for the backup or if a spare storage device is available, it is possible to do a raw copy of the MDT or OST from one block device to the other, as long as the new device is at least as large as the original device.", "mimetype": "text/plain", "start_char_idx": 10180, "end_char_idx": 13654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7f781d4-ccb1-4fdf-a7e8-14684c118690": {"__data__": {"id_": "e7f781d4-ccb1-4fdf-a7e8-14684c118690", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42eceeee-6cc2-4570-8be8-f6e1ff1b2531", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f8a0ed63f1467379b6f7a6b8548be28c05a60d482ce26c3fc2ba72c30e628312", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e0cf0d9-6ebf-451c-af03-a5fb0566f715", "node_type": "1", "metadata": {}, "hash": "2f150331d695d3dad770a4e4fa01943f8279b96d8ad2487d45109f376f20035e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the MDT file system, it may also be the fastest way to perform the backup and restore, since it can do large streaming read and write operations at the maximum bandwidth of the underlying devices.\n\n**Note**\n\nKeeping an updated full backup of the MDT is especially important because permanent failure or corruption of the MDT file system renders the much larger amount of data in all the OSTs largely inaccessible and unusable. The storage needed for one or two full MDT device backups is much smaller than doing a full filesystem backup, and can use less expensive storage than the actual MDT device(s) since it only needs to have good streaming read/write speed instead of high random IOPS.\n\nIf hardware replacement is the reason for the backup or if a spare storage device is available, it is possible to do a raw copy of the MDT or OST from one block device to the other, as long as the new device is at least as large as the original device. To do this, run:\n\n```\ndd if=/dev/{original} of=/dev/{newdev} bs=4M\n```\n\nIf hardware errors cause read problems on the original device, use the command below to allow as much data as possible to be read from the original device while skipping sections of the disk with errors:\n\n```\ndd if=/dev/{original} of=/dev/{newdev} bs=4k conv=sync,noerror /\n      count={original size in 4kB blocks}\n```\n\nEven in the face of hardware errors, the `ldiskfs` file system is very robust and it may be possible to recover the file system data after running `e2fsck -fy /dev/{newdev}` on the new device.\n\nWith Lustre software version 2.6 and later,  the `LFSCK` scanning will automatically move objects from `lost+found` back into its correct location on the OST after directory corruption.\n\nIn order to ensure that the backup is fully consistent, the MDT or OST must be unmounted, so that there are no changes being made to the device while the data is being transferred. If the reason for the backup is preventative (i.e. MDT backup on a running MDS in case of future failures) then it is possible to perform a consistent backup from an LVM snapshot. If an LVM snapshot is not available, and taking the MDS offline for a backup is unacceptable, it is also possible to perform a backup from the raw MDT block device. While the backup from the raw device will not be fully consistent due to ongoing changes, the vast majority of ldiskfs metadata is statically allocated, and inconsistencies in the backup can be fixed by running `e2fsck` on the backup device, and is still much better than not having any backup at all.\n\n## Backing Up an OST or MDT (Backend File System Level)\n\nThis procedure provides an alternative to backup or migrate the data of an OST or MDT at the file level. At the file-level, unused space is omitted from the backup and the process may be completed quicker with a smaller total backup size. Backing up a single OST device is not necessarily the best way to perform backups of the Lustre file system, since the files stored in the backup are not usable without metadata stored on the MDT and additional file stripes that may be on other OSTs. However, it is the preferred method for migration of OST devices, especially when it is desirable to reformat the underlying file system with different configuration options or to reduce fragmentation.\n\n**Note**\n\nSince Lustre stores internal metadata that maps FIDs to local inode numbers via the Object Index file, they need to be rebuilt at first mount after a restore is detected so that file-level MDT backup and restore is supported. The OI Scrub rebuilds these automatically at first mount after a restore is detected, which may affect MDT performance after mount until the rebuild is completed. Progress can be monitored via `lctl get_param osd-*.*.oi_scrub` on the MDS or OSS node where the target filesystem was restored.\n\n\n\nIntroduced in Lustre 2.11\n\n### Backing Up an OST or MDT (Backend File System Level)\n\nPrior to Lustre software release 2.11.0, we can only do the backend file system level backup and restore process for ldiskfs-based systems. The ability to perform a zfs-based MDT/OST file system level backup and restore is introduced beginning in Lustre software release 2.11.0. Differing from an ldiskfs-based system, index objects must be backed up before the unmount of the target (MDT or OST) in order to be able to restore the file system successfully.", "mimetype": "text/plain", "start_char_idx": 12705, "end_char_idx": 17081, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e0cf0d9-6ebf-451c-af03-a5fb0566f715": {"__data__": {"id_": "6e0cf0d9-6ebf-451c-af03-a5fb0566f715", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7f781d4-ccb1-4fdf-a7e8-14684c118690", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "99d6b375db3c2ca1333a64b770d3ce84ba702533f72c87f12c093c2ff598c7d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4928e89b-90e4-4ebd-a5f5-8fb076e35b35", "node_type": "1", "metadata": {}, "hash": "971f7ce21f9d248e091f7471b121131548cc27892f8af4218ac24846b137e3ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Progress can be monitored via `lctl get_param osd-*.*.oi_scrub` on the MDS or OSS node where the target filesystem was restored.\n\n\n\nIntroduced in Lustre 2.11\n\n### Backing Up an OST or MDT (Backend File System Level)\n\nPrior to Lustre software release 2.11.0, we can only do the backend file system level backup and restore process for ldiskfs-based systems. The ability to perform a zfs-based MDT/OST file system level backup and restore is introduced beginning in Lustre software release 2.11.0. Differing from an ldiskfs-based system, index objects must be backed up before the unmount of the target (MDT or OST) in order to be able to restore the file system successfully. To enable index backup on the target, execute the following command on the target server:\n\n```\n# lctl set_param osd-*.${fsname}-${target}.index_backup=1\n```\n\n*${target}* is composed of the target type (MDT or OST) plus the target index, such as `MDT0000`, `OST0001`, and so on.\n\n**Note**\n\nThe index_backup is also valid for an ldiskfs-based system, that will be used when migrating data between ldiskfs-based and zfs-based systems as described in [the section called \u201c Migration Between ZFS and ldiskfs Target Filesystems \u201d](#migration-between-zfs-and-ldiskfs-target-filesystems).\n\n### Backing Up an OST or MDT\n\nThe below examples show backing up an OST filesystem. When backing up an MDT, substitute `mdt` for `ost` in the instructions below.\n\n1. **Umount the target**\n\n2. **Make a mountpoint for the file system.**\n\n   ```\n   [oss]# mkdir -p /mnt/ost\n   ```\n\n3. **Mount the file system.**\n\n   For ldiskfs-based systems:\n\n   ```\n   [oss]# mount -t ldiskfs /dev/{ostdev} /mnt/ost\n   ```\n\n   For zfs-based systems:\n\n   1. Import the pool for the target if it is exported. For example:\n\n      ```\n      [oss]# zpool import lustre-ost [-d ${ostdev_dir}]\n      ```\n\n      \n\n   2. Enable the `canmount` property on the target filesystem. For example:\n\n      ```\n      [oss]# zfs set canmount=on ${fsname}-ost/ost\n      ```\n\n      You also can specify the mountpoint property. By default, it will be: `/${fsname}-ost/ost`\n\n   3. Mount the target as 'zfs'. For example:\n\n      ```\n      [oss]# zfs mount ${fsname}-ost/ost\n      ```\n\n      \n\n4. **Change to the mountpoint being backed up.**\n\n   ```\n   [oss]# cd /mnt/ost\n   ```\n\n5. **Back up the extended attributes.**\n\n   ```\n   [oss]# getfattr -R -d -m '.*' -e hex -P . > ea-$(date +%Y%m%d).bak\n   ```\n\n   ### Note\n\n   If the `tar(1)` command supports the `--xattr` option (see below), the `getfattr` step may be unnecessary as long as tar correctly backs up the `trusted.*` attributes. However, completing this step is not harmful and can serve as an added safety measure.\n\n   ### Note\n\n   In most distributions, the `getfattr` command is part of the `attr` package. If the `getfattr` command returns errors like `Operation not supported`, then the kernel does not correctly support EAs. Stop and use a different backup method.\n\n6. **Verify that the ea-$date.bak file has properly backed up the EA data on the OST.**\n\n   Without this attribute data, the MDT restore process will fail and result in an unusable filesystem. The OST restore process may be missing extra data that can be very useful in case of later file system corruption. Look at this file with `more` or a text editor. Each object file should have a corresponding item similar to this:\n\n   ```\n   [oss]# file: O/0/d0/100992\n   trusted.fid= \\\n   0x0d822200000000004a8a73e500000000808a0100000000000000000000000000\n   ```\n\n7. **Back up all file system data.", "mimetype": "text/plain", "start_char_idx": 16407, "end_char_idx": 19947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4928e89b-90e4-4ebd-a5f5-8fb076e35b35": {"__data__": {"id_": "4928e89b-90e4-4ebd-a5f5-8fb076e35b35", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e0cf0d9-6ebf-451c-af03-a5fb0566f715", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1b487b99e63b86b7792cfaafc76307f1433efb1c7eac8b787bb043154f8a3bee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14af0eac-4713-406f-9899-10e23567b00d", "node_type": "1", "metadata": {}, "hash": "70fe1b97111c3038d8cb536eaf3afba65e27a6eab8782b05fc67a3b363f35d6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Stop and use a different backup method.\n\n6. **Verify that the ea-$date.bak file has properly backed up the EA data on the OST.**\n\n   Without this attribute data, the MDT restore process will fail and result in an unusable filesystem. The OST restore process may be missing extra data that can be very useful in case of later file system corruption. Look at this file with `more` or a text editor. Each object file should have a corresponding item similar to this:\n\n   ```\n   [oss]# file: O/0/d0/100992\n   trusted.fid= \\\n   0x0d822200000000004a8a73e500000000808a0100000000000000000000000000\n   ```\n\n7. **Back up all file system data.**\n\n   ```\n   [oss]# tar czvf {backup file}.tgz [--xattrs] [--xattrs-include=\"trusted.*\" --sparse .\n   ```\n\n   ### Note\n\n   The tar `--sparse` option is vital for backing up an MDT. Very old versions of tar may not support the `--sparse` option correctly, which may cause the MDT backup to take a long time. Known-working versions include the tar from Red Hat Enterprise Linux distribution (RHEL version 6.3 or newer) or GNU tar version 1.25 and newer.\n\n   ### Warning\n\n   The tar `--xattrs` option is only available in GNU tar version 1.27 or later or in RHEL 6.3 or newer. The `--xattrs-include=\"trusted.*\"` option is *required* for correct restoration of the xattrs when using GNU tar 1.27 or RHEL 7 and newer.\n\n8. **Change directory out of the file system.**\n\n   ```\n   [oss]# cd -\n   ```\n\n9. **Unmount the file system.**\n\n   ```\n   [oss]# umount /mnt/ost\n   ```\n\n   ### Note\n\n   When restoring an OST backup on a different node as part of an OST migration, you also have to change server NIDs and use the `--writeconf` command to re-generate the configuration logs. See [*Lustre Maintenance*](03.03-Lustre%20Maintenance.md)(Changing a Server NID).\n\n## Restoring a File-Level Backup\n\nTo restore data from a file-level backup, you need to format the device, restore the file data and then restore the EA data.\n\n1. Format the new device.\n\n   ```\n   [oss]# mkfs.lustre --ost --index {OST index}\n   --replace --fstype=${fstype} {other options} /dev/{newdev}\n   ```\n\n2. Set the file system label (**ldiskfs-based systems only**).\n\n   ```\n   [oss]# e2label {fsname}-OST{index in hex} /mnt/ost\n   ```\n\n3. Mount the file system.\n\n   For ldiskfs-based systems:\n\n   ```\n   [oss]# mount -t ldiskfs /dev/{newdev} /mnt/ost\n   ```\n\n   For zfs-based systems:\n\n   1. Import the pool for the target if it is exported. For example:\n\n      ```\n      [oss]# zpool import lustre-ost [-d ${ostdev_dir}]\n      ```\n\n   2. Enable the canmount property on the target filesystem. For example:\n\n      ```\n      [oss]# zfs set canmount=on ${fsname}-ost/ost\n      ```\n\n      You also can specify the `mountpoint` property. By default, it will be: `/${fsname}-ost/ost`\n\n   3. Mount the target as 'zfs'. For example:\n\n      ```\n      [oss]# zfs mount ${fsname}-ost/ost\n      ```\n\n4. Change to the new file system mount point.\n\n   ```\n   [oss]# cd /mnt/ost\n   ```\n\n5. Restore the file system backup.\n\n   ```\n   [oss]# tar xzvpf {backup file} [--xattrs] [--xattrs-include=\"trusted.*\"] --sparse\n   ```\n\n   ### Warning\n\n   The tar `--xattrs` option is only available in GNU tar version 1.27 or later or in RHEL 6.3 or newer. The `--xattrs-include=\"trusted.*\"` option is *required* for correct restoration of the MDT xattrs when using GNU tar 1.27 or RHEL 7 and newer.", "mimetype": "text/plain", "start_char_idx": 19315, "end_char_idx": 22682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14af0eac-4713-406f-9899-10e23567b00d": {"__data__": {"id_": "14af0eac-4713-406f-9899-10e23567b00d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4928e89b-90e4-4ebd-a5f5-8fb076e35b35", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "92e0672e5924e1d846b4514abf5de83c0b3acb5f916593dd6ca9993598557934", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09e7ffba-5ab5-45a3-a9c7-6f06464de225", "node_type": "1", "metadata": {}, "hash": "77946f6566b5e45d1910db34090443ed206d3b4a5e4d19cc75ba9ee2c9228f3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mount the target as 'zfs'. For example:\n\n      ```\n      [oss]# zfs mount ${fsname}-ost/ost\n      ```\n\n4. Change to the new file system mount point.\n\n   ```\n   [oss]# cd /mnt/ost\n   ```\n\n5. Restore the file system backup.\n\n   ```\n   [oss]# tar xzvpf {backup file} [--xattrs] [--xattrs-include=\"trusted.*\"] --sparse\n   ```\n\n   ### Warning\n\n   The tar `--xattrs` option is only available in GNU tar version 1.27 or later or in RHEL 6.3 or newer. The `--xattrs-include=\"trusted.*\"` option is *required* for correct restoration of the MDT xattrs when using GNU tar 1.27 or RHEL 7 and newer. Otherwise, the `setfattr` step below should be used.\n\n6. If not using a version of tar that supports direct xattr backups, restore the file system extended attributes.\n\n   ```\n   [oss]# setfattr --restore=ea-${date}.bak\n   ```\n\n   ### Note\n\n   If `--xattrs` option is supported by tar and specified in the step above, this step is redundant.\n\n7. Verify that the extended attributes were restored.\n\n   ```\n   [oss]# getfattr -d -m \".*\" -e hex O/0/d0/100992 trusted.fid= \\\n   0x0d822200000000004a8a73e500000000808a0100000000000000000000000000\n   ```\n\n8. Remove old OI and LFSCK files.\n\n   ```\n   [oss]# rm -rf oi.16* lfsck_* LFSCK\n   ```\n\n9. Remove old CATALOGS.\n\n   ```\n   [oss]# rm -f CATALOGS\n   ```\n\n   ### Note\n\n   This is optional for the MDT side only. The CATALOGS record the llog file handlers that are used for recovering cross-server updates. Before OI scrub rebuilds the OI mappings for the llog files, the related recovery will get a failure if it runs faster than the background OI scrub. This will result in a failure of the whole mount process. OI scrub is an online tool, therefore, a mount failure means that the OI scrub will be stopped. Removing the old CATALOGS will avoid this potential trouble. The side-effect of removing old CATALOGS is that the recovery for related cross-server updates will be aborted. However, this can be handled by LFSCK after the system mount is up.\n\n10. Change directory out of the file system.\n\n    ```\n    [oss]# cd -\n    ```\n\n11. Unmount the new file system.\n\n    ```\n    [oss]# umount /mnt/ost\n    ```\n\n    ### Note\n\n    If the restored system has a different NID from the backup system, please change the NID. For detail, please refer to [the section called \u201c Changing a Server NID\u201d](03.03-Lustre%20Maintenance.md#changing-a-server-nid). For example:\n\n    ```\n    [oss]# mount -t lustre -o nosvc ${fsname}-ost/ost /mnt/ost\n    [oss]# lctl replace_nids ${fsname}-OSTxxxx $new_nids\n    [oss]# umount /mnt/ost\n    ```\n\n12. Mount the target as `lustre`.\n\n    Usually, we will use the `-o abort_recov` option to skip unnecessary recovery. For example:\n\n    ```\n    [oss]# mount -t lustre -o abort_recov #{fsname}-ost/ost /mnt/ost\n    ```\n\n    Lustre can detect the restore automatically when mounting the target, and then trigger OI scrub to rebuild the OIs and index objects asynchronously in the background. You can check the OI scrub status with the following command:\n\n    ```\n    [oss]# lctl get_param -n osd-${fstype}.${fsname}-${target}.oi_scrub\n    ```\n\nIf the file system was used between the time the backup was made and when it was restored, then the online `LFSCK` tool will automatically be run to ensure the filesystem is coherent. If all of the device filesystems were backed up at the same time after Lustre was was stopped, this step is unnecessary.", "mimetype": "text/plain", "start_char_idx": 22096, "end_char_idx": 25496, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09e7ffba-5ab5-45a3-a9c7-6f06464de225": {"__data__": {"id_": "09e7ffba-5ab5-45a3-a9c7-6f06464de225", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14af0eac-4713-406f-9899-10e23567b00d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bb697b1773ef7bdc7f8f5e2816220e3680a67e2d0d0c1a113e9d897f3db88d2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d746be72-d5ee-4fad-8fb6-185be883418b", "node_type": "1", "metadata": {}, "hash": "3efc61da9f5b8a2f742ec2900066f94f344a64776cd4a0240ac357edc31d70b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Usually, we will use the `-o abort_recov` option to skip unnecessary recovery. For example:\n\n    ```\n    [oss]# mount -t lustre -o abort_recov #{fsname}-ost/ost /mnt/ost\n    ```\n\n    Lustre can detect the restore automatically when mounting the target, and then trigger OI scrub to rebuild the OIs and index objects asynchronously in the background. You can check the OI scrub status with the following command:\n\n    ```\n    [oss]# lctl get_param -n osd-${fstype}.${fsname}-${target}.oi_scrub\n    ```\n\nIf the file system was used between the time the backup was made and when it was restored, then the online `LFSCK` tool will automatically be run to ensure the filesystem is coherent. If all of the device filesystems were backed up at the same time after Lustre was was stopped, this step is unnecessary. In either case, the filesystem will be immediately although there may be I/O errors reading from files that are present on the MDT but not the OSTs, and files that were created after the MDT backup will not be accessible or visible. See [the section called \u201c Checking the file system with LFSCK\u201d](05.02-Troubleshooting%20Recovery.md#checking-the-file-system-with-lfsck)for details on using LFSCK.\n\n## Using LVM Snapshots with the Lustre File System\n\nIf you want to perform disk-based backups (because, for example, access to the backup system needs to be as fast as to the primary Lustre file system), you can use the Linux LVM snapshot tool to maintain multiple, incremental file system backups.\n\nBecause LVM snapshots cost CPU cycles as new files are written, taking snapshots of the main Lustre file system will probably result in unacceptable performance losses. You should create a new, backup Lustre file system and periodically (e.g., nightly) back up new/changed files to it. Periodic snapshots can be taken of this backup file system to create a series of \"full\" backups.\n\n**Note**\n\nCreating an LVM snapshot is not as reliable as making a separate backup, because the LVM snapshot shares the same disks as the primary MDT device, and depends on the primary MDT device for much of its data. If the primary MDT device becomes corrupted, this may result in the snapshot being corrupted.\n\n### Creating an LVM-based Backup File System\n\nUse this procedure to create a backup Lustre file system for use with the LVM snapshot mechanism.\n\n1. Create LVM volumes for the MDT and OSTs.\n\n   Create LVM devices for your MDT and OST targets. Make sure not to use the entire disk for the targets; save some room for the snapshots. The snapshots start out as 0 size, but grow as you make changes to the current file system. If you expect to change 20% of the file system between backups, the most recent snapshot will be 20% of the target size, the next older one will be 40%, etc. Here is an example:\n\n   ```\n   cfs21:~# pvcreate /dev/sda1\n      Physical volume \"/dev/sda1\" successfully created\n   cfs21:~# vgcreate vgmain /dev/sda1\n      Volume group \"vgmain\" successfully created\n   cfs21:~# lvcreate -L200G -nMDT0 vgmain\n      Logical volume \"MDT0\" created\n   cfs21:~# lvcreate -L200G -nOST0 vgmain\n      Logical volume \"OST0\" created\n   cfs21:~# lvscan\n      ACTIVE                  '/dev/vgmain/MDT0' [200.00 GB] inherit\n      ACTIVE                  '/dev/vgmain/OST0' [200.00 GB] inherit\n   ```\n\n2. Format the LVM volumes as Lustre targets.\n\n   In this example, the backup file system is called `main` and designates the current, most up-to-date backup.\n\n   ```\n   cfs21:~# mkfs.lustre --fsname=main --mdt --index=0 /dev/vgmain/MDT0\n    No management node specified, adding MGS to this MDT.", "mimetype": "text/plain", "start_char_idx": 24690, "end_char_idx": 28287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d746be72-d5ee-4fad-8fb6-185be883418b": {"__data__": {"id_": "d746be72-d5ee-4fad-8fb6-185be883418b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09e7ffba-5ab5-45a3-a9c7-6f06464de225", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "269c88e401be6c0dc5bd0f4578af04c2bc4f20213773f332e8e6bf0ed6347c95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b89bed59-d1d4-43f1-84c2-3e484f521bf8", "node_type": "1", "metadata": {}, "hash": "4225d65e747a9f4ca1348270177b5266e6aa1709af91d6b7e1988775e1b08f3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Format the LVM volumes as Lustre targets.\n\n   In this example, the backup file system is called `main` and designates the current, most up-to-date backup.\n\n   ```\n   cfs21:~# mkfs.lustre --fsname=main --mdt --index=0 /dev/vgmain/MDT0\n    No management node specified, adding MGS to this MDT.\n       Permanent disk data:\n    Target:     main-MDT0000\n    Index:      0\n    Lustre FS:  main\n    Mount type: ldiskfs\n    Flags:      0x75\n                  (MDT MGS first_time update )\n    Persistent mount opts: errors=remount-ro,iopen_nopriv,user_xattr\n    Parameters:\n   checking for existing Lustre data\n    device size = 200GB\n    formatting backing filesystem ldiskfs on /dev/vgmain/MDT0\n            target name  main-MDT0000\n            4k blocks     0\n            options        -i 4096 -I 512 -q -O dir_index -F\n    mkfs_cmd = mkfs.ext2 -j -b 4096 -L main-MDT0000  -i 4096 -I 512 -q\n     -O dir_index -F /dev/vgmain/MDT0\n    Writing CONFIGS/mountdata\n   cfs21:~# mkfs.lustre --mgsnode=cfs21 --fsname=main --ost --index=0\n   /dev/vgmain/OST0\n       Permanent disk data:\n    Target:     main-OST0000\n    Index:      0\n    Lustre FS:  main\n    Mount type: ldiskfs\n    Flags:      0x72\n                  (OST first_time update )\n    Persistent mount opts: errors=remount-ro,extents,mballoc\n    Parameters: mgsnode=192.168.0.21@tcp\n   checking for existing Lustre data\n    device size = 200GB\n    formatting backing filesystem ldiskfs on /dev/vgmain/OST0\n            target name  main-OST0000\n            4k blocks     0\n            options        -I 256 -q -O dir_index -F\n    mkfs_cmd = mkfs.ext2 -j -b 4096 -L lustre-OST0000 -J size=400 -I 256 \n     -i 262144 -O extents,uninit_bg,dir_nlink,huge_file,flex_bg -G 256 \n     -E resize=4290772992,lazy_journal_init, -F /dev/vgmain/OST0\n    Writing CONFIGS/mountdata\n   cfs21:~# mount -t lustre /dev/vgmain/MDT0 /mnt/mdt\n   cfs21:~# mount -t lustre /dev/vgmain/OST0 /mnt/ost\n   cfs21:~# mount -t lustre cfs21:/main /mnt/main\n   ```\n\n### Backing up New/Changed Files to the Backup File System\n\nAt periodic intervals e.g., nightly, back up new and changed files to the LVM-based backup file system.\n\n```\ncfs21:~# cp /etc/passwd /mnt/main \n \ncfs21:~# cp /etc/fstab /mnt/main \n \ncfs21:~# ls /mnt/main \nfstab  passwd\n```\n\n### Creating Snapshot Volumes\n\nWhenever you want to make a \"checkpoint\" of the main Lustre file system, create LVM snapshots of all target MDT and OSTs in the LVM-based backup file system. You must decide the maximum size of a snapshot ahead of time, although you can dynamically change this later. The size of a daily snapshot is dependent on the amount of data changed daily in the main Lustre file system. It is likely that a two-day old snapshot will be twice as big as a one-day old snapshot.\n\nYou can create as many snapshots as you have room for in the volume group. If necessary, you can dynamically add disks to the volume group.\n\nThe snapshots of the target MDT and OSTs should be taken at the same point in time. Make sure that the cronjob updating the backup file system is not running, since that is the only thing writing to the disks.", "mimetype": "text/plain", "start_char_idx": 27996, "end_char_idx": 31108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b89bed59-d1d4-43f1-84c2-3e484f521bf8": {"__data__": {"id_": "b89bed59-d1d4-43f1-84c2-3e484f521bf8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d746be72-d5ee-4fad-8fb6-185be883418b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bfd441865ed90017aae02dd66a2cd8eede442b87cd99d51f83237154cdd866a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f70c5049-1c69-4717-b0a8-f9e55b96637b", "node_type": "1", "metadata": {}, "hash": "f808d08026582bdbf83bdd8896d36465c90896961a7d05519653f5dc8ac2b616", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You must decide the maximum size of a snapshot ahead of time, although you can dynamically change this later. The size of a daily snapshot is dependent on the amount of data changed daily in the main Lustre file system. It is likely that a two-day old snapshot will be twice as big as a one-day old snapshot.\n\nYou can create as many snapshots as you have room for in the volume group. If necessary, you can dynamically add disks to the volume group.\n\nThe snapshots of the target MDT and OSTs should be taken at the same point in time. Make sure that the cronjob updating the backup file system is not running, since that is the only thing writing to the disks. Here is an example:\n\n```\ncfs21:~# modprobe dm-snapshot\ncfs21:~# lvcreate -L50M -s -n MDT0.b1 /dev/vgmain/MDT0\n   Rounding up size to full physical extent 52.00 MB\n   Logical volume \"MDT0.b1\" created\ncfs21:~# lvcreate -L50M -s -n OST0.b1 /dev/vgmain/OST0\n   Rounding up size to full physical extent 52.00 MB\n   Logical volume \"OST0.b1\" created\n```\n\nAfter the snapshots are taken, you can continue to back up new/changed files to \"main\". The snapshots will not contain the new files.\n\n```\ncfs21:~# cp /etc/termcap /mnt/main\ncfs21:~# ls /mnt/main\nfstab  passwd  termcap\n```\n\n### Restoring the File System From a Snapshot\n\nUse this procedure to restore the file system from an LVM snapshot.\n\n1. Rename the LVM snapshot.\n\n   Rename the file system snapshot from \"main\" to \"back\" so you can mount it without unmounting \"main\". This is recommended, but not required. Use the `--reformat` flag to `tunefs.lustre` to force the name change.", "mimetype": "text/plain", "start_char_idx": 30448, "end_char_idx": 32039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f70c5049-1c69-4717-b0a8-f9e55b96637b": {"__data__": {"id_": "f70c5049-1c69-4717-b0a8-f9e55b96637b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b89bed59-d1d4-43f1-84c2-3e484f521bf8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b2e77f2946f338e165abce30568197849986abb5eccfce8cd16b936e8c05f155", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6077241-ef53-4122-a268-1c30a2f954d5", "node_type": "1", "metadata": {}, "hash": "e7cc0bec199c9db8cf6c54e7523bbea7fbd6fc4edac7aa417294f8a6386b22cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The snapshots will not contain the new files.\n\n```\ncfs21:~# cp /etc/termcap /mnt/main\ncfs21:~# ls /mnt/main\nfstab  passwd  termcap\n```\n\n### Restoring the File System From a Snapshot\n\nUse this procedure to restore the file system from an LVM snapshot.\n\n1. Rename the LVM snapshot.\n\n   Rename the file system snapshot from \"main\" to \"back\" so you can mount it without unmounting \"main\". This is recommended, but not required. Use the `--reformat` flag to `tunefs.lustre` to force the name change. For example:\n\n   ```\n   cfs21:~# tunefs.lustre --reformat --fsname=back --writeconf /dev/vgmain/MDT0.b1\n    checking for existing Lustre data\n    found Lustre data\n    Reading CONFIGS/mountdata\n   Read previous values:\n    Target:     main-MDT0000\n    Index:      0\n    Lustre FS:  main\n    Mount type: ldiskfs\n    Flags:      0x5\n                 (MDT MGS )\n    Persistent mount opts: errors=remount-ro,iopen_nopriv,user_xattr\n    Parameters:\n   Permanent disk data:\n    Target:     back-MDT0000\n    Index:      0\n    Lustre FS:  back\n    Mount type: ldiskfs\n    Flags:      0x105\n                 (MDT MGS writeconf )\n    Persistent mount opts: errors=remount-ro,iopen_nopriv,user_xattr\n    Parameters:\n   Writing CONFIGS/mountdata\n   cfs21:~# tunefs.lustre --reformat --fsname=back --writeconf /dev/vgmain/OST0.b1\n    checking for existing Lustre data\n    found Lustre data\n    Reading CONFIGS/mountdata\n   Read previous values:\n    Target:     main-OST0000\n    Index:      0\n    Lustre FS:  main\n    Mount type: ldiskfs\n    Flags:      0x2\n                 (OST )\n    Persistent mount opts: errors=remount-ro,extents,mballoc\n    Parameters: mgsnode=192.168.0.21@tcp\n   Permanent disk data:\n    Target:     back-OST0000\n    Index:      0\n    Lustre FS:  back\n    Mount type: ldiskfs\n    Flags:      0x102\n                 (OST writeconf )\n    Persistent mount opts: errors=remount-ro,extents,mballoc\n    Parameters: mgsnode=192.168.0.21@tcp\n   Writing CONFIGS/mountdata\n   ```\n\n   When renaming a file system, we must also erase the last_rcvd file from the snapshots\n\n   ```\n   cfs21:~# mount -t ldiskfs /dev/vgmain/MDT0.b1 /mnt/mdtback\n   cfs21:~# rm /mnt/mdtback/last_rcvd\n   cfs21:~# umount /mnt/mdtback\n   cfs21:~# mount -t ldiskfs /dev/vgmain/OST0.b1 /mnt/ostback\n   cfs21:~# rm /mnt/ostback/last_rcvd\n   cfs21:~# umount /mnt/ostback\n   ```\n\n2. Mount the file system from the LVM snapshot. For example:\n\n   ```\n   cfs21:~# mount -t lustre /dev/vgmain/MDT0.b1 /mnt/mdtback\n   cfs21:~# mount -t lustre /dev/vgmain/OST0.b1 /mnt/ostback\n   cfs21:~# mount -t lustre cfs21:/back /mnt/back\n   ```\n\n3. Note the old directory contents, as of the snapshot time. For example:\n\n   ```\n   cfs21:~/cfs/b1_5/lustre/utils# ls /mnt/back\n   fstab  passwds\n   ```\n\n### Deleting Old Snapshots\n\nTo reclaim disk space, you can erase old snapshots as your backup policy dictates. Run:\n\n```\nlvremove /dev/vgmain/MDT0.b1\n```\n\n### Changing Snapshot Volume Size\n\nYou can also extend or shrink snapshot volumes if you find your daily deltas are smaller or larger than expected.", "mimetype": "text/plain", "start_char_idx": 31545, "end_char_idx": 34597, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6077241-ef53-4122-a268-1c30a2f954d5": {"__data__": {"id_": "d6077241-ef53-4122-a268-1c30a2f954d5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "761cdce3-575f-4bdb-94ac-ae187e5ef7e9", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9aad84123ae623ff062abe67f5953d86cdb76c48c09cc19d5d5127a50e9d0035", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f70c5049-1c69-4717-b0a8-f9e55b96637b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ffefd7036a0904bf43d5c637b85817abd59cffae9f506f04f84be0a7dd8bdfdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note the old directory contents, as of the snapshot time. For example:\n\n   ```\n   cfs21:~/cfs/b1_5/lustre/utils# ls /mnt/back\n   fstab  passwds\n   ```\n\n### Deleting Old Snapshots\n\nTo reclaim disk space, you can erase old snapshots as your backup policy dictates. Run:\n\n```\nlvremove /dev/vgmain/MDT0.b1\n```\n\n### Changing Snapshot Volume Size\n\nYou can also extend or shrink snapshot volumes if you find your daily deltas are smaller or larger than expected. Run:\n\n```\nlvextend -L10G /dev/vgmain/MDT0.b1\n```\n\n**Note**\n\nExtending snapshots seems to be broken in older LVM. It is working in LVM v2.02.01.\n\n\n\nIntroduced in Lustre 2.11\n\n## Migration Between ZFS and ldiskfs Target Filesystems\n\nBeginning with Lustre 2.11.0, it is possible to migrate between ZFS and ldiskfs backends. For migrating OSTs, it is best to use\u00a0`lfs find`/`lfs_migrate`\u00a0to empty out an OST while the filesystem is in use and then reformat it with the new fstype. For instructions on removing the OST, please see\u00a0[the section called \u201cRemoving an OST from the File System\u201d](03.03-Lustre%20Maintenance.md#removing-an-ost-from-the-file-system).\n\n### Migrate from a ZFS to an ldiskfs based filesystem\n\nThe first step of the process is to make a ZFS backend backup using `tar` as described in [the section called \u201c Backing Up an OST or MDT (Backend File System Level)\u201d](#backing-up-an-ost-or-mdt-backend-file-system-level).\n\nNext, restore the backup to an ldiskfs-based system as described in [the section called \u201c Restoring a File-Level Backup\u201d](#restoring-a-file-level-backup).\n\n### Migrate from an ldiskfs to a ZFS based filesystem\n\nThe first step of the process is to make an ldiskfs backend backup using `tar` as described in [the section called \u201c Backing Up an OST or MDT (Backend File System Level)\u201d](#backing-up-an-ost-or-mdt-backend-file-system-level).\n\n**Caution:**For a migration from ldiskfs to zfs, it is required to enable index_backup before the unmount of the target. This is an additional step for a regular ldiskfs-based backup/restore and easy to be missed.\n\nNext, restore the backup to an ldiskfs-based system as described in [the section called \u201c Restoring a File-Level Backup\u201d](#restoring-a-file-level-backup).", "mimetype": "text/plain", "start_char_idx": 34142, "end_char_idx": 36338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ba4f0b0-9b74-4cea-a015-9a6d8e8e1710": {"__data__": {"id_": "4ba4f0b0-9b74-4cea-a015-9a6d8e8e1710", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9059c12-5b98-4e65-9ded-730980f9276d", "node_type": "1", "metadata": {}, "hash": "c25920d8acb45c457dbc667a49b37c51e7e1b29e7ac55a546ad3859207493365", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Managing File Layout (Striping) and Free Space\n\n- [Managing File Layout (Striping) and Free Space](#managing-file-layout-striping-and-free-space)\n  * [How Lustre File System Striping Works](#how-lustre-file-system-striping-works)\n  * [Lustre File Layout (Striping) Considerations](#lustre-file-layout-striping-considerations)\n    + [Choosing a Stripe Size](#choosing-a-stripe-size)\n  * [Setting the File Layout/Striping Configuration (`lfs setstripe`)](#setting-the-file-layoutstriping-configuration-lfs-setstripe)\n    + [Specifying a File Layout (Striping Pattern) for a Single File](#specifying-a-file-layout-striping-pattern-for-a-single-file)\n      - [Setting the Stripe Size](#setting-the-stripe-size)\n      - [Setting the Stripe Count](#setting-the-stripe-count)\n    + [Setting the Striping Layout for a Directory](#setting-the-striping-layout-for-a-directory)\n    + [Setting the Striping Layout for a File System](#setting-the-striping-layout-for-a-file-system)\n    + [Creating a File on a Specific OST](#creating-a-file-on-a-specific-ost)\n  * [Retrieving File Layout/Striping Information (`getstripe`)](#retrieving-file-layoutstriping-information-getstripe)\n    + [Displaying the Current Stripe Size](#displaying-the-current-stripe-size)\n    + [Inspecting the File Tree](#inspecting-the-file-tree)\n    + [Locating the MDT for a remote directory](#locating-the-mdt-for-a-remote-directory)\n  * [Progressive File Layout(PFL)](#progressive-file-layoutpfl)L 2.10\n    + [`lfs setstripe`](#lfs-setstripe)\n      - [Create a PFL file](#create-a-pfl-file)\n      - [Add component(s) to an existing composite file](#add-components-to-an-existing-composite-file)\n      - [Delete component(s) from an existing file](#delete-components-from-an-existing-file)\n      - [Set default PFL layout to an existing directory](#set-default-pfl-layout-to-an-existing-directory)\n    + [`lfs migrate`](#lfs-migrate)\n    + [`lfs getstripe`](#lfs-getstripe)\n    + [`lfs find`](#lfs-find)\n  * [Managing Free Space](#managing-free-space)\n    + [Checking File System Free Space](#checking-file-system-free-space)\n    + [Stripe Allocation Methods](#stripe-allocation-methods)\n    + [Adjusting the Weighting Between Free Space and Location](#adjusting-the-weighting-between-free-space-and-location)\n  * [Lustre Striping Internals](#lustre-striping-internals)\n\nThis chapter describes file layout (striping) and I/O options, and includes the following sections:\n\n* [the section called \u201c How Lustre File System Striping Works\u201d](#how-lustre-file-system-striping-works)\n* [the section called \u201c Lustre File Layout (Striping) Considerations\u201d](#lustre-file-layout-striping-considerations)\n* [the section called \u201cSetting the File Layout/Striping Configuration (`lfs setstripe`)\u201d](#setting-the-file-layoutstriping-configuration-lfs-setstripe)\n* [the section called \u201cRetrieving File Layout/Striping Information (getstripe)\u201d](#retrieving-file-layoutstriping-information-getstripe)\n* [the section called \u201cManaging Free Space\u201d](#managing-free-space)\n* [the section called \u201cLustre Striping Internals\u201d](#lustre-striping-internals)\n\n## How Lustre File System Striping Works\n\nIn a Lustre file system, the MDS allocates objects to OSTs using either a round-robin algorithm or a weighted algorithm. When the amount of free space is well balanced (i.e., by default, when the free space across OSTs differs by less than 17%), the round-robin algorithm is used to select the next OST to which a stripe is to be written. Periodically, the MDS adjusts the striping layout to eliminate some degenerated cases in which applications that create very regular file layouts (striping patterns) preferentially use a particular OST in the sequence.\n\nNormally the usage of OSTs is well balanced. However, if users create a small number of exceptionally large files or incorrectly specify striping parameters, imbalanced OST usage may result.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9059c12-5b98-4e65-9ded-730980f9276d": {"__data__": {"id_": "e9059c12-5b98-4e65-9ded-730980f9276d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ba4f0b0-9b74-4cea-a015-9a6d8e8e1710", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1e842fb13969521d0e253fb9218ff2c1c56c833f230a13f98121c8a261dbeaac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06055661-3e92-48f2-83cf-ddf96872f49c", "node_type": "1", "metadata": {}, "hash": "0d3d81126c6e806661bbd51ee0760cc3cb45b15374c7704de070816363998ee8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When the amount of free space is well balanced (i.e., by default, when the free space across OSTs differs by less than 17%), the round-robin algorithm is used to select the next OST to which a stripe is to be written. Periodically, the MDS adjusts the striping layout to eliminate some degenerated cases in which applications that create very regular file layouts (striping patterns) preferentially use a particular OST in the sequence.\n\nNormally the usage of OSTs is well balanced. However, if users create a small number of exceptionally large files or incorrectly specify striping parameters, imbalanced OST usage may result. When the free space across OSTs differs by more than a specific amount (17% by default), the MDS then uses weighted random allocations with a preference for allocating objects on OSTs with more free space. (This can reduce I/O performance until space usage is rebalanced again.) For a more detailed description of how striping is allocated, see [*the section called \u201cManaging Free Space\u201d*](#managing-free-space).\n\nFiles can only be striped over a finite number of OSTs, based on the maximum size of the attributes that can be stored on the MDT. If the MDT is ldiskfs-based without the `ea_inode` feature, a file can be striped across at most 160 OSTs. With a ZFS-based MDT, or if the `ea_inode` feature is enabled for an ldiskfs-based MDT, a file can be striped across up to 2000 OSTs. For more information, see [*the section called \u201cLustre Striping Internals\u201d*](#lustre-striping-internals).\n\n## Lustre File Layout (Striping) Considerations\n\nWhether you should set up file striping and what parameter values you select depends on your needs. A good rule of thumb is to stripe over as few objects as will meet those needs and no more.\n\nSome reasons for using striping include:\n\n- **Providing high-bandwidth access.** Many applications require high-bandwidth access to a single file, which may be more bandwidth than can be provided by a single OSS. Examples are a scientific application that writes to a single file from hundreds of nodes, or a binary executable that is loaded by many nodes when an application starts.\n\n  In cases like these, a file can be striped over as many OSSs as it takes to achieve the required peak aggregate bandwidth for that file. Striping across a larger number of OSSs should only be used when the file size is very large and/or is accessed by many nodes at a time. Currently, Lustre files can be striped across up to 2000 OSTs.\n\n- **Improving performance when OSS bandwidth is exceeded.** Striping across many OSSs can improve performance if the aggregate client bandwidth exceeds the server bandwidth and the application reads and writes data fast enough to take advantage of the additional OSS bandwidth. The largest useful stripe count is bounded by the I/O rate of the clients/jobs divided by the performance per OSS.\n\n- **Matching stripes to I/O pattern.** When writing to a single file from multiple nodes, having more than one client writing to a stripe can lead to issues with lock exchange, where clients contend over writing to that stripe, even if their I/Os do not overlap. This can be avoided if I/O can be stripe aligned so that each stripe is accessed by only one client. Since Lustre 2.13, the 'overstriping' feature is available, allowing more than stripe per OST. This is particularly helpful for the case where thread count exceeds OST count, making it possible to match stripe count to thread count even in this case.\n\n- **Providing space for very large files.** Striping is useful when a single OST does not have enough free space to hold the entire file.\n\nSome reasons to minimize or avoid striping:\n\n- **Increased overhead.** Striping results in more locks and extra network operations during common operations such as `stat` and `unlink`. Even when these operations are performed in parallel, one network operation takes less time than 100 operations.\n\n  Increased overhead also results from server contention. Consider a cluster with 100 clients and 100 OSSs, each with one OST. If each file has exactly one object and the load is distributed evenly, there is no contention and the disks on each server can manage sequential I/O. If each file has 100 objects, then the clients all compete with one another for the attention of the servers, and the disks on each node seek in 100 different directions resulting in needless contention.\n\n- **Increased risk.** When files are striped across all servers and one of the servers breaks down, a small part of each striped file is lost.", "mimetype": "text/plain", "start_char_idx": 3254, "end_char_idx": 7816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06055661-3e92-48f2-83cf-ddf96872f49c": {"__data__": {"id_": "06055661-3e92-48f2-83cf-ddf96872f49c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9059c12-5b98-4e65-9ded-730980f9276d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "de7599891584379fa07122b8f7ae2c644802ea79a76e937e10dfe51bcf2a7930", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a73df900-f4b5-4c92-94a1-b35c0c18ef5d", "node_type": "1", "metadata": {}, "hash": "f599eb8cdd43e86663caff05b3a2d972c49564b0a52630eb070e1b9b48efdf37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some reasons to minimize or avoid striping:\n\n- **Increased overhead.** Striping results in more locks and extra network operations during common operations such as `stat` and `unlink`. Even when these operations are performed in parallel, one network operation takes less time than 100 operations.\n\n  Increased overhead also results from server contention. Consider a cluster with 100 clients and 100 OSSs, each with one OST. If each file has exactly one object and the load is distributed evenly, there is no contention and the disks on each server can manage sequential I/O. If each file has 100 objects, then the clients all compete with one another for the attention of the servers, and the disks on each node seek in 100 different directions resulting in needless contention.\n\n- **Increased risk.** When files are striped across all servers and one of the servers breaks down, a small part of each striped file is lost. By comparison, if each file has exactly one stripe, fewer files are lost, but they are lost in their entirety. Many users would prefer to lose some of their files entirely than all of their files partially.\n\n### Choosing a Stripe Size\n\nChoosing a stripe size is a balancing act, but reasonable defaults are described below. The stripe size has no effect on a single-stripe file.\n\n- **The stripe size must be a multiple of the page size.** Lustre software tools enforce a multiple of 64 KB (the maximum page size on ia64 and PPC64 nodes) so that users on platforms with smaller pages do not accidentally create files that might cause problems for ia64 clients.\n- **The smallest recommended stripe size is 512 KB.** Although you can create files with a stripe size of 64 KB, the smallest practical stripe size is 512 KB because the Lustre file system sends 1MB chunks over the network. Choosing a smaller stripe size may result in inefficient I/O to the disks and reduced performance.\n- **A good stripe size for sequential I/O using high-speed networks is between 1 MB and 4 MB.** In most situations, stripe sizes larger than 4 MB may result in longer lock hold times and contention during shared file access.\n- **The maximum stripe size is 4 GB.** Using a large stripe size can improve performance when accessing very large files. It allows each client to have exclusive access to its own part of a file. However, a large stripe size can be counterproductive in cases where it does not match your I/O pattern.\n- **Choose a stripe pattern that takes into account the write patterns of your application.** Writes that cross an object boundary are slightly less efficient than writes that go entirely to one server. If the file is written in a consistent and aligned way, make the stripe size a multiple of the `write()` size.\n\n## Setting the File Layout/Striping Configuration (`lfs setstripe`)\n\nUse the `lfs setstripe` command to create new files with a specific file layout (stripe pattern) configuration.\n\n```\nlfs setstripe [--size|-s stripe_size] [--stripe-count|-c stripe_count] [--overstripe-count|-C stripe_count] \n[--index|-i start_ost] [--pool|-p pool_name] filename|dirname \n```\n\n**stripe_size**\n\nThe `stripe_count` indicates how many stripes to use. The default `stripe_count` value is 1. Setting `stripe_count` to 0 causes the default stripe count to be used. Setting `stripe_count` to -1 means stripe over all available OSTs (full OSTs are skipped). When --overstripe-count is used, per OST if necessary.\n\n**stripe_count (--stripe-count, --overstripe-count)**\n\nThe `stripe_count` indicates how many OSTs to use. The default `stripe_count` value is 1. Setting `stripe_count` to 0 causes the default stripe count to be used. Setting `stripe_count` to -1 means stripe over all available OSTs (full OSTs are skipped).\n\n**start_ost**\n\nThe start OST is the first OST to which files are written. The default value for `start_ost` is -1, which allows the MDS to choose the starting index. This setting is strongly recommended, as it allows space and load balancing to be done by the MDS as needed. If the value of `start_ost` is set to a value other than -1, the file starts on the specified OST index. OST index numbering starts at 0.\n\n**Note**\n\nIf the specified OST is inactive or in a degraded mode, the MDS will silently choose another target.", "mimetype": "text/plain", "start_char_idx": 6892, "end_char_idx": 11166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a73df900-f4b5-4c92-94a1-b35c0c18ef5d": {"__data__": {"id_": "a73df900-f4b5-4c92-94a1-b35c0c18ef5d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06055661-3e92-48f2-83cf-ddf96872f49c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "de69cce962bfbbb8d8aa53ab25ce635f6fb5160eb1353e334dc6947f1b086014", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfcc1e97-ef78-4850-b7ac-1f9ffa217a33", "node_type": "1", "metadata": {}, "hash": "fde6cd9274db2539eb2f1e795e68f0b29b27c5814de1f8600cd2acc412d69bd1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The default `stripe_count` value is 1. Setting `stripe_count` to 0 causes the default stripe count to be used. Setting `stripe_count` to -1 means stripe over all available OSTs (full OSTs are skipped).\n\n**start_ost**\n\nThe start OST is the first OST to which files are written. The default value for `start_ost` is -1, which allows the MDS to choose the starting index. This setting is strongly recommended, as it allows space and load balancing to be done by the MDS as needed. If the value of `start_ost` is set to a value other than -1, the file starts on the specified OST index. OST index numbering starts at 0.\n\n**Note**\n\nIf the specified OST is inactive or in a degraded mode, the MDS will silently choose another target.\n\n**Note**\n\nIf you pass a `start_ost` value of 0 and a `stripe_count` value of *1*, all files are written to OST 0, until space is exhausted. This is probably not what you meant to do. If you only want to adjust the stripe count and keep the other parameters at their default settings, do not specify any of the other parameters:\n\n```\nclient# lfs setstripe -c stripe_count filename\n```\n\n**pool_name**\n\nThe `pool_name` specifies the OST pool to which the file will be written. This allows limiting the OSTs used to a subset of all OSTs in the file system. For more details about using OST pools, see [*Creating and Managing OST Pools*](03.12-Managing%20the%20File%20System%20and%20IO.md).\n\n### Specifying a File Layout (Striping Pattern) for a Single File\n\nIt is possible to specify the file layout when a new file is created using the command\u00a0`lfs setstripe`. This allows users to override the file system default parameters to tune the file layout more optimally for their application. Execution of an\u00a0`lfs setstripe`command fails if the file already exists.\n\n#### Setting the Stripe Size\n\nThe command to create a new file with a specified stripe size is similar to:\n\n```\n[client]# lfs setstripe -s 4M /mnt/lustre/new_file\n```\n\nThis example command creates the new file `/mnt/lustre/new_file` with a stripe size of 4 MB.\n\nNow, when the file is created, the new stripe setting creates the file on a single OST with a stripe size of 4M:\n\n```\n [client]# lfs getstripe /mnt/lustre/new_file\n/mnt/lustre/4mb_file\nlmm_stripe_count:   1\nlmm_stripe_size:    4194304\nlmm_pattern:        1\nlmm_layout_gen:     0\nlmm_stripe_offset:  1\nobdidx     objid        objid           group\n1          690550       0xa8976         0 \n```\n\nIn this example, the stripe size is 4 MB.\n\n#### Setting the Stripe Count\n\nThe command below creates a new file with a stripe count of `-1` to specify striping over all available OSTs:\n\n```\n[client]# lfs setstripe -c -1 /mnt/lustre/full_stripe\n```\n\nThe example below indicates that the file `full_stripe` is striped over all six active OSTs in the configuration:\n\n```\n[client]# lfs getstripe /mnt/lustre/full_stripe\n/mnt/lustre/full_stripe\n  obdidx   objid   objid   group\n  0        8       0x8     0\n  1        4       0x4     0\n  2        5       0x5     0\n  3        5       0x5     0\n  4        4       0x4     0\n  5        2       0x2     0\n```\n\nThis is in contrast to the output in [*the section called \u201cSetting the Stripe Size\u201d*](#setting-the-stripe-size), which shows only a single object for the file.\n\n### Setting the Striping Layout for a Directory\n\nIn a directory, the `lfs setstripe` command sets a default striping configuration for files created in the directory. The usage is the same as `lfs setstripe` for a regular file, except that the directory must exist prior to setting the default striping configuration.", "mimetype": "text/plain", "start_char_idx": 10439, "end_char_idx": 14013, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bfcc1e97-ef78-4850-b7ac-1f9ffa217a33": {"__data__": {"id_": "bfcc1e97-ef78-4850-b7ac-1f9ffa217a33", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a73df900-f4b5-4c92-94a1-b35c0c18ef5d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "26dd00b9e1e86665d9d7db082ffc06eb96e13e41fc54e1a42f86609e793191e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5204ea24-9dd2-465e-b720-79d81bdf750b", "node_type": "1", "metadata": {}, "hash": "15f087f9d5703d86881337d06603d84041780e466a18cc1e55066ca4039b870a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Setting the Striping Layout for a Directory\n\nIn a directory, the `lfs setstripe` command sets a default striping configuration for files created in the directory. The usage is the same as `lfs setstripe` for a regular file, except that the directory must exist prior to setting the default striping configuration. If a file is created in a directory with a default stripe configuration (without otherwise specifying striping), the Lustre file system uses those striping parameters instead of the file system default for the new file.\n\nTo change the striping pattern for a sub-directory, create a directory with desired file layout as described above. Sub-directories inherit the file layout of the root/parent directory.\n\n### Setting the Striping Layout for a File System\n\nSetting the striping specification on the `root` directory determines the striping for all new files created in the file system unless an overriding striping specification takes precedence (such as a striping layout specified by the application, or set using `lfs setstripe`, or specified for the parent directory).\n\n**Note**\n\nThe striping settings for a `root` directory are, by default, applied to any new child directories created in the root directory, unless striping settings have been specified for the child directory.\n\n### Creating a File on a Specific OST\n\nYou can use `lfs setstripe` to create a file on a specific OST. In the following example, the file `file1` is created on the first OST (OST index is 0).\n\n```\n$ lfs setstripe --stripe-count 1 --index 0 file1\n$ dd if=/dev/zero of=file1 count=1 bs=100M\n1+0 records in\n1+0 records out\n\n$ lfs getstripe file1\n/mnt/testfs/file1\nlmm_stripe_count:   1\nlmm_stripe_size:    1048576\nlmm_pattern:        1\nlmm_layout_gen:     0\nlmm_stripe_offset:  0               \n     obdidx    objid   objid    group                    \n     0         37364   0x91f4   0\n```\n\n## Retrieving File Layout/Striping Information (`getstripe`)\n\nThe\u00a0`lfs getstripe`\u00a0command is used to display information that shows over which OSTs a file is distributed. For each OST, the index and UUID is displayed, along with the OST index and object ID for each stripe in the file. For directories, the default settings for files created in that directory are displayed.\n\n### Displaying the Current Stripe Size\n\nTo see the current stripe size for a Lustre file or directory, use the `lfs getstripe` command. For example, to view information for a directory, enter a command similar to:\n\n```\n[client]# lfs getstripe /mnt/lustre \n```\n\nThis command produces output similar to:\n\n```\n/mnt/lustre \n(Default) stripe_count: 1 stripe_size: 1M stripe_offset: -1\n```\n\nIn this example, the default stripe count is `1` (data blocks are striped over a single OST), the default stripe size is 1 MB, and the objects are created over all available OSTs.\n\nTo view information for a file, enter a command similar to:\n\n```\n$ lfs getstripe /mnt/lustre/foo\n/mnt/lustre/foo\nlmm_stripe_count:   1\nlmm_stripe_size:    1048576\nlmm_pattern:        1\nlmm_layout_gen:     0\nlmm_stripe_offset:  0\n  obdidx   objid    objid      group\n  2        835487   m0xcbf9f   0 \n```\n\nIn this example, the file is located on `obdidx 2`, which corresponds to the OST `lustre-OST0002`. To see which node is serving that OST, run:\n\n```\n$ lctl get_param osc.lustre-OST0002-osc.ost_conn_uuid\nosc.lustre-OST0002-osc.ost_conn_uuid=192.168.20.1@tcp\n```\n\n### Inspecting the File Tree\n\nTo inspect an entire tree of files, use the `lfs find` command:\n\n```\nlfs find [--recursive | -r] file|directory ...\n```\n\n### Locating the MDT for a remote directory\n\nLustre can be configured with multiple MDTs in the same file system.  Each directory and file could be located on a different MDT.", "mimetype": "text/plain", "start_char_idx": 13696, "end_char_idx": 17424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5204ea24-9dd2-465e-b720-79d81bdf750b": {"__data__": {"id_": "5204ea24-9dd2-465e-b720-79d81bdf750b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfcc1e97-ef78-4850-b7ac-1f9ffa217a33", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a1d4683ca75e8a7d32d29ca0baf9a380f109434891945de63e80f804d9197ba7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "426b4faf-6ad8-473a-850a-4c5e3dcf22c9", "node_type": "1", "metadata": {}, "hash": "ed31e46aff4d00d8e6cd5aaee23ec997b4dc45d83dce36dc837613c549bf2b8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To see which node is serving that OST, run:\n\n```\n$ lctl get_param osc.lustre-OST0002-osc.ost_conn_uuid\nosc.lustre-OST0002-osc.ost_conn_uuid=192.168.20.1@tcp\n```\n\n### Inspecting the File Tree\n\nTo inspect an entire tree of files, use the `lfs find` command:\n\n```\nlfs find [--recursive | -r] file|directory ...\n```\n\n### Locating the MDT for a remote directory\n\nLustre can be configured with multiple MDTs in the same file system.  Each directory and file could be located on a different MDT. To identify which MDT a given subdirectory is located, pass the `getstripe [--mdt-index|-M]` parameters to `lfs`. An example of this command is provided in the section [*the section called \u201cRemoving an MDT from the File System\u201d*](03.03-Lustre%20Maintenance.md#removing-an-mdt-from-the-file-system).\n\nIntroduced in Lustre 2.10\n\n## Progressive File Layout(PFL)\n\nThe Lustre Progressive File Layout (PFL) feature simplifies the use of Lustre so that users can expect reasonable performance for a variety of normal file IO patterns without the need to explicitly understand their IO model or Lustre usage details in advance. In particular, users do not necessarily need to know the size or concurrency of output files in advance of their creation and explicitly specify an optimal layout for each file in order to achieve good performance for both highly concurrent shared-single-large-file IO or parallel IO to many smaller per-process files.\n\nThe layout of a PFL file is stored on disk as `composite layout`. A PFL file is essentially an array of `sub-layout components`, with each sub-layout component being a plain layout covering different and non-overlapped extents of the file. For PFL files, the file layout is composed of a series of components, therefore it's possible that there are some file extents are not described by any components.\n\nAn example of how data blocks of PFL files are mapped to OST objects of components is shown in the following PFL object mapping diagram:\n\n##### Figure 10. PFL object mapping diagram\n\n![PFL object mapping diagram](figures/PFL_object_mapping_diagram.png)\n\nThe PFL file in [Figure 10, \u201cPFL object mapping diagram\u201d](#figure-10-pfl-object-mapping-diagram)  has 3 components and shows the mapping for the blocks of a 2055MB file. The stripe size for the first two components is 1MB, while the stripe size for the third component is 4MB. The stripe count is increasing for each successive component. The first component only has two 1MB blocks and the single object has a size of 2MB. The second component holds the next 254MB of the file spread over 4 separate OST objects in RAID-0, each one will have a size of 256MB / 4 objects = 64MB per object. Note the first two objects `obj 2,0` and `obj 2,1` have a 1MB hole at the start where the data is stored in the first component. The final component holds the next 1800MB spread over 32 OST objects. There is a 256MB / 32 = 8MB hole at the start each one for the data stored in the first two components. Each object will be 2048MB / 32 objects = 64MB per object, except the `obj 3,0` that holds an extra 4MB chunk and `obj 3,1` that holds an extra 3MB chunk. If more data was written to the file, only the objects in component 3 would increase in size.\n\nWhen a file range with defined but not instantiated component is accessed, clients will send a Layout Intent RPC to the MDT, and the MDT would instantiate the objects of the components covering that range.\n\nNext, some commands for user to operate PFL files are introduced and some examples of possible composite layout are illustrated as well. Lustre provides commands `lfs setstripe` and `lfs migrate` for users to operate PFL files. `lfs setstripe`commands are used to create PFL files, add or delete components to or from an existing composite file; `lfs migrate` commands are used to re-layout the data in existing files using the new layout parameter by copying the data from the existing OST(s) to the new OST(s).", "mimetype": "text/plain", "start_char_idx": 16936, "end_char_idx": 20886, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "426b4faf-6ad8-473a-850a-4c5e3dcf22c9": {"__data__": {"id_": "426b4faf-6ad8-473a-850a-4c5e3dcf22c9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5204ea24-9dd2-465e-b720-79d81bdf750b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "db3efa38a29b97422811f0697563d248fc9dc08aaad098609b433abd714d74bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0c601e0-ba51-45f0-b2c8-6c6cccc23221", "node_type": "1", "metadata": {}, "hash": "ccb0a767f665881367fd613d54fe71e52ba38c3f7deee68b80613c0cc2a30ebb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If more data was written to the file, only the objects in component 3 would increase in size.\n\nWhen a file range with defined but not instantiated component is accessed, clients will send a Layout Intent RPC to the MDT, and the MDT would instantiate the objects of the components covering that range.\n\nNext, some commands for user to operate PFL files are introduced and some examples of possible composite layout are illustrated as well. Lustre provides commands `lfs setstripe` and `lfs migrate` for users to operate PFL files. `lfs setstripe`commands are used to create PFL files, add or delete components to or from an existing composite file; `lfs migrate` commands are used to re-layout the data in existing files using the new layout parameter by copying the data from the existing OST(s) to the new OST(s). Also, as introduced in the previous sections, `lfs getstripe` commands can be used to list the striping/component information for a given PFL file, and `lfs find` commands can be used to search the directory tree rooted at the given directory or file name for the files that match the given PFL component parameters.\n\n**Note**\n\nUsing PFL files requires both the client and server to understand the PFL file layout, which isn't available for Lustre 2.9 and earlier. And it will not prevent older clients from accessing non-PFL files in the filesystem.\n\n### `lfs setstripe`\n\n`lfs setstripe` commands are used to create PFL files, add or delete components to or from an existing composite file. (Suppose we have 8 OSTs in the following examples and stripe size is 1MB by default.)\n\n#### Create a PFL file\n\n**Command**\n\n```\nlfs setstripe\n[--component-end|-E end1] [STRIPE_OPTIONS]\n[--component-end|-E end2] [STRIPE_OPTIONS] ... filename\n```\n\nThe `-E` option is used to specify the end offset (in bytes or using a suffix \u201ckMGTP\u201d, e.g. 256M) of each component, and it also indicates the following `STRIPE_OPTIONS` are for this component. Each component defines the stripe pattern of the file in the range of [start, end). The first component must start from offset 0 and all components must be adjacent with each other, no holes are allowed, so each extent will start at the end of previous extent. A `-1` end offset or `eof` indicates this is the last component extending to the end of file.\n\n**Example**\n\n```\n$ lfs setstripe -E 4M -c 1 -E 64M -c 4 -E -1 -c -1 -i 4 \\\n/mnt/testfs/create_comp\n```\n\nThis command creates a file with composite layout illustrated in the following figure. The first component has 1 stripe and covers [0, 4M), the second component has 4 stripes and covers [4M, 64M), and the last component stripes start at OST4, cross over all available OSTs and covers [64M, EOF).\n\n**Figure 11. Example: create a composite file**\n\n !", "mimetype": "text/plain", "start_char_idx": 20072, "end_char_idx": 22827, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0c601e0-ba51-45f0-b2c8-6c6cccc23221": {"__data__": {"id_": "d0c601e0-ba51-45f0-b2c8-6c6cccc23221", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "426b4faf-6ad8-473a-850a-4c5e3dcf22c9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "374efc8bfdc28cdc40b1b0abd6c879c93e2ca88f410bd69b0d6474ad5e1ef7e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afa03b52-1847-426a-8a76-e9f29182f7c6", "node_type": "1", "metadata": {}, "hash": "b519170de12ae9d9317d06227fb57f0c735c72594029d2eb2b4bf234a2368aea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first component must start from offset 0 and all components must be adjacent with each other, no holes are allowed, so each extent will start at the end of previous extent. A `-1` end offset or `eof` indicates this is the last component extending to the end of file.\n\n**Example**\n\n```\n$ lfs setstripe -E 4M -c 1 -E 64M -c 4 -E -1 -c -1 -i 4 \\\n/mnt/testfs/create_comp\n```\n\nThis command creates a file with composite layout illustrated in the following figure. The first component has 1 stripe and covers [0, 4M), the second component has 4 stripes and covers [4M, 64M), and the last component stripes start at OST4, cross over all available OSTs and covers [64M, EOF).\n\n**Figure 11. Example: create a composite file**\n\n ![Example: create a composite file](figures/PFL_createfile.png) \n\nThe composite layout can be output by the following command:\n\n```\n$ lfs getstripe /mnt/testfs/create_comp\n/mnt/testfs/create_comp\n  lcm_layout_gen:  3\n  lcm_entry_count: 3\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x2:0x0] }\n\n    lcme_id:             2\n    lcme_flags:          0\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   67108864\n      lmm_stripe_count:  4\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n    lcme_id:             3\n    lcme_flags:          0\n    lcme_extent.e_start: 67108864\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  -1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n```\n\n**Note**\n\nOnly the first component\u2019s OST objects of the PFL file are instantiated when the layout is being set. Other instantiation is delayed to later write/truncate operations.", "mimetype": "text/plain", "start_char_idx": 22103, "end_char_idx": 24105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afa03b52-1847-426a-8a76-e9f29182f7c6": {"__data__": {"id_": "afa03b52-1847-426a-8a76-e9f29182f7c6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0c601e0-ba51-45f0-b2c8-6c6cccc23221", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f02099e9fc290f92c4d841e1e67bdc6c204acfb2e539e9b2ebd12d206e0fdeb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49378cae-63ec-4e61-aa89-bef5e683103b", "node_type": "1", "metadata": {}, "hash": "bb482572a4311efc2d50613472e41e477221f96a8c784cbb3295baa22d0147bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Other instantiation is delayed to later write/truncate operations.\n\nIf we write 128M data to this PFL file, the second and third components will be instantiated:\n\n```\n$ dd if=/dev/zero of=/mnt/testfs/create_comp bs=1M count=128\n$ lfs getstripe /mnt/testfs/create_comp\n/mnt/testfs/create_comp\n  lcm_layout_gen:  5\n  lcm_entry_count: 3\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x2:0x0] }\n\n    lcme_id:             2\n    lcme_flags:          init\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   67108864\n      lmm_stripe_count:  4\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 1\n      lmm_objects:\n      - 0: { l_ost_idx: 1, l_fid: [0x100010000:0x2:0x0] }\n      - 1: { l_ost_idx: 2, l_fid: [0x100020000:0x2:0x0] }\n      - 2: { l_ost_idx: 3, l_fid: [0x100030000:0x2:0x0] }\n      - 3: { l_ost_idx: 4, l_fid: [0x100040000:0x2:0x0] }\n\n    lcme_id:             3\n    lcme_flags:          init\n    lcme_extent.e_start: 67108864\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  8\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n      lmm_objects:\n      - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x3:0x0] }\n      - 1: { l_ost_idx: 5, l_fid: [0x100050000:0x2:0x0] }\n      - 2: { l_ost_idx: 6, l_fid: [0x100060000:0x2:0x0] }\n      - 3: { l_ost_idx: 7, l_fid: [0x100070000:0x2:0x0] }\n      - 4: { l_ost_idx: 0, l_fid: [0x100000000:0x3:0x0] }\n      - 5: { l_ost_idx: 1, l_fid: [0x100010000:0x3:0x0] }\n      - 6: { l_ost_idx: 2, l_fid: [0x100020000:0x3:0x0] }\n      - 7: { l_ost_idx: 3, l_fid: [0x100030000:0x3:0x0] }\n```\n\n#### Add component(s) to an existing composite file\n\n**Command**\n\n```\nlfs setstripe --component-add\n[--component-end|-E end1] [STRIPE_OPTIONS]\n[--component-end|-E end2] [STRIPE_OPTIONS] ... filename\n```\n\nThe option `--component-add` is used to add components to an existing composite file. The extent start of the first component to be added is equal to the extent end of last component in the existing file, and all components to be added must be adjacent with each other.\n\n**Note**\n\nIf the last existing component is specified by `-E -1` or `-E eof`, which covers to the end of the file, it must be deleted before a new one is added.", "mimetype": "text/plain", "start_char_idx": 24039, "end_char_idx": 26610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49378cae-63ec-4e61-aa89-bef5e683103b": {"__data__": {"id_": "49378cae-63ec-4e61-aa89-bef5e683103b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afa03b52-1847-426a-8a76-e9f29182f7c6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "89f08b5c0e7ba0ea2a4bb84430b6291baa785085d2be7e62248bdc3f4fd914d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8a22a3c-0adf-4397-a82b-3657c343eb35", "node_type": "1", "metadata": {}, "hash": "525b5cef29fd8fe28fe45f5115bf92c0fac8143a86a7afa052d814a4f9fff5d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The extent start of the first component to be added is equal to the extent end of last component in the existing file, and all components to be added must be adjacent with each other.\n\n**Note**\n\nIf the last existing component is specified by `-E -1` or `-E eof`, which covers to the end of the file, it must be deleted before a new one is added.\n\n**Example**\n\n```\n$ lfs setstripe -E 4M -c 1 -E 64M -c 4 /mnt/testfs/add_comp\n$ lfs setstripe --component-add -E -1 -c 4 -o 6-7,0,5 \\\n/mnt/testfs/add_comp\n```\n\nThis command adds a new component which starts from the end of the last existing component to the end of file. The layout of this example is illustrated in [Figure 12, \u201cExample: add a component to an existing composite file\u201d](#figure-12-example-add-a-component-to-an-existing-composite-file). The last component stripes across 4 OSTs in sequence OST6, OST7, OST0 and OST5, covers [64M, EOF).\n\n##### Figure 12. Example: add a component to an existing composite file\n\n ![Example: add a component to an existing composite file](figures/PFL_addcomp.png)Example: add a component to an existing composite file \n\nThe layout can be printed out by the following command:\n\n```\n$ lfs getstripe /mnt/testfs/add_comp\n/mnt/testfs/add_comp\n  lcm_layout_gen:  5\n  lcm_entry_count: 3\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x2:0x0] }\n\n    lcme_id:             2\n    lcme_flags:          init\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   67108864\n      lmm_stripe_count:  4\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 1\n      lmm_objects:\n      - 0: { l_ost_idx: 1, l_fid: [0x100010000:0x2:0x0] }\n      - 1: { l_ost_idx: 2, l_fid: [0x100020000:0x2:0x0] }\n      - 2: { l_ost_idx: 3, l_fid: [0x100030000:0x2:0x0] }\n      - 3: { l_ost_idx: 4, l_fid: [0x100040000:0x2:0x0] }\n\n    lcme_id:             5\n    lcme_flags:          0\n    lcme_extent.e_start: 67108864\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  4\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n```\n\nThe component ID \"lcme_id\" changes as layout generation changes. It is not necessarily sequential and does not imply ordering of individual components.\n\n**Note**\n\nSimilar to specifying a full-file composite layout at file creation time, `--component-add` won't instantiate OST objects, the instantiation is delayed to later write/truncate operations.", "mimetype": "text/plain", "start_char_idx": 26265, "end_char_idx": 29005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8a22a3c-0adf-4397-a82b-3657c343eb35": {"__data__": {"id_": "f8a22a3c-0adf-4397-a82b-3657c343eb35", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49378cae-63ec-4e61-aa89-bef5e683103b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7df4a0cb10098c6914b75b28a8158fab028d35fb3a71817a213202dec4646200", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1a2369d-52cd-4b13-9254-b0d43ee89149", "node_type": "1", "metadata": {}, "hash": "df91a85555388ee9539308581e85b4a950565afba61d0efa3b3ab8bd9a35c86f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is not necessarily sequential and does not imply ordering of individual components.\n\n**Note**\n\nSimilar to specifying a full-file composite layout at file creation time, `--component-add` won't instantiate OST objects, the instantiation is delayed to later write/truncate operations. For example, after writing beyond the 64MB start of the file's last component, the new component has had objects allocated:\n\n```\n$ lfs getstripe -I5 /mnt/testfs/add_comp\n/mnt/testfs/add_comp\n  lcm_layout_gen:  6\n  lcm_entry_count: 3\n    lcme_id:             5\n    lcme_flags:          init\n    lcme_extent.e_start: 67108864\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  4\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 6\n      lmm_objects:\n      - 0: { l_ost_idx: 6, l_fid: [0x100060000:0x4:0x0] }\n      - 1: { l_ost_idx: 7, l_fid: [0x100070000:0x4:0x0] }\n      - 2: { l_ost_idx: 0, l_fid: [0x100000000:0x5:0x0] }\n      - 3: { l_ost_idx: 5, l_fid: [0x100050000:0x4:0x0] }\n```\n\n#### Delete component(s) from an existing file\n\n**Command**\n\n```\nlfs setstripe --component-del\n[--component-id|-I comp_id | --component-flags comp_flags]\nfilename\n```\n\nThe option `--component-del` is used to remove the component(s) specified by component ID or flags from an existing file. This operation will result in any data stored in the deleted component will be lost.\n\nThe ID specified by `-I` option is the numerical unique ID of the component, which can be obtained by command `lfs getstripe -I` command, and the flag specified by `--component-flags` option is a certain type of components, which can be obtained by command `lfs getstripe --component-flags`. For now, we only have two flags `init` and `^init` for instantiated and un-instantiated components respectively.\n\n**Note**\n\nDeletion must start with the last component because hole is not allowed.\n\n**Example**\n\n```\n$ lfs getstripe -I /mnt/testfs/del_comp\n1\n2\n5\n$ lfs setstripe --component-del -I 5 /mnt/testfs/del_comp\n```\n\nThis example deletes the component with ID 5 from file `/mnt/testfs/del_comp`. If we still use the last example, the final result is illustrated in [Figure 13, \u201cExample: delete a component from an existing file\u201d](#figure-13-example-delete-a-component-from-an-existing-file).\n\n##### Figure 13. Example: delete a component from an existing file\n\n  ![Example: delete a component from an existing file](figures/PFL_delcomp.png)\n\nIf you try to delete a non-last component, you will see the following error:\n\n```\n$ lfs setstripe -component-del -I 2 /mnt/testfs/del_comp\nDelete component 0x2 from /mnt/testfs/del_comp failed. Invalid argument\nerror: setstripe: delete component of file '/mnt/testfs/del_comp' failed: Invalid argument\n```\n\n#### Set default PFL layout to an existing directory\n\nSimilar to create a PFL file, you can set default PFL layout to an existing directory. After that, all the files created will inherit this layout by default.", "mimetype": "text/plain", "start_char_idx": 28720, "end_char_idx": 31691, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1a2369d-52cd-4b13-9254-b0d43ee89149": {"__data__": {"id_": "f1a2369d-52cd-4b13-9254-b0d43ee89149", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8a22a3c-0adf-4397-a82b-3657c343eb35", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5691303e807f4a033cabca4fe883c49dd4f1c6b4b1dbf84e4ad3adb60625c59e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef7feccf-90c7-4254-8868-1b9aee2e1759", "node_type": "1", "metadata": {}, "hash": "635df22a84102599f5eb2421ce64f47cf89c23158b54660785f589de0c9c752d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "##### Figure 13. Example: delete a component from an existing file\n\n  ![Example: delete a component from an existing file](figures/PFL_delcomp.png)\n\nIf you try to delete a non-last component, you will see the following error:\n\n```\n$ lfs setstripe -component-del -I 2 /mnt/testfs/del_comp\nDelete component 0x2 from /mnt/testfs/del_comp failed. Invalid argument\nerror: setstripe: delete component of file '/mnt/testfs/del_comp' failed: Invalid argument\n```\n\n#### Set default PFL layout to an existing directory\n\nSimilar to create a PFL file, you can set default PFL layout to an existing directory. After that, all the files created will inherit this layout by default.\n\n**Command**\n\n```\nlfs setstripe\n[--component-end|-E end1] [STRIPE_OPTIONS]\n[--component-end|-E end2] [STRIPE_OPTIONS] ... dirname\n```\n\n**Example**\n\n```\n$ mkdir /mnt/testfs/pfldir\n$ lfs setstripe -E 256M -c 1 -E 16G -c 4 -E -1 -S 4M -c -1 /mnt/testfs/pfldir\n```\n\nWhen you run `lfs getstripe`, you will see:\n\n```\n$ lfs getstripe /mnt/testfs/pfldir\n/mnt/testfs/pfldir\n  lcm_layout_gen:  0\n  lcm_entry_count: 3\n    lcme_id:             N/A\n    lcme_flags:          0\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   268435456\n      stripe_count:  1       stripe_size:   1048576       stripe_offset: -1\n    lcme_id:             N/A\n    lcme_flags:          0\n    lcme_extent.e_start: 268435456\n    lcme_extent.e_end:   17179869184\n      stripe_count:  4       stripe_size:   1048576       stripe_offset: -1\n    lcme_id:             N/A\n    lcme_flags:          0\n    lcme_extent.e_start: 17179869184\n    lcme_extent.e_end:   EOF\n      stripe_count:  -1       stripe_size:   4194304       stripe_offset: -1\n```\n\nIf you create a file under `/mnt/testfs/pfldir`, the layout of that file will inherit the layout from its parent directory:\n\n```\n$ touch /mnt/testfs/pfldir/pflfile\n$ lfs getstripe /mnt/testfs/pfldir/pflfile\n/mnt/testfs/pfldir/pflfile\n  lcm_layout_gen:  2\n  lcm_entry_count: 3\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   268435456\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 1\n      lmm_objects:\n      - 0: { l_ost_idx: 1, l_fid: [0x100010000:0xa:0x0] }\n\n    lcme_id:             2\n    lcme_flags:          0\n    lcme_extent.e_start: 268435456\n    lcme_extent.e_end:   17179869184\n      lmm_stripe_count:  4\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n\n    lcme_id:             3\n    lcme_flags:          0\n    lcme_extent.e_start: 17179869184\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  -1\n      lmm_stripe_size:   4194304\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n```\n\n**Note**\n\n`lfs setstripe --component-add/del` can't be run on a directory, because default layout in directory is likea config, which can be arbitrarily changed by `lfs setstripe`, while layout in file may have data (OST objects) attached.", "mimetype": "text/plain", "start_char_idx": 31024, "end_char_idx": 34103, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef7feccf-90c7-4254-8868-1b9aee2e1759": {"__data__": {"id_": "ef7feccf-90c7-4254-8868-1b9aee2e1759", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1a2369d-52cd-4b13-9254-b0d43ee89149", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e5478ffcb8e13fd89caf04e3b6bd38677eebe68369556ac766ecfdfdd2952013", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5109ff20-66d7-4c3d-b305-b5688f11b425", "node_type": "1", "metadata": {}, "hash": "c238f8857bde9708733e29b14837b3eff29ed776cc6b2b0bb66452269a4e3710", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you want to delete default layout in a directory, run `lfs setstripe -d *dirname*` to return the directory to the filesystem-wide defaults, like:\n\n```\n$ lfs setstripe -d /mnt/testfs/pfldir\n$ lfs getstripe -d /mnt/testfs/pfldir\n/mnt/testfs/pfldir\nstripe_count:  1 stripe_size:   1048576 stripe_offset: -1\n/mnt/testfs/pfldir/commonfile\nlmm_stripe_count:  1\nlmm_stripe_size:   1048576\nlmm_pattern:       1\nlmm_layout_gen:    0\nlmm_stripe_offset: 0\n\tobdidx\t\t objid\t\t objid\t\t group\n\t     2\t             9\t          0x9\t             0\n```\n\n### `lfs migrate`\n\n`lfs migrate` commands are used to re-layout the data in the existing files with the new layout parameter by copying the data from the existing OST(s) to the new OST(s).\n\n**Command**\n\n```\nlfs migrate [--component-end|-E comp_end] [STRIPE_OPTIONS] ...\nfilename\n```\n\nThe difference between `migrate` and `setstripe` is that `migrate` is to re-layout the data in the existing files, while `setstripe` is to create new files with the specified layout.\n\n**Example**\n\n**Case1. Migrate a normal one to a composite layout**\n\n```\n$ lfs setstripe -c 1 -S 128K /mnt/testfs/norm_to_2comp\n$ dd if=/dev/urandom of=/mnt/testfs/norm_to_2comp bs=1M count=5\n$ lfs getstripe /mnt/testfs/norm_to_2comp --yaml\n/mnt/testfs/norm_to_comp\nlmm_stripe_count:  1\nlmm_stripe_size:   131072\nlmm_pattern:       1\nlmm_layout_gen:    0\nlmm_stripe_offset: 7\nlmm_objects:\n      - l_ost_idx: 7\n        l_fid:     0x100070000:0x2:0x0\n$ lfs migrate -E 1M -S 512K -c 1 -E -1 -S 1M -c 2 \\\n/mnt/testfs/norm_to_2comp\n```\n\nIn this example, a 5MB size file with 1 stripe and 128K stripe size is migrated to a composite layout file with 2 components, illustrated in [Figure 14, \u201cExample: migrate normal to composite\u201d](#figure-14-example-migrate-normal-to-composite).\n\n##### Figure 14. Example: migrate normal to composite\n\n![Example: migrate normal to composite](figures/PFL_norm_to_comp.png)\n\nThe stripe information after migration is like:\n\n```\n$ lfs getstripe /mnt/testfs/norm_to_2comp\n/mnt/testfs/norm_to_2comp\n  lcm_layout_gen:  4\n  lcm_entry_count: 2\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   1048576\n      lmm_stripe_count:  1\n      lmm_stripe_size:   524288\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x2:0x0] }\n\n    lcme_id:             2\n    lcme_flags:          init\n    lcme_extent.e_start: 1048576\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 2\n      lmm_objects:\n      - 0: { l_ost_idx: 2, l_fid: [0x100020000:0x2:0x0] }\n      - 1: { l_ost_idx: 3, l_fid: [0x100030000:0x2:0x0] }\n```\n\n**Case2.", "mimetype": "text/plain", "start_char_idx": 34104, "end_char_idx": 36915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5109ff20-66d7-4c3d-b305-b5688f11b425": {"__data__": {"id_": "5109ff20-66d7-4c3d-b305-b5688f11b425", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef7feccf-90c7-4254-8868-1b9aee2e1759", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "517f0701ff226202de8b8be64a7325c8776d315fd934fe67c23aa6efa96f017f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d1bcdd4-6ff1-4e18-b37e-d6a8cfad0c7f", "node_type": "1", "metadata": {}, "hash": "9a423e19f0bede461ab67552b1bdbc320250f2f5d1d7db2a70a43c5b16ffa8fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Migrate a composite layout to another composite layout**\n\n```\n$ lfs setstripe -E 1M -S 512K -c 1 -E -1 -S 1M -c 2 \\\n/mnt/testfs/2comp_to_3comp\n$ dd if=/dev/urandom of=/mnt/testfs/norm_to_2comp bs=1M count=5\n$ lfs migrate -E 1M -S 1M -c 2 -E 4M -S 1M -c 2 -E -1 -S 3M -c 3 \\\n/mnt/testfs/2comp_to_3comp\n```\n\nIn this example, a composite layout file with 2 components is migrated a composite layout file with 3 components. If we still use the example in case1, the migration process is illustrated in [Figure 15, \u201cExample: migrate composite to composite\u201d](#figure-15-example-migrate-composite-to-composite).\n\n##### Figure 15. Example: migrate composite to composite\n\n![Example: migrate composite to composite](figures/PFL_comp_to_comp.png)\n\nThe stripe information is like:\n\n```\n$ lfs getstripe /mnt/testfs/2comp_to_3comp\n/mnt/testfs/2comp_to_3comp\n  lcm_layout_gen:  6\n  lcm_entry_count: 3\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   1048576\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n      lmm_objects:\n      - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x2:0x0] }\n      - 1: { l_ost_idx: 5, l_fid: [0x100050000:0x2:0x0] }\n\n    lcme_id:             2\n    lcme_flags:          init\n    lcme_extent.e_start: 1048576\n    lcme_extent.e_end:   4194304\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 6\n      lmm_objects:\n      - 0: { l_ost_idx: 6, l_fid: [0x100060000:0x2:0x0] }\n      - 1: { l_ost_idx: 7, l_fid: [0x100070000:0x3:0x0] }\n\n    lcme_id:             3\n    lcme_flags:          init\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  3\n      lmm_stripe_size:   3145728\n      lmm_pattern:       1\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x3:0x0] }\n      - 1: { l_ost_idx: 1, l_fid: [0x100010000:0x2:0x0] }\n      - 2: { l_ost_idx: 2, l_fid: [0x100020000:0x3:0x0] }\n```\n\n**Case3. Migrate a composite layout to a normal one**\n\n```\n$ lfs migrate -E 1M -S 1M -c 2 -E 4M -S 1M -c 2 -E -1 -S 3M -c 3 \\\n/mnt/testfs/3comp_to_norm\n$ dd if=/dev/urandom of=/mnt/testfs/norm_to_2comp bs=1M count=5\n$ lfs migrate -c 2 -S 2M /mnt/testfs/3comp_to_normal\n```\n\nIn this example, a composite file with 3 components is migrated to a normal file with 2 stripes and 2M stripe size.", "mimetype": "text/plain", "start_char_idx": 36916, "end_char_idx": 39444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d1bcdd4-6ff1-4e18-b37e-d6a8cfad0c7f": {"__data__": {"id_": "6d1bcdd4-6ff1-4e18-b37e-d6a8cfad0c7f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5109ff20-66d7-4c3d-b305-b5688f11b425", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f9d7617081940808064a8d1b3c39b26e4992d28ceee5f34c62bb05ab2a36c001", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa08f408-8d6c-4741-80e2-31141bde438d", "node_type": "1", "metadata": {}, "hash": "4c51a50030d45eaeabeedfa4b922621bf8929bfd48ceba2f376a44fd8f2cb124", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Migrate a composite layout to a normal one**\n\n```\n$ lfs migrate -E 1M -S 1M -c 2 -E 4M -S 1M -c 2 -E -1 -S 3M -c 3 \\\n/mnt/testfs/3comp_to_norm\n$ dd if=/dev/urandom of=/mnt/testfs/norm_to_2comp bs=1M count=5\n$ lfs migrate -c 2 -S 2M /mnt/testfs/3comp_to_normal\n```\n\nIn this example, a composite file with 3 components is migrated to a normal file with 2 stripes and 2M stripe size. If we still use the example in Case2, the migration process is illustrated in [Figure 16, \u201cExample: migrate composite to normal\u201d](#figure-16-example-migrate-composite-to-normal).\n\n##### Figure 16. Example: migrate composite to normal\n\n ![Example: migrate composite to normal](figures/PFL_comp_to_norm.png)\n\nThe stripe information is like:\n\n```\n$ lfs getstripe /mnt/testfs/3comp_to_norm --yaml\n/mnt/testfs/3comp_to_norm\nlmm_stripe_count:  2\nlmm_stripe_size:   2097152\nlmm_pattern:       1\nlmm_layout_gen:    7\nlmm_stripe_offset: 4\nlmm_objects:\n      - l_ost_idx: 4\n        l_fid:     0x100040000:0x3:0x0\n      - l_ost_idx: 5\n        l_fid:     0x100050000:0x3:0x0\n```\n\n### `lfs getstripe`\n\n`lfs getstripe` commands can be used to list the striping/component information for a given PFL file. Here, only those parameters new for PFL files are shown.\n\n**Command**\n\n```\nlfs getstripe\n[--component-id|-I [comp_id]]\n[--component-flags [comp_flags]]\n[--component-count]\n[--component-start [+-][N][kMGTPE]]\n[--component-end|-E [+-][N][kMGTPE]]\ndirname|filename\n```\n\n**Example**\n\nSuppose we already have a composite file `/mnt/testfs/3comp`, created by the following command:\n\n```\n$ lfs setstripe -E 4M -c 1 -E 64M -c 4 -E -1 -c -1 -i 4 \\\n/mnt/testfs/3comp\n```\n\nAnd write some data\n\n```\n$ dd if=/dev/zero of=/mnt/testfs/3comp bs=1M count=5\n```\n\n**Case1.", "mimetype": "text/plain", "start_char_idx": 39064, "end_char_idx": 40789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa08f408-8d6c-4741-80e2-31141bde438d": {"__data__": {"id_": "fa08f408-8d6c-4741-80e2-31141bde438d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d1bcdd4-6ff1-4e18-b37e-d6a8cfad0c7f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4a681d5875b570bc788333f733457aae36a9fe5699b206cb4ea6a17f24f01b2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67d36f7e-f87b-452a-8490-2a1583a45a4d", "node_type": "1", "metadata": {}, "hash": "b354bcff958ea3831750b2aba19639772c936b067c6886d93346857d3aa99567", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, only those parameters new for PFL files are shown.\n\n**Command**\n\n```\nlfs getstripe\n[--component-id|-I [comp_id]]\n[--component-flags [comp_flags]]\n[--component-count]\n[--component-start [+-][N][kMGTPE]]\n[--component-end|-E [+-][N][kMGTPE]]\ndirname|filename\n```\n\n**Example**\n\nSuppose we already have a composite file `/mnt/testfs/3comp`, created by the following command:\n\n```\n$ lfs setstripe -E 4M -c 1 -E 64M -c 4 -E -1 -c -1 -i 4 \\\n/mnt/testfs/3comp\n```\n\nAnd write some data\n\n```\n$ dd if=/dev/zero of=/mnt/testfs/3comp bs=1M count=5\n```\n\n**Case1. List component ID and its related information**\n\n- List all the components ID\n\n  ```\n  $ lfs getstripe -I /mnt/testfs/3comp\n  1\n  2\n  3\n  ```\n\n- List the detailed striping information of component ID=2\n\n  ```\n  $ lfs getstripe -I2 /mnt/testfs/3comp\n  /mnt/testfs/3comp\n    lcm_layout_gen:  4\n    lcm_entry_count: 3\n      lcme_id:             2\n      lcme_flags:          init\n      lcme_extent.e_start: 4194304\n      lcme_extent.e_end:   67108864\n        lmm_stripe_count:  4\n        lmm_stripe_size:   1048576\n        lmm_pattern:       1\n        lmm_layout_gen:    0\n        lmm_stripe_offset: 5\n        lmm_objects:\n        - 0: { l_ost_idx: 5, l_fid: [0x100050000:0x2:0x0] }\n        - 1: { l_ost_idx: 6, l_fid: [0x100060000:0x2:0x0] }\n        - 2: { l_ost_idx: 7, l_fid: [0x100070000:0x2:0x0] }\n        - 3: { l_ost_idx: 0, l_fid: [0x100000000:0x2:0x0] }\n  ```\n\n- List the stripe offset and stripe count of component ID=2\n\n  ```\n  $ lfs getstripe -I2 -i -c /mnt/testfs/3comp\n        lmm_stripe_count:  4\n        lmm_stripe_offset: 5\n  ```\n\n**Case2. List the component which contains the specified flag**\n\n- List the flag of each component\n\n  ```\n  $ lfs getstripe -component-flag -I /mnt/testfs/3comp\n      lcme_id:             1\n      lcme_flags:          init\n      lcme_id:             2\n      lcme_flags:          init\n      lcme_id:             3\n      lcme_flags:          0\n  ```\n\n- List component(s) who is not instantiated\n\n  ```\n  $ lfs getstripe --component-flags=^init /mnt/testfs/3comp\n  /mnt/testfs/3comp\n    lcm_layout_gen:  4\n    lcm_entry_count: 3\n      lcme_id:             3\n      lcme_flags:          0\n      lcme_extent.e_start: 67108864\n      lcme_extent.e_end:   EOF\n        lmm_stripe_count:  -1\n        lmm_stripe_size:   1048576\n        lmm_pattern:       1\n        lmm_layout_gen:    4\n        lmm_stripe_offset: 4\n  ```\n\n**Case3. List the total number of all the component(s)**\n\n- List the total number of all the components\n\n  ```\n  $ lfs getstripe --component-count /mnt/testfs/3comp\n  3\n  ```\n\n**Case4.", "mimetype": "text/plain", "start_char_idx": 40236, "end_char_idx": 42827, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67d36f7e-f87b-452a-8490-2a1583a45a4d": {"__data__": {"id_": "67d36f7e-f87b-452a-8490-2a1583a45a4d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa08f408-8d6c-4741-80e2-31141bde438d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f85a5cc87f013531aafc56c9a31c3dd134fb0448823b500afa969539c6ad4a36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d80970f5-c462-4381-a340-5a1bf000238b", "node_type": "1", "metadata": {}, "hash": "e964e1b6c3be7f0355c23ff5df72be7ec0a908fa6c20f56c66bf27c646c4db0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "List the total number of all the component(s)**\n\n- List the total number of all the components\n\n  ```\n  $ lfs getstripe --component-count /mnt/testfs/3comp\n  3\n  ```\n\n**Case4. List the component with the specified extent start or end positions**\n\n- List the start position in bytes of each component\n\n  ```\n  $ lfs getstripe --component-start /mnt/testfs/3comp\n  0\n  4194304\n  67108864\n  ```\n\n- List the start position in bytes of component ID=3\n\n  ```\n  $ lfs getstripe --component-start -I3 /mnt/testfs/3comp\n  67108864\n  ```\n\n- List the component with start = 64M\n\n  ```\n  $ lfs getstripe --component-start=64M /mnt/testfs/3comp\n  /mnt/testfs/3comp\n    lcm_layout_gen:  4\n    lcm_entry_count: 3\n      lcme_id:             3\n      lcme_flags:          0\n      lcme_extent.e_start: 67108864\n      lcme_extent.e_end:   EOF\n        lmm_stripe_count:  -1\n        lmm_stripe_size:   1048576\n        lmm_pattern:       1\n        lmm_layout_gen:    4\n        lmm_stripe_offset: 4\n  ```\n\n- List the component(s) with start > 5M\n\n  ```\n  $ lfs getstripe --component-start=+5M /mnt/testfs/3comp\n  /mnt/testfs/3comp\n    lcm_layout_gen:  4\n    lcm_entry_count: 3\n      lcme_id:             3\n      lcme_flags:          0\n      lcme_extent.e_start: 67108864\n      lcme_extent.e_end:   EOF\n        lmm_stripe_count:  -1\n        lmm_stripe_size:   1048576\n        lmm_pattern:       1\n        lmm_layout_gen:    4\n        lmm_stripe_offset: 4\n  ```\n\n- List the component(s) with start < 5M\n\n  ```\n  $ lfs getstripe --component-start=-5M /mnt/testfs/3comp\n  /mnt/testfs/3comp\n    lcm_layout_gen:  4\n    lcm_entry_count: 3\n      lcme_id:             1\n      lcme_flags:          init\n      lcme_extent.e_start: 0\n      lcme_extent.e_end:   4194304\n        lmm_stripe_count:  1\n        lmm_stripe_size:   1048576\n        lmm_pattern:       1\n        lmm_layout_gen:    0\n        lmm_stripe_offset: 4\n        lmm_objects:\n        - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x2:0x0] }\n  \n      lcme_id:             2\n      lcme_flags:          init\n      lcme_extent.e_start: 4194304\n      lcme_extent.e_end:   67108864\n        lmm_stripe_count:  4\n        lmm_stripe_size:   1048576\n        lmm_pattern:       1\n        lmm_layout_gen:    0\n        lmm_stripe_offset: 5\n        lmm_objects:\n        - 0: { l_ost_idx: 5, l_fid: [0x100050000:0x2:0x0] }\n        - 1: { l_ost_idx: 6, l_fid: [0x100060000:0x2:0x0] }\n        - 2: { l_ost_idx: 7, l_fid: [0x100070000:0x2:0x0] }\n        - 3: { l_ost_idx: 0,", "mimetype": "text/plain", "start_char_idx": 42652, "end_char_idx": 45132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d80970f5-c462-4381-a340-5a1bf000238b": {"__data__": {"id_": "d80970f5-c462-4381-a340-5a1bf000238b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67d36f7e-f87b-452a-8490-2a1583a45a4d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a5c5d48c029dfd052063c749bdca12fd680ebfdd32f21366b01482c31cd463ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f83d6f6b-5703-4cd1-bcf9-3db13dcf4a3a", "node_type": "1", "metadata": {}, "hash": "95324628850dde74ed585e7c45f150d7e38ec4120cc9b000b91f51781a8b762a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "e_start: 4194304\n      lcme_extent.e_end:   67108864\n        lmm_stripe_count:  4\n        lmm_stripe_size:   1048576\n        lmm_pattern:       1\n        lmm_layout_gen:    0\n        lmm_stripe_offset: 5\n        lmm_objects:\n        - 0: { l_ost_idx: 5, l_fid: [0x100050000:0x2:0x0] }\n        - 1: { l_ost_idx: 6, l_fid: [0x100060000:0x2:0x0] }\n        - 2: { l_ost_idx: 7, l_fid: [0x100070000:0x2:0x0] }\n        - 3: { l_ost_idx: 0, l_fid: [0x100000000:0x2:0x0] }\n  ```\n\n- List the component(s) with start > 3M and end < 70M\n\n  ```\n  $ lfs getstripe --component-start=+3M --component-end=-70M \\\n  /mnt/testfs/3comp\n  /mnt/testfs/3comp\n    lcm_layout_gen:  4\n    lcm_entry_count: 3\n      lcme_id:             2\n      lcme_flags:          init\n      lcme_extent.e_start: 4194304\n      lcme_extent.e_end:   67108864\n        lmm_stripe_count:  4\n        lmm_stripe_size:   1048576\n        lmm_pattern:       1\n        lmm_layout_gen:    0\n        lmm_stripe_offset: 5\n        lmm_objects:\n        - 0: { l_ost_idx: 5, l_fid: [0x100050000:0x2:0x0] }\n        - 1: { l_ost_idx: 6, l_fid: [0x100060000:0x2:0x0] }\n        - 2: { l_ost_idx: 7, l_fid: [0x100070000:0x2:0x0] }\n        - 3: { l_ost_idx: 0, l_fid: [0x100000000:0x2:0x0] }\n  ```\n\n### `lfs find`\n\n`lfs find` commands can be used to search the directory tree rooted at the given directory or file name for the files that match the given PFL component parameters. Here, only those parameters new for PFL files are shown. Their usages are similar to `lfs getstripe` commands.\n\n**Command**\n\n```\nlfs find directory|filename\n[[!] --component-count [+-=]comp_cnt]\n[[!] --component-start [+-=]N[kMGTPE]]\n[[!] --component-end|-E [+-=]N[kMGTPE]]\n[[!] --component-flags=comp_flags]\n```\n\n**Note**\n\nIf you use `--component-xxx` options, only the composite files will be searched; but if you use `! --component-xxx` options, all the files will be searched.\n\n**Example**\n\nWe use the following directory and composite files to show how `lfs find` works.\n\n```\n$ mkdir /mnt/testfs/testdir\n$ lfs setstripe -E 1M -E 10M -E eof /mnt/testfs/testdir/3comp\n$ lfs setstripe -E 4M -E 20M -E 30M -E eof /mnt/testfs/testdir/4comp\n$ mkdir -p /mnt/testfs/testdir/dir_3comp\n$ lfs setstripe -E 6M -E 30M -E eof /mnt/testfs/testdir/dir_3comp\n$ lfs setstripe -E 8M -E eof /mnt/testfs/testdir/dir_3comp/2comp\n$ lfs setstripe -c 1 /mnt/testfs/testdir/dir_3comp/commnfile\n```\n\n**Case1. Find the files that match the specified component count condition**\n\nFind the files under directory /mnt/testfs/testdir whose number of components is not equal to 3.\n\n```\n$ lfs find /mnt/testfs/testdir !", "mimetype": "text/plain", "start_char_idx": 44699, "end_char_idx": 47303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f83d6f6b-5703-4cd1-bcf9-3db13dcf4a3a": {"__data__": {"id_": "f83d6f6b-5703-4cd1-bcf9-3db13dcf4a3a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d80970f5-c462-4381-a340-5a1bf000238b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3909dd4dfa790f6b593ca1cb904f7f336885850c1b93105da279bc5581b41889", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89481538-57aa-4755-a40f-ca883786125e", "node_type": "1", "metadata": {}, "hash": "f185bf0b2af27c53b21093430cfa36a8a4bf3d2e29591af2c101140c309b63b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Find the files that match the specified component count condition**\n\nFind the files under directory /mnt/testfs/testdir whose number of components is not equal to 3.\n\n```\n$ lfs find /mnt/testfs/testdir ! --component-count=3\n/mnt/testfs/testdir\n/mnt/testfs/testdir/4comp\n/mnt/testfs/testdir/dir_3comp/2comp\n/mnt/testfs/testdir/dir_3comp/commonfile\n```\n\n**Case2. Find the files/dirs that match the specified component start/end condition**\n\nFind the file(s) under directory /mnt/testfs/testdir with component start = 4M and end < 70M\n\n```\n$ lfs find /mnt/testfs/testdir --component-start=4M -E -30M\n/mnt/testfs/testdir/4comp\n```\n\n**Case3. Find the files/dirs that match the specified component flag condition**\n\nFind the file(s) under directory /mnt/testfs/testdir whose component flags contain `init`\n\n```\n$ lfs find /mnt/testfs/testdir --component-flag=init\n/mnt/testfs/testdir/3comp\n/mnt/testfs/testdir/4comp\n/mnt/testfs/testdir/dir_3comp/2comp\n```\n\n**Note**\n\nSince `lfs find` uses \"`!`\" to do negative search, we don\u2019t support flag `^init` here.\n\n## Self-Extending Layout (SEL)\n\nThe Lustre Self-Extending Layout (SEL) feature is an extension of the Section 19.5, \u201cProgressive File Layout(PFL)\u201d feature, which allows the MDS to change the defined PFL layout dynamically. With this feature, the MDS monitors the used space on OSTs and swaps the OSTs for the current file when they are low on space. This avoids `ENOSPC` problems for SEL files when applications are writing to them. \n\nWhereas PFL delays the instantiation of some components until an IO operation occurs on this region, SEL allows splitting such non-instantiated components in two parts: an \u201cextendable\u201d component and an \u201cextension\u201d component. The extendable component is a regular PFL component, covering just a part of the region, which is small originally. The extension (or SEL) component is a new component type which is always non-instantiated and unassigned, covering the other part of the region. When a write reaches this unassigned space, and the client calls the MDS to have it instantiated, the MDS makes a decision as to whether to grant additional space to the extendable component. The granted region moves from the head of the extension component to the tail of the extendable component, thus the extendable component grows and the SEL one is shortened. Therefore, it allows the file to continue on the same OSTs, or in the case where space is low on one of the current OSTs, to modify the layout to switch to a new component on new OSTs. In particular, it lets IO automatically spill over to a large HDD OST pool once a small SSD OST pool is getting low on space. \n\nThe default extension policy modifies the layout in the following ways: \n\n1. Extension: continue on the same OSTs \u2013 used when not low on space on any of the OSTs of the current component; a particular extent is granted to the extendable component. \n\n2. Spill over: switch to next component OSTs \u2013 it is used only for not the last component when *at least one* of the current OSTs is low on space; the whole region of the SEL component moves to the next component and the SEL component is removed in its turn. \n\n3. Repeating: create a new component with the same layout but on free OSTs \u2013 it is used only for the last component when *at least one* of the current OSTs is low on space; a new component has the same layout but instantiated on different OSTs (from the same pool) which have enough space. \n\n4. Forced extension: continue with the current component OSTs despite the low on space condition \u2013 it is used only for the last component when a repeating attempt detected low on space condition as well - spillover is impossible and there is no sense in the repeating.\n\n   **Note**\n\n   The SEL feature does not require clients to understand the SEL format of already created files, only the MDS support is needed which is introduced in Lustre 2.13. However, old clients will have some limitations as the Lustre tools will not support it.", "mimetype": "text/plain", "start_char_idx": 47100, "end_char_idx": 51088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89481538-57aa-4755-a40f-ca883786125e": {"__data__": {"id_": "89481538-57aa-4755-a40f-ca883786125e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f83d6f6b-5703-4cd1-bcf9-3db13dcf4a3a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "838504bda000f89e657df1ad0f7d229707341918766662f701107b1977aa9dad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcd1e421-103e-4735-a200-17803c811991", "node_type": "1", "metadata": {}, "hash": "2e3286c1bea12563baf7e669cef96f91b99be93637b308ba080bd94c51fbe2e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. Repeating: create a new component with the same layout but on free OSTs \u2013 it is used only for the last component when *at least one* of the current OSTs is low on space; a new component has the same layout but instantiated on different OSTs (from the same pool) which have enough space. \n\n4. Forced extension: continue with the current component OSTs despite the low on space condition \u2013 it is used only for the last component when a repeating attempt detected low on space condition as well - spillover is impossible and there is no sense in the repeating.\n\n   **Note**\n\n   The SEL feature does not require clients to understand the SEL format of already created files, only the MDS support is needed which is introduced in Lustre 2.13. However, old clients will have some limitations as the Lustre tools will not support it.\n\n### `lfs setstripe`\n\nThe `lfs setstripe` command is used to create files with composite layouts, as well as add or delete components to or from an existing file. It is extended to support SEL components.\n\n#### Create a SEL file\n\n**Command**\n\n```\nlfs setstripe\n[--component-end|-E end1] [STRIPE_OPTIONS] ... filename\nSTRIPE OPTIONS:\n--extension-size, --ext-size, -z <ext_size>\n```\n\nThe `-z` option is added to specify the size of the region which is granted to the extendable component on each iteration. While declaring any component, this option turns the declared component to a pair of components: extendable and extension ones.\n\n**Example**\n\nThe following command creates 2 pairs of extendable and extension components:\n\n```\n# lfs setstripe -E 1G -z 64M -E -1 -z 256M /mnt/lustre/file\n```\n\n**Figure 19.8. Example: create a SEL file**\n\n![Example: create a SEL file](figures/19_8_figures_SEL_Createfile.png)\n\n**Note**\n\nAs usual, only the first PFL component is instantiated at the creation time, thus it is immediately extended to the extension size (64M for the first component), whereas the third component is left zero-length.\n\n```\n# lfs getstripe /mnt/lustre/file\n/mnt/lustre/file\n lcm_layout_gen: 4\n lcm_mirror_count: 1\n lcm_entry_count: 4\n lcme_id: 1\n lcme_mirror_id: 0\n lcme_flags: init\n lcme_extent.e_start: 0\n lcme_extent.e_end: 67108864\n lmm_stripe_count: 1\n lmm_stripe_size: 1048576\n lmm_pattern: raid0\n lmm_layout_gen: 0\n  lmm_stripe_offset: 0\n lmm_objects:\n - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x5:0x0] }\n lcme_id: 2\n lcme_mirror_id: 0\n lcme_flags: extension\n lcme_extent.e_start: 67108864\n lcme_extent.e_end: 1073741824\n lmm_stripe_count: 0\n lmm_extension_size: 67108864\n lmm_pattern: raid0\n lmm_layout_gen: 0\n lmm_stripe_offset: -1\n lcme_id: 3\n lcme_mirror_id: 0\n lcme_flags: 0\n lcme_extent.e_start: 1073741824\n lcme_extent.e_end: 1073741824\n lmm_stripe_count: 1\n lmm_stripe_size: 1048576\n lmm_pattern: raid0\n lmm_layout_gen: 0\n lmm_stripe_offset: -1\n lcme_id: 4\n lcme_mirror_id: 0\n lcme_flags: extension\n lcme_extent.e_start: 1073741824\n lcme_extent.e_end: EOF\n lmm_stripe_count: 0\n lmm_extension_size: 268435456\n lmm_pattern: raid0\n lmm_layout_gen: 0\n lmm_stripe_offset: -1\n```\n\n#### Create a SEL layout template\n\nSimilar to PFL, it is possible to set a SEL layout template to a directory. After that, all the files created under it will inherit this layout by default.", "mimetype": "text/plain", "start_char_idx": 50259, "end_char_idx": 53483, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bcd1e421-103e-4735-a200-17803c811991": {"__data__": {"id_": "bcd1e421-103e-4735-a200-17803c811991", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89481538-57aa-4755-a40f-ca883786125e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fe2c2600311ce5b77c0c0725c8133d06b413b941349769c5da5766684e705562", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3225dfe7-8db0-4070-afb6-7ec70b4a26de", "node_type": "1", "metadata": {}, "hash": "54eca17f8e5d752c7d89276d561efa00d645812b04d4164afe61e6fabf177b4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After that, all the files created under it will inherit this layout by default.\n\n```\n# lfs setstripe -E 1G -z 64M -E -1 -z 256M /mnt/lustre/dir\n# ./lustre/utils/lfs getstripe /mnt/lustre/dir\n/mnt/lustre/dir\n lcm_layout_gen: 0\n lcm_mirror_count: 1\n lcm_entry_count: 4\n lcme_id: N/A\n lcme_mirror_id: N/A\n lcme_flags: 0\n lcme_extent.e_start: 0\n lcme_extent.e_end: 67108864\n stripe_count: 1 stripe_size: 1048576 pattern: raid0\n  lcme_id: N/A\n lcme_mirror_id: N/A\n lcme_flags: extension\n lcme_extent.e_start: 67108864\n lcme_extent.e_end: 1073741824\n stripe_count: 1 extension_size: 67108864 pattern: raid0 stripe_offset: -1\n lcme_id: N/A\n lcme_mirror_id: N/A\n lcme_flags: 0\n lcme_extent.e_start: 1073741824\n lcme_extent.e_end: 1073741824\n stripe_count: 1 stripe_size: 1048576 pattern: raid0 stripe_offset: -1\n lcme_id: N/A\n lcme_mirror_id: N/A\n lcme_flags: extension\n lcme_extent.e_start: 1073741824\n lcme_extent.e_end: EOF\n stripe_count: 1 extension_size: 268435456 pattern: raid0 \n```\n\n### `lfs getstripe`\n\n`lfs getstripe` commands can be used to list the striping/component information for a given SEL file. Here, only those parameters new for SEL files are shown.\n\n**Command**\n\n```\nlfs getstripe\n[--extension-size|--ext-size|-z] filename\n```\n\nThe `-z` option is added to print the extension size in bytes. For composite files this is the extension size of the first extension component. If a particular component is identified by other options (`--componentid, --component-start`, etc...), this component extension size is printed.\n\n**Example 1: List a SEL component information**\n\nSuppose we already have a composite file `/mnt/lustre/file`, created by the following command:\n\n```\n# lfs setstripe -E 1G -z 64M -E -1 -z 256M /mnt/lustre/file\n```\n\nThe 2nd component could be listed with the following command:\n\n```\n# lfs getstripe -I2 /mnt/lustre/file\n/mnt/lustre/file\n lcm_layout_gen: 4\n lcm_mirror_count: 1\n lcm_entry_count: 4\n lcme_id: 2\n lcme_mirror_id: 0\n lcme_flags: extension\n lcme_extent.e_start: 67108864\n lcme_extent.e_end: 1073741824\n  lmm_stripe_count: 0\n lmm_extension_size: 67108864\n lmm_pattern: raid0\n lmm_layout_gen: 0\n lmm_stripe_offset: -1\n```\n\n\u200b\t**Note**\n\n\u200b\tAs you can see the SEL components are marked by the `extension` flag and `lmm_extension_size` field keeps the specified extension size.\n\n**Example 2: List the extension size**\n\nHaving the same file as in the above example, the extension size of the second component could be listed with:\n\n```\n# lfs getstripe -z -I2 /mnt/lustre/file\n67108864\n```\n\n**Example 3: Extension**\n\nHaving the same file as in the above example, suppose there is a write which crosses the end of the first component (64M), and then another write another write which crosses the end of the first component (128M) again, the layout changes as following:\n\n**Figure 19.9. Example: an extension of a SEL file**\n\n!", "mimetype": "text/plain", "start_char_idx": 53404, "end_char_idx": 56261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3225dfe7-8db0-4070-afb6-7ec70b4a26de": {"__data__": {"id_": "3225dfe7-8db0-4070-afb6-7ec70b4a26de", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcd1e421-103e-4735-a200-17803c811991", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2acc1626216e192971b0648ef32beca73d44ed81f587160c2348dbffb7de9bbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5994e311-edfc-439b-939b-9df3e60d27f6", "node_type": "1", "metadata": {}, "hash": "2eab7a7f5a063bb45cd219c937a14cdb9f7d9785d012ce60b3151d4e7769e71d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Example 2: List the extension size**\n\nHaving the same file as in the above example, the extension size of the second component could be listed with:\n\n```\n# lfs getstripe -z -I2 /mnt/lustre/file\n67108864\n```\n\n**Example 3: Extension**\n\nHaving the same file as in the above example, suppose there is a write which crosses the end of the first component (64M), and then another write another write which crosses the end of the first component (128M) again, the layout changes as following:\n\n**Figure 19.9. Example: an extension of a SEL file**\n\n![Example: an extension of a SEL file](figures/19_9_figures_SEL_extension.png)\n\nThe layout can be printed out by the following command:\n\n```\n# lfs getstripe /mnt/lustre/file\n/mnt/lustre/file\nlcm_layout_gen: 6\n lcm_mirror_count: 1\n lcm_entry_count: 4\n \tlcme_id: 1\n \tlcme_mirror_id: 0\n \tlcme_flags: init\n \tlcme_extent.e_start: 0\n \tlcme_extent.e_end: 201326592\n \t\tlmm_stripe_count: 1\n \t\tlmm_stripe_size: 1048576\n \t\tlmm_pattern: raid0\n \t\tlmm_layout_gen: 0\n \t\tlmm_stripe_offset: 0\n \t\tlmm_objects:\n \t\t- 0: { l_ost_idx: 0, l_fid: [0x100000000:0x5:0x0] }\n \t\t\n \tlcme_id: 2\n \tlcme_mirror_id: 0\n \tlcme_flags: extension\n \tlcme_extent.e_start: 201326592\n \tlcme_extent.e_end: 1073741824\n \t\tlmm_stripe_count: 0\n \t\tlmm_extension_size: 67108864\n \t\tlmm_pattern: raid0\n \t\tlmm_layout_gen: 0\n \t\tlmm_stripe_offset: -1\n \t\t\n \tlcme_id: 3\n \tlcme_mirror_id: 0\n \tlcme_flags: 0\n \tlcme_extent.e_start: 1073741824\n \tlcme_extent.e_end: 1073741824\n \t\tlmm_stripe_count: 1\n \t\tlmm_stripe_size: 1048576\n \t\tlmm_pattern: raid0\n \t\tlmm_layout_gen: 0\n \t\tlmm_stripe_offset: -1\n \t\t\n \tlcme_id: 4\n \tlcme_mirror_id: 0\n \tlcme_flags: extension\n \tlcme_extent.e_start: 1073741824\n \tlcme_extent.e_end: EOF\n \t\tlmm_stripe_count: 0\n \t\tlmm_extension_size: 268435456\n \t\tlmm_pattern: raid0\n \t\tlmm_layout_gen: 0\n \t\tlmm_stripe_offset: -1\n```\n\n**Example 4: Spillover**\n\nIn case where `OST0` is low on space and an IO happens to a SEL component, a spillover happens: the full region of the SEL component is added to the next component, e.g. in the example above the next layout modification will look like:\n\n**Figure 19.10. Example: a spillover in a SEL file**\n\n![Example: a spillover in a SEL file](figures/19_10_figures_SEL_Spillover.png)\n\n**Note**\n\nDespite the fact the third component was [1G, 1G] originally, while it is not instantiated, instead of getting extended backward, it is moved backward to the start of the previous SEL component (192M) and extended on its extension size (256M) from that position, thus it becomes `[192M, 448M]`.", "mimetype": "text/plain", "start_char_idx": 55717, "end_char_idx": 58244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5994e311-edfc-439b-939b-9df3e60d27f6": {"__data__": {"id_": "5994e311-edfc-439b-939b-9df3e60d27f6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3225dfe7-8db0-4070-afb6-7ec70b4a26de", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2a86584c6a9e38a83733fc146470352391a639d88437d598db082fe09dd9f2e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccff69a2-86d9-4206-8eae-47ab9df1fd31", "node_type": "1", "metadata": {}, "hash": "f30b9c0075f7dbc631f128c8ceef519d8764b42c01b9bad154fbe7507eb33cde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in the example above the next layout modification will look like:\n\n**Figure 19.10. Example: a spillover in a SEL file**\n\n![Example: a spillover in a SEL file](figures/19_10_figures_SEL_Spillover.png)\n\n**Note**\n\nDespite the fact the third component was [1G, 1G] originally, while it is not instantiated, instead of getting extended backward, it is moved backward to the start of the previous SEL component (192M) and extended on its extension size (256M) from that position, thus it becomes `[192M, 448M]`.\n\n```\n# lfs getstripe /mnt/lustre/file\n/mnt/lustre/file\n lcm_layout_gen: 7\n lcm_mirror_count: 1\n lcm_entry_count: 3\n    lcme_id: 1\n \tlcme_mirror_id: 0\n\tlcme_flags: init\n \tlcme_extent.e_start: 0\n \tlcme_extent.e_end: 201326592\n \t\tlmm_stripe_count: 1\n \t\tlmm_stripe_size: 1048576\n \t\tlmm_pattern: raid0\n \t\tlmm_layout_gen: 0\n \t\tlmm_stripe_offset: 0\n \t\tlmm_objects:\n \t\t- 0: { l_ost_idx: 0, l_fid: [0x100000000:0x5:0x0] }\n \tlcme_id: 3\n \tlcme_mirror_id: 0\n \tlcme_flags: init\n \tlcme_extent.e_start: 201326592\n \tlcme_extent.e_end: 469762048\n \t\tlmm_stripe_count: 1\n \t\tlmm_stripe_size: 1048576\n \t\tlmm_pattern: raid0\n \t\tlmm_layout_gen: 0\n \t\tlmm_stripe_offset: 1\n \t\tlmm_objects:\n \t\t- 0: { l_ost_idx: 1, l_fid: [0x100010000:0x8:0x0] }\n \tlcme_id: 4\n \tlcme_mirror_id: 0\n \tlcme_flags: extension\n \tlcme_extent.e_start: 469762048\n \tlcme_extent.e_end: EOF\n \t\tlmm_stripe_count: 0\n \t\tlmm_extension_size: 268435456\n \t\tlmm_pattern: raid0\n \t\tlmm_layout_gen: 0\n \t\tlmm_stripe_offset: -1\n```\n\n**Example 5: Repeating**\n\nSuppose in the example above, `OST0` got enough free space back but `OST1` is low on space, the following write to the last SEL component leads to a new component allocation before the SEL component, which repeats the previous component layout but instantiated on free OSTs:\n\n**Figure 19.11. Example: repeat a SEL component**\n\n![Example: repeat a SEL component](figures/19_11_figures_SEL_repeating.png)", "mimetype": "text/plain", "start_char_idx": 57739, "end_char_idx": 59635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccff69a2-86d9-4206-8eae-47ab9df1fd31": {"__data__": {"id_": "ccff69a2-86d9-4206-8eae-47ab9df1fd31", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5994e311-edfc-439b-939b-9df3e60d27f6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "85a46ba7959521d8e7b431faafb6d24992f5673bc9f9481436784d36d844cc59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59290b1e-1817-4bf7-bee4-ebfac7d14792", "node_type": "1", "metadata": {}, "hash": "ddf3e3e4995fe106819221af88efd13e00649dc6cc9cef81b890b511e57d2cd5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Example: repeat a SEL component**\n\n![Example: repeat a SEL component](figures/19_11_figures_SEL_repeating.png)\n\n\n\n```\n# lfs getstripe /mnt/lustre/file\n/mnt/lustre/file\n \tlcm_layout_gen: 9\n \tlcm_mirror_count: 1\n \tlcm_entry_count: 4\n \t\tlcme_id: 1\n \t\tlcme_mirror_id: 0\n \t\tlcme_flags: init\n \t\tlcme_extent.e_start: 0\n \t\tlcme_extent.e_end: 201326592\n \t\t\tlmm_stripe_count: 1\n \t\t\tlmm_stripe_size: 1048576\n \t\t\tlmm_pattern: raid0\n \t\t\tlmm_layout_gen: 0\n \t\t\tlmm_stripe_offset: 0\n \t\t\tlmm_objects:\n \t\t\t- 0: { l_ost_idx: 0, l_fid: [0x100000000:0x5:0x0] }\n \t\tlcme_id: 3\n \t\tlcme_mirror_id: 0\n \t\tlcme_flags: init\n \t\tlcme_extent.e_start: 201326592\n \t\tlcme_extent.e_end: 469762048\n \t\t\tlmm_stripe_count: 1\n \t\t\tlmm_stripe_size: 1048576\n \t\t\tlmm_pattern: raid0\n \t\t\tlmm_layout_gen: 0\n \t\t\tlmm_stripe_offset: 1\n \t\t\tlmm_objects:\n\t \t\t- 0: { l_ost_idx: 1, l_fid: [0x100010000:0x8:0x0] }\n \t\tlcme_id: 8\n \t\tlcme_mirror_id: 0\n \t\tlcme_flags: init\n \t\tlcme_extent.e_start: 469762048\n \t\tlcme_extent.e_end: 738197504\n \t\t\tlmm_stripe_count: 1\n \t\t\tlmm_stripe_size: 1048576\n \t\t\tlmm_pattern: raid0\n \t\t\tlmm_layout_gen: 65535\n \t\t\tlmm_stripe_offset: 0\n \t\t\tlmm_objects:\n \t\t\t- 0: { l_ost_idx: 0, l_fid: [0x100000000:0x6:0x0] }\n \t\tlcme_id: 4\n \t\tlcme_mirror_id: 0\n \t\tlcme_flags: extension\n \t\tlcme_extent.e_start: 738197504\n \t\tlcme_extent.e_end: EOF\n \t\t\tlmm_stripe_count: 0\n \t\t\tlmm_extension_size: 268435456\n \t\t\tlmm_pattern: raid0\n \t\t\tlmm_layout_gen: 0\n \t\t\tlmm_stripe_offset: -1\n\n```\n\n**Example 6: Forced extension**\n\nSuppose in the example above, both `OST0` and `OST1` are low on space, the following write to the last SEL component will behave as an extension as there is no sense to repeat.\n\n**Figure 19.12. Example: forced extension in a SEL file**\n\n!", "mimetype": "text/plain", "start_char_idx": 59525, "end_char_idx": 61229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59290b1e-1817-4bf7-bee4-ebfac7d14792": {"__data__": {"id_": "59290b1e-1817-4bf7-bee4-ebfac7d14792", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccff69a2-86d9-4206-8eae-47ab9df1fd31", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b30499fb5f605f4146a7d3c2ae80c58486ce0bb34e4f8989c572fe16cf709b01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c543585-de51-4e1a-9275-308e98d43465", "node_type": "1", "metadata": {}, "hash": "2fc24c1c93330083ef661be7b19786696f13951883c8389886ea325dc5681829", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 19.12. Example: forced extension in a SEL file**\n\n![Example: forced extension in a SEL file](figures/19_12_figures_SEL_forced.png)\n\n```\n# lfs getstripe /mnt/lustre/file\n/mnt/lustre/file\n \tlcm_layout_gen: 11\n \tlcm_mirror_count: 1\n \tlcm_entry_count: 4\n \t\tlcme_id: 1\n \t\tlcme_mirror_id: 0\n \t\tlcme_flags: init\n \t\tlcme_extent.e_start: 0\n \t\tlcme_extent.e_end: 201326592\n \t\t\tlmm_stripe_count: 1\n \t\t\tlmm_stripe_size: 1048576\n \t\t\tlmm_pattern: raid0\n \t\t\tlmm_layout_gen: 0\n \t\t\tlmm_stripe_offset: 0\n \t\t\tlmm_objects:\n \t\t\t- 0: { l_ost_idx: 0, l_fid: [0x100000000:0x5:0x0] }\n \t\tlcme_id: 3\n \t\tlcme_mirror_id: 0\n \t\tlcme_flags: init\n \t\tlcme_extent.e_start: 201326592\n \t\tlcme_extent.e_end: 469762048\n \t\t\tlmm_stripe_count: 1\n \t\t\tlmm_stripe_size: 1048576\n \t\t\tlmm_pattern: raid0\n \t\t\tlmm_layout_gen: 0\n \t\t\tlmm_stripe_offset: 1\n\t\t \tlmm_objects:\n \t\t\t- 0: { l_ost_idx: 1, l_fid: [0x100010000:0x8:0x0] }\n \t\t\t\n \t\tlcme_id: 8\n \t\tlcme_mirror_id: 0\n \t\tlcme_flags: init\n \t\tlcme_extent.e_start: 469762048\n \t\tlcme_extent.e_end: 1006632960\n \t\t\tlmm_stripe_count: 1\n \t\t\tlmm_stripe_size: 1048576\n \t\t\tlmm_pattern: raid0\n \t\t\tlmm_layout_gen: 65535\n \t\t\tlmm_stripe_offset: 0\n \t\t\tlmm_objects:\n \t\t\t- 0: { l_ost_idx: 0, l_fid: [0x100000000:0x6:0x0] }\n \t\tlcme_id: 4\n \t\tlcme_mirror_id: 0\n \t\tlcme_flags: extension\n \t\tlcme_extent.e_start: 1006632960\n \t\tlcme_extent.e_end: EOF\n \t\t\tlmm_stripe_count: 0\n \t\t\tlmm_extension_size: 268435456\n \t\t\tlmm_pattern: raid0\n \t\t\tlmm_layout_gen: 0\n \t\t\tlmm_stripe_offset: -1\n```\n\n### `lfs find`\n\n`lfs find` commands can be used to search for the files that match the given SEL component paremeters. Here, only those parameters new for the SEL files are shown.\n\n```\nlfs find\n[[!] --extension-size|--ext-size|-z [+-]ext-size[KMG]\n[[!] --component-flags=extension]\n```\n\nThe `-z` option is added to specify the extension size to search for. The files which have any component with the extension size matched the given criteria are printed out. As always \u201c+\u201d and \u201c-\u201c signs are allowed to specify the least and the most size.\n\nA new `extension` component flag is added. Only files which have at least one SEL component are printed.\n\n\u200b\t\tNote \n\n\u200b\t\tThe negative search for flags searches the files which **have** a non-SEL component (not files which **do not have** any SEL component).\n\n**Example**\n\n```\n# lfs setstripe --extension-size 64M -c 1 -E -1 /mnt/lustre/file\n\n# lfs find --comp-flags extension /mnt/lustre/*\n/mnt/lustre/file\n\n# lfs find !", "mimetype": "text/plain", "start_char_idx": 61169, "end_char_idx": 63595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c543585-de51-4e1a-9275-308e98d43465": {"__data__": {"id_": "5c543585-de51-4e1a-9275-308e98d43465", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59290b1e-1817-4bf7-bee4-ebfac7d14792", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "932a9ac6928325dfa7760da26dc5852fc122279b899f7f1406096520b43c5555", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e2adddf-5564-47d7-812c-c57416a423b7", "node_type": "1", "metadata": {}, "hash": "168b20f461c1be72ec532c35f2024a6220441f3eaf0f48dbc1b8c3cee5a7bd0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "--component-flags=extension]\n```\n\nThe `-z` option is added to specify the extension size to search for. The files which have any component with the extension size matched the given criteria are printed out. As always \u201c+\u201d and \u201c-\u201c signs are allowed to specify the least and the most size.\n\nA new `extension` component flag is added. Only files which have at least one SEL component are printed.\n\n\u200b\t\tNote \n\n\u200b\t\tThe negative search for flags searches the files which **have** a non-SEL component (not files which **do not have** any SEL component).\n\n**Example**\n\n```\n# lfs setstripe --extension-size 64M -c 1 -E -1 /mnt/lustre/file\n\n# lfs find --comp-flags extension /mnt/lustre/*\n/mnt/lustre/file\n\n# lfs find ! --comp-flags extension /mnt/lustre/*\n/mnt/lustre/file\n\n# lfs find -z 64M /mnt/lustre/*\n/mnt/lustre/file\n\n# lfs find -z +64M /mnt/lustre/*\n\n# lfs find -z -64M /mnt/lustre/*\n\n# lfs find -z +63M /mnt/lustre/*\n/mnt/lustre/file\n\n# lfs find -z -65M /mnt/lustre/*\n/mnt/lustre/file\n\n# lfs find -z 65M /mnt/lustre/*\n\n# lfs find ! -z 64M /mnt/lustre/*\n\n# lfs find ! -z +64M /mnt/lustre/*\n/mnt/lustre/file\n\n# lfs find ! -z -64M /mnt/lustre/*\n/mnt/lustre/file\n\n# lfs find ! -z +63M /mnt/lustre/*\n\n# lfs find ! -z -65M /mnt/lustre/*\n\n# lfs find ! -z 65M /mnt/lustre/*\n/mnt/lustre/file\n```\n\n##  Foreign Layout\n\nThe Lustre Foreign Layout feature is an extension of both the LOV and LMV formats which allows the creation of empty files and directories with the necessary specifications to point to corresponding objects outside from Lustre namespace. \n\nThe new LOV/LMV foreign internal format can be represented as:\n\n**Figure 19.13. LOV/LMV foreign format**\n\n![Example: LOV/LMV foreign format](figures/19_13_figures_Foreign_Format.png)\n\n### `lfs set[dir]stripe`\n\nThe `lfs set[dir]stripe` commands are used to create files or directories with foreign layouts, by calling the corresponding API, itself invoking the appropriate ioctl().\n\n### Create a Foreign file/dir\n\n**Command**\n\n```\nlfs set[dir]stripe \\\n--foreign[=<foreign_type>] --xattr|-x <layout_string> \\\n[--flags <hex_bitmask>] [--mode <mode_bits>] \\\n{file,dir}name\n```\n\nBoth the `--foreign` and `--xattr|-x` options are mandatory. The  \\<foreign_type\\>(default is \"none\", meaning no special behavior), and both `--flags` and `--mode` (default is 0666) options are optional.\n\n**Example**\n\nThe following command creates a foreign file of \"none\" type and with \"foo@bar\" LOV content and specific mode and flags:\n\n```\n# lfs setstripe --foreign=none --flags=0xda08 --mode=0640 \\\n--xattr=foo@bar /mnt/lustre/file\n```\n\n![Example: LOV/LMV foreign format](figures/19_14_figures_Foreign_Createfile.png)\n\n### `lfs get[dir]stripe`\n\n`lfs get[dir]stripe` commands can be used to retrieve foreign LOV/LMV informations and content.", "mimetype": "text/plain", "start_char_idx": 62889, "end_char_idx": 65651, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e2adddf-5564-47d7-812c-c57416a423b7": {"__data__": {"id_": "8e2adddf-5564-47d7-812c-c57416a423b7", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c543585-de51-4e1a-9275-308e98d43465", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "483e70b7f869b95367b6a39821753e9fe57e73a993107376ef8473323568b711", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01caa303-9e21-42b3-bff5-44b86af5fa73", "node_type": "1", "metadata": {}, "hash": "e3e50682dab66395ac80597ae43688b2f9a675f644f6f8d55293b0025bda5a41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The  \\<foreign_type\\>(default is \"none\", meaning no special behavior), and both `--flags` and `--mode` (default is 0666) options are optional.\n\n**Example**\n\nThe following command creates a foreign file of \"none\" type and with \"foo@bar\" LOV content and specific mode and flags:\n\n```\n# lfs setstripe --foreign=none --flags=0xda08 --mode=0640 \\\n--xattr=foo@bar /mnt/lustre/file\n```\n\n![Example: LOV/LMV foreign format](figures/19_14_figures_Foreign_Createfile.png)\n\n### `lfs get[dir]stripe`\n\n`lfs get[dir]stripe` commands can be used to retrieve foreign LOV/LMV informations and content.\n\n**Command**\n\n```\nlfs get[dir]stripe [-v] filename\n```\n\n**List foreign layout information**\n\nSuppose we already have a foreign file `/mnt/lustre/file`, created by the following command:\n\n```\n# lfs setstripe --foreign=none --flags=0xda08 --mode=0640 \\\n--xattr=foo@bar /mnt/lustre/file\n```\n\nThe full foreign layout informations can be listed using the following command:\n\n```\n# lfs getstripe -v /mnt/lustre/file\n/mnt/lustre/file\n lfm_magic: 0x0BD70BD0\n lfm_length: 7\n lfm_type: none\n lfm_flags: 0x0000DA08\n lfm_value: foo@bar\n```\n\n\u200b\t**Note**\n\n\u200b\tAs you can see the `lfm_length` field value is the characters number in the variable length `lfm_value` field.\n\n### `lfs find`\n\n`lfs find` commands can be used to search for all the foreign files/directories or those that match the given selection paremeters. \n\n```\nlfs find\n[[!] --foreign[=<foreign_type>]\n```\n\nThe `--foreign[=<foreign_type>] ` option has been added to specify that all [!,but not] files and/ or directories with a foreign layout [and [!,but not] of \\<foreign_type\\>] will be retrieved. \n\n**Example**\n\n```\n# lfs setstripe --foreign=none --xattr=foo@bar /mnt/lustre/file\n\n# touch /mnt/lustre/file2\n\n# lfs find --foreign /mnt/lustre/*\n/mnt/lustre/file\n\n# lfs find ! --foreign /mnt/lustre/*\n/mnt/lustre/file2\n\n# lfs find --foreign=none /mnt/lustre/*\n/mnt/lustre/file\n```\n\n## Managing Free Space\n\nTo optimize file system performance, the MDT assigns file stripes to OSTs based on two allocation algorithms. The round-robin allocator gives preference to location (spreading out stripes across OSSs to increase network bandwidth utilization) and the weighted allocator gives preference to available space (balancing loads across OSTs). Threshold and weighting factors for these two algorithms can be adjusted by the user. The MDT reserves 0.1 percent of total OST space and 32 inodes for each OST. The MDT stops object allocation for the OST if available space is less than reserved or the OST has fewer than 32 free inodes. The MDT starts object allocation when available space is twice as big as the reserved space and the OST has more than 64 free inodes. Note, clients could append existing files no matter what object allocation state is.\n\nIntroduced in Lustre 2.9The reserved space for each OST can be adjusted by the user. Use the `lctl set_param` command, for example the next command reserve 1GB space for all OSTs.`lctl set_param -P osp.*.reserved_mb_low=1024`\n\nThis section describes how to check available free space on disks and how free space is allocated. It then describes how to set the threshold and weighting factors for the allocation algorithms.\n\n### Checking File System Free Space\n\nFree space is an important consideration in assigning file stripes. The `lfs df` command can be used to show available disk space on the mounted Lustre file system and space consumption per OST. If multiple Lustre file systems are mounted, a path may be specified, but is not required. Options to the`lfs df` command are shown below.", "mimetype": "text/plain", "start_char_idx": 65068, "end_char_idx": 68645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01caa303-9e21-42b3-bff5-44b86af5fa73": {"__data__": {"id_": "01caa303-9e21-42b3-bff5-44b86af5fa73", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e2adddf-5564-47d7-812c-c57416a423b7", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f6061959f8b09809055b4aef6b0551dd493df2bd510ae5c99d6d1feb0bc9836", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2178baf0-2e71-4958-9144-1216f2f1f9c9", "node_type": "1", "metadata": {}, "hash": "096efbb7bb78fbe03229bedba7a934d60108af49b017dbdbf73128562c845524", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note, clients could append existing files no matter what object allocation state is.\n\nIntroduced in Lustre 2.9The reserved space for each OST can be adjusted by the user. Use the `lctl set_param` command, for example the next command reserve 1GB space for all OSTs.`lctl set_param -P osp.*.reserved_mb_low=1024`\n\nThis section describes how to check available free space on disks and how free space is allocated. It then describes how to set the threshold and weighting factors for the allocation algorithms.\n\n### Checking File System Free Space\n\nFree space is an important consideration in assigning file stripes. The `lfs df` command can be used to show available disk space on the mounted Lustre file system and space consumption per OST. If multiple Lustre file systems are mounted, a path may be specified, but is not required. Options to the`lfs df` command are shown below.\n\n| **Option**             | **Description**                                              |\n| ---------------------- | ------------------------------------------------------------ |\n| `-h, --human-readable` | Displays sizes in human readable format (for example: 1K, 234M, 5G) using base-2 (binary) values (i.e. 1G = 1024M). |\n| `-H, --si`             | Like `-h`, this displays counts in human readable format, but using base-10 (decimal) values (i.e. 1G = 1000M). |\n| `-i, --inodes`         | Lists inodes instead of block usage.                         |\n| `-l, --lazy`           | Do not attempt to contact any OST or MDT not currently connected to the client. This avoids blocking the `lfs df` output if a target is offline or unreachable, and only returns the space on OSTs that can currently be accessed. |\n| `-p, --pool`           | Limit the usage to report only OSTs that are in the specified *pool*. If multiple Lustre filesystems are mounted, list the OSTs in *pool* for each filesystem, or limit the display to only a pool for a specific filesystem if *fsname.pool* is given. Specifying both *fsname* and *pool* is equivalent to providing a specific mountpoint. |\n| `-v, --verbose`        | Display verbose status of MDTs and OSTs. This may include one or more optional flags at the end of each line. |\n\n`lfs df` may also report additional target status as the last column in the display, if there are issues with that target. Target states include:\n\n* `D`: OST/MDT is `Degraded`. The target has a failed drive in the RAID device, or is undergoing RAID reconstruction. This state is marked on the server automatically for ZFS targets via `zed`, or a (user-supplied) script that monitors the target device and sets \"`lctl set_param obdfilter.target.degraded=1`\" on the OST. This target will be avoided for new allocations, but will still be used to read existing files located there or if there are not enough non-degraded OSTs to make up a widely-striped file.\n* `R`: OST/MDT is `Read-only`. The target filesystem is marked read-only due to filesystem corruption detected by ldiskfs or ZFS. No modifications are allowed on this OST, and it needs to be unmounted and e2fsck or zpool scrub run to repair the underlying filesystem.\n* `N`: OST/MDT is `No-precreate`. The target is configured to deny object precreation set by \"lctl set_param obdfilter.target.no_precreate=1\" parameter or the \"-o no_precreate\" mount option. This may be done to add an OST to the filesystem without allowing objects to be allocated on it yet, or for other reasons.\n* `S`: OST/MDT is out of `Space`. The target filesystem has less than the minimum required free space and will not be used for new object allocations until it has more free space.\n* `I`: OST/MDT is out of `Inodes`. The target filesystem has less than the minimum required free inodes and will not be used for new object allocations until it has more free inodes.\n* `f`: OST/MDT is on `flash`. The target filesystem is using a flash (non-rotational) storage device. This is normally detected from the underlying Linux block device, but can be set manually with \"`lctl set_param osd-*.*.nonrotational=1` on the respective OSTs.", "mimetype": "text/plain", "start_char_idx": 67766, "end_char_idx": 71826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2178baf0-2e71-4958-9144-1216f2f1f9c9": {"__data__": {"id_": "2178baf0-2e71-4958-9144-1216f2f1f9c9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01caa303-9e21-42b3-bff5-44b86af5fa73", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "95924eff69a5a74f1e8733d0d8c1b2b655ab1b804a292e004f7bf314f5064aef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdcd0665-afef-4d12-84a3-80eb97def906", "node_type": "1", "metadata": {}, "hash": "ad28014ed32b6d64a66b066c95ffa513a206f7062821069c679c22875cda3c1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This may be done to add an OST to the filesystem without allowing objects to be allocated on it yet, or for other reasons.\n* `S`: OST/MDT is out of `Space`. The target filesystem has less than the minimum required free space and will not be used for new object allocations until it has more free space.\n* `I`: OST/MDT is out of `Inodes`. The target filesystem has less than the minimum required free inodes and will not be used for new object allocations until it has more free inodes.\n* `f`: OST/MDT is on `flash`. The target filesystem is using a flash (non-rotational) storage device. This is normally detected from the underlying Linux block device, but can be set manually with \"`lctl set_param osd-*.*.nonrotational=1` on the respective OSTs. This lower-case status is only shown in conjunction with the -v option, since it is not an error condition.\n\n**Note**\n\nThe `df -i` and `lfs df -i` commands show the minimum number of inodes that can be created in the file system at the current time. If the total number of objects available across all of the OSTs is smaller than those available on the MDT(s), taking into account the default file striping, then `df -i` will also report a smaller number of inodes than could be created. Running `lfs df -i` will report the actual number of inodes that are free on each target.\n\nFor ZFS file systems, the number of inodes that can be created is dynamic and depends on the free space in the file system. The Free and Total inode counts reported for a ZFS file system are only an estimate based on the current usage for each target. The Used inode count is the actual number of inodes used by the file system.", "mimetype": "text/plain", "start_char_idx": 71078, "end_char_idx": 72734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bdcd0665-afef-4d12-84a3-80eb97def906": {"__data__": {"id_": "bdcd0665-afef-4d12-84a3-80eb97def906", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2178baf0-2e71-4958-9144-1216f2f1f9c9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9d2d2f2835c91fa78ba4beda6be0ff1e1d0eea992d649aaeb05eeb6323a79435", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80c0a361-b35b-40b6-b798-65ce79ea5f35", "node_type": "1", "metadata": {}, "hash": "5827bc7f4d7d9c4200817a521d36d974e2f3f2b5ab18d1ddb2f2c9b88561fb54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Note**\n\nThe `df -i` and `lfs df -i` commands show the minimum number of inodes that can be created in the file system at the current time. If the total number of objects available across all of the OSTs is smaller than those available on the MDT(s), taking into account the default file striping, then `df -i` will also report a smaller number of inodes than could be created. Running `lfs df -i` will report the actual number of inodes that are free on each target.\n\nFor ZFS file systems, the number of inodes that can be created is dynamic and depends on the free space in the file system. The Free and Total inode counts reported for a ZFS file system are only an estimate based on the current usage for each target. The Used inode count is the actual number of inodes used by the file system.\n\n**Examples**\n\n```\nclient$ lfs df\nUUID 1K-blocks Used Available Use% Mounted on\ntestfs-OST0000_UUID 9174328 1020024 8154304 11% /mnt/lustre[MDT:0]\ntestfs-OST0000_UUID 94181368 56330708 37850660 59% /mnt/lustre[OST:0]\ntestfs-OST0001_UUID 94181368 56385748 37795620 59% /mnt/lustre[OST:1]\ntestfs-OST0002_UUID 94181368 54352012 39829356 57% /mnt/lustre[OST:2]\nfilesystem summary: 282544104 167068468 39829356 57% /mnt/lustre\n[client1] $ lfs df -hv\nUUID bytes Used Available Use% Mounted on\ntestfs-MDT0000_UUID 8.7G 996.1M 7.8G 11% /mnt/lustre[MDT:0]\ntestfs-OST0000_UUID 89.8G 53.7G 36.1G 59% /mnt/lustre[OST:0] f\ntestfs-OST0001_UUID 89.8G 53.8G 36.0G 59% /mnt/lustre[OST:1] f\ntestfs-OST0002_UUID 89.8G 51.8G 38.0G 57% /mnt/lustre[OST:2] f\nfilesystem summary: 269.5G 159.3G 110.1G 59% /mnt/lustre\n[client1] $ lfs df -iH\nUUID Inodes IUsed IFree IUse% Mounted on\ntestfs-MDT0000_UUID 2.21M 41.9k 2.17M 1% /mnt/lustre[MDT:0]\ntestfs-OST0000_UUID 737.3k 12.1k 725.1k 1% /mnt/lustre[OST:0]\ntestfs-OST0001_UUID 737.3k 12.2k 725.0k 1% /mnt/lustre[OST:1]\ntestfs-OST0002_UUID 737.3k 12.2k 725.0k 1% /mnt/lustre[OST:2]\nfilesystem summary: 2.21M 41.9k 2.17M 1% /mnt/lustre[OST:2]\n```\n\n### Stripe Allocation Methods\n\nTwo stripe allocation methods are provided:\n\n- **Round-robin allocator** - When the OSTs have approximately the same amount of free space, the round-robin allocator alternates stripes between OSTs on different OSSs, so the OST used for stripe 0 of each file is evenly distributed among OSTs, regardless of the stripe count.", "mimetype": "text/plain", "start_char_idx": 71936, "end_char_idx": 74257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80c0a361-b35b-40b6-b798-65ce79ea5f35": {"__data__": {"id_": "80c0a361-b35b-40b6-b798-65ce79ea5f35", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b3ff7df-5b84-46ae-b374-90e4f2cfed78", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ebcc870bb602e7f287c676e73b552e5cd2cdb0e5ac3308a3e1e44a9346dfbe27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdcd0665-afef-4d12-84a3-80eb97def906", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2191e9aaf38b3f2855db7ede1c7336d74403bff17340d38d8f366f90417b7c96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In a simple example with eight OSTs numbered 0-7, objects would be allocated like this:\n\n  \n\n  ```\n  File 1: OST1, OST2, OST3, OST4\n  File 2: OST5, OST6, OST7\n  File 3: OST0, OST1, OST2, OST3, OST4, OST5\n  File 4: OST6, OST7, OST0\n  ```\n\n  \n\n  Here are several more sample round-robin stripe orders (each letter represents a different OST on a single OSS):\n\n  | 3: AAA           | One 3-OST OSS                           |\n  | ---------------- | --------------------------------------- |\n  | 3x3: ABABAB      | Two 3-OST OSSs                          |\n  | 3x4: BBABABA     | One 3-OST OSS (A) and one 4-OST OSS (B) |\n  | 3x5: BBABBABA    | One 3-OST OSS (A) and one 5-OST OSS (B) |\n  | 3x3x3: ABCABCABC | Three 3-OST OSSs                        |\n\n- **Weighted allocator** - When the free space difference between the OSTs becomes significant, the weighting algorithm is used to influence OST ordering based on size (amount of free space available on each OST) and location (stripes evenly distributed across OSTs). The weighted allocator fills the emptier OSTs faster, but uses a weighted random algorithm, so the OST with the most free space is not necessarily chosen each time.\n\nThe allocation method is determined by the amount of free-space imbalance on the OSTs. When free space is relatively balanced across OSTs, the faster round-robin allocator is used, which maximizes network balancing. The weighted allocator is used when any two OSTs are out of balance by more than the specified threshold (17% by default). The threshold between the two allocation methods is defined by the `qos_threshold_rr` parameter.\n\nTo temporarily set the `qos_threshold_r` to `25`, enter this command on each MDS:\n\n```\nmds# lctl set_param lod.fsname*.qos_threshold_rr=25\n```\n\n### Adjusting the Weighting Between Free Space and Location\n\nThe weighting priority used by the weighted allocator is set by the the `qos_prio_free` parameter. Increasing the value of `qos_prio_free` puts more weighting on the amount of free space available on each OST and less on how stripes are distributed across OSTs. The default value is `91` (percent). When the free space priority is set to `100` (percent), weighting is based entirely on free space and location is no longer used by the striping algorithm.\n\nTo permanently change the allocator weighting to `100`, enter this command on the MGS:\n\n```\nlctl conf_param fsname-MDT0000-*.lod.qos_prio_free=100\n```\n\n.\n\n**Note**\n\nWhen `qos_prio_free` is set to `100`, a weighted random algorithm is still used to assign stripes, so, for example, if OST2 has twice as much free space as OST1, OST2 is twice as likely to be used, but it is not guaranteed to be used.\n\n\n\n## Lustre Striping Internals\n\nIndividual files can only be striped over a finite number of OSTs, based on the maximum size of the attributes that can be stored on the MDT. If the MDT is ldiskfs-based without the `ea_inode` feature, a file can be striped across at most 160 OSTs. With ZFS-based MDTs, or if the `ea_inode` feature is enabled for an ldiskfs-based MDT, a file can be striped across up to 2000 OSTs.\n\nLustre inodes use an extended attribute to record on which OST each object is located, and the identifier each object on that OST. The size of the extended attribute is a function of the number of stripes.\n\nIf using an ldiskfs-based MDT, the maximum number of OSTs over which files can be striped can been raised to 2000 by enabling the `ea_inode` feature on the MDT:\n\n```\ntune2fs -O ea_inode /dev/mdtdev\n```\n\n**Note** \n\nSince Lustre 2.13 the `ea_inode` feature is enabled by default on all newly formatted ldiskfs MDT filesystems.\n\n**Note**\n\nThe maximum stripe count for a single file does not limit the maximum number of OSTs that are in the filesystem as a whole, only the maximum possible size and maximum aggregate bandwidth for the file.", "mimetype": "text/plain", "start_char_idx": 74258, "end_char_idx": 78098, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99d3d5ba-d4d0-4259-bc8c-705a0c3678e9": {"__data__": {"id_": "99d3d5ba-d4d0-4259-bc8c-705a0c3678e9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d2da4308c4c2038893c82e26d86f2d48a61d2c0bb621064910b010b55bae63a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5bd9449-2f9f-4ecd-8977-7ba6aab35435", "node_type": "1", "metadata": {}, "hash": "64d6ccfdc2ce29ddbfaa5f5fb273edc3634378535ee487003ca8fd4e4d1f4b08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in Lustre 2.11 \n\n# Data on MDT (DoM)\n\n- [Data on MDT (DoM)](#data-on-mdt-dom)\n  * [Introduction to Data on MDT (DoM)](#introduction-to-data-on-mdt-dom)\n  * [User Commands](#user-commands)\n    + [lfs setstripe for DoM files](#lfs-setstripe-for-dom-files)\n      - [Command](#command)\n      - [Example](#example)\n    + [Setting a default DoM layout to an existing directory](#setting-a-default-dom-layout-to-an-existing-directory)\n      - [Command](#command-1)\n      - [Example](#example-1)\n    + [DoM Stripe Size Restrictions](#dom-stripe-size-restrictions)\n      - [LFS limits for DoM component size](#lfs-limits-for-dom-component-size)\n      - [MDT Server Limits](#mdt-server-limits)\n    + [lfs getstripe for DoM files](#lfs-getstripe-for-dom-files)\n      - [Command](#command-2)\n      - [Examples](#examples)\n    + [lfs find for DoM files](#lfs-find-for-dom-files)\n      - [Command](#command-3)\n      - [Examples](#examples-1)\n    + [The dom_stripesize parameter](#the-dom_stripesize-parameter)\n      - [Get Command](#get-command)\n      - [Get Examples](#get-examples)\n      - [Temporary Set Command](#temporary-set-command)\n      - [Temporary Set Examples](#temporary-set-examples)\n      - [Persistent Set Command](#persistent-set-command)\n      - [Persistent Set Examples](#persistent-set-examples)\n    + [Disable DoM](#disable-dom)\n\n\nThis chapter describes Data on MDT (DoM).\n\n## Introduction to Data on MDT (DoM)\n\nThe Lustre Data on MDT (DoM) feature improves small file IO by placing small files directly on the MDT, and also improves large file IO by avoiding the OST being affected by small random IO that can cause device seeking and hurt the streaming IO performance. Therefore, users can expect more consistent performance for both small file IO and mixed IO patterns.\n\nThe layout of a DoM file is stored on disk as a composite layout and is a special case of Progressive File Layout (PFL). Please see [*the section called \u201cProgressive File Layout(PFL)\u201d*](#progressive-file-layoutpfl) for more information on PFL. For DoM files, the file layout is composed of the component of the file, which is placed on an MDT, and the rest of components are placed on OSTs, if needed. The first component is placed on the MDT in the MDT object data blocks. This component always has one stripe with size equal to the component size. Such a component with an MDT layout can be only the first component in composite layout. The rest of components are placed over OSTs as usual with a RAID0 layout. The OST components are not instantiated until a client writes or truncates the file beyond the size of the MDT component.\n\n## User Commands\n\nLustre provides the\u00a0`lfs setstripe`\u00a0command for users to create DoM files. Also, as usual,\u00a0`lfs getstripe`\u00a0command can be used to list the striping/component information for a given file, while\u00a0`lfs find`\u00a0command can be used to search the directory tree rooted at the given directory or file name for the files that match the given DoM component parameters, e.g. layout type.\n\n### lfs setstripe for DoM files\n\nThe `lfs setstripe` command is used to create DoM files.\n\n#### Command\n\n```\nlfs setstripe --component-end|-E end1 --layout|-L mdt \\\n        [--component-end|-E end2 [STRIPE_OPTIONS] ...] <filename>\n              \n```\n\nThe command above creates a file with the special composite layout, which defines the first component as an MDT component. The MDT component must start from offset 0 and ends at *end1*. The *end1* is also the stripe size of this component, and is limited by the `lod.*.dom_stripesize` of the MDT the file is created on. No other options are required for this component. The rest of the components use the normal syntax for composite files creation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3722, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5bd9449-2f9f-4ecd-8977-7ba6aab35435": {"__data__": {"id_": "d5bd9449-2f9f-4ecd-8977-7ba6aab35435", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d2da4308c4c2038893c82e26d86f2d48a61d2c0bb621064910b010b55bae63a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99d3d5ba-d4d0-4259-bc8c-705a0c3678e9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8257010ac73abe4610511a94ea78a142d28b5692a2a18cf81f6cb2dd992b43c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e24f2a49-8d1b-4530-942b-7167a89bf90b", "node_type": "1", "metadata": {}, "hash": "a1169a9718509539a50383c8adedca993a664bda9df3903baf38a7da3ae35a1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "layout type.\n\n### lfs setstripe for DoM files\n\nThe `lfs setstripe` command is used to create DoM files.\n\n#### Command\n\n```\nlfs setstripe --component-end|-E end1 --layout|-L mdt \\\n        [--component-end|-E end2 [STRIPE_OPTIONS] ...] <filename>\n              \n```\n\nThe command above creates a file with the special composite layout, which defines the first component as an MDT component. The MDT component must start from offset 0 and ends at *end1*. The *end1* is also the stripe size of this component, and is limited by the `lod.*.dom_stripesize` of the MDT the file is created on. No other options are required for this component. The rest of the components use the normal syntax for composite files creation.\n\n**Note**\n\nIf the next component doesn't specify striping, such as:\n\n```\nlfs setstripe -E 1M -L mdt -E EOF <filename>\n```\n\nThen that component get its settings from the default filesystem striping.\n\n#### Example\n\nThe command below creates a file with a DoM layout. The first component has an `mdt` layout and is placed on the MDT, covering [0, 1M). The second component covers [1M, EOF) and is striped over all available OSTs.\n\n```\nclient$ lfs setstripe -E 1M -L mdt -E -1 -S 4M -c -1 \\\n          /mnt/lustre/domfile\n```\n\nThe resulting layout is illustrated by [Figure 17, \u201cResulting file layout\u201d](#figure-17-resulting-file-layout).\n\n##### Figure 17. Resulting file layout\n\n ![Resulting file layout](./figures/DoM_Layout1.png)\n\nThe resulting can also be checked with `lfs getstripe` as shown below:\n\n```\nclient$ lfs getstripe /mnt/lustre/domfile\n/mnt/lustre/domfile\n  lcm_layout_gen:   2\n  lcm_mirror_count: 1\n  lcm_entry_count:  2\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   1048576\n      lmm_stripe_count:  0\n      lmm_stripe_size:   1048576\n      lmm_pattern:       mdt\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_objects:\n      \n    lcme_id:             2\n    lcme_flags:          0\n    lcme_extent.e_start: 1048576\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  -1\n      lmm_stripe_size:   4194304\n      lmm_pattern:       raid0\n      lmm_layout_gen:    65535\n      lmm_stripe_offset: -1\n```\n\nThe output above shows that the first component has size 1MB and pattern is 'mdt'. The second component is not instantiated yet, which is seen by `lcme_flags: 0`.", "mimetype": "text/plain", "start_char_idx": 3009, "end_char_idx": 5375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e24f2a49-8d1b-4530-942b-7167a89bf90b": {"__data__": {"id_": "e24f2a49-8d1b-4530-942b-7167a89bf90b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d2da4308c4c2038893c82e26d86f2d48a61d2c0bb621064910b010b55bae63a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5bd9449-2f9f-4ecd-8977-7ba6aab35435", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4677ec8f56f3f4b33148d8f64222e51ac826c462c11dd9a549ca25d5a993404c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0d07f09-f859-493e-b6cb-5c55c56d668c", "node_type": "1", "metadata": {}, "hash": "f33361746ec473d9ebb3a81795f0e5fe04be87f0b25afb117cf4d005075590b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second component is not instantiated yet, which is seen by `lcme_flags: 0`.\n\nIf more than 1MB of data is written to the file, then `lfs getstripe` output is changed accordingly:\n\n```\nclient$ lfs getstripe /mnt/lustre/domfile\n/mnt/lustre/domfile\n  lcm_layout_gen:   3\n  lcm_mirror_count: 1\n  lcm_entry_count:  2\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   1048576\n      lmm_stripe_count:  0\n      lmm_stripe_size:   1048576\n      lmm_pattern:       mdt\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 2\n      lmm_objects:\n      \n    lcme_id:             2\n    lcme_flags:          init\n    lcme_extent.e_start: 1048576\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   4194304\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x2:0x0] }\n      - 1: { l_ost_idx: 1, l_fid: [0x100010000:0x2:0x0] }\n```\n\nThe output above shows that the second component now has objects on OSTs with a 4MB stripe.\n\n### Setting a default DoM layout to an existing directory\n\nA DoM layout can be set on an existing directory as well. When set, all the files created after that will inherit this layout by default.\n\n#### Command\n\n```\nlfs setstripe --component-end|-E end1 --layout|-L mdt \\\n[--component-end|-E end2 [STRIPE_OPTIONS] ...] <dirname>\n```\n\n#### Example\n\n```\nclient$ mkdir /mnt/lustre/domdir\nclient$ touch /mnt/lustre/domdir/normfile\nclient$ lfs setstripe -E 1M -L mdt -E -1 /mnt/lustre/domdir/\nclient$ lfs getstripe -d /mnt/lustre/domdir\n  lcm_layout_gen:   0\n  lcm_mirror_count: 1\n  lcm_entry_count:  2\n    lcme_id:             N/A\n    lcme_flags:          0\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   1048576\n      stripe_count:  0    stripe_size:   1048576    \\\n      pattern:  mdt    stripe_offset:  -1\n    \n    lcme_id:             N/A\n    lcme_flags:          0\n    lcme_extent.e_start: 1048576\n    lcme_extent.e_end:   EOF\n      stripe_count:  1    stripe_size:   1048576    \\\n      pattern:  raid0    stripe_offset:  -1\n              \n```\n\nIn the output above, it can be seen that the directory has a default layout with a DoM component.", "mimetype": "text/plain", "start_char_idx": 5296, "end_char_idx": 7542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0d07f09-f859-493e-b6cb-5c55c56d668c": {"__data__": {"id_": "f0d07f09-f859-493e-b6cb-5c55c56d668c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d2da4308c4c2038893c82e26d86f2d48a61d2c0bb621064910b010b55bae63a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e24f2a49-8d1b-4530-942b-7167a89bf90b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4b441da2e403fa501c41697a69fedd4ac21cfb57bdec30d17af6b733d37fe45d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bef8dbb8-829d-49e4-b438-81772ad0d899", "node_type": "1", "metadata": {}, "hash": "d7f2c8484caa1512a8b5466853d51caaa7cd79a4702250491c0bfb79e6183f56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following example will check layouts of files in that directory:\n\n```\nclient$ touch /mnt/lustre/domdir/domfile\nclient$ lfs getstripe /mnt/lustre/domdir/normfile\n/mnt/lustre/domdir/normfile\nlmm_stripe_count:  2\nlmm_stripe_size:   1048576\nlmm_pattern:       raid0\nlmm_layout_gen:    0\nlmm_stripe_offset: 1\n  obdidx   objid   objid   group\n       1              3           0x3              0\n       0              3           0x3              0\n\nclient$ lfs getstripe /mnt/lustre/domdir/domfile\n/mnt/lustre/domdir/domfile\n  lcm_layout_gen:   2\n  lcm_mirror_count: 1\n  lcm_entry_count:  2\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   1048576\n      lmm_stripe_count:  0\n      lmm_stripe_size:   1048576\n      lmm_pattern:       mdt\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 2\n      lmm_objects:\n      \n    lcme_id:             2\n    lcme_flags:          0\n    lcme_extent.e_start: 1048576\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    65535\n      lmm_stripe_offset: -1\n```\n\nWe can see that first file **normfile** in that directory has an ordinary layout, whereas the file **domfile** inherits the directory default layout and is a DoM file.\n\n**Note**\n\nThe directory default layout setting will be inherited by new files even if the server DoM size limit will be set to a lower value.\n\n### DoM Stripe Size Restrictions\n\nThe maximum size of a DoM component is restricted in several ways to protect the MDT from being eventually filled with large files.\n\n#### LFS limits for DoM component size\n\n`lfs setstripe` allows for setting the component size for MDT layouts up to 1GB (this is a compile-time limit to avoid improper configuration), however, the size must also be aligned by 64KB due to the minimum stripe size in Lustre (see [Table 4, \u201cFile and file system limits\u201d](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#table-4-file-and-file-system-limits) `Minimum stripe size`). There is also a limit imposed on each file by `lfs setstripe -E end` that may be smaller than the MDT-imposed limit if this is better for a particular usage.\n\n#### MDT Server Limits\n\nThe `lod.$fsname-MDTxxxx.dom_stripesize` is used to control the per-MDT maximum size for a DoM component. Larger DoM components specified by the user will be truncated to the MDT-specified limit, and as such may be different on each MDT to balance DoM space usage on each MDT separately, if needed. It is 1MB by default and can be changed with the `lctl` tool. For more information on setting `dom_stripesize` please see [*the section called \u201c The dom_stripesize parameter\u201d*](#the-dom_stripesize-parameter).\n\n### lfs getstripe for DoM files\n\nThe `lfs getstripe` command is used to list the striping/component information for a given file. For DoM files, it can be used to check its layout and size.", "mimetype": "text/plain", "start_char_idx": 7544, "end_char_idx": 10522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bef8dbb8-829d-49e4-b438-81772ad0d899": {"__data__": {"id_": "bef8dbb8-829d-49e4-b438-81772ad0d899", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d2da4308c4c2038893c82e26d86f2d48a61d2c0bb621064910b010b55bae63a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0d07f09-f859-493e-b6cb-5c55c56d668c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "16044a6f1ba078391348f893b57dea7e521813d1a1ce5abd877c96900a895872", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4aa9d5d-217f-4f40-9bc7-3812d8ce8aa6", "node_type": "1", "metadata": {}, "hash": "92184115199e1fa5525ec88e0af26b900b97dbfc8805d362fd4c1dbbd5a5a305", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### MDT Server Limits\n\nThe `lod.$fsname-MDTxxxx.dom_stripesize` is used to control the per-MDT maximum size for a DoM component. Larger DoM components specified by the user will be truncated to the MDT-specified limit, and as such may be different on each MDT to balance DoM space usage on each MDT separately, if needed. It is 1MB by default and can be changed with the `lctl` tool. For more information on setting `dom_stripesize` please see [*the section called \u201c The dom_stripesize parameter\u201d*](#the-dom_stripesize-parameter).\n\n### lfs getstripe for DoM files\n\nThe `lfs getstripe` command is used to list the striping/component information for a given file. For DoM files, it can be used to check its layout and size.\n\n#### Command\n\n```\nlfs getstripe [--component-id|-I [comp_id]] [--layout|-L] \\\n              [--stripe-size|-S] <dirname|filename>\n```\n\n#### Examples\n\n```\nclient$ lfs getstripe -I1 /mnt/lustre/domfile\n/mnt/lustre/domfile\n  lcm_layout_gen:   3\n  lcm_mirror_count: 1\n  lcm_entry_count:  2\n    lcme_id:             1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   1048576\n      lmm_stripe_count:  0\n      lmm_stripe_size:   1048576\n      lmm_pattern:       mdt\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 2\n      lmm_objects:\n```\n\nShort info about the layout and size of DoM component can be obtained with the use of the `-L` option along with `-S` or `-E` options:\n\n```\nclient$ lfs getstripe -I1 -L -S /mnt/lustre/domfile\n      lmm_stripe_size:   1048576\n      lmm_pattern:       mdt\nclient$ lfs getstripe -I1 -L -E /mnt/lustre/domfile\n    lcme_extent.e_end:   1048576\n      lmm_pattern:       mdt\n```\n\nBoth commands return layout type and its size. The stripe size is equal to the extent size of component in case of DoM files, so both can be used to get size on the MDT.\n\n### lfs find for DoM files\n\nThe\u00a0`lfs find`\u00a0command can be used to search the directory tree rooted at the given directory or file name for the files that match the given parameters. The command below shows the new parameters for DoM files and their usages are similar to the\u00a0`lfs getstripe`\u00a0command.\n\n#### Command\n\n```\nlfs find <directory|filename> [--layout|-L] [...]\n```\n\n#### Examples\n\nFind all files with DoM layout under directory `/mnt/lustre`:\n\n```\nclient$ lfs find -L mdt /mnt/lustre\n/mnt/lustre/domfile\n/mnt/lustre/domdir\n/mnt/lustre/domdir/domfile\n                          \nclient$ lfs find -L mdt -type f /mnt/lustre\n/mnt/lustre/domfile\n/mnt/lustre/domdir/domfile\n                          \nclient$ lfs find -L mdt -type d /mnt/lustre\n/mnt/lustre/domdir\n```\n\nBy using this command you can find all DoM objects, only DoM files, or only directories with default DoM layout.\n\nFind the DoM files/dirs with a particular stripe size:\n\n```\nclient$ lfs find -L mdt -S -1200K -type f /mnt/lustre\n/mnt/lustre/domfile\n/mnt/lustre/domdir/domfile\n                          \nclient$ lfs find -L mdt -S +200K -type f /mnt/lustre\n/mnt/lustre/domfile\n/mnt/lustre/domdir/domfile\n```\n\nThe first command finds all DoM files with stripe size less than 1200KB. The second command above does the same for files with a stripe size greater than 200KB. In both cases, all DoM files are found because their DoM size is 1MB.\n\n### The dom_stripesize parameter\n\nThe MDT controls the default maximum DoM size on the server via the parameter\u00a0`dom_stripesize`\u00a0in the LOD device.", "mimetype": "text/plain", "start_char_idx": 9800, "end_char_idx": 13195, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4aa9d5d-217f-4f40-9bc7-3812d8ce8aa6": {"__data__": {"id_": "f4aa9d5d-217f-4f40-9bc7-3812d8ce8aa6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a83a9ded-1bf5-47a3-b6d1-9aba91141757", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d2da4308c4c2038893c82e26d86f2d48a61d2c0bb621064910b010b55bae63a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bef8dbb8-829d-49e4-b438-81772ad0d899", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "87be114f559ef9642165601cd3734fdbb2ebd82f5e93503a1ec1d477f1c31426", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Find the DoM files/dirs with a particular stripe size:\n\n```\nclient$ lfs find -L mdt -S -1200K -type f /mnt/lustre\n/mnt/lustre/domfile\n/mnt/lustre/domdir/domfile\n                          \nclient$ lfs find -L mdt -S +200K -type f /mnt/lustre\n/mnt/lustre/domfile\n/mnt/lustre/domdir/domfile\n```\n\nThe first command finds all DoM files with stripe size less than 1200KB. The second command above does the same for files with a stripe size greater than 200KB. In both cases, all DoM files are found because their DoM size is 1MB.\n\n### The dom_stripesize parameter\n\nThe MDT controls the default maximum DoM size on the server via the parameter\u00a0`dom_stripesize`\u00a0in the LOD device. The\u00a0`dom_stripesize`\u00a0can be set differently for each MDT, if necessary. The default value of the parameter is 1MB and can be changed with\u00a0`lctl`\u00a0tool.\n\n#### Get Command\n\n```\nlctl get_param lod.*MDT<index>*.dom_stripesize\n```\n\n#### Get Examples\n\nThe commands below get the maximum allowed DoM size on the server. The final command is an attempt to create a file with a larger size than the parameter setting and correctly fails.\n\n```\nmds# lctl get_param lod.*MDT0000*.dom_stripesize\nlod.lustre-MDT0000-mdtlov.dom_stripesize=1048576\n\nmds# lctl get_param -n lod.*MDT0000*.dom_stripesize\n1048576\n\nclient$ lfs setstripe -E 2M -L mdt /mnt/lustre/dom2mb\nCreate composite file /mnt/lustre/dom2mb failed. Invalid argument\nerror: setstripe: create composite file '/mnt/lustre/dom2mb' failed:\nInvalid argument\n```\n\n#### Temporary Set Command\n\nTo temporarily set the value of the parameter, the `lctl set_param` is used:\n\n```\nlctl set_param lod.*MDT<index>*.dom_stripesize=<value>\n              \n```\n\n#### Temporary Set Examples\n\nThe example below shows a change to the default DoM limit on the server to 64KB and try to create a file with 1MB DoM size after that.\n\n```\nmds# lctl set_param -n lod.*MDT0000*.dom_stripesize=64K\nmds# lctl get_param -n lod.*MDT0000*.dom_stripesize\n65536\n\nclient$ lfs setstripe -E 1M -L mdt /mnt/lustre/dom\nCreate composite file /mnt/lustre/dom failed. Invalid argument\nerror: setstripe: create composite file '/mnt/lustre/dom' failed:\nInvalid argument\n```\n\n#### Persistent Set Command\n\nTo persistently set the value of the parameter, the `lctl conf_param` command is used:\n\n```\nlctl conf_param <fsname>-MDT<index>.lod.dom_stripesize=<value>\n```\n\n#### Persistent Set Examples\n\nThe new value of the parameter is saved in config log permanently:\n\n```\nmgs# lctl conf_param lustre-MDT0000.lod.dom_stripesize=512K\nmds# lctl get_param -n lod.*MDT0000*.dom_stripesize\n524288\n```\n\nNew settings are applied in few seconds and saved persistently in server config.\n\n### Disable DoM\n\nWhen `lctl set_param` or `lctl conf_param` sets `dom_stripesize` to `0`, DoM component creation will be disabled on the selected server, and any *new* layouts with a specified DoM component will have that component removed from the file layout. Existing files and layouts with DoM components on that MDT are not changed.\n\n**Note**\n\nDoM files can still be created in existing directories with a default DoM layout.", "mimetype": "text/plain", "start_char_idx": 12523, "end_char_idx": 15598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "543bf6bc-1eac-4ae5-bf47-56de26276ade": {"__data__": {"id_": "543bf6bc-1eac-4ae5-bf47-56de26276ade", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/21-Lazy Size on MDT (LSoM).md", "file_name": "21-Lazy Size on MDT (LSoM).md", "file_type": "text/markdown", "file_size": 7262, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa0d9664-d941-45d1-8d3c-afe24618fbb8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/21-Lazy Size on MDT (LSoM).md", "file_name": "21-Lazy Size on MDT (LSoM).md", "file_type": "text/markdown", "file_size": 7262, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e5b36b6e8a99229817f5aee6738f7124ffcee0c046aa5af6d8d32ae5f26acf05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b003578-7bd1-4359-af6c-e8c6b9d49041", "node_type": "1", "metadata": {}, "hash": "e2202b564b18ea9421f133b524807cc0e2f5bca890c386eaa75cc00d9668d0cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in Lustre 2.12 \n\n## Lazy Size on MDT (LSoM)\n\n- [Lazy Size on MDT (LSoM)](#lazy-size-on-mdt-lsom)\n- [Introduction to Lazy Size on MDT (LSoM)](#introduction-to-lazy-size-on-mdt-lsom)\n- [Enable LSoM](#enable-lsom)\n- [User Commands](#user-commands)\n  * [lfs getsom for LSoM data](#lfs-getsom-for-lsom-data)\n    + [lfs getsom Command](#lfs-getsom-command)\n  * [Syncing LSoM data](#syncing-lsom-data)\n    + [llsom_sync Command](#llsom_sync-command)\n\nThis chapter describes Lazy Size on MDT (LSoM).\n\n## Introduction to Lazy Size on MDT (LSoM)\n\nIn the Lustre file system, MDSs store the ctime, mtime, owner, and other file attributes. The OSSs store the size and number of blocks used for each file. To obtain the correct file size, the client must contact each OST that the file is stored across, which means multiple RPCs to get the size and blocks for a file when a file is striped over multiple OSTs. The Lazy Size on MDT (LSoM) feature stores the file size on the MDS and avoids the need to fetch the file size from the OST(s) in cases where the application understands that the size may not be accurate. Lazy means there is no guarantee of the accuracy of the attributes stored on the MDS.\n\nSince many Lustre installations use SSD for MDT storage, the motivation for the LSoM work is to speed up the time it takes to get the size of a file from the Lustre file system by storing that data on the MDTs. We expect this feature to be initially used by Lustre policy engines that scan the backend MDT storage, make decisions based on broad size categories, and do not depend on a totally accurate file size. Examples include Lester, Robinhood, Zester, and various vendor offerings. Future improvements will allow the LSoM data to be accessed by tools such as `lfs find`.\n\n## Enable LSoM\n\nLSoM is always enabled and nothing needs to be done to enable the feature for fetching the LSoM data when scanning the MDT inodes with a policy engine. It is also possible to access the LSoM data on the client via the `lfs getsom` command. Because the LSoM data is currently accessed on the client via the xattr interface, the`xattr_cache` will cache the file size and block count on the client as long as the inode is cached. In most cases this is desirable, since it improves access to the LSoM data. However, it also means that the LSoM data may be stale if the file size is changed after the xattr is first accessed or if the xattr is accessed shortly after the file is first created.\n\nIf it is necessary to access up-to-date LSoM data that has gone stale, it is possible to flush the xattr cache from the client by cancelling the MDC locks via `lctl set_param ldlm.namespaces.*mdc*.lru_size=clear`. Otherwise, the file attributes will be dropped from the client cache if the file has not been accessed before the LDLM lock timeout. The timeout is stored via `lctl get_param ldlm.namespaces.*mdc*.lru_max_age`.\n\nIf repeated access to LSoM attributes for files that are recently created or frequently modified from a specific client, such as an HSM agent node, it is possible to disable xattr caching on a client via: `lctl set_param llite.*.xattr_cache=0`. This may cause extra overhead when accessing files, and is not recommended for normal usage.\n\n## User Commands\n\nLustre provides the `lfs getsom` command to list file attributes that are stored on the MDT.\n\nThe `llsom_sync` command allows the user to sync the file attributes on the MDT with the valid/up-to-date data on the OSTs. `llsom_sync` is called on the client with the Lustre file system mount point. `llsom_sync` uses Lustre MDS changelogs and, thus, a changelog user must be registered to use this utility.\n\n### lfs getsom for LSoM data\nThe `lfs getsom` command lists file attributes that are stored on the MDT.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b003578-7bd1-4359-af6c-e8c6b9d49041": {"__data__": {"id_": "5b003578-7bd1-4359-af6c-e8c6b9d49041", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/21-Lazy Size on MDT (LSoM).md", "file_name": "21-Lazy Size on MDT (LSoM).md", "file_type": "text/markdown", "file_size": 7262, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa0d9664-d941-45d1-8d3c-afe24618fbb8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/21-Lazy Size on MDT (LSoM).md", "file_name": "21-Lazy Size on MDT (LSoM).md", "file_type": "text/markdown", "file_size": 7262, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e5b36b6e8a99229817f5aee6738f7124ffcee0c046aa5af6d8d32ae5f26acf05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "543bf6bc-1eac-4ae5-bf47-56de26276ade", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/21-Lazy Size on MDT (LSoM).md", "file_name": "21-Lazy Size on MDT (LSoM).md", "file_type": "text/markdown", "file_size": 7262, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "57862089c3731789c3a5240d5492a3c84b56bf4bac1d98023247b41e674852f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8f8d992-cf93-4c9e-81de-9729f18de176", "node_type": "1", "metadata": {}, "hash": "47e801a1da33e820328293ccc0453830b527bde29674fdee5a0afcc747d99295", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This may cause extra overhead when accessing files, and is not recommended for normal usage.\n\n## User Commands\n\nLustre provides the `lfs getsom` command to list file attributes that are stored on the MDT.\n\nThe `llsom_sync` command allows the user to sync the file attributes on the MDT with the valid/up-to-date data on the OSTs. `llsom_sync` is called on the client with the Lustre file system mount point. `llsom_sync` uses Lustre MDS changelogs and, thus, a changelog user must be registered to use this utility.\n\n### lfs getsom for LSoM data\nThe `lfs getsom` command lists file attributes that are stored on the MDT. `lfs getsom` is called with the full path and file name for a file on the Lustre file system. If no flags are used, then all file attributes stored on the MDS will be shown.\n\n#### lfs getsom Command\n\n```\nlfs getsom [-s] [-b] [-f] <filename>\n```\n\nThe various `lfs getsom` options are listed and described below.\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `-s`       | Only show the size value of the LSoM data for a given file. This is an optional flag |\n| `-b`       | Only show the blocks value of the LSoM data for a given file. This is an optional flag |\n| `-f`       | Only show the flag value of the LSoM data for a given file. This is an optional flag. Valid flags are:SOM_FL_UNKNOWN = 0x0000 - Unknown or no SoM data, must get size from OSTs.SOM_FL_STRICT = 0x0001 - Known strictly correct, FLR file (SoM guaranteed)SOM_FL_STALE = 0x0002 - Known stale -was right at some point in the past, but it is known (or likely) to be incorrect now (e.g. opened for write)SOM_FL_LAZY= 0x0004 - Approximate, may never have been strictly correct, need to sync SOM data to achieve eventual consistency. |\n\n### Syncing LSoM data\n\nThe `llsom_sync` command allows the user to sync the file attributes on the MDT with the valid/up-to-date data on the OSTs. `llsom_sync` is called on the client with the client mount point for the Lustre file system. `llsom_sync` uses Lustre MDS changelogs and, thus, a changelog user must be registered to use this utility.\n\n#### llsom_sync Command\n\n```\nllsom_sync --mdt|-m <mdt> --user|-u <user_id>\n              [--daemonize|-d] [--verbose|-v] [--interval|-i] [--min-age|-a]\n              [--max-cache|-c] [--sync|-s] <lustre_mount_point>\n```\n\nThe various `llsom_sync` options are listed and described below.\n\n| **Option**              | **Description**                                              |\n| ----------------------- | ------------------------------------------------------------ |\n| `--mdt | -m <mdt>`      | The metadata device which need to be synced the LSoM xattr of files. A changelog user must be registered for this device.Required flag. |\n| `--user | -u <user_id>` | The changelog user id for the MDT device. Required flag.     |\n| `--daemonize | -d`      | Optional flag to \u201cdaemonize\u201d the program. In daemon mode, the utility will scan, process the changelog records and sync the LSoM xattr for files periodically. |\n| `--verbose | -v`        | Optional flag to produce verbose output.                     |\n| `--interval | -i`       | Optional flag for the time interval to scan the Lustre changelog and process the log record in daemon mode. |\n| `--min-age | -a`        | Optional flag for the time that `llsom_sync` tool will not try to sync the LSoM data for any files closed less than this many seconds old. The default min-age value is 600s(10 minutes). |\n| `--max-cache | -c`      | Optional flag for the total memory used for the FID cache which can be with a suffix [KkGgMm].The default max-cache value is 256MB. For the parameter value < 100, it is taken as the percentage of total memory size used for the FID cache instead of the cache size.", "mimetype": "text/plain", "start_char_idx": 3154, "end_char_idx": 6990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8f8d992-cf93-4c9e-81de-9729f18de176": {"__data__": {"id_": "c8f8d992-cf93-4c9e-81de-9729f18de176", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/21-Lazy Size on MDT (LSoM).md", "file_name": "21-Lazy Size on MDT (LSoM).md", "file_type": "text/markdown", "file_size": 7262, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa0d9664-d941-45d1-8d3c-afe24618fbb8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/21-Lazy Size on MDT (LSoM).md", "file_name": "21-Lazy Size on MDT (LSoM).md", "file_type": "text/markdown", "file_size": 7262, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e5b36b6e8a99229817f5aee6738f7124ffcee0c046aa5af6d8d32ae5f26acf05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b003578-7bd1-4359-af6c-e8c6b9d49041", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/21-Lazy Size on MDT (LSoM).md", "file_name": "21-Lazy Size on MDT (LSoM).md", "file_type": "text/markdown", "file_size": 7262, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5f8dc2ee9f0b6120b3f8e0f7ab899e26d3863f3f2655954bab6c096d14f3e471", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `--verbose | -v`        | Optional flag to produce verbose output.                     |\n| `--interval | -i`       | Optional flag for the time interval to scan the Lustre changelog and process the log record in daemon mode. |\n| `--min-age | -a`        | Optional flag for the time that `llsom_sync` tool will not try to sync the LSoM data for any files closed less than this many seconds old. The default min-age value is 600s(10 minutes). |\n| `--max-cache | -c`      | Optional flag for the total memory used for the FID cache which can be with a suffix [KkGgMm].The default max-cache value is 256MB. For the parameter value < 100, it is taken as the percentage of total memory size used for the FID cache instead of the cache size. |\n| `--sync | -s`           | Optional flag to sync file data to make the dirty data out of cache to ensure the blocks count is correct when update the file LSoM xattr. This option could hurt server performance significantly if thousands of fsync requests are sent. |", "mimetype": "text/plain", "start_char_idx": 6252, "end_char_idx": 7258, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bbad918d-30ae-4ff3-8757-5e7eaa4cfaaf": {"__data__": {"id_": "bbad918d-30ae-4ff3-8757-5e7eaa4cfaaf", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bee065b7-2b55-4c0f-840e-80159963033b", "node_type": "1", "metadata": {}, "hash": "8a4696bd42ad464cfb5a8d48d2165616bc28ed10b0b0e429e916cc3df5138785", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in Lustre 2.11\n\n## File Level Redundancy (FLR)\n\n- [File Level Redundancy (FLR)](#file-level-redundancy-flr)\n- [Introduction](#introduction)\n- [Operations](#operations)\n  * [Creating a Mirrored File or Directory](#creating-a-mirrored-file-or-directory)\n  * [Extending a Mirrored File](#extending-a-mirrored-file)\n  * [Splitting a Mirrored File](#splitting-a-mirrored-file)\n  * [Resynchronizing out-of-sync Mirrored File(s)](#resynchronizing-out-of-sync-mirrored-files)\n  * [Verifying Mirrored File(s)](#verifying-mirrored-files)\n  * [Finding Mirrored File(s)](#finding-mirrored-files)\n- [Interoperability](#interoperability)\n\nThis chapter describes File Level Redundancy (FLR).\n\n## Introduction\n\nThe Lustre file system was initially designed and implemented for HPC use. It has been working well on high-end storage that has internal redundancy and fault-tolerance. However, despite the expense and complexity of these storage systems, storage failures still occur, and before release 2.11, Lustre could not be more reliable than the individual storage and servers\u2019 components on which it was based. The Lustre file system had no mechanism to mitigate storage hardware failures and files would become inaccessible if a server was inaccessible or otherwise out of service.\n\nWith the File Level Redundancy (FLR) feature introduced in Lustre Release 2.11, any Lustre file can store the same data on multiple OSTs in order for the system to be robust in the event of storage failures or other outages. With the choice of multiple mirrors, the best suited mirror can be chosen to satisfy an individual request, which has a direct impact on IO availability. Furthermore, for files that are concurrently read by many clients (e.g. input decks, shared libraries, or executables) the aggregate parallel read performance of a single file can be improved by creating multiple mirrors of the file data.\n\nThe first phase of the FLR feature has been implemented with delayed write ([Figure 18, \u201cFLR Delayed Write\u201d](#figure-18-flr-delayed-write)). While writing to a mirrored file, only one primary or preferred mirror will be updated directly during the write, while other mirrors will be simply marked as stale. The file can subsequently return to a mirrored state again by synchronizing among mirrors with command line tools (run by the user or administrator directly or via automated monitoring tools).\n\n##### Figure 18. FLR Delayed Write\n\n![FLR Delayed Write Diagram](figures/FLR_DelayedWrite.png) \n\n## Operations\n\nLustre provides\u00a0`lfs mirror`\u00a0command line tools for users to operate on mirrored files or directories.\n\n### Creating a Mirrored File or Directory\n\n**Command:**\n\n```\nlfs mirror create <--mirror-count|-N[mirror_count]\n[setstripe_options|[--flags<=flags>]]> ... <filename|directory>\n```\n\nThe above command will create a mirrored file or directory specified by *filename* or *directory*, respectively.\n\n| Option                           | Description                                                  |\n| -------------------------------- | ------------------------------------------------------------ |\n| --mirror-count\\|-N[mirror_count] | Indicates the number of mirrors to be created with the following setstripe options. It can be repeated multiple times to separate mirrors that have different layouts.The *mirror_count* argument is optional and defaults to `1` if it is not specified; if specified, it must follow the option without a space. |\n| setstripe_options                | Specifies a specific layout for the mirror. It can be a plain layout with a specific striping pattern or a composite layout, such as [the section called \u201cProgressive File Layout(PFL)\u201d](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#progressive-file-layoutpfl). The options are the same as those for the `lfs setstripe` command.If *setstripe_options* are not specified, then the stripe options inherited from the previous component will be used. If there is no previous component, then the `stripe_count` and`stripe_size` options inherited from the filesystem-wide default values will be used, and the OST `pool_name` inherited from the parent directory will be used. |\n| --flags<=flags>                  | Sets flags to the mirror to be created.Only the `prefer` flag is supported at this time. This flag will be set to all components that belong to the corresponding mirror.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bee065b7-2b55-4c0f-840e-80159963033b": {"__data__": {"id_": "bee065b7-2b55-4c0f-840e-80159963033b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbad918d-30ae-4ff3-8757-5e7eaa4cfaaf", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "78f2fe4ac265b1d8ea6f51974e9e41ec1078197d8d36dba9223d8fa938f20a53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15170e97-1ce3-4a72-843d-4eaa53b9cfaa", "node_type": "1", "metadata": {}, "hash": "4eaca83c9328317cec30de23f5463dd0bb07bebbb473a18ae2be9ea097a3b790", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The options are the same as those for the `lfs setstripe` command.If *setstripe_options* are not specified, then the stripe options inherited from the previous component will be used. If there is no previous component, then the `stripe_count` and`stripe_size` options inherited from the filesystem-wide default values will be used, and the OST `pool_name` inherited from the parent directory will be used. |\n| --flags<=flags>                  | Sets flags to the mirror to be created.Only the `prefer` flag is supported at this time. This flag will be set to all components that belong to the corresponding mirror. The `prefer` flag gives a hint to Lustre for which mirrors should be used to serve I/O. When a mirrored file is being read, the component(s) with the `prefer` flag is likely to be picked to serve the read; and when a mirrored file is prepared to be written, the MDT will tend to choose the component with the `prefer` flag set and mark the other components with overlapping extents as stale. This flag just provides a hint to Lustre, which means Lustre may still choose mirrors without this flag set, for instance, if all preferred mirrors are unavailable when the I/O occurs. This flag can be set on multiple components.**Note:** This flag will be set to all components that belong to the corresponding mirror. The `--comp-flags` option also exists, which can be set to individual components at mirror creation time. |\n\n**Note:** For redundancy and fault-tolerance, users need to make sure that different mirrors must be on different OSTs, even OSSs and racks. An understanding of cluster topology is necessary to achieve this architecture. In the initial implementation the use of the existing OST pools mechanism will allow separating OSTs by any arbitrary criteria: i.e. fault domain. In practice, users can take advantage of OST pools by grouping OSTs by topological information. Therefore, when creating a mirrored file, users can indicate which OST pools can be used by mirrors.", "mimetype": "text/plain", "start_char_idx": 3779, "end_char_idx": 5779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15170e97-1ce3-4a72-843d-4eaa53b9cfaa": {"__data__": {"id_": "15170e97-1ce3-4a72-843d-4eaa53b9cfaa", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bee065b7-2b55-4c0f-840e-80159963033b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "eefebf8cde786f52a872edab34ae77143ca90868258127629814fb5995e59ef8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eab2a285-0e6e-4f09-a752-c24cc6fe9a82", "node_type": "1", "metadata": {}, "hash": "54a4348ac2df78c02b601e18896d4bc4226ecf74a1cb50bbd9e8e2ae55c1ead4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This flag can be set on multiple components.**Note:** This flag will be set to all components that belong to the corresponding mirror. The `--comp-flags` option also exists, which can be set to individual components at mirror creation time. |\n\n**Note:** For redundancy and fault-tolerance, users need to make sure that different mirrors must be on different OSTs, even OSSs and racks. An understanding of cluster topology is necessary to achieve this architecture. In the initial implementation the use of the existing OST pools mechanism will allow separating OSTs by any arbitrary criteria: i.e. fault domain. In practice, users can take advantage of OST pools by grouping OSTs by topological information. Therefore, when creating a mirrored file, users can indicate which OST pools can be used by mirrors.\n\n**Examples:**\n\nThe following command creates a mirrored file with 2 plain layout mirrors:\n\n```\nclient# lfs mirror create -N -S 4M -c 2 -p flash \\\n                          -N -c -1 -p archive /mnt/testfs/file1\n```\n\nThe following command displays the layout information of the mirrored file `/mnt/testfs/file1`:\n\n```\nclient# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    2\n  lcm_mirror_count:  2\n  lcm_entry_count:   2\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   4194304\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 1\n      lmm_pool:          flash\n      lmm_objects:\n      - 0: { l_ost_idx: 1, l_fid: [0x100010000:0x2:0x0] }\n      - 1: { l_ost_idx: 0, l_fid: [0x100000000:0x2:0x0] }\n\n    lcme_id:             131074\n    lcme_mirror_id:      2\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  6\n      lmm_stripe_size:   4194304\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 3\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 3, l_fid: [0x100030000:0x2:0x0] }\n      - 1: { l_ost_idx: 4, l_fid: [0x100040000:0x2:0x0] }\n      - 2: { l_ost_idx: 5, l_fid: [0x100050000:0x2:0x0] }\n      - 3: { l_ost_idx: 6, l_fid: [0x100060000:0x2:0x0] }\n      - 4: { l_ost_idx: 7, l_fid: [0x100070000:0x2:0x0] }\n      - 5: { l_ost_idx: 2, l_fid: [0x100020000:0x2:0x0] }\n```\n\nThe first mirror has 4MB stripe size and two stripes across OSTs in the \u201cflash\u201d OST pool. The second mirror has 4MB stripe size inherited from the first mirror, and stripes across all of the available OSTs in the \u201carchive\u201d OST pool.\n\nAs mentioned above, it is recommended to use the `--pool|-p` option (one of the `lfs setstripe` options) with OST pools configured with independent fault domains to ensure different mirrors will be placed on different OSTs, servers, and/or racks, thereby improving availability and performance. If the setstripe options are not specified, it is possible to create mirrors with objects on the same OST(s), which would remove most of the benefit of using replication.\n\nIn the layout information printed by `lfs getstripe`, `lcme_mirror_id` shows mirror ID, which is the unique numerical identifier for a mirror. And `lcme_flags` shows mirrored component flags.", "mimetype": "text/plain", "start_char_idx": 4971, "end_char_idx": 8267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eab2a285-0e6e-4f09-a752-c24cc6fe9a82": {"__data__": {"id_": "eab2a285-0e6e-4f09-a752-c24cc6fe9a82", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15170e97-1ce3-4a72-843d-4eaa53b9cfaa", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "671ab83a3a6d23e69ea442a5ea0971da1abe1fcd604b34486b886df3c63bded9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74b23b5e-9aac-44f5-825c-3abf739e1aac", "node_type": "1", "metadata": {}, "hash": "7cb46efa452b05380fb0698783c3c776d64c507939092ea6f8188627351d22d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second mirror has 4MB stripe size inherited from the first mirror, and stripes across all of the available OSTs in the \u201carchive\u201d OST pool.\n\nAs mentioned above, it is recommended to use the `--pool|-p` option (one of the `lfs setstripe` options) with OST pools configured with independent fault domains to ensure different mirrors will be placed on different OSTs, servers, and/or racks, thereby improving availability and performance. If the setstripe options are not specified, it is possible to create mirrors with objects on the same OST(s), which would remove most of the benefit of using replication.\n\nIn the layout information printed by `lfs getstripe`, `lcme_mirror_id` shows mirror ID, which is the unique numerical identifier for a mirror. And `lcme_flags` shows mirrored component flags. Valid flag names are:\n\n- `init` - indicates mirrored component has been initialized (has allocated OST objects).\n- `stale` - indicates mirrored component does not have up-to-date data. Stale components will not be used for read or write operations, and need to be resynchronized by running `lfs mirror resync` command before they can be accessed again.\n- `prefer` - indicates mirrored component is preferred for read or write. For example, the mirror is located on SSD-based OSTs or is closer, fewer hops, on the network to the client. This flag can be set by users at mirror creation time.\n\nThe following command creates a mirrored file with 3 PFL mirrors:\n\n```\nclient# lfs mirror create -N -E 4M -p flash --flags=prefer -E eof -c 2 \\\n-N -E 16M -S 8M -c 4 -p archive --comp-flags=prefer -E eof -c -1 \\\n-N -E 32M -c 1 -p none -E eof -c -1 /mnt/testfs/file2\n```\n\nThe following command displays the layout information of the mirrored file `/mnt/testfs/file2`:\n\n```\nclient# lfs getstripe /mnt/testfs/file2\n/mnt/testfs/file2\n  lcm_layout_gen:    6\n  lcm_mirror_count:  3\n  lcm_entry_count:   6\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init,prefer\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 1\n      lmm_pool:          flash\n      lmm_objects:\n      - 0: { l_ost_idx: 1, l_fid: [0x100010000:0x3:0x0] }\n\n    lcme_id:             65538\n    lcme_mirror_id:      1\n    lcme_flags:          prefer\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n      lmm_pool:          flash\n\n    lcme_id:             131075\n    lcme_mirror_id:      2\n    lcme_flags:          init,prefer\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   16777216\n      lmm_stripe_count:  4\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x3:0x0] }\n      - 1: { l_ost_idx: 5, l_fid: [0x100050000:0x3:0x0] }\n      - 2: { l_ost_idx: 6, l_fid: [0x100060000:0x3:0x0] }\n      - 3: { l_ost_idx: 7,", "mimetype": "text/plain", "start_char_idx": 7465, "end_char_idx": 10651, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74b23b5e-9aac-44f5-825c-3abf739e1aac": {"__data__": {"id_": "74b23b5e-9aac-44f5-825c-3abf739e1aac", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eab2a285-0e6e-4f09-a752-c24cc6fe9a82", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ab8ba523be525e4124db01a528b34a245a266b4d98cfb0a26bcd38ffd18794a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee3f4a17-ccb1-4467-ac74-247980d83251", "node_type": "1", "metadata": {}, "hash": "8a80233df54696a5807ac45f2098e671e9c72262042364c1b99bd1cdda2650f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "e_start: 0\n    lcme_extent.e_end:   16777216\n      lmm_stripe_count:  4\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x3:0x0] }\n      - 1: { l_ost_idx: 5, l_fid: [0x100050000:0x3:0x0] }\n      - 2: { l_ost_idx: 6, l_fid: [0x100060000:0x3:0x0] }\n      - 3: { l_ost_idx: 7, l_fid: [0x100070000:0x3:0x0] }\n\n    lcme_id:             131076\n    lcme_mirror_id:      2\n    lcme_flags:          0\n    lcme_extent.e_start: 16777216\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  6\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n      lmm_pool:          archive\n\n    lcme_id:             196613\n    lcme_mirror_id:      3\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   33554432\n      lmm_stripe_count:  1\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x3:0x0] }\n\n    lcme_id:             196614\n    lcme_mirror_id:      3\n    lcme_flags:          0\n    lcme_extent.e_start: 33554432\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  -1\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n```\n\nFor the first mirror, the first component inherits the stripe count and stripe size from filesystem-wide default values. The second component inherits the stripe size and OST pool from the first component, and has two stripes. Both of the components are allocated from the \u201cflash\u201d OST pool. Also, the flag `prefer` is applied to all the components of the first mirror, which tells the client to read data from those components whenever they are available.\n\nFor the second mirror, the first component has an 8MB stripe size and 4 stripes across OSTs in the \u201carchive\u201d OST pool. The second component inherits the stripe size and OST pool from the first component, and stripes across all of the available OSTs in the \u201carchive\u201d OST pool. The flag `prefer` is only applied to the first component.\n\nFor the third mirror, the first component inherits the stripe size of 8MB from the last component of the second mirror, and has one single stripe. The OST pool name is cleared and inherited from the parent directory (if it was set with OST pool name). The second component inherits stripe size from the first component, and stripes across all of the available OSTs.\n\n### Extending a Mirrored File\n\n**Command:**\n\n```\nlfs mirror extend [--no-verify] <--mirror-count|-N[mirror_count]\n[setstripe_options|-f <victim_file>]> ... <filename>\n```\n\nThe above command will append mirror(s) indicated by `setstripe options` or just take the layout from existing file *victim_file* into the file *filename*. The *filename* must be an existing file, however, it can be a mirrored or regular non-mirrored file. If it is a non-mirrored file, the command will convert it to a mirrored file.", "mimetype": "text/plain", "start_char_idx": 10209, "end_char_idx": 13352, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee3f4a17-ccb1-4467-ac74-247980d83251": {"__data__": {"id_": "ee3f4a17-ccb1-4467-ac74-247980d83251", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74b23b5e-9aac-44f5-825c-3abf739e1aac", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "779ded8d767cc87bb64ade4b2a940c194eea284e30c0a221ce7e89c5720a96d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73c8af7c-febb-4309-bd71-f09475fdfd85", "node_type": "1", "metadata": {}, "hash": "102021147997912acb81b41bdda13ee40c063e2714288fcad3851d2ab6598c21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The OST pool name is cleared and inherited from the parent directory (if it was set with OST pool name). The second component inherits stripe size from the first component, and stripes across all of the available OSTs.\n\n### Extending a Mirrored File\n\n**Command:**\n\n```\nlfs mirror extend [--no-verify] <--mirror-count|-N[mirror_count]\n[setstripe_options|-f <victim_file>]> ... <filename>\n```\n\nThe above command will append mirror(s) indicated by `setstripe options` or just take the layout from existing file *victim_file* into the file *filename*. The *filename* must be an existing file, however, it can be a mirrored or regular non-mirrored file. If it is a non-mirrored file, the command will convert it to a mirrored file.\n\n| Option                           | Description                                                  |\n| -------------------------------- | ------------------------------------------------------------ |\n| --mirror-count\\|-N[mirror_count] | Indicates the number of mirrors to be added with the following `setstripe options`. It can be repeated multiple times to separate mirrors that have different layouts.The *mirror_count* argument is optional and defaults to `1` if it is not specified; if specified, it must follow the option without a space. |\n| setstripe_options                | Specifies a specific layout for the mirror. It can be a plain layout with specific striping pattern or a composite layout, such as [the section called \u201cProgressive File Layout(PFL)\u201d](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#progressive-file-layoutpfl). The options are the same as those for the `lfs setstripe` command. If *setstripe_options* are not specified, then the stripe options inherited from the previous component will be used. If there is no previous component, then the `stripe_count` and`stripe_size` options inherited from filesystem-wide default values will be used, and the OST`pool_name` inherited from parent directory will be used. |\n| -f <victim_file>                 | If *victim_file* exists, the command will split the layout from that file and use it as a mirror added to the mirrored file. After the command is finished, the *victim_file* will be removed.                                                                                                               **Note**:                                                                                                                                                                                                                       The *setstripe_options* cannot be specified with `-f <victim_file>` option in one command line. |\n| --no-verify                      | If *victim_file* is specified, the command will verify that the file contents from *victim_file* are the same as *filename*. Otherwise, the command will return a failure. However, the option `--no-verify` can be used to override this verification. This option can save significant time on file comparison if the file size is large, but use it only when the file contents are known to be the same. |\n\n**Note**: The `lfs mirror extend` operation won't be applied to the directory.\n\n**Examples:**\n\nThe following commands create a non-mirrored file, convert it to a mirrored file, and extend it with a plain layout mirror:\n\n```\n# lfs setstripe -p flash /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\nlmm_stripe_count:  1\nlmm_stripe_size:   1048576\nlmm_pattern:       raid0\nlmm_layout_gen:    0\nlmm_stripe_offset: 0\nlmm_pool:          flash\n        obdidx           objid           objid           group\n             0               4            0x4                0\n\n# lfs mirror extend -N -S 8M -c -1 -p archive /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    2\n  lcm_mirror_count:  2\n  lcm_entry_count:   2\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_pool:          flash\n      lmm_objects:\n      - 0: { l_ost_idx: 0,", "mimetype": "text/plain", "start_char_idx": 12626, "end_char_idx": 16850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73c8af7c-febb-4309-bd71-f09475fdfd85": {"__data__": {"id_": "73c8af7c-febb-4309-bd71-f09475fdfd85", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee3f4a17-ccb1-4467-ac74-247980d83251", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "774461af606aef0eba676317863e19c91e3d5e4bc497a9ab5247f77e4740835f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "436b5466-ee41-4372-9565-a3e3131c2949", "node_type": "1", "metadata": {}, "hash": "7830d06a70ac9c394e9957790d8924a399e74d96ff88befd2f7cd7ce2748a228", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_pool:          flash\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x4:0x0] }\n\n    lcme_id:             131073\n    lcme_mirror_id:      2\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  6\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 3\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 3, l_fid: [0x100030000:0x3:0x0] }\n      - 1: { l_ost_idx: 4, l_fid: [0x100040000:0x4:0x0] }\n      - 2: { l_ost_idx: 5, l_fid: [0x100050000:0x4:0x0] }\n      - 3: { l_ost_idx: 6, l_fid: [0x100060000:0x4:0x0] }\n      - 4: { l_ost_idx: 7, l_fid: [0x100070000:0x4:0x0] }\n      - 5: { l_ost_idx: 2, l_fid: [0x100020000:0x3:0x0] }\n```\n\nThe following commands split the PFL layout from a *victim_file* and use it as a mirror added to the mirrored file `/mnt/testfs/file1` created in the above example without data verification:\n\n```\n# lfs setstripe -E 16M -c 2 -p none \\\n                -E eof -c -1 /mnt/testfs/victim_file\n# lfs getstripe /mnt/testfs/victim_file\n/mnt/testfs/victim_file\n  lcm_layout_gen:    2\n  lcm_mirror_count:  1\n  lcm_entry_count:   2\n    lcme_id:             1\n    lcme_mirror_id:      0\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   16777216\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 5\n      lmm_objects:\n      - 0: { l_ost_idx: 5, l_fid: [0x100050000:0x5:0x0] }\n      - 1: { l_ost_idx: 6, l_fid: [0x100060000:0x5:0x0] }\n\n    lcme_id:             2\n    lcme_mirror_id:      0\n    lcme_flags:          0\n    lcme_extent.e_start: 16777216\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  -1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n\n# lfs mirror extend --no-verify -N -f /mnt/testfs/victim_file \\\n                    /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    3\n  lcm_mirror_count:  3\n  lcm_entry_count:   4\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.", "mimetype": "text/plain", "start_char_idx": 16589, "end_char_idx": 19070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "436b5466-ee41-4372-9565-a3e3131c2949": {"__data__": {"id_": "436b5466-ee41-4372-9565-a3e3131c2949", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73c8af7c-febb-4309-bd71-f09475fdfd85", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d674de73eb036e34cabaa1bde2e64548825fe1ceae5f7b66539572ea3df417d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8283437d-d295-44bf-8eff-70868a7153d6", "node_type": "1", "metadata": {}, "hash": "1961b199c68d59dc5f1fc19690a83f378c0889e379ee9cc6a79d3c2115d72363", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "e_start: 16777216\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  -1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n\n# lfs mirror extend --no-verify -N -f /mnt/testfs/victim_file \\\n                    /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    3\n  lcm_mirror_count:  3\n  lcm_entry_count:   4\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_pool:          flash\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x4:0x0] }\n\n    lcme_id:             131073\n    lcme_mirror_id:      2\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  6\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 3\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 3, l_fid: [0x100030000:0x3:0x0] }\n      - 1: { l_ost_idx: 4, l_fid: [0x100040000:0x4:0x0] }\n      - 2: { l_ost_idx: 5, l_fid: [0x100050000:0x4:0x0] }\n      - 3: { l_ost_idx: 6, l_fid: [0x100060000:0x4:0x0] }\n      - 4: { l_ost_idx: 7, l_fid: [0x100070000:0x4:0x0] }\n      - 5: { l_ost_idx: 2, l_fid: [0x100020000:0x3:0x0] }\n\n    lcme_id:             196609\n    lcme_mirror_id:      3\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   16777216\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 5\n      lmm_objects:\n      - 0: { l_ost_idx: 5, l_fid: [0x100050000:0x5:0x0] }\n      - 1: { l_ost_idx: 6, l_fid: [0x100060000:0x5:0x0] }\n\n    lcme_id:             196610\n    lcme_mirror_id:      3\n    lcme_flags:          0\n    lcme_extent.e_start: 16777216\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  -1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n```\n\nAfter extending,", "mimetype": "text/plain", "start_char_idx": 18521, "end_char_idx": 20776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8283437d-d295-44bf-8eff-70868a7153d6": {"__data__": {"id_": "8283437d-d295-44bf-8eff-70868a7153d6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "436b5466-ee41-4372-9565-a3e3131c2949", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "78d3b45828a981c4e1066fdd49c3c51df6ac92ef314bc4d858d4a44839d5da5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce4ce2f7-8911-437a-86c3-50d1e21497f3", "node_type": "1", "metadata": {}, "hash": "4ac1acdfa79d869a184bcd8a6dd5e7fd4d7f597bbcad30044a854376a355579e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "l_fid: [0x100050000:0x5:0x0] }\n      - 1: { l_ost_idx: 6, l_fid: [0x100060000:0x5:0x0] }\n\n    lcme_id:             196610\n    lcme_mirror_id:      3\n    lcme_flags:          0\n    lcme_extent.e_start: 16777216\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  -1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n```\n\nAfter extending, the *victim_file* was removed:\n\n```\n# ls /mnt/testfs/victim_file\nls: cannot access /mnt/testfs/victim_file: No such file or directory\n```\n\n### Splitting a Mirrored File\n\n**Command:**\n\n```\nlfs mirror split <--mirror-id <mirror_id>>\n[--destroy|-d] [-f <new_file>] <mirrored_file>\n```\n\nThe above command will split a specified mirror with ID *<mirror_id>* out of an existing mirrored file specified by*mirrored_file*. By default, a new file named `<mirrored_file>.mirror~<mirror_id>` will be created with the layout of the split mirror. If the `--destroy|-d` option is specified, then the split mirror will be destroyed. If the `-f <new_file>` option is specified, then a file named *new_file* will be created with the layout of the split mirror. If *mirrored_file* has only one mirror existing after split, it will be converted to a regular non-mirrored file. If the original *mirrored_file* is not a mirrored file, then the command will return an error.\n\n| Option                  | Description                                                  |\n| ----------------------- | ------------------------------------------------------------ |\n| --mirror-id <mirror_id> | The unique numerical identifier for a mirror. The mirror ID is unique within a mirrored file and is automatically assigned at file creation or extension time. It can be fetched by the `lfs getstripe` command. |\n| --destroy\\|-d           | Indicates the split mirror will be destroyed.                |\n| -f <new_file>           | Indicates a file named *new_file* will be created with the layout of the split mirror. |\n\n**Examples:**\n\nThe following commands create a mirrored file with 4 mirrors, then split 3 mirrors separately from the mirrored file.\n\nCreating a mirrored file with 4 mirrors:\n\n```\n# lfs mirror create -N2 -E 4M -p flash -E eof -c -1 \\\n                    -N2 -S 8M -c 2 -p archive /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    6\n  lcm_mirror_count:  4\n  lcm_entry_count:   6\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 1\n      lmm_pool:          flash\n      lmm_objects:\n      - 0: { l_ost_idx: 1, l_fid: [0x100010000:0x4:0x0] }\n\n    lcme_id:             65538\n    lcme_mirror_id:      1\n    lcme_flags:          0\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n      lmm_pool:          flash\n\n    lcme_id:             131075\n    lcme_mirror_id:      2\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.", "mimetype": "text/plain", "start_char_idx": 20369, "end_char_idx": 23639, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce4ce2f7-8911-437a-86c3-50d1e21497f3": {"__data__": {"id_": "ce4ce2f7-8911-437a-86c3-50d1e21497f3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8283437d-d295-44bf-8eff-70868a7153d6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e257795710755b210e2f14a3021e605289bc66a594b94f3e164bc00cfae8060a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41054fef-3fab-4adb-9ac4-88a24e989a66", "node_type": "1", "metadata": {}, "hash": "a1429daf3b0544bf49bc7b64f3fa14f1ebb69fe027b20b44289db7a908f29025", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "l_fid: [0x100010000:0x4:0x0] }\n\n    lcme_id:             65538\n    lcme_mirror_id:      1\n    lcme_flags:          0\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n      lmm_pool:          flash\n\n    lcme_id:             131075\n    lcme_mirror_id:      2\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 0\n      lmm_pool:          flash\n      lmm_objects:\n      - 0: { l_ost_idx: 0, l_fid: [0x100000000:0x5:0x0] }\n\n    lcme_id:             131076\n    lcme_mirror_id:      2\n    lcme_flags:          0\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n      lmm_pool:          flash\n\n    lcme_id:             196613\n    lcme_mirror_id:      3\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x5:0x0] }\n      - 1: { l_ost_idx: 5, l_fid: [0x100050000:0x6:0x0] }\n\n    lcme_id:             262150\n    lcme_mirror_id:      4\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 7\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 7, l_fid: [0x100070000:0x5:0x0] }\n      - 1: { l_ost_idx: 2, l_fid: [0x100020000:0x4:0x0] }\n```\n\nSplitting the mirror with ID `1` from `/mnt/testfs/file1` and creating `/mnt/testfs/file1.mirror~1` with the layout of the split mirror:\n\n```\n# lfs mirror split --mirror-id 1 /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1.mirror~1\n/mnt/testfs/file1.mirror~1\n  lcm_layout_gen:    1\n  lcm_mirror_count:  1\n  lcm_entry_count:   2\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.", "mimetype": "text/plain", "start_char_idx": 23150, "end_char_idx": 25621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41054fef-3fab-4adb-9ac4-88a24e989a66": {"__data__": {"id_": "41054fef-3fab-4adb-9ac4-88a24e989a66", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce4ce2f7-8911-437a-86c3-50d1e21497f3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77615194a18e76e4c19796dedc0bca9e97a6d775679c1a883740d8af6435229f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfdd6e3e-9c2a-4e29-8c73-7564c5397aa4", "node_type": "1", "metadata": {}, "hash": "1d5815a2550c6458f3427feb87bb140d06b2ff6c76a5c3e9b13b3211e2984b28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "l_fid: [0x100020000:0x4:0x0] }\n```\n\nSplitting the mirror with ID `1` from `/mnt/testfs/file1` and creating `/mnt/testfs/file1.mirror~1` with the layout of the split mirror:\n\n```\n# lfs mirror split --mirror-id 1 /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1.mirror~1\n/mnt/testfs/file1.mirror~1\n  lcm_layout_gen:    1\n  lcm_mirror_count:  1\n  lcm_entry_count:   2\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 1\n      lmm_pool:          flash\n      lmm_objects:\n      - 0: { l_ost_idx: 1, l_fid: [0x100010000:0x4:0x0] }\n\n    lcme_id:             65538\n    lcme_mirror_id:      1\n    lcme_flags:          0\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n      lmm_pool:          flash\n```\n\nSplitting the mirror with ID `2` from `/mnt/testfs/file1` and destroying it:\n\n```\n# lfs mirror split --mirror-id 2 -d /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    8\n  lcm_mirror_count:  2\n  lcm_entry_count:   2\n    lcme_id:             196613\n    lcme_mirror_id:      3\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x5:0x0] }\n      - 1: { l_ost_idx: 5, l_fid: [0x100050000:0x6:0x0] }\n\n    lcme_id:             262150\n    lcme_mirror_id:      4\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 7\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 7, l_fid: [0x100070000:0x5:0x0] }\n      - 1: { l_ost_idx: 2,", "mimetype": "text/plain", "start_char_idx": 25122, "end_char_idx": 27363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfdd6e3e-9c2a-4e29-8c73-7564c5397aa4": {"__data__": {"id_": "cfdd6e3e-9c2a-4e29-8c73-7564c5397aa4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41054fef-3fab-4adb-9ac4-88a24e989a66", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "671feba41736abf02d1fbc479a1f5591c0f65fb8eabaddc41357a93acaf2c4a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7448192-91d4-4829-bfaf-af421667b63c", "node_type": "1", "metadata": {}, "hash": "b4e815244dfaafff92bf80aaa6950b4702e484c519621755117697f1ab0ad9c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "l_fid: [0x100050000:0x6:0x0] }\n\n    lcme_id:             262150\n    lcme_mirror_id:      4\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 7\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 7, l_fid: [0x100070000:0x5:0x0] }\n      - 1: { l_ost_idx: 2, l_fid: [0x100020000:0x4:0x0] }\n```\n\nSplitting the mirror with ID `3` from `/mnt/testfs/file1` and creating `/mnt/testfs/file2` with the layout of the split mirror:\n\n```\n# lfs mirror split --mirror-id 3 -f /mnt/testfs/file2 \\\n                   /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file2\n/mnt/testfs/file2\n  lcm_layout_gen:    1\n  lcm_mirror_count:  1\n  lcm_entry_count:   1\n    lcme_id:             196613\n    lcme_mirror_id:      3\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x5:0x0] }\n      - 1: { l_ost_idx: 5, l_fid: [0x100050000:0x6:0x0] }\n\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    9\n  lcm_mirror_count:  1\n  lcm_entry_count:   1\n    lcme_id:             262150\n    lcme_mirror_id:      4\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  2\n      lmm_stripe_size:   8388608\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 7\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 7, l_fid: [0x100070000:0x5:0x0] }\n      - 1: { l_ost_idx: 2, l_fid: [0x100020000:0x4:0x0] }\n```\n\nThe above layout information showed that mirrors with ID `1, 2, and 3` were all split from the mirrored file`/mnt/testfs/file1`.\n\n### Resynchronizing out-of-sync Mirrored File(s)\n\n**Command:**\n\n```\nlfs mirror resync [--only <mirror_id[,...]>]\n<mirrored_file> [<mirrored_file2>...]\n```\n\nThe above command will resynchronize out-of-sync mirrored file(s) specified by *mirrored_file*. It supports specifying multiple mirrored files in one command line.\n\nIf there is no stale mirror for the specified mirrored file(s), then the command does nothing. Otherwise, it will copy data from synced mirror to the stale mirror(s), and mark all successfully copied mirror(s) as SYNC. If the `--only <mirror_id[,...]>` option is specified, then the command will only resynchronize the mirror(s) specified by the*mirror_id(s)*. This option cannot be used when multiple mirrored files are specified.\n\n| Option                   | Description                                                  |\n| ------------------------ | ------------------------------------------------------------ |\n| --only <mirror_id[,...]> | Indicates which mirror(s) specified by *mirror_id(s)* needs to be resynchronized. The *mirror_id* is the unique numerical identifier for a mirror.", "mimetype": "text/plain", "start_char_idx": 26905, "end_char_idx": 30033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7448192-91d4-4829-bfaf-af421667b63c": {"__data__": {"id_": "c7448192-91d4-4829-bfaf-af421667b63c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfdd6e3e-9c2a-4e29-8c73-7564c5397aa4", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4520aa2ec4fe3a510a2df5c9ab8ea70cae28671eb924eb19089c8fc063814024", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35fd6598-17c4-4583-8762-ac9e23dcaee4", "node_type": "1", "metadata": {}, "hash": "bdb15be343c888ad1483f03962dc9f9777597d2abf0726697a16ae2e116810ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It supports specifying multiple mirrored files in one command line.\n\nIf there is no stale mirror for the specified mirrored file(s), then the command does nothing. Otherwise, it will copy data from synced mirror to the stale mirror(s), and mark all successfully copied mirror(s) as SYNC. If the `--only <mirror_id[,...]>` option is specified, then the command will only resynchronize the mirror(s) specified by the*mirror_id(s)*. This option cannot be used when multiple mirrored files are specified.\n\n| Option                   | Description                                                  |\n| ------------------------ | ------------------------------------------------------------ |\n| --only <mirror_id[,...]> | Indicates which mirror(s) specified by *mirror_id(s)* needs to be resynchronized. The *mirror_id* is the unique numerical identifier for a mirror. Multiple *mirror_ids* are separated by comma. This option cannot be used when multiple mirrored files are specified. |\n\n**Note:** With delayed write implemented in FLR phase 1, after writing to a mirrored file, users need to run `lfs mirror resync` command to get all mirrors synchronized.\n\n**Examples:**\n\nThe following commands create a mirrored file with 3 mirrors, then write some data into the file and resynchronizes stale mirrors.\n\nCreating a mirrored file with 3 mirrors:\n\n```\n# lfs mirror create -N -E 4M -p flash -E eof \\\n                    -N2 -p archive /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    4\n  lcm_mirror_count:  3\n  lcm_entry_count:   4\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 1\n      lmm_pool:          flash\n      lmm_objects:\n      - 0: { l_ost_idx: 1, l_fid: [0x100010000:0x5:0x0] }\n\n    lcme_id:             65538\n    lcme_mirror_id:      1\n    lcme_flags:          0\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: -1\n      lmm_pool:          flash\n\n    lcme_id:             131075\n    lcme_mirror_id:      2\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 3\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 3, l_fid: [0x100030000:0x4:0x0] }\n\n    lcme_id:             196612\n    lcme_mirror_id:      3\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x6:0x0] }\n```\n\nWriting some data into the mirrored file `/mnt/testfs/file1`:\n\n```\n# yes | dd of=/mnt/testfs/file1 bs=1M count=2\n2+0 records in\n2+0 records out\n2097152 bytes (2.1 MB) copied, 0.0320613 s, 65.", "mimetype": "text/plain", "start_char_idx": 29172, "end_char_idx": 32463, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35fd6598-17c4-4583-8762-ac9e23dcaee4": {"__data__": {"id_": "35fd6598-17c4-4583-8762-ac9e23dcaee4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7448192-91d4-4829-bfaf-af421667b63c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3f0a3cd5d4a21c6808f8528dd6797f488321571c127483a7975b6bca72154e75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25514e69-c941-4475-95c5-dfa182eaee1c", "node_type": "1", "metadata": {}, "hash": "4ea04a92c06705438ef09f320bf7e0e21ce75bd4763269926b9219a17bca4470", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "e_start: 0\n    lcme_extent.e_end:   EOF\n      lmm_stripe_count:  1\n      lmm_stripe_size:   1048576\n      lmm_pattern:       raid0\n      lmm_layout_gen:    0\n      lmm_stripe_offset: 4\n      lmm_pool:          archive\n      lmm_objects:\n      - 0: { l_ost_idx: 4, l_fid: [0x100040000:0x6:0x0] }\n```\n\nWriting some data into the mirrored file `/mnt/testfs/file1`:\n\n```\n# yes | dd of=/mnt/testfs/file1 bs=1M count=2\n2+0 records in\n2+0 records out\n2097152 bytes (2.1 MB) copied, 0.0320613 s, 65.4 MB/s\n\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    5\n  lcm_mirror_count:  3\n  lcm_entry_count:   4\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n    .\n\n    lcme_id:             65538\n    lcme_mirror_id:      1\n    lcme_flags:          0\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n    .\n\n    lcme_id:             131075\n    lcme_mirror_id:      2\n    lcme_flags:          init,stale\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n    .\n\n    lcme_id:             196612\n    lcme_mirror_id:      3\n    lcme_flags:          init,stale\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n    .\n```\n\nThe above layout information showed that data were written into the first component of mirror with ID `1`, and mirrors with ID `2` and `3` were marked with \u201cstale\u201d flag.\n\nResynchronizing the stale mirror with ID `2` for the mirrored file `/mnt/testfs/file1`:\n\n```\n# lfs mirror resync --only 2 /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    7\n  lcm_mirror_count:  3\n  lcm_entry_count:   4\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n    ......\n\n    lcme_id:             65538\n    lcme_mirror_id:      1\n    lcme_flags:          0\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n    ......\n\n    lcme_id:             131075\n    lcme_mirror_id:      2\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n    ......\n\n    lcme_id:             196612\n    lcme_mirror_id:      3\n    lcme_flags:          init,stale\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n    ......\n```\n\nThe above layout information showed that after resynchronizing, the \u201cstale\u201d flag was removed from mirror with ID `2`.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25514e69-c941-4475-95c5-dfa182eaee1c": {"__data__": {"id_": "25514e69-c941-4475-95c5-dfa182eaee1c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35fd6598-17c4-4583-8762-ac9e23dcaee4", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bc83451f1d64567d6dec5265b1f3a553f2414dad2fd4483a0b99ba5f0a40dad1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c2b1a7a-31ac-49c0-a2ed-38379e4df154", "node_type": "1", "metadata": {}, "hash": "b565fcf9dc66526c89a0e01b657179e5e21dfe6058d6bc34a17c4715ebf2ff9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Resynchronizing all of the stale mirrors for the mirrored file `/mnt/testfs/file1`:\n\n```\n# lfs mirror resync /mnt/testfs/file1\n# lfs getstripe /mnt/testfs/file1\n/mnt/testfs/file1\n  lcm_layout_gen:    9\n  lcm_mirror_count:  3\n  lcm_entry_count:   4\n    lcme_id:             65537\n    lcme_mirror_id:      1\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   4194304\n    ......\n\n    lcme_id:             65538\n    lcme_mirror_id:      1\n    lcme_flags:          0\n    lcme_extent.e_start: 4194304\n    lcme_extent.e_end:   EOF\n    ......\n\n    lcme_id:             131075\n    lcme_mirror_id:      2\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n    ......\n\n    lcme_id:             196612\n    lcme_mirror_id:      3\n    lcme_flags:          init\n    lcme_extent.e_start: 0\n    lcme_extent.e_end:   EOF\n    ......\n```\n\nThe above layout information showed that after resynchronizing, none of the mirrors were marked as stale.\n\n### Verifying Mirrored File(s)\n\n**Command:**\n\n```\nlfs mirror verify [--only <mirror_id,mirror_id2[,...]>]\n[--verbose|-v] <mirrored_file> [<mirrored_file2> ...]\n```\n\nThe above command will verify that each SYNC mirror (contains up-to-date data) of a mirrored file, specified by*mirrored_file*, has exactly the same data. It supports specifying multiple mirrored files in one command line.\n\nThis is a scrub tool that should be run on regular basis to make sure that mirrored files are not corrupted. The command won't repair the file if it turns out to be corrupted. Usually, an administrator should check the file content from each mirror and decide which one is correct and then invoke `lfs mirror resync` to repair it manually.\n\n| Option                              | Description                                                  |\n| ----------------------------------- | ------------------------------------------------------------ |\n| --only <mirror_id,mirror_id2[,...]> | Indicates which mirrors specified by *mirror_ids* need to be verified. The *mirror_id*is the unique numerical identifier for a mirror. Multiple *mirror_ids* are separated by comma.Note: At least two *mirror_ids* are required. This option cannot be used when multiple mirrored files are specified. |\n| --verbose\\|-v                       | Indicates the command will print where the differences are if the data do not match. Otherwise, the command will just return an error in that case. This option can be repeated for multiple times to print more information. |\n\n**Note:**\n\nMirror components that have \u201cstale\u201d or \u201coffline\u201d flags will be skipped and not verified.", "mimetype": "text/plain", "start_char_idx": 34435, "end_char_idx": 37066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c2b1a7a-31ac-49c0-a2ed-38379e4df154": {"__data__": {"id_": "2c2b1a7a-31ac-49c0-a2ed-38379e4df154", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25514e69-c941-4475-95c5-dfa182eaee1c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cf8a0abea3e7951817657a01bf4aa20dbacb28a51c8769234a2ee92d1cc10548", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a00cc16-ae52-423a-a5a1-6d33128bd6a1", "node_type": "1", "metadata": {}, "hash": "ee6d2fbf94f04cb9ac0166d5a15db09785e25ea00e1abc6316fe19b9c9c445c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Option                              | Description                                                  |\n| ----------------------------------- | ------------------------------------------------------------ |\n| --only <mirror_id,mirror_id2[,...]> | Indicates which mirrors specified by *mirror_ids* need to be verified. The *mirror_id*is the unique numerical identifier for a mirror. Multiple *mirror_ids* are separated by comma.Note: At least two *mirror_ids* are required. This option cannot be used when multiple mirrored files are specified. |\n| --verbose\\|-v                       | Indicates the command will print where the differences are if the data do not match. Otherwise, the command will just return an error in that case. This option can be repeated for multiple times to print more information. |\n\n**Note:**\n\nMirror components that have \u201cstale\u201d or \u201coffline\u201d flags will be skipped and not verified.\n\n**Examples:**\n\nThe following command verifies that each mirror of a mirrored file contains exactly the same data:\n\n```\n# lfs mirror verify /mnt/testfs/file1\n```\n\nThe following command has the `-v` option specified to print where the differences are if the data does not match:\n\n```\n# lfs mirror verify -vvv /mnt/testfs/file2\nChunks to be verified in /mnt/testfs/file2:\n[0, 0x200000)   [1, 2, 3, 4]    4\n[0x200000, 0x400000)    [1, 2, 3, 4]    4\n[0x400000, 0x600000)    [1, 2, 3, 4]    4\n[0x600000, 0x800000)    [1, 2, 3, 4]    4\n[0x800000, 0xa00000)    [1, 2, 3, 4]    4\n[0xa00000, 0x1000000)   [1, 2, 3, 4]    4\n[0x1000000, 0xffffffffffffffff) [1, 2, 3, 4]    4\n\nVerifying chunk [0, 0x200000) on mirror: 1 2 3 4\nCRC-32 checksum value for chunk [0, 0x200000):\nMirror 1:       0x207b02f1\nMirror 2:       0x207b02f1\nMirror 3:       0x207b02f1\nMirror 4:       0x207b02f1\n\nVerifying chunk [0, 0x200000) on mirror: 1 2 3 4 PASS\n\nVerifying chunk [0x200000, 0x400000) on mirror: 1 2 3 4\nCRC-32 checksum value for chunk [0x200000, 0x400000):\nMirror 1:       0x207b02f1\nMirror 2:       0x207b02f1\nMirror 3:       0x207b02f1\nMirror 4:       0x207b02f1\n\nVerifying chunk [0x200000, 0x400000) on mirror: 1 2 3 4 PASS\n\nVerifying chunk [0x400000, 0x600000) on mirror: 1 2 3 4\nCRC-32 checksum value for chunk [0x400000, 0x600000):\nMirror 1:       0x42571b66\nMirror 2:       0x42571b66\nMirror 3:       0x42571b66\nMirror 4:       0xabdaf92\n\nlfs mirror verify: chunk [0x400000, 0x600000) has different\nchecksum value on mirror 1 and mirror 4.\nVerifying chunk [0x600000, 0x800000) on mirror: 1 2 3 4\nCRC-32 checksum value for chunk [0x600000, 0x800000):\nMirror 1:       0x1f8ad0d8\nMirror 2:       0x1f8ad0d8\nMirror 3:       0x1f8ad0d8\nMirror 4:       0x18975bf9\n\nlfs mirror verify: chunk [0x600000, 0x800000) has different\nchecksum value on mirror 1 and mirror 4.", "mimetype": "text/plain", "start_char_idx": 36157, "end_char_idx": 38911, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a00cc16-ae52-423a-a5a1-6d33128bd6a1": {"__data__": {"id_": "9a00cc16-ae52-423a-a5a1-6d33128bd6a1", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c2b1a7a-31ac-49c0-a2ed-38379e4df154", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9c9b2938a62d5e042a95c9a32068be2c3a80c0319abe242de26d4fbc3ab3843a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a760f3d-88a7-4517-8486-c51269a38888", "node_type": "1", "metadata": {}, "hash": "0b20e48caa77bce63305caa1a550421d201cc154a2e6ae13f735b2ac17a62ce2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Verifying chunk [0x600000, 0x800000) on mirror: 1 2 3 4\nCRC-32 checksum value for chunk [0x600000, 0x800000):\nMirror 1:       0x1f8ad0d8\nMirror 2:       0x1f8ad0d8\nMirror 3:       0x1f8ad0d8\nMirror 4:       0x18975bf9\n\nlfs mirror verify: chunk [0x600000, 0x800000) has different\nchecksum value on mirror 1 and mirror 4.\nVerifying chunk [0x800000, 0xa00000) on mirror: 1 2 3 4\nCRC-32 checksum value for chunk [0x800000, 0xa00000):\nMirror 1:       0x69c17478\nMirror 2:       0x69c17478\nMirror 3:       0x69c17478\nMirror 4:       0x69c17478\n\nVerifying chunk [0x800000, 0xa00000) on mirror: 1 2 3 4 PASS\n\nlfs mirror verify: '/mnt/testfs/file2' chunk [0xa00000, 0x1000000]\nexceeds file size 0xa00000: skipped\n```\n\nThe following command uses the `--only` option to only verify the specified mirrors:\n\n```\n# lfs mirror verify -v --only 1,4 /mnt/testfs/file2\nCRC-32 checksum value for chunk [0, 0x200000):\nMirror 1:       0x207b02f1\nMirror 4:       0x207b02f1\n\nCRC-32 checksum value for chunk [0x200000, 0x400000):\nMirror 1:       0x207b02f1\nMirror 4:       0x207b02f1\n\nCRC-32 checksum value for chunk [0x400000, 0x600000):\nMirror 1:       0x42571b66\nMirror 4:       0xabdaf92\n\nlfs mirror verify: chunk [0x400000, 0x600000) has different\nchecksum value on mirror 1 and mirror 4.\nCRC-32 checksum value for chunk [0x600000, 0x800000):\nMirror 1:       0x1f8ad0d8\nMirror 4:       0x18975bf9\n\nlfs mirror verify: chunk [0x600000, 0x800000) has different\nchecksum value on mirror 1 and mirror 4.\nCRC-32 checksum value for chunk [0x800000, 0xa00000):\nMirror 1:       0x69c17478\nMirror 4:       0x69c17478\n\nlfs mirror verify: '/mnt/testfs/file2' chunk [0xa00000, 0x1000000]\nexceeds file size 0xa00000: skipped\n```\n\n### Finding Mirrored File(s)\n\nThe `lfs find` command is used to list files and directories with specific attributes. The following two attribute parameters are specific to a mirrored file or directory:\n\n```\nlfs find <directory|filename ...>\n    [[!] --mirror-count|-N [+-]n]\n    [[!] --mirror-state <[^]state>]\n```\n\n| Option                    | Description                                                  |\n| ------------------------- | ------------------------------------------------------------ |\n| --mirror-count\\|-N [+-]n  | Indicates mirror count.                                      |\n| --mirror-state <[^]state> | Indicates mirrored file state.If *^state* is used, print only files not matching *state*. Only one state can be specified.Valid state names are:`ro` \u2013 indicates the mirrored file is in read-only state. All of the mirrors contain the up-to-date data.`wp` \u2013 indicates the mirrored file is in a state of being written.`sp` \u2013 indicates the mirrored file is in a state of being resynchronized. |\n\n**Note:**\n\nSpecifying `!` before an option negates its meaning (files NOT matching the parameter). Using `+` before a numeric value means 'more than n', while `-` before a numeric value means 'less than n'. If neither is used, it means 'equal to n', within the bounds of the unit specified (if any).", "mimetype": "text/plain", "start_char_idx": 38592, "end_char_idx": 41606, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a760f3d-88a7-4517-8486-c51269a38888": {"__data__": {"id_": "7a760f3d-88a7-4517-8486-c51269a38888", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37222704-0a59-4da0-925a-e2f4ab7c0729", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "537212b1c6ea0efcba9e17d45f4b633c44c47ed47d2572269ba1b5aba389bc47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a00cc16-ae52-423a-a5a1-6d33128bd6a1", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "415d49fdbcb4b787eccfc912183c4259c41096a8a75129a60b32d72472eede9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| --mirror-state <[^]state> | Indicates mirrored file state.If *^state* is used, print only files not matching *state*. Only one state can be specified.Valid state names are:`ro` \u2013 indicates the mirrored file is in read-only state. All of the mirrors contain the up-to-date data.`wp` \u2013 indicates the mirrored file is in a state of being written.`sp` \u2013 indicates the mirrored file is in a state of being resynchronized. |\n\n**Note:**\n\nSpecifying `!` before an option negates its meaning (files NOT matching the parameter). Using `+` before a numeric value means 'more than n', while `-` before a numeric value means 'less than n'. If neither is used, it means 'equal to n', within the bounds of the unit specified (if any).\n\n**Examples:**\n\nThe following command recursively lists all mirrored files that have more than 2 mirrors under directory `/mnt/testfs`:\n\n```\n# lfs find --mirror-count +2 --type f /mnt/testfs\n```\n\nThe following command recursively lists all out-of-sync mirrored files under directory `/mnt/testfs`:\n\n```\n# lfs find --mirror-state=^ro --type f /mnt/testfs\n```\n\n## Interoperability\n\nIntroduced in Lustre release 2.11.0, the FLR feature is based on the [the section called \u201cProgressive File Layout(PFL)\u201d](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#progressive-file-layoutpfl) feature introduced in Lustre 2.10.0\n\nFor Lustre release 2.9 and older clients, which do not understand the PFL layout, they cannot access and open mirrored files created in the Lustre 2.11 filesystem.\n\nThe following example shows the errors returned by accessing and opening a mirrored file (created in Lustre 2.11 filesystem) on a Lustre 2.9 client:\n\n```\n# ls /mnt/testfs/mirrored_file\nls: cannot access /mnt/testfs/mirrored_file: Invalid argument\n\n# cat /mnt/testfs/mirrored_file\ncat: /mnt/testfs/mirrored_file: Operation not supported\n```\n\nFor Lustre release 2.10 clients, which understand the PFL layout, but do not understand a mirrored layout, they can access mirrored files created in Lustre 2.11 filesystem, however, they cannot open them. This is because the Lustre 2.10 clients do not verify overlapping components so they would read and write mirrored files just as if they were normal PFL files, which will cause a problem where synced mirrors actually contain different data.\n\nThe following example shows the results returned by accessing and opening a mirrored file (created in Lustre 2.11 filesystem) on a Lustre 2.10 client:\n\n```\n# ls /mnt/testfs/mirrored_file\n/mnt/testfs/mirrored_file\n\n# cat /mnt/testfs/mirrored_file\ncat: /mnt/testfs/mirrored_file: Operation not supported\n```", "mimetype": "text/plain", "start_char_idx": 40883, "end_char_idx": 43499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0cdb5fde-7a5d-442b-a88e-6ad85806fad2": {"__data__": {"id_": "0cdb5fde-7a5d-442b-a88e-6ad85806fad2", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "934347200c3346ae799feceff93b7acf9b02814452049c7667cfb47038e60497", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bc4fd67-9769-4da1-bbab-12760a299a4d", "node_type": "1", "metadata": {}, "hash": "aa0809f17e5c3cd2a0d6d8dcbab5186012edb5e0dbdd597122b566459ea665c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Managing the File System and I/O\n\n- [Managing the File System and I/O](#managing-the-file-system-and-io)\n  * [Handling Full OSTs](#handling-full-osts)\n    + [Checking OST Space Usage](#checking-ost-space-usage)\n    + [Disabling creates on a Full OST](#disabling-creates-on-a-full-ost)\n    + [Migrating Data within a File System](#migrating-data-within-a-file-system)\n    + [Returning an Inactive OST Back Online](#returning-an-inactive-ost-back-online)\n    + [Migrating Metadata within a Filesystem](#migrating-metadata-within-a-filesystem)\n      - [Whole Directory Migration](#whole-directory-migration)\n      - [Striped Directory Migration](#striped-directory-migration)\n  * [Creating and Managing OST Pools](#creating-and-managing-ost-pools)\n    + [Working with OST Pools](#working-with-ost-pools)\n      - [Using the lfs Command with OST Pools](#using-the-lfs-command-with-ost-pools)\n    + [Tips for Using OST Pools](#tips-for-using-ost-pools)\n  * [Adding an OST to a Lustre File System](#adding-an-ost-to-a-lustre-file-system)\n  * [Performing Direct I/O](#performing-direct-io)\n    + [Making File System Objects Immutable](#making-file-system-objects-immutable)\n  * [Other I/O Options](#other-io-options)\n    + [Lustre Checksums](#lustre-checksums)\n      - [Changing Checksum Algorithms](#changing-checksum-algorithms)\n    + [Ptlrpc Client Thread Pool](#ptlrpc-client-thread-pool)\n      - [ptlrpcd parameters](#ptlrpcd-parameters)\n\n## Handling Full OSTs\n\nSometimes a Lustre file system becomes unbalanced, often due to incorrectly-specified stripe settings, or when very large files are created that are not striped over all of the OSTs. Lustre will automatically avoid allocating new files on OSTs that are full. If an OST is completely full and more data is written to files already located on that OST, an error occurs. The procedures below describe how to handle a full OST.\n\nThe MDS will normally handle space balancing automatically at file creation time, and this procedure is normally not needed, but manual data migration may be desirable in some cases (e.g. creating very large files that would consume more than the total free space of the full OSTs).\n\n Checking OST Space Usage\n\nThe example below shows an unbalanced file system:\n\n```\nclient# lfs df -h\nUUID                       bytes           Used            Available       \\\nUse%            Mounted on\ntestfs-MDT0000_UUID        4.4G            214.5M          3.9G            \\\n4%              /mnt/testfs[MDT:0]\ntestfs-OST0000_UUID        2.0G            751.3M          1.1G            \\\n37%             /mnt/testfs[OST:0]\ntestfs-OST0001_UUID        2.0G            755.3M          1.1G            \\\n37%             /mnt/testfs[OST:1]\ntestfs-OST0002_UUID        2.0G            1.7G            155.1M          \\\n86%             /mnt/testfs[OST:2] ****\ntestfs-OST0003_UUID        2.0G            751.3M          1.1G            \\\n37%             /mnt/testfs[OST:3]\ntestfs-OST0004_UUID        2.0G            747.3M          1.1G            \\\n37%             /mnt/testfs[OST:4]\ntestfs-OST0005_UUID        2.0G            743.3M          1.1G            \\\n36%             /mnt/testfs[OST:5]\n \nfilesystem summary:        11.8G           5.4G            5.8G            \\\n45%             /mnt/testfs\n```\n\nIn this case, OST0002 is almost full and when an attempt is made to write additional information to the file system (even with uniform striping over all the OSTs),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3439, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9bc4fd67-9769-4da1-bbab-12760a299a4d": {"__data__": {"id_": "9bc4fd67-9769-4da1-bbab-12760a299a4d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "934347200c3346ae799feceff93b7acf9b02814452049c7667cfb47038e60497", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cdb5fde-7a5d-442b-a88e-6ad85806fad2", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5fd48f837c704b874eead4c813498db4511dbced432721db130e7a2a67af880d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "737c143f-2576-4f45-a854-e39a5ade0454", "node_type": "1", "metadata": {}, "hash": "78daf5228340e2306fb7d7bfad98a62eb8141f1cbb896a1634015e6d40a1878e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0G            751.3M          1.1G            \\\n37%             /mnt/testfs[OST:3]\ntestfs-OST0004_UUID        2.0G            747.3M          1.1G            \\\n37%             /mnt/testfs[OST:4]\ntestfs-OST0005_UUID        2.0G            743.3M          1.1G            \\\n36%             /mnt/testfs[OST:5]\n \nfilesystem summary:        11.8G           5.4G            5.8G            \\\n45%             /mnt/testfs\n```\n\nIn this case, OST0002 is almost full and when an attempt is made to write additional information to the file system (even with uniform striping over all the OSTs), the write command fails as follows:\n\n```\nclient# lfs setstripe /mnt/testfs 4M 0 -1\nclient# dd if=/dev/zero of=/mnt/testfs/test_3 bs=10M count=100\ndd: writing '/mnt/testfs/test_3': No space left on device\n98+0 records in\n97+0 records out\n1017192448 bytes (1.0 GB) copied, 23.2411 seconds, 43.8 MB/s\n```\n### Checking OST Space Usage\n\nThe example below shows an unbalanced file system:\n\n```\nclient# lfs df -h\nUUID                       bytes           Used            Available       \\\nUse%            Mounted on\ntestfs-MDT0000_UUID        4.4G            214.5M          3.9G            \\\n4%              /mnt/testfs[MDT:0]\ntestfs-OST0000_UUID        2.0G            751.3M          1.1G            \\\n37%             /mnt/testfs[OST:0]\ntestfs-OST0001_UUID        2.0G            755.3M          1.1G            \\\n37%             /mnt/testfs[OST:1]\ntestfs-OST0002_UUID        2.0G            1.7G            155.1M          \\\n86%             /mnt/testfs[OST:2] ****\ntestfs-OST0003_UUID        2.0G            751.3M          1.1G            \\\n37%             /mnt/testfs[OST:3]\ntestfs-OST0004_UUID        2.0G            747.3M          1.1G            \\\n37%             /mnt/testfs[OST:4]\ntestfs-OST0005_UUID        2.0G            743.3M          1.1G            \\\n36%             /mnt/testfs[OST:5]\n \nfilesystem summary:        11.8G           5.4G            5.8G            \\\n45%             /mnt/testfs\n```\n\nIn this case, OST0002 is almost full and when an attempt is made to write additional information to the file system (even with uniform striping over all the OSTs), the write command fails as follows:\n\n```\nclient# lfs setstripe /mnt/testfs 4M 0 -1\nclient# dd if=/dev/zero of=/mnt/testfs/test_3 bs=10M count=100\ndd: writing '/mnt/testfs/test_3': No space left on device\n98+0 records in\n97+0 records out\n1017192448 bytes (1.0 GB) copied, 23.2411 seconds, 43.8 MB/s\n```\n\n### Disabling creates on a Full OST\n\nTo avoid running out of space in the file system, if the OST usage is imbalanced and one or more OSTs are close to being full while there are others that have a lot of space, the MDS will typically avoid file creation on the full OST(s) automatically. The full OSTs may optionally be deactivated manually on the MDS to ensure the MDS will not allocate new objects there.\n\n1. Log into the MDS server and use the `lctl` command to stop new object creation on the full OST(s):\n\n   ```\n   mds# lctl set_param osp.fsname-OSTnnnn*.max_create_count=0\n   ```\n\nWhen new files are created in the file system, they will only use the remaining OSTs. Either manual space rebalancing can be done by migrating data to other OSTs, as shown in the next section, or normal file deletion and creation can passively rebalance the space usage.", "mimetype": "text/plain", "start_char_idx": 2857, "end_char_idx": 6177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "737c143f-2576-4f45-a854-e39a5ade0454": {"__data__": {"id_": "737c143f-2576-4f45-a854-e39a5ade0454", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "934347200c3346ae799feceff93b7acf9b02814452049c7667cfb47038e60497", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bc4fd67-9769-4da1-bbab-12760a299a4d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a58b564741db6f0c1bf454f11ffe893eeec6a1e50c9cac6294f9fe6cc47b16d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ff60224-6fb9-41f0-8c34-92ce4ede4172", "node_type": "1", "metadata": {}, "hash": "bb7d0ce2c7bb3f969bf9a6562fe94209dbc8cfafaf88f020323672f254a4fce4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "if the OST usage is imbalanced and one or more OSTs are close to being full while there are others that have a lot of space, the MDS will typically avoid file creation on the full OST(s) automatically. The full OSTs may optionally be deactivated manually on the MDS to ensure the MDS will not allocate new objects there.\n\n1. Log into the MDS server and use the `lctl` command to stop new object creation on the full OST(s):\n\n   ```\n   mds# lctl set_param osp.fsname-OSTnnnn*.max_create_count=0\n   ```\n\nWhen new files are created in the file system, they will only use the remaining OSTs. Either manual space rebalancing can be done by migrating data to other OSTs, as shown in the next section, or normal file deletion and creation can passively rebalance the space usage.\n\n### Migrating Data within a File System\n\nIf there is a need to move the file data from the current OST(s) to new OST(s), the data must be migrated (copied) to the new location. The simplest way to do this is to use the `lfs_migrate` command, as described in [the section called \u201c Adding a New OST to a Lustre File System\u201d](03.03-Lustre%20Maintenance.md#adding-a-new-ost-to-a-lustre-file-system).\n\n### Returning an Inactive OST Back Online\n\nOnce the full OST(s) no longer are severely imbalanced, due to either active or passive data redistribution, they should be reactivated so they will again have new files allocated on them.\n\n```\n[mds]# lctl set_param osp.testfs-OST0002.max_create_count=20000\n```\n\n### Migrating Metadata within a Filesystem\n\nIntroduced in Lustre 2.8\n\n#### Whole Directory Migration\n\nLustre software version 2.8 includes a feature to migrate metadata (directories and inodes therein) between MDTs. This migration can only be performed on whole directories. Striped directories are not supported until Lustre 2.12. For example, to migrate the contents of the `/testfs/remotedir` directory from the MDT where it currently is located to MDT0000 to allow that MDT to be removed, the sequence of commands is as follows:\n\n```\n$ cd /testfs \n$ lfs getdirstripe -m ./remotedir *which MDT is dir on?* \n1 \n$ touch ./remotedir/file.{1,2,3}.txt*create test files* \n$ lfs getstripe -m ./remotedir/file.*.txt*check files are on MDT0001* \n1 \n1 \n1 \n$ lfs migrate -m 0 ./remotedir *migrate testremote to MDT0000* \n$ lfs getdirstripe -m ./remotedir *which MDT is dir on now?* \n0 \n$ lfs getstripe -m ./remotedir/file.*.txt*check files are on MDT0000* \n0 \n0 \n0\n```\n\nFor more information, see `man lfs-migrate`.\n\n##### Warning\n\nDuring migration each file receives a new identifier (FID). As a consequence, the file will report a new inode number to userspace applications. Some system tools (for example, backup and archiving tools, NFS, Samba) that identify files by inode number may consider the migrated files to be new, even though the contents are unchanged. If a Lustre system is re-exporting to NFS, the migrated files may become inaccessible during and after migration if the client or server are caching a stale file handle with the old FID. Restarting the NFS service will flush the local file handle cache, but clients may also need to be restarted as they may cache stale file handles as well.\n\n\n\nIntroduced in Lustre 2.12\n\n#### Striped Directory Migration\n\nLustre 2.8 included a feature to migrate metadata (directories and inodes therein) between MDTs, however it did not support migration of striped directories, or changing the stripe count of an existing directory. Lustre 2.12 adds support for migrating and restriping directories. The `lfs migrate -m` command can only only be performed on whole directories, though it will migrate both the specified directory and its sub-entries recursively.", "mimetype": "text/plain", "start_char_idx": 5405, "end_char_idx": 9090, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ff60224-6fb9-41f0-8c34-92ce4ede4172": {"__data__": {"id_": "6ff60224-6fb9-41f0-8c34-92ce4ede4172", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "934347200c3346ae799feceff93b7acf9b02814452049c7667cfb47038e60497", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "737c143f-2576-4f45-a854-e39a5ade0454", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "60621656a96b064997629a78c4fa599af85c619640256bb936d4293b34f58f87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6e6edbe-b8e5-4c4f-bf19-7b8a1ff405b3", "node_type": "1", "metadata": {}, "hash": "332ca207152d3bb3927cdaba6ed7911d02605ea27cd874e33815b48ebcb66fc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If a Lustre system is re-exporting to NFS, the migrated files may become inaccessible during and after migration if the client or server are caching a stale file handle with the old FID. Restarting the NFS service will flush the local file handle cache, but clients may also need to be restarted as they may cache stale file handles as well.\n\n\n\nIntroduced in Lustre 2.12\n\n#### Striped Directory Migration\n\nLustre 2.8 included a feature to migrate metadata (directories and inodes therein) between MDTs, however it did not support migration of striped directories, or changing the stripe count of an existing directory. Lustre 2.12 adds support for migrating and restriping directories. The `lfs migrate -m` command can only only be performed on whole directories, though it will migrate both the specified directory and its sub-entries recursively. For example, to migrate the contents of a large directory `/testfs/largedir` from its current location on MDT0000 to MDT0001 and MDT0003, run the following command:\n\n`$ lfs migrate -m 1,3 /testfs/largedir`\n\nMetadata migration will migrate file dirent and inode to other MDTs, but it won't touch file data. During migration, directory and its sub-files can be accessed like normal ones, though the same warning above applies to tools that depend on the file inode number. Migration may fail for various reasons such as MDS restart, or disk full. In those cases, some of the sub-files may have been migrated to the new MDTs, while others are still on the original MDT. The files can be accessed normally. The same `lfs migrate -m` command should be executed again when these issues are fixed to finish this migration. However, you cannot abort a failed migration, or migrate to different MDTs from previous migration command.\n\n#### Directory Restriping\n\nLustre 2.14 includs a feature to change the stripe count of an existing directory. The `lfs setdirstripe -c`command can be performed on an existing directory to change its stripe count. For example, a directory `/testfs/testdir` is becoming large, run the following command to increase its stripe count to 2:\n\n```\n$ lfs setdirstripe -c 2 /testfs/testdir\n```\n\nBy default directory restriping will migrate sub-file dirents only, but it won't move inodes. To enable moving both dirents and inodes, run the following command on all MDS's:\n\n```\nmds$ lctl set_param mdt.*.dir_restripe_nsonly=0\n```\n\nIt's not allowed to specify MDTs in directory restriping, instead server will pick MDTs for the added stripes by space and inode usages. During restriping, directory and its sub-files can be accessed like normal ones, which is the same as directory migration. Similarly you cannot abort a failed restriping, and server will resume the failed restriping automatically when it notices an unfinished restriping.\n\n#### 23.1.5.4. Directory Auto-Split\n\nLustre 2.14 includs a feature to automatically increase the stripe count of a directory when it becomes large. This can be enabled by the following command:\n\n```\nmds$ lctl set_param mdt.*.enable_dir_auto_split=1\n```\n\nThe sub file count that triggers directory auto-split is 50k, and it can be changed by the following command:\n\n```\nmds$ lctl set_param mdt.*.dir_split_count=value\n```\n\nThe directory stripe count will be increased from 0 to 4 if it's a plain directory, and from 4 to 8 upon the second split, and so on. However the final stripe count won't exceed total MDT count, and it will stop splitting when it's distributed among all MDTs. This delta value can be changed by the following command:\n\n```\nmds$ lctl set_param mdt.*.dir_split_delta=value\n```\n\n## Creating and Managing OST Pools\n\nThe OST pools feature enables users to group OSTs together to make object placement more flexible. A 'pool' is the name associated with an arbitrary subset of OSTs in a Lustre cluster.\n\nOST pools follow these rules:\n\n- An OST can be a member of multiple pools.\n- No ordering of OSTs in a pool is defined or implied.\n- Stripe allocation within a pool follows the same rules as the normal stripe allocator.\n- OST membership in a pool is flexible, and can change over time.", "mimetype": "text/plain", "start_char_idx": 8242, "end_char_idx": 12350, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6e6edbe-b8e5-4c4f-bf19-7b8a1ff405b3": {"__data__": {"id_": "b6e6edbe-b8e5-4c4f-bf19-7b8a1ff405b3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "934347200c3346ae799feceff93b7acf9b02814452049c7667cfb47038e60497", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ff60224-6fb9-41f0-8c34-92ce4ede4172", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "de4ce48021ff3b3e1f0066f3868739207dccaf8f023fb56ff5f62dfef0f37e89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac88c352-89cb-4475-8269-132388625273", "node_type": "1", "metadata": {}, "hash": "2bdc2ad5d88d6371b9c7af25674e3bbbaf4456f5533449a15a8c5eeb284a1414", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However the final stripe count won't exceed total MDT count, and it will stop splitting when it's distributed among all MDTs. This delta value can be changed by the following command:\n\n```\nmds$ lctl set_param mdt.*.dir_split_delta=value\n```\n\n## Creating and Managing OST Pools\n\nThe OST pools feature enables users to group OSTs together to make object placement more flexible. A 'pool' is the name associated with an arbitrary subset of OSTs in a Lustre cluster.\n\nOST pools follow these rules:\n\n- An OST can be a member of multiple pools.\n- No ordering of OSTs in a pool is defined or implied.\n- Stripe allocation within a pool follows the same rules as the normal stripe allocator.\n- OST membership in a pool is flexible, and can change over time.\n\nWhen an OST pool is defined, it can be used to allocate files. When file or directory striping is set to a pool, only OSTs in the pool are candidates for striping. If a stripe_index is specified which refers to an OST that is not a member of the pool, an error is returned.\n\nOST pools are used only at file creation. If the definition of a pool changes (an OST is added or removed or the pool is destroyed), already-created files are not affected.\n\n**Note**\n\nAn error ( `EINVAL`) results if you create a file using an empty pool.\n\n**Note**\n\nIf a directory has pool striping set and the pool is subsequently removed, the new files created in this directory have the (non-pool) default striping pattern for that directory applied and no error is returned.\n\n### Working with OST Pools\n\nOST pools are defined in the configuration log on the MGS. Use the lctl command to:\n\n- Create/destroy a pool\n- Add/remove OSTs in a pool\n- List pools and OSTs in a specific pool\n\nThe lctl command MUST be run on the MGS. Another requirement for managing OST pools is to either have the MDT and MGS on the same node or have a Lustre client mounted on the MGS node, if it is separate from the MDS. This is needed to validate the pool commands being run are correct.\n\n**Caution**\n\nRunning the `writeconf` command on the MDS erases all pools information (as well as any other parameters set using `lctl conf_param`). We recommend that the pools definitions (and `conf_param`settings) be executed using a script, so they can be reproduced easily after a `writeconf` is performed.\n\nTo create a new pool, run:\n\n```\nmgs# lctl pool_new \nfsname.\npoolname\n```\n\n**Note**\n\nThe pool name is an ASCII string up to 15 characters.\n\nTo add the named OST to a pool, run:\n\n```\nmgs# lctl pool_add \nfsname.\npoolname \nost_list\n```\n\nWhere:\n\n- `*ost_list*is *fsname*-OST *index_range*`\n- `*index_range*is *ost_index_start*- *ost_index_end[,index_range]*` or `*ost_index_start*-*ost_index_end/step*`\n\nIf the leading `*fsname* `and/or ending `_UUID` are missing, they are automatically added.\n\nFor example, to add even-numbered OSTs to `pool1` on file system `testfs`, run a single command ( `pool_add`) to add many OSTs to the pool at one time:\n\n```\nlctl pool_add testfs.pool1 OST[0-10/2]\n```\n\n**Note**\n\nEach time an OST is added to a pool, a new `llog` configuration record is created. For convenience, you can run a single command.\n\nTo remove a named OST from a pool, run:\n\n```\nmgs# lctl pool_remove \nfsname.\npoolname \nost_list\n```\n\nTo destroy a pool, run:\n\n```\nmgs# lctl pool_destroy \nfsname.\npoolname\n```\n\n**Note**\n\nAll OSTs must be removed from a pool before it can be destroyed.\n\nTo list pools in the named file system, run:\n\n```\nmgs# lctl pool_list \nfsname|pathname\n```\n\nTo list OSTs in a named pool, run:\n\n```\nlctl pool_list \nfsname.\npoolname\n```\n\n#### Using the lfs Command with OST Pools\n\nSeveral lfs commands can be run with OST pools. Use the `lfs setstripe` command to associate a directory with an OST pool. This causes all new regular files and directories in the directory to be created in the pool. The lfs command can be used to list pools in a file system and OSTs in a named pool.", "mimetype": "text/plain", "start_char_idx": 11602, "end_char_idx": 15508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac88c352-89cb-4475-8269-132388625273": {"__data__": {"id_": "ac88c352-89cb-4475-8269-132388625273", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "934347200c3346ae799feceff93b7acf9b02814452049c7667cfb47038e60497", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6e6edbe-b8e5-4c4f-bf19-7b8a1ff405b3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0927884fa2ce778cba5c37f3e52a4e61162fcf9a49c5b4894159a3828177377f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b800ea5-0623-4e7e-a5d5-ae6b0f83adb8", "node_type": "1", "metadata": {}, "hash": "f85e6782023eecdff910446681c765e7355155492225258cf28eecc65501d7db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "poolname \nost_list\n```\n\nTo destroy a pool, run:\n\n```\nmgs# lctl pool_destroy \nfsname.\npoolname\n```\n\n**Note**\n\nAll OSTs must be removed from a pool before it can be destroyed.\n\nTo list pools in the named file system, run:\n\n```\nmgs# lctl pool_list \nfsname|pathname\n```\n\nTo list OSTs in a named pool, run:\n\n```\nlctl pool_list \nfsname.\npoolname\n```\n\n#### Using the lfs Command with OST Pools\n\nSeveral lfs commands can be run with OST pools. Use the `lfs setstripe` command to associate a directory with an OST pool. This causes all new regular files and directories in the directory to be created in the pool. The lfs command can be used to list pools in a file system and OSTs in a named pool.\n\nTo associate a directory with a pool, so all new files and directories will be created in the pool, run:\n\n```\nclient# lfs setstripe --pool|-p pool_name \nfilename|dirname \n```\n\nTo set striping patterns, run:\n\n```\nclient# lfs setstripe [--size|-s stripe_size] [--offset|-o start_ost]\n           [--stripe-count|-c stripe_count] [--overstripe-count|-C stripe_count]\n \t\t   [--pool|-p pool_name]\n           \ndir|filename\n```\n\n**Note **\n\nIf you specify striping with an invalid pool name, because the pool does not exist or the pool name was mistyped, `lfs setstripe` returns an error. Run `lfs pool_list` to make sure the pool exists and the pool name is entered correctly.\n\n**Note**\n\nThe `--pool` option for lfs setstripe is compatible with other modifiers. For example, you can set striping on a directory to use an explicit starting index.\n\n### Tips for Using OST Pools\n\nHere are several suggestions for using OST pools.\n\n- A directory or file can be given an extended attribute (EA), that restricts striping to a pool.\n- Pools can be used to group OSTs with the same technology or performance (slower or faster), or that are preferred for certain jobs. Examples are SATA OSTs versus SAS OSTs or remote OSTs versus local OSTs.\n- A file created in an OST pool tracks the pool by keeping the pool name in the file LOV EA.\n\n## Adding an OST to a Lustre File System\n\nTo add an OST to existing Lustre file system:\n\n1. Add a new OST by passing on the following commands, run:\n\n   ```\n   oss# mkfs.lustre --fsname=testfs --mgsnode=mds16@tcp0 --ost --index=12 /dev/sda\n   oss# mkdir -p /mnt/testfs/ost12\n   oss# mount -t lustre /dev/sda /mnt/testfs/ost12\n   ```\n\n2. Migrate the data (possibly).\n\n   The file system is quite unbalanced when new empty OSTs are added. New file creations are automatically balanced. If this is a scratch file system or files are pruned at a regular interval, then no further work may be needed. Files existing prior to the expansion can be rebalanced with an in-place copy, which can be done with a simple script.\n\n   The basic method is to copy existing files to a temporary file, then move the temp file over the old one. This should not be attempted with files which are currently being written to by users or applications. This operation redistributes the stripes over the entire set of OSTs.\n\n   A very clever migration script would do the following:\n\n   - Examine the current distribution of data.\n   - Calculate how much data should move from each full OST to the empty ones.\n   - Search for files on a given full OST (using `lfs getstripe`).\n   - Force the new destination OST (using `lfs setstripe`).\n   - Copy only enough files to address the imbalance.\n\nIf a Lustre file system administrator wants to explore this approach further, per-OST disk-usage statistics can be found under `/proc/fs/lustre/osc/*/rpc_stats`\n\n## Performing Direct I/O\n\nThe Lustre software supports the `O_DIRECT` flag to open.\n\nApplications using the `read()` and `write()` calls must supply buffers aligned on a page boundary (usually 4 K). If the alignment is not correct, the call returns `-EINVAL`. Direct I/O may help performance in cases where the client is doing a large amount of I/O and is CPU-bound (CPU utilization 100%).", "mimetype": "text/plain", "start_char_idx": 14819, "end_char_idx": 18746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b800ea5-0623-4e7e-a5d5-ae6b0f83adb8": {"__data__": {"id_": "6b800ea5-0623-4e7e-a5d5-ae6b0f83adb8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "934347200c3346ae799feceff93b7acf9b02814452049c7667cfb47038e60497", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac88c352-89cb-4475-8269-132388625273", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "15958934a02e6850a4afbefe580656bd73ff7326ac8f2a2ae5229a2c074e302a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7518710f-9ca7-4291-9ba3-f007d085ef0d", "node_type": "1", "metadata": {}, "hash": "3d6b5225fcc9d15fba6fcf82453b22a54e351fa10e92526c86c80a04629ff308", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Calculate how much data should move from each full OST to the empty ones.\n   - Search for files on a given full OST (using `lfs getstripe`).\n   - Force the new destination OST (using `lfs setstripe`).\n   - Copy only enough files to address the imbalance.\n\nIf a Lustre file system administrator wants to explore this approach further, per-OST disk-usage statistics can be found under `/proc/fs/lustre/osc/*/rpc_stats`\n\n## Performing Direct I/O\n\nThe Lustre software supports the `O_DIRECT` flag to open.\n\nApplications using the `read()` and `write()` calls must supply buffers aligned on a page boundary (usually 4 K). If the alignment is not correct, the call returns `-EINVAL`. Direct I/O may help performance in cases where the client is doing a large amount of I/O and is CPU-bound (CPU utilization 100%).\n\n### Making File System Objects Immutable\n\nAn immutable file or directory is one that cannot be modified, renamed or removed. To do this:\n\n```\nchattr +i \nfile\n```\n\nTo remove this flag, use `chattr -i`\n\n## Other I/O Options\n\nThis section describes other I/O options, including checksums, and the ptlrpcd thread pool.\n\n### Lustre Checksums\n\nTo guard against network data corruption, a Lustre client can perform two types of data checksums: in-memory (for data in client memory) and wire (for data sent over the network). For each checksum type, a 32-bit checksum of the data read or written on both the client and server is computed, to ensure that the data has not been corrupted in transit over the network. The `ldiskfs` backing file system does NOT do any persistent checksumming, so it does not detect corruption of data in the OST file system.\n\nThe checksumming feature is enabled, by default, on individual client nodes. If the client or OST detects a checksum mismatch, then an error is logged in the syslog of the form:\n\n```\nLustreError: BAD WRITE CHECKSUM: changed in transit before arrival at OST: \\\nfrom 192.168.1.1@tcp inum 8991479/2386814769 object 1127239/0 extent [10240\\\n0-106495]\n```\n\nIf this happens, the client will re-read or re-write the affected data up to five times to get a good copy of the data over the network. If it is still not possible, then an I/O error is returned to the application.\n\nTo enable both types of checksums (in-memory and wire), run:\n\n```\nlctl set_param llite.*.checksum_pages=1\n```\n\nTo disable both types of checksums (in-memory and wire), run:\n\n```\nlctl set_param llite.*.checksum_pages=0\n```\n\nTo check the status of a wire checksum, run:\n\n```\nlctl get_param osc.*.checksums\n```\n\n#### Changing Checksum Algorithms\n\nBy default, the Lustre software uses the adler32 checksum algorithm, because it is robust and has a lower impact on performance than crc32. The Lustre file system administrator can change the checksum algorithm via `lctl get_param`, depending on what is supported in the kernel.\n\nTo check which checksum algorithm is being used by the Lustre software, run:\n\n```\n$ lctl get_param osc.*.checksum_type\n```\n\nTo change the wire checksum algorithm, run:\n\n```\n$ lctl set_param osc.*.checksum_type=\nalgorithm\n```\n\n**Note **\n\nThe in-memory checksum always uses the adler32 algorithm, if available, and only falls back to crc32 if adler32 cannot be used.\n\nIn the following example, the `lctl get_param` command is used to determine that the Lustre software is using the adler32 checksum algorithm. Then the `lctl set_param` command is used to change the checksum algorithm to crc32. A second `lctl get_param` command confirms that the crc32 checksum algorithm is now in use.\n\n```\n$ lctl get_param osc.*.checksum_type\nosc.testfs-OST0000-osc-ffff81012b2c48e0.checksum_type=crc32 [adler]\n$ lctl set_param osc.*.checksum_type=crc32\nosc.testfs-OST0000-osc-ffff81012b2c48e0.checksum_type=crc32\n$ lctl get_param osc.", "mimetype": "text/plain", "start_char_idx": 17937, "end_char_idx": 21708, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7518710f-9ca7-4291-9ba3-f007d085ef0d": {"__data__": {"id_": "7518710f-9ca7-4291-9ba3-f007d085ef0d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a35258f4-eb58-4e5a-b66b-4814eca7f5b0", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "934347200c3346ae799feceff93b7acf9b02814452049c7667cfb47038e60497", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b800ea5-0623-4e7e-a5d5-ae6b0f83adb8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6166d88f78a29979597bef2e2c953f89cf7916871e313b867efa0f0c72a62a4f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the following example, the `lctl get_param` command is used to determine that the Lustre software is using the adler32 checksum algorithm. Then the `lctl set_param` command is used to change the checksum algorithm to crc32. A second `lctl get_param` command confirms that the crc32 checksum algorithm is now in use.\n\n```\n$ lctl get_param osc.*.checksum_type\nosc.testfs-OST0000-osc-ffff81012b2c48e0.checksum_type=crc32 [adler]\n$ lctl set_param osc.*.checksum_type=crc32\nosc.testfs-OST0000-osc-ffff81012b2c48e0.checksum_type=crc32\n$ lctl get_param osc.*.checksum_type\nosc.testfs-OST0000-osc-ffff81012b2c48e0.checksum_type=[crc32] adler\n```\n### Ptlrpc Client Thread Pool\n\nThe use of large SMP nodes for Lustre clients requires significant parallelism within the kernel to avoid cases where a single CPU would be 100% utilized and other CPUs would be relativity idle. This is especially noticeable when a single thread traverses a large directory.\n\nThe Lustre client implements a PtlRPC daemon thread pool, so that multiple threads can be created to serve asynchronous RPC requests, even if only a single userspace thread is running. The number of ptlrpcd threads spawned is controlled at module load time using module options. By default two service threads are spawned per CPU socket.\n\nOne of the issues with thread operations is the cost of moving a thread context from one CPU to another with the resulting loss of CPU cache warmth. To reduce this cost, PtlRPC threads can be bound to a CPU. However, if the CPUs are busy, a bound thread may not be able to respond quickly, as the bound CPU may be busy with other tasks and the thread must wait to schedule.\n\nBecause of these considerations, the pool of ptlrpcd threads can be a mixture of bound and unbound threads. The system operator can balance the thread mixture based on system size and workload.\n\n#### ptlrpcd parameters\n\nThese parameters should be set in `/etc/modprobe.conf` or in the `etc/modprobe.d` directory, as options for the ptlrpc module.\n\n```\noptions ptlrpcd ptlrpcd_per_cpt_max=XXX\n```\n\nSets the number of ptlrpcd threads created per socket. The default if not specified is two threads per CPU socket, including hyper-threaded CPUs. The lower bound is 2 threads per socket.\n\n```\noptions ptlrpcd ptlrpcd_bind_policy=[1-4]\n```\n\nControls the binding of threads to CPUs. There are four policy options.\n\n- `PDB_POLICY_NONE`(ptlrpcd_bind_policy=1) All threads are unbound.\n- `PDB_POLICY_FULL`(ptlrpcd_bind_policy=2) All threads attempt to bind to a CPU.\n- `PDB_POLICY_PAIR`(ptlrpcd_bind_policy=3) This is the default policy. Threads are allocated as a bound/unbound pair. Each thread (bound or free) has a partner thread. The partnering is used by the ptlrpcd load policy, which determines how threads are allocated to CPUs.\n- `PDB_POLICY_NEIGHBOR`(ptlrpcd_bind_policy=4) Threads are allocated as a bound/unbound pair. Each thread (bound or free) has two partner threads.", "mimetype": "text/plain", "start_char_idx": 21155, "end_char_idx": 24092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ec64c04-8560-473f-b0f8-a38b2e3251fe": {"__data__": {"id_": "6ec64c04-8560-473f-b0f8-a38b2e3251fe", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/24-Lustre File System Failover and Multiple-Mount Protection.md", "file_name": "24-Lustre File System Failover and Multiple-Mount Protection.md", "file_type": "text/markdown", "file_size": 3302, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67c64319-6584-4bdc-b600-62c718831df6", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/24-Lustre File System Failover and Multiple-Mount Protection.md", "file_name": "24-Lustre File System Failover and Multiple-Mount Protection.md", "file_type": "text/markdown", "file_size": 3302, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc5722147e9cc7f7b1e30c10dbfa724f643674114b733aab0f5bbfaad1ccb737", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Lustre File System Failover and Multiple-Mount Protection\n\n- [Lustre File System Failover and Multiple-Mount Protection](#lustre-file-system-failover-and-multiple-mount-protection)\n  * [Overview of Multiple-Mount Protection](#overview-of-multiple-mount-protection)\n  * [Working with Multiple-Mount Protection](#working-with-multiple-mount-protection)\n\nThis chapter describes the multiple-mount protection (MMP) feature, which protects the file system from being mounted simultaneously to more than one node. It includes the following sections:\n\n- [the section called \u201c Overview of Multiple-Mount Protection\u201d](#overview-of-multiple-mount-protection)\n- [the section called \u201c Working with Multiple-Mount Protection\u201d](#working-with-multiple-mount-protection)\n\n**Note**\nFor information about configuring a Lustre file system for failover, see Configuring Failover in a Lustre File System\n\n## Overview of Multiple-Mount Protection\n\nThe multiple-mount protection (MMP) feature protects the Lustre file system from being mounted simultaneously to more than one node. This feature is important in a shared storage environment (for example, when a failover pair of OSSs share a LUN).\n\nThe backend file system, `ldiskfs`, supports the MMP mechanism. A block in the file system is updated by a `kmmpd`daemon at one second intervals, and a sequence number is written in this block. If the file system is cleanly unmounted, then a special \"clean\" sequence is written to this block. When mounting the file system, `ldiskfs`checks if the MMP block has a clean sequence or not.\n\nEven if the MMP block has a clean sequence, `ldiskfs` waits for some interval to guard against the following situations:\n\n- If I/O traffic is heavy, it may take longer for the MMP block to be updated.\n- If another node is trying to mount the same file system, a \"race\" condition may occur.\n\nWith MMP enabled, mounting a clean file system takes at least 10 seconds. If the file system was not cleanly unmounted, then the file system mount may require additional time.\n\n**Note**\n\nThe MMP feature is only supported on Linux kernel versions newer than 2.6.9.\n\n## Working with Multiple-Mount Protection\n\nOn a new Lustre file system, MMP is automatically enabled by `mkfs.lustre` at format time if failover is being used and the kernel and `e2fsprogs` version support it. On an existing file system, a Lustre file system administrator can manually enable MMP when the file system is unmounted.\n\nUse the following commands to determine whether MMP is running in the Lustre file system and to enable or disable the MMP feature.\n\nTo determine if MMP is enabled, run:\n\n```\ndumpe2fs -h /dev/block_device | grep mmp\n```\n\nHere is a sample command:\n\n```\ndumpe2fs -h /dev/sdc | grep mmp \nFilesystem features: has_journal ext_attr resize_inode dir_index \nfiletype extent mmp sparse_super large_file uninit_bg\n```\n\nTo manually disable MMP, run:\n\n```\ntune2fs -O ^mmp /dev/block_device\n```\n\nTo manually enable MMP, run:\n\n```\ntune2fs -O mmp /dev/block_device\n```\n\nWhen MMP is enabled, if `ldiskfs` detects multiple mount attempts after the file system is mounted, it blocks these later mount attempts and reports the time when the MMP block was last updated, the node name, and the device name of the node where the file system is currently mounted.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70dfe8e7-61f0-4329-8e86-bba6c99b3384": {"__data__": {"id_": "70dfe8e7-61f0-4329-8e86-bba6c99b3384", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2e3055a-18ce-4694-9f0d-96b35b5732b0", "node_type": "1", "metadata": {}, "hash": "31a579edc6e9dcf8272a881e82742fae8b394a8ab38715321a6c921024fe4ca5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Configuring and Managing Quotas\n\n- [Configuring and Managing Quotas](#configuring-and-managing-quotas)\n  * [Working with Quotas](#working-with-quotas)\n  * [Enabling Disk Quotas](#enabling-disk-quotas)\n    + [Enabling Disk Quotas (Lustre Software Release 2.4 and later)](#enabling-disk-quotas-lustre-software-release-24-and-later)\n      - [Quota Verification](#quota-verification)\n  * [Quota Administration](#quota-administration)\n  * [Default Quota](#Default Quota)\n  * [Quota Allocation](#quota-allocation)\n  * [Quotas and Version Interoperability](#quotas-and-version-interoperability)\n  * [Granted Cache and Quota Limits](#granted-cache-and-quota-limits)\n  * [Lustre Quota Statistics](#lustre-quota-statistics)\n    + [Interpreting Quota Statistics](#interpreting-quota-statistics)\n  * [Pool Quotas] (#Pool Quotas)\n\n## Working with Quotas\n\nQuotas allow a system administrator to limit the amount of disk space a user, group, or project can use. Quotas are set by root, and can be specified for individual users, groups, and/or projects. Before a file is written to a partition where quotas are set, the quota of the creator's group is checked. If a quota exists, then the file size counts towards the group's quota. If no quota exists, then the owner's user quota is checked before the file is written. Similarly, inode usage for specific functions can be controlled if a user over-uses the allocated space.\n\nLustre quota enforcement differs from standard Linux quota enforcement in several ways:\n\n- Quotas are administered via the `lfs` and `lctl` commands (post-mount).\n\n- The quota feature in Lustre software is distributed throughout the system (as the Lustre file system is a distributed file system). Because of this, quota setup and behavior on Lustre is somewhat different from local disk quotas in the following ways:\n  - No single point of administration: some commands must be executed on the MGS, other commands on the MDSs and OSSs, and still other commands on the client.\n  - Granularity: a local quota is typically specified for kilobyte resolution, Lustre uses one megabyte as the smallest quota resolution.\n  - Accuracy: quota information is distributed throughout the file system and can only be accurately calculated with a quiescent file system in order to minimize performance overhead during normal use.\n  \n- Quotas are allocated and consumed in a quantized fashion.\n\n- Client does not set the `usrquota` or `grpquota` options to mount. Space accounting is enabled by default and quota enforcement can be enabled/disabled on a per-filesystem basis with `lctl conf_param`.\n\n   It is worth noting that both `lfs quotaon`,`lfs quotaoff`, `lfs quotacheck` and `quota_type` sub-commands are deprecated as of Lustre 2.4.0, and removed completely in Lustre 2.8.0.\n\n**Caution**\n\nAlthough a quota feature is available in the Lustre software, root quotas are NOT enforced.\n\n`lfs setquota -u root` (limits are not enforced)\n\n`lfs quota -u root` (usage includes internal Lustre data that is dynamic in size and does not accurately reflect mount point visible block and inode usage).\n\n## Enabling Disk Quotas\n\nThe design of quotas on Lustre has management and enforcement separated from resource usage and accounting. Lustre software is responsible for management and enforcement. The back-end file system is responsible for resource usage and accounting. Because of this, it is necessary to begin enabling quotas by enabling quotas on the back-end disk system. \n\n**Caution**\n\nQuota setup is orchestrated by the MGS and *all setup commands in this section must be run directly on the MGS*. Support for project quotas specifically requires Lustre Release 2.10 or later. A *patched server* may be required, depending on the kernel version and backend filesystem type:\n\n| **Configuration**                             | **Patched Server Required?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3856, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2e3055a-18ce-4694-9f0d-96b35b5732b0": {"__data__": {"id_": "c2e3055a-18ce-4694-9f0d-96b35b5732b0", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70dfe8e7-61f0-4329-8e86-bba6c99b3384", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5d7edc048f5f3fb29f0de8a590456abdb9c6ea2072d156023eefd7a3e2c56a1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be88f882-9e95-4063-b60b-ea860976d866", "node_type": "1", "metadata": {}, "hash": "7adc55e8330247de9f36be869882c04f745f912bfed13147344a2d583fe7a2cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Enabling Disk Quotas\n\nThe design of quotas on Lustre has management and enforcement separated from resource usage and accounting. Lustre software is responsible for management and enforcement. The back-end file system is responsible for resource usage and accounting. Because of this, it is necessary to begin enabling quotas by enabling quotas on the back-end disk system. \n\n**Caution**\n\nQuota setup is orchestrated by the MGS and *all setup commands in this section must be run directly on the MGS*. Support for project quotas specifically requires Lustre Release 2.10 or later. A *patched server* may be required, depending on the kernel version and backend filesystem type:\n\n| **Configuration**                             | **Patched Server Required?** |\n| --------------------------------------------- | ---------------------------- |\n| *ldiskfs with kernel version < 4.5*           | Yes                          |\n| *ldiskfs with kernel version >= 4.5*          | No                           |\n| *zfs version >=0.8 with kernel version < 4.5* | Yes                          |\n| *zfs version >=0.8 with kernel version > 4.5* | No                           |\n\n**Note**: Project quotas are not supported on zfs versions earlier than 0.8.\n\nOnce setup, verification of the quota state must be performed on the MDT. Although quota enforcement is managed by the Lustre software, each OSD implementation relies on the back-end file system to maintain per-user/group/project block and inode usage. Hence, differences exist when setting up quotas with ldiskfs or ZFS back-ends:\n\n- For ldiskfs backends, `mkfs.lustre` now creates empty quota files and enables the QUOTA feature flag in the superblock which turns quota accounting on at mount time automatically. e2fsck was also modified to fix the quota files when the QUOTA feature flag is present. The project quota feature is disabled by default, and`tune2fs` needs to be run to enable every target manually. If user, group, and project quota usage is inconsistent, run `e2fsck -f` on all unmounted MDTs and OSTs.\n- For ZFS backend, *the project quota feature is not supported on zfs versions less than 0.8.0.* Accounting ZAPs are created and maintained by the ZFS file system itself. While ZFS tracks per-user and group block usage, it does not handle inode accounting for ZFS versions prior to zfs-0.7.0. The ZFS OSD previously implemented its own support for inode tracking. Two options are available:\n  1. The ZFS OSD can estimate the number of inodes in-use based on the number of blocks used by a given user or group. This mode can be enabled by running the following command on the server running the target: `lctl set_param osd-zfs.${FSNAME}-${TARGETNAME}.quota_iused_estimate=1`.\n  2. Similarly to block accounting, dedicated ZAPs are also created the ZFS OSD to maintain per-user and group inode usage. This is the default mode which corresponds to `quota_iused_estimate` set to 0.\n\n**Note**\n\nTo (re-)enable space usage quota on ldiskfs filesystems, run `tune2fs -O quota` against all targets. This command sets the QUOTA feature flag in the superblock and runs e2fsck internally. As a result, the target must be offline to build the per-UID/GID disk usage database.\n\nIntroduced in Lustre 2.10\n\nLustre filesystems formatted with a Lustre release prior to 2.10 can be still safely upgraded to release 2.10, but will not have project quota usage reporting functional until 2.15.0 or `tune2fs -O project` is run against all ldiskfs backend targets. This command sets the PROJECT feature flag in the superblock and runs e2fsck (as a result, the target must be offline). See *the section called \u201c Quotas and Version Interoperability\u201d* for further important considerations.\n\n**Caution**\n\nLustre requires a version of e2fsprogs that supports quota to be installed on the server nodes when using ldiskfs backend (e2fsprogs is not needed with ZFS backend). In general, we recommend to use the latest e2fsprogs version available on [http://downloads.whamcloud.com/public/e2fsprogs/](http://downloads.whamcloud.com/e2fsprogs/).\n\nThe ldiskfs OSD relies on the standard Linux quota to maintain accounting information on disk.", "mimetype": "text/plain", "start_char_idx": 3098, "end_char_idx": 7273, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be88f882-9e95-4063-b60b-ea860976d866": {"__data__": {"id_": "be88f882-9e95-4063-b60b-ea860976d866", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2e3055a-18ce-4694-9f0d-96b35b5732b0", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ba24d3d2b2350e0ddaecb5de88038c9cc412072e4fb23eaa78f91851167959bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae7ea765-7866-4f26-9f1b-64e116e1241f", "node_type": "1", "metadata": {}, "hash": "7899c57363d4c85045e37538044443ba7bdf71ef1d4865b78496e1219249e630", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This command sets the PROJECT feature flag in the superblock and runs e2fsck (as a result, the target must be offline). See *the section called \u201c Quotas and Version Interoperability\u201d* for further important considerations.\n\n**Caution**\n\nLustre requires a version of e2fsprogs that supports quota to be installed on the server nodes when using ldiskfs backend (e2fsprogs is not needed with ZFS backend). In general, we recommend to use the latest e2fsprogs version available on [http://downloads.whamcloud.com/public/e2fsprogs/](http://downloads.whamcloud.com/e2fsprogs/).\n\nThe ldiskfs OSD relies on the standard Linux quota to maintain accounting information on disk. As a consequence, the Linux kernel running on the Lustre servers using ldiskfs backend must have`CONFIG_QUOTA`, `CONFIG_QUOTACTL` and `CONFIG_QFMT_V2` enabled.\n\nQuota enforcement is turned on/off independently of space accounting which is always enabled. There is a single per-file system quota parameter controlling inode/block quota enforcement. Like all permanent parameters, this quota parameter can be set via `lctl conf_param` on the MGS via the following syntax:\n\n```\nlctl conf_param fsname.quota.ost|mdt=u|g|p|ugp|none\n```\n\n- `ost` -- to configure block quota managed by OSTs\n- `mdt` -- to configure inode quota managed by MDTs\n- `u` -- to enable quota enforcement for users only\n- `g` -- to enable quota enforcement for groups only\n- `p` -- to enable quota enforcement for projects only\n- `ugp` -- to enable quota enforcement for all users, groups and projects\n- `none` -- to disable quota enforcement for all users, groups and projects\n\nExamples:\n\nTo turn on user, group, and project quotas for block only on file system `testfs1`, *on the MGS* run:\n\n```\nmgs# lctl conf_param testfs1.quota.ost=ugp \n```\n\nTo turn on group quotas for inodes on file system `testfs2`, on the MGS run:\n\n```\nmgs# lctl conf_param testfs2.quota.mdt=g \n```\n\nTo turn off user, group, and project quotas for both inode and block on file system `testfs3`, on the MGS run:\n\n```\nmgs# lctl conf_param testfs3.quota.ost=none\nmgs# lctl conf_param testfs3.quota.mdt=none\n```\n#### Quota Verification\n\nOnce the quota parameters have been configured, all targets which are part of the file system will be automatically notified of the new quota settings and enable/disable quota enforcement as needed. The per-target enforcement status can still be verified by running the following *command on the MDS(s)*:\n\n```\n$ lctl get_param osd-*.*.quota_slave.info\nosd-zfs.testfs-MDT0000.quota_slave.info=\ntarget name:    testfs-MDT0000\npool ID:        0\ntype:           md\nquota enabled:  ug\nconn to master: setup\nuser uptodate:  glb[1],slv[1],reint[0]\ngroup uptodate: glb[1],slv[1],reint[0]\n```\n\n## Quota Administration\n\nOnce the file system is up and running, quota limits on blocks and inodes can be set for user, group, and project. This is *controlled entirely from a client* via three quota parameters:\n\n**Grace period**-- The period of time (in seconds) within which users are allowed to exceed their soft limit. There are six types of grace periods:\n\n- user block soft limit\n- user inode soft limit\n- group block soft limit\n- group inode soft limit\n- project block soft limit\n- project inode soft limit\n\nThe grace period applies to all users. The user block soft limit is for all users who are using a blocks quota.\n\n**Soft limit** -- The grace timer is started once the soft limit is exceeded. At this point, the user/group/project can still allocate block/inode. When the grace time expires and if the user is still above the soft limit, the soft limit becomes a hard limit and the user/group/project can't allocate any new block/inode any more. The user/group/project should then delete files to be under the soft limit. The soft limit MUST be smaller than the hard limit. If the soft limit is not needed, it should be set to zero (0).", "mimetype": "text/plain", "start_char_idx": 6607, "end_char_idx": 10484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae7ea765-7866-4f26-9f1b-64e116e1241f": {"__data__": {"id_": "ae7ea765-7866-4f26-9f1b-64e116e1241f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be88f882-9e95-4063-b60b-ea860976d866", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "497b4b6675d36f647c1d3ed37a7eb4c79bd6693443115c1d1a59c7e75b82129f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac99226f-31a3-4bad-9db9-363ac8bac564", "node_type": "1", "metadata": {}, "hash": "3677882e6cf0cb85c260989494ad70f4991d68159bac15bb9d08a308f97d19cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are six types of grace periods:\n\n- user block soft limit\n- user inode soft limit\n- group block soft limit\n- group inode soft limit\n- project block soft limit\n- project inode soft limit\n\nThe grace period applies to all users. The user block soft limit is for all users who are using a blocks quota.\n\n**Soft limit** -- The grace timer is started once the soft limit is exceeded. At this point, the user/group/project can still allocate block/inode. When the grace time expires and if the user is still above the soft limit, the soft limit becomes a hard limit and the user/group/project can't allocate any new block/inode any more. The user/group/project should then delete files to be under the soft limit. The soft limit MUST be smaller than the hard limit. If the soft limit is not needed, it should be set to zero (0).\n\n**Hard limit** -- Block or inode allocation will fail with `EDQUOT`(i.e. quota exceeded) when the hard limit is reached. The hard limit is the absolute limit. When a grace period is set, one can exceed the soft limit within the grace period if under the hard limit.\n\nDue to the distributed nature of a Lustre file system and the need to maintain performance under load, those quota parameters may not be 100% accurate. The quota settings can be manipulated via the `lfs` command, executed on a client, and includes several options to work with quotas:\n\n- `quota` -- displays general quota information (disk usage and limits)\n- `setquota` -- specifies quota limits and tunes the grace period. By default, the grace period is one week.\n\nUsage:\n\n```\nlfs quota [-q] [-v] [-h] [-o obd_uuid] [-u|-g|-p uname|uid|gname|gid|projid] /mount_point\nlfs quota -t {-u|-g|-p} /mount_point\nlfs setquota {-u|--user|-g|--group|-p|--project} username|groupname [-b block-softlimit] \\\n             [-B block_hardlimit] [-i inode_softlimit] \\\n             [-I inode_hardlimit] /mount_point\n```\n\nTo display general quota information (disk usage and limits) for the user running the command and his primary group, run:\n\n```\n$ lfs quota /mnt/testfs\n```\n\nTo display general quota information for a specific user (\" `bob`\" in this example), run:\n\n```\n$ lfs quota -u bob /mnt/testfs\n```\n\nTo display general quota information for a specific user (\" `bob`\" in this example) and detailed quota statistics for each MDT and OST, run:\n\n```\n$ lfs quota -u bob -v /mnt/testfs\n```\n\nTo display general quota information for a specific project (\" `1`\" in this example), run:\n\n```\n$ lfs quota -p 1 /mnt/testfs\n```\n\nTo display general quota information for a specific group (\" `eng`\" in this example), run:\n\n```\n$ lfs quota -g eng /mnt/testfs\n```\n\nTo limit quota usage for a specific project ID on a specific directory (\"`/mnt/testfs/dir`\" in this example), run:\n\n```\n$ lfs project -s -p 1 -r /mnt/testfs/dir\n$ lfs setquota -p 1 -b 307200 -B 309200 -i 10000 -I 11000 /mnt/testfs\n```\n\nRecursively list all descendants'(of the directory) project attribute on directory (\"/mnt/testfs/dir\" in this example), run:\n\n```\n$ lfs project -r /mnt/testfs/dir\n```\n\nPlease note that if it is desired to have lfs quota -p show the space/inode usage under the directory properly (much faster than du), then the user/admin needs to use different project IDs for different directories.\n\nTo display block and inode grace times for user quotas, run:\n\n```\n$ lfs quota -t -u /mnt/testfs\n```\n\nTo set user or group quotas for a specific ID (\"bob\" in this example), run:\n\n```\n$ lfs setquota -u bob -b 307200 -B 309200 -i 10000 -I 11000 /mnt/testfs\n```\n\nIn this example, the quota for user \"bob\" is set to 300 MB (309200*1024) and the hard limit is 11,000 files. Therefore, the inode hard limit should be 11000.\n\nThe quota command displays the quota allocated and consumed by each Lustre target.", "mimetype": "text/plain", "start_char_idx": 9658, "end_char_idx": 13415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac99226f-31a3-4bad-9db9-363ac8bac564": {"__data__": {"id_": "ac99226f-31a3-4bad-9db9-363ac8bac564", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae7ea765-7866-4f26-9f1b-64e116e1241f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "253374b7d835ac46f6023b9c218e6af8616fb3203b5a91dc3411ae8e2392b54b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ae994c4-2e69-45a9-bfcf-029cb6b69e20", "node_type": "1", "metadata": {}, "hash": "5e0562a9c9aba1efdcccfd7d2f3e159b87ecc8f560fbc09836eb3a0b67f49a6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To display block and inode grace times for user quotas, run:\n\n```\n$ lfs quota -t -u /mnt/testfs\n```\n\nTo set user or group quotas for a specific ID (\"bob\" in this example), run:\n\n```\n$ lfs setquota -u bob -b 307200 -B 309200 -i 10000 -I 11000 /mnt/testfs\n```\n\nIn this example, the quota for user \"bob\" is set to 300 MB (309200*1024) and the hard limit is 11,000 files. Therefore, the inode hard limit should be 11000.\n\nThe quota command displays the quota allocated and consumed by each Lustre target. Using the previous `setquota`example, running this `lfs` quota command:\n\n```\n$ lfs quota -u bob -v /mnt/testfs\n```\n\ndisplays this command output:\n\n```\nDisk quotas for user bob (uid 6000):\nFilesystem          kbytes quota limit grace files quota limit grace\n/mnt/testfs         0      30720 30920 -     0     10000 11000 -\ntestfs-MDT0000_UUID 0      -      8192 -     0     -     2560  -\ntestfs-OST0000_UUID 0      -      8192 -     0     -     0     -\ntestfs-OST0001_UUID 0      -      8192 -     0     -     0     -\nTotal allocated inode limit: 2560, total allocated block limit: 24576\n```\n\nGlobal quota limits are stored in dedicated index files (there is one such index per quota type) on the quota master target (aka QMT). The QMT runs on MDT0000 and exports the global indices via *lctl get_param*. The global indices can thus be dumped via the following command:\n\n```\n# lctl get_param qmt.testfs-QMT0000.*.glb-*\n```\n\nThe format of global indexes depends on the OSD type. The ldiskfs OSD uses an IAM files while the ZFS OSD creates dedicated ZAPs.\n\nEach slave also stores a copy of this global index locally. When the global index is modified on the master, a glimpse callback is issued on the global quota lock to notify all slaves that the global index has been modified. This glimpse callback includes information about the identifier subject to the change. If the global index on the QMT is modified while a slave is disconnected, the index version is used to determine whether the slave copy of the global index isn't up to date any more. If so, the slave fetches the whole index again and updates the local copy. The slave copy of the global index can also be accessed via the following command:\n\n```\nlctl get_param osd-*.*.quota_slave.limit*\n```\n\n## Default Quota\n\nThe default quota is used to enforce the quota limits for any user, group, or project that do not have quotas set by administrator. The default quota can be disabled by setting limits to 0.", "mimetype": "text/plain", "start_char_idx": 12915, "end_char_idx": 15382, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ae994c4-2e69-45a9-bfcf-029cb6b69e20": {"__data__": {"id_": "8ae994c4-2e69-45a9-bfcf-029cb6b69e20", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac99226f-31a3-4bad-9db9-363ac8bac564", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ad7c66634c94e35814b043ac0bbef8d6686e844bb1addd4e455312a47cef97af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a64024c2-d8b2-40a9-9b41-165050b39e6b", "node_type": "1", "metadata": {}, "hash": "c994b33e987690d58d2ab7b0c5625fac27133e6717e085ac740ac2e461d60d7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each slave also stores a copy of this global index locally. When the global index is modified on the master, a glimpse callback is issued on the global quota lock to notify all slaves that the global index has been modified. This glimpse callback includes information about the identifier subject to the change. If the global index on the QMT is modified while a slave is disconnected, the index version is used to determine whether the slave copy of the global index isn't up to date any more. If so, the slave fetches the whole index again and updates the local copy. The slave copy of the global index can also be accessed via the following command:\n\n```\nlctl get_param osd-*.*.quota_slave.limit*\n```\n\n## Default Quota\n\nThe default quota is used to enforce the quota limits for any user, group, or project that do not have quotas set by administrator. The default quota can be disabled by setting limits to 0.\n\n### Usage\n\n```\nlfs quota [-U|--default-usr|-G|--default-grp|-P|--default-prj] /mount_point\nlfs setquota {-U|--default-usr|-G|--default-grp|-P|--default-prj} [-b block-softlimit] \\\n [-B block_hardlimit] [-i inode_softlimit] [-I inode_hardlimit] /mount_point\nlfs setquota {-u|-g|-p} username|groupname -d /mount_point\n```\n\nTo set the default user quota: \n\n```\n# lfs setquota -U -b 10G -B 11G -i 100K -I 105K /mnt/testfs \n```\n\nTo set the default group quota: \n```\n# lfs setquota -G -b 10G -B 11G -i 100K -I 105K /mnt/testfs \n```\nTo set the default project quota: \n```\n# lfs setquota -P -b 10G -B 11G -i 100K -I 105K /mnt/testfs\n```\nTo disable the default user quota: \n```\n# lfs setquota -U -b 0 -B 0 -i 0 -I 0 /mnt/testfs\n```\nTo disable the default group quota: \n```\n# lfs setquota -G -b 0 -B 0 -i 0 -I 0 /mnt/testfs\n```\nTo disable the default project quota: \n```\n# lfs setquota -P -b 0 -B 0 -i 0 -I 0 /mnt/testfs\n```\n**Note**\nIf quota limits are set for some user, group or project, it will use those specific quota limits instead of the default quota. Quota limits for any user, group or project will use the default quota by setting its quota limits to 0.\n\n## Quota Allocation\n\nIn a Lustre file system, quota must be properly allocated or users may experience unnecessary failures. The file system block quota is divided up among the OSTs within the file system. Each OST requests an allocation which is increased up to the quota limit. The quota allocation is then quantized to reduce the number of quota-related request traffic.\n\nThe Lustre quota system distributes quotas from the Quota Master Target (aka QMT). Only one QMT instance is supported for now and only runs on the same node as MDT0000. All OSTs and MDTs set up a Quota Slave Device (aka QSD) which connects to the QMT to allocate/release quota space. The QSD is setup directly from the OSD layer.\n\nTo reduce quota requests, quota space is initially allocated to QSDs in very large chunks. How much unused quota space can be held by a target is controlled by the qunit size. When quota space for a given ID is close to exhaustion on the QMT, the qunit size is reduced and QSDs are notified of the new qunit size value via a glimpse callback. Slaves are then responsible for releasing quota space above the new qunit value. The qunit size isn't shrunk indefinitely and there is a minimal value of 1MB for blocks and 1,024 for inodes. This means that the quota space rebalancing process will stop when this minimum value is reached. As a result, quota exceeded can be returned while many slaves still have 1MB or 1,024 inodes of spare quota space.", "mimetype": "text/plain", "start_char_idx": 14470, "end_char_idx": 17993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a64024c2-d8b2-40a9-9b41-165050b39e6b": {"__data__": {"id_": "a64024c2-d8b2-40a9-9b41-165050b39e6b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ae994c4-2e69-45a9-bfcf-029cb6b69e20", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0020e9f050f98fad2c07d0af2e6da8107b182eb81179ffe5bf4a912c6b7e8a78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f4c9d8e-4d88-4a1f-90ea-d8721ff2c572", "node_type": "1", "metadata": {}, "hash": "592ac40a6361a8e46c4aaf541fee8678787f470bb2da81be77bc2e0a31eea207", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The QSD is setup directly from the OSD layer.\n\nTo reduce quota requests, quota space is initially allocated to QSDs in very large chunks. How much unused quota space can be held by a target is controlled by the qunit size. When quota space for a given ID is close to exhaustion on the QMT, the qunit size is reduced and QSDs are notified of the new qunit size value via a glimpse callback. Slaves are then responsible for releasing quota space above the new qunit value. The qunit size isn't shrunk indefinitely and there is a minimal value of 1MB for blocks and 1,024 for inodes. This means that the quota space rebalancing process will stop when this minimum value is reached. As a result, quota exceeded can be returned while many slaves still have 1MB or 1,024 inodes of spare quota space.\n\nIf we look at the `setquota` example again, running this `lfs quota` command:\n\n```\n# lfs quota -u bob -v /mnt/testfs\n```\n\ndisplays this command output:\n\n```\nDisk quotas for user bob (uid 500):\nFilesystem          kbytes quota limit grace       files  quota limit grace\n/mnt/testfs         30720* 30720 30920 6d23h56m44s 10101* 10000 11000\n6d23h59m50s\ntestfs-MDT0000_UUID 0      -     0     -           10101  -     10240\ntestfs-OST0000_UUID 0      -     1024  -           -      -     -\ntestfs-OST0001_UUID 30720* -     29896 -           -      -     -\nTotal allocated inode limit: 10240, total allocated block limit: 30920\n```\n\nThe total quota limit of 30,920 is allocated to user bob, which is further distributed to two OSTs.\n\nValues appended with ' `*`' show that the quota limit has been exceeded, causing the following error when trying to write or create a file:\n\n```\n$ cp: writing `/mnt/testfs/foo`: Disk quota exceeded.\n```\n\n**Note**\n\nIt is very important to note that the block quota is consumed per OST and the inode quota per MDS. Therefore, when the quota is consumed on one OST (resp. MDT), the client may not be able to create files regardless of the quota available on other OSTs (resp. MDTs).\n\nSetting the quota limit below the minimal qunit size may prevent the user/group from all file creation. It is thus recommended to use soft/hard limits which are a multiple of the number of OSTs * the minimal qunit size.\n\nTo determine the total number of inodes, use `lfs df -i`(and also `lctl get_param *.*.filestotal`). For more information on using the `lfs df -i` command and the command output, see *the section called \u201cChecking File System Free Space\u201d*.\n\nUnfortunately, the `statfs` interface does not report the free inode count directly, but instead reports the total inode and used inode counts. The free inode count is calculated for `df` from (total inodes - used inodes). It is not critical to know the total inode count for a file system. Instead, you should know (accurately), the free inode count and the used inode count for a file system. The Lustre software manipulates the total inode count in order to accurately report the other two values.\n\n## Quotas and Version Interoperability\n\nIntroduced in Lustre 2.10\n\nTo use the project quota functionality introduced in Lustre 2.10, **all Lustre servers and clients must be upgraded to Lustre release 2.10 or later for project quota to work correctly**. Otherwise, project quota will be inaccessible on clients and not be accounted for on OSTs. Furthermore, the **servers may be required to use a patched kernel,** for more information see *the section called \u201cEnabling Disk Quotas (Lustre Software Release 2.4 and later)\u201d*\n\nIntroduced in Lustre 2.14\n\n`df` and `lfs df` will return the amount of space available to that project rather than the total filesystem space, if the project quota limit is smaller. **Only client need be upgraded to Lustre release 2.14 or later to apply this new behavior.**\n\n## Granted Cache and Quota Limits\n\nIn a Lustre file system, granted cache does not respect quota limits. In this situation, OSTs grant cache to a Lustre client to accelerate I/O.", "mimetype": "text/plain", "start_char_idx": 17200, "end_char_idx": 21147, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f4c9d8e-4d88-4a1f-90ea-d8721ff2c572": {"__data__": {"id_": "5f4c9d8e-4d88-4a1f-90ea-d8721ff2c572", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a64024c2-d8b2-40a9-9b41-165050b39e6b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "abe2b5a012c8e81af55aa134de35e2d665c37268599e483a0358ffa9114d0fea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47c151d5-aa4f-482d-8278-9923dd87a84c", "node_type": "1", "metadata": {}, "hash": "86350dea7b3a67ca25a799723aa38074bd0577964e30865e624ee130394f5ee2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Otherwise, project quota will be inaccessible on clients and not be accounted for on OSTs. Furthermore, the **servers may be required to use a patched kernel,** for more information see *the section called \u201cEnabling Disk Quotas (Lustre Software Release 2.4 and later)\u201d*\n\nIntroduced in Lustre 2.14\n\n`df` and `lfs df` will return the amount of space available to that project rather than the total filesystem space, if the project quota limit is smaller. **Only client need be upgraded to Lustre release 2.14 or later to apply this new behavior.**\n\n## Granted Cache and Quota Limits\n\nIn a Lustre file system, granted cache does not respect quota limits. In this situation, OSTs grant cache to a Lustre client to accelerate I/O. Granting cache causes writes to be successful in OSTs, even if they exceed the quota limits, and will overwrite them.\n\nThe sequence is:\n\n1. A user writes files to the Lustre file system.\n2. If the Lustre client has enough granted cache, then it returns 'success' to users and arranges the writes to the OSTs.\n3. Because Lustre clients have delivered success to users, the OSTs cannot fail these writes.\n\nBecause of granted cache, writes always overwrite quota limitations. For example, if you set a 400 GB quota on user A and use IOR to write for user A from a bundle of clients, you will write much more data than 400 GB, and cause an out-of-quota error ( `EDQUOT`).\n\n**Note**\n\nThe effect of granted cache on quota limits can be mitigated, but not eradicated. Reduce the maximum amount of dirty data on the clients (minimal value is 1MB):\n\n- `lctl set_param osc.*.max_dirty_mb=8`\n\n## Lustre Quota Statistics\n\nThe Lustre software includes statistics that monitor quota activity, such as the kinds of quota RPCs sent during a specific period, the average time to complete the RPCs, etc. These statistics are useful to measure performance of a Lustre file system.\n\nEach quota statistic consists of a quota event and `min_time`, `max_time` and `sum_time` values for the event.\n\n| **Quota Event**                                              | **Description**                                              |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| **sync_acq_req**                                             | Quota slaves send a acquiring_quota request and wait for its return. |\n| **sync_rel_req**                                             | Quota slaves send a releasing_quota request and wait for its return. |\n| **async_acq_req**                                            | Quota slaves send an acquiring_quota request and do not wait for its return. |\n| **async_rel_req**                                            | Quota slaves send a releasing_quota request and do not wait for its return. |\n| **wait_for_blk_quota (lquota_chkquota)**                     | Before data is written to OSTs, the OSTs check if the remaining block quota is sufficient. This is done in the lquota_chkquota function. |\n| **wait_for_ino_quota (lquota_chkquota)**                     | Before files are created on the MDS, the MDS checks if the remaining inode quota is sufficient. This is done in the lquota_chkquota function. |\n| **wait_for_blk_quota (lquota_pending_commit)**               | After blocks are written to OSTs, relative quota information is updated. This is done in the lquota_pending_commit function. |\n| **wait_for_ino_quota (lquota_pending_commit)**               | After files are created, relative quota information is updated. This is done in the lquota_pending_commit function. |\n| **wait_for_pending_blk_quota_req (qctxt_wait_pending_dqacq)** | On the MDS or OSTs, there is one thread sending a quota request for a specific UID/GID for block quota at any time. At that time, if other threads need to do this too, they should wait. This is done in the qctxt_wait_pending_dqacq function. |\n| **wait_for_pending_ino_quota_req (qctxt_wait_pending_dqacq)** | On the MDS, there is one thread sending a quota request for a specific UID/GID for inode quota at any time. If other threads need to do this too, they should wait. This is done in the qctxt_wait_pending_dqacq function. |\n| **nowait_for_pending_blk_quota_req (qctxt_wait_pending_dqacq)** | On the MDS or OSTs, there is one thread sending a quota request for a specific UID/GID for block quota at any time.", "mimetype": "text/plain", "start_char_idx": 20422, "end_char_idx": 24805, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "47c151d5-aa4f-482d-8278-9923dd87a84c": {"__data__": {"id_": "47c151d5-aa4f-482d-8278-9923dd87a84c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f4c9d8e-4d88-4a1f-90ea-d8721ff2c572", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7a180c1e62123808bdc44bcb964a8d5245513be666d5ef5bcdb4917d145e0935", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0234c91-95c9-48eb-9c53-dcdbbd4be96a", "node_type": "1", "metadata": {}, "hash": "3353f577b7bdc196e47349340bd71d28a56e1797e0a29b7207784386ebf4352d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At that time, if other threads need to do this too, they should wait. This is done in the qctxt_wait_pending_dqacq function. |\n| **wait_for_pending_ino_quota_req (qctxt_wait_pending_dqacq)** | On the MDS, there is one thread sending a quota request for a specific UID/GID for inode quota at any time. If other threads need to do this too, they should wait. This is done in the qctxt_wait_pending_dqacq function. |\n| **nowait_for_pending_blk_quota_req (qctxt_wait_pending_dqacq)** | On the MDS or OSTs, there is one thread sending a quota request for a specific UID/GID for block quota at any time. When threads enter qctxt_wait_pending_dqacq, they do not need to wait. This is done in the qctxt_wait_pending_dqacq function. |\n| **nowait_for_pending_ino_quota_req (qctxt_wait_pending_dqacq)** | On the MDS, there is one thread sending a quota request for a specific UID/GID for inode quota at any time. When threads enter qctxt_wait_pending_dqacq, they do not need to wait. This is done in the qctxt_wait_pending_dqacq function. |\n| **quota_ctl**                                                | The quota_ctl statistic is generated when lfs `setquota`, `lfs quota` and so on, are issued. |\n| **adjust_qunit**                                             | Each time qunit is adjusted, it is counted.                  |\n\n### Interpreting Quota Statistics\n\nQuota statistics are an important measure of the performance of a Lustre file system. Interpreting these statistics correctly can help you diagnose problems with quotas, and may indicate adjustments to improve system performance.\n\nFor example, if you run this command on the OSTs:\n\n```\nlctl get_param lquota.testfs-OST0000.stats\n```\n\nYou will get a result similar to this:\n\n```\nsnapshot_time                                1219908615.506895 secs.usecs\nasync_acq_req                              1 samples [us]  32 32 32\nasync_rel_req                              1 samples [us]  5 5 5\nnowait_for_pending_blk_quota_req(qctxt_wait_pending_dqacq) 1 samples [us] 2\\\n 2 2\nquota_ctl                          4 samples [us]  80 3470 4293\nadjust_qunit                               1 samples [us]  70 70 70\n....\n```\n\nIn the first line, `snapshot_time` indicates when the statistics were taken. The remaining lines list the quota events and their associated data.\n\nIn the second line, the `async_acq_req` event occurs one time. The `min_time`, `max_time` and `sum_time` statistics for this event are 32, 32 and 32, respectively. The unit is microseconds (\u03bcs).\n\nIn the fifth line, the quota_ctl event occurs four times. The `min_time`, `max_time` and `sum_time` statistics for this event are 80, 3470 and 4293, respectively. The unit is microseconds (\u03bcs).\n\n## Pool Quotas\n\nOST Pool Quotas feature gives an ability to limit user's (group's/project's) disk usage at OST pool level. Each OST Pool Quota (PQ) maps directly to the OST pool of the same name. Thus PQ could be tuned with standard lctl pool_new/add/remove/erase commands. All PQ are subset of a global pool that includes all OSTs and MDTs (DOM case). It may be initially confusing to be prevented from using \"all of\" one quota due to a different quota setting. In Lustre, a quota is a limit, not a right to use an amount. You don't always get to use your quota - an OST may be out of space, or some other quota is limiting. For example, if there is an inode quota and a space quota, and you hit your inode limit while you still have plenty of space, you can't use the space. For another example, quotas may easily be overallocated: everyone gets 10PB of quota, in a 15PB system. That does not give them the right to use 10PB, it means they cannot use more than 10PB. They may very well get ENOSPC long before that - but they will not get EDQUOT.", "mimetype": "text/plain", "start_char_idx": 24208, "end_char_idx": 27956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0234c91-95c9-48eb-9c53-dcdbbd4be96a": {"__data__": {"id_": "d0234c91-95c9-48eb-9c53-dcdbbd4be96a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb87823864b2186f9c809399454affb559754d2afe02a631b20c65cc6d97335f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47c151d5-aa4f-482d-8278-9923dd87a84c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6f1bb6e1443acf7519e150d305016bdc7b6670d9c2f74b433c9714b0bbff0e83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It may be initially confusing to be prevented from using \"all of\" one quota due to a different quota setting. In Lustre, a quota is a limit, not a right to use an amount. You don't always get to use your quota - an OST may be out of space, or some other quota is limiting. For example, if there is an inode quota and a space quota, and you hit your inode limit while you still have plenty of space, you can't use the space. For another example, quotas may easily be overallocated: everyone gets 10PB of quota, in a 15PB system. That does not give them the right to use 10PB, it means they cannot use more than 10PB. They may very well get ENOSPC long before that - but they will not get EDQUOT. This behavior already exists in Lustre today, but pool quotas increase the number of limits in play: user, group or project global space quota and now all of those limits can also be defined for each pool. In all cases, the net effect is that the actual amount of space you can use is limited to the smallest (min) quota out of everything that is applicable. See more details in OST Pool Quotas HLD [http://wiki.lustre.org/OST_Pool_Quotas_HLD]\n\n### DOM and MDT pools\n\nFrom Quota Master point of view, \"data\" MDTs are regular members together with OSTs. However Pool Quotas support only OSTs as there is currently no mechanism to group MDTs in pools.\n\n### Lfs quota/setquota options to setup quota pools\n\nThe same long option `--pool` is used to setup and report Pool Quotas with `lfs setquota` and `lfs setquota`.\n\n`lfs setquota --pool` pool_name is used to set the block and soft usage limit for the user, group, or project for the specified pool name. \n\n`lfs quota --pool` pool_name shows the user, group, or project usage for the specified pool name.\n\n### Quota pools interoperability\n\nBoth client and server should have at least Lustre 2.14 to support Pool Quotas.\n\n**Note**\n\nPool Quotas may be able to work with older clients if server supports Pool Quotas. Pool quotas cannot be viewed or modified by older clients. Since the quota enforcement is done on the servers, only a single client is needed to configure the quotas. This could be done by mounting a client directly on the MDS if needed.\n\n###  Pool Quotas Hard Limit setup example\n\nLet's imagine you need to setup quota usage for already existed OST pool flash_pool:\n\n```\n# it is a limit for global pool. PQ don't work properly without that\nlfs setquota -u ivan -B100T /mnt/testfs\n# set 1TiB block hard limit for ivan in a flash_pool\nlfs setquota -u ivan --pool flash_pool -B1T /mnt/testfs\n```\n\n**Note**\n\nSystem-side hard limit is required before setting Quota Pool limit. If you do not need to limit user at all OSTs and MDTs at system, only per pool, it is recommended to set some unrealistic big hard limit. Without a global limit in place the Quota Pool limit will not be enforced. No matter hard or soft global limit - at least one of them should be set.\n\n### Pool Quotas Soft Limit setup example\n\n```\n# notify OSTs to enforce quota for ivan\nlfs setquota -u ivan -B10T /mnt/testfs\n# soft limit 10MiB for ivan in a pool flash_pool\nlfs setquota -u ivan --pool flash_pool -b1T /mnt/testfs\n# set block grace 600 s for all users at flash_pool\nlfs setquota -t -u --block-grace 600 --pool flash_pool /mnt/testfs\n```", "mimetype": "text/plain", "start_char_idx": 27262, "end_char_idx": 30533, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4694d615-158c-4a50-9edb-cf2069562aad": {"__data__": {"id_": "4694d615-158c-4a50-9edb-cf2069562aad", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3445594814cd167537ba0d74b2637b6e324f1044231f3ef0572628ca0da292e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8f89da6-3dfc-4280-adb3-fec694077ca5", "node_type": "1", "metadata": {}, "hash": "4f84967157690a7940eb6f33652b09b97da042da17e63489fd75c27646e91470", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in Lustre 2.5\n\n# Hierarchical Storage Management (HSM)\n\n- [Hierarchical Storage Management (HSM)](#hierarchical-storage-management-hsm)\n  * [Introduction](#introduction)\n  * [Setup](#setup)\n    + [Requirements](#requirements)\n    + [Coordinator](#coordinator)\n    + [Agents](#agents)\n  * [Agents and copytool](#agents-and-copytool)\n    + [Archive ID, multiple backends](#archive-id-multiple-backends)\n    + [Registered agents](#registered-agents)\n    + [Timeout](#timeout)\n  * [Requests](#requests)\n    + [Commands](#commands)\n    + [Automatic restore](#automatic-restore)\n    + [Request monitoring](#request-monitoring)\n  * [File states](#file-states)\n  * [Tuning](#tuning)\n    + [`hsm_controlpolicy`](#hsm_controlpolicy)\n    + [`max_requests`](#max_requests)\n    + [`policy`](#policy)\n    + [`grace_delay`](#grace_delay)\n  * [change logs](#change-logs)\n  * [Policy engine](#policy-engine)\n    + [Robinhood](#robinhood)\n\nThis chapter describes how to bind Lustre to a Hierarchical Storage Management (HSM) solution.\n\n## Introduction\n\nThe Lustre file system can bind to a Hierarchical Storage Management (HSM) solution using a specific set of functions. These functions enable connecting a Lustre file system to one or more external storage systems, typically HSMs. With a Lustre file system bound to a HSM solution, the Lustre file system acts as a high speed cache in front of these slower HSM storage systems.\n\nThe Lustre file system integration with HSM provides a mechanism for files to simultaneously exist in a HSM solution and have a metadata entry in the Lustre file system that can be examined. Reading, writing or truncating the file will trigger the file data to be fetched from the HSM storage back into the Lustre file system.\n\nThe process of copying a file into the HSM storage is known as *archive*. Once the archive is complete, the Lustre file data can be deleted (known as *release*.) The process of returning data from the HSM storage to the Lustre file system is called *restore*. The archive and restore operations require a Lustre file system component called an *Agent*.\n\nAn Agent is a specially designed Lustre client node that mounts the Lustre file system in question. On an Agent, a user space program called a copytool is run to coordinate the archive and restore of files between the Lustre file system and the HSM solution.\n\nRequests to restore a given file are registered and dispatched by a facet on the MDT called the Coordinator.\n\n##### Figure 19. Overview of the Lustre file system HSM\n\n![Overview of the Lustre file system HSM](figures/HSM_copytool.svg)\n\n## Setup\n\n### Requirements\n\nTo setup a Lustre/HSM configuration you need:\n\n- a standard Lustre file system (version 2.5.0 and above)\n- a minimum of 2 clients, 1 used for your chosen computation task that generates useful data, and 1 used as an agent.\n\nMultiple agents can be employed. All the agents need to share access to their backend storage. For the POSIX copytool, a POSIX namespace like NFS or another Lustre file system is suitable.\n\n### Coordinator\n\nTo bind a Lustre file system to a HSM system a coordinator must be activated on each of your filesystem MDTs. This can be achieved with the command:\n\n```\n$ lctl set_param mdt.$FSNAME-MDT0000.hsm_control=enabled\nmdt.lustre-MDT0000.hsm_control=enabled\n```\n\nTo verify that the coordinator is running correctly\n\n```\n$ lctl get_param mdt.$FSNAME-MDT0000.hsm_control\nmdt.lustre-MDT0000.hsm_control=enabled\n```\n\n### Agents\n\nOnce a coordinator is started, launch the copytool on each agent node to connect to your HSM storage. If your HSM storage has POSIX access this command will be of the form:\n\n```\nlhsmtool_posix --daemon --hsm-root $HSMPATH --archive=1 $LUSTREPATH\n```\n\nThe POSIX copytool must be stopped by sending it a TERM signal.\n\n## Agents and copytool\n\nAgents are Lustre file system clients running copytool. copytool is a userspace daemon that transfers data between Lustre and a HSM solution.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3960, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8f89da6-3dfc-4280-adb3-fec694077ca5": {"__data__": {"id_": "b8f89da6-3dfc-4280-adb3-fec694077ca5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3445594814cd167537ba0d74b2637b6e324f1044231f3ef0572628ca0da292e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4694d615-158c-4a50-9edb-cf2069562aad", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e5b0b386f247e1f678a6becdb3fba508d7ba5c8f256ef7764ad67dace232b7a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8a9f542-64c3-4e43-8ed0-89b3db7c222b", "node_type": "1", "metadata": {}, "hash": "762b94c545c5a44bc3cfebb7fba2ee0dee86de45c7b2e7ed890c7dd81777a361", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If your HSM storage has POSIX access this command will be of the form:\n\n```\nlhsmtool_posix --daemon --hsm-root $HSMPATH --archive=1 $LUSTREPATH\n```\n\nThe POSIX copytool must be stopped by sending it a TERM signal.\n\n## Agents and copytool\n\nAgents are Lustre file system clients running copytool. copytool is a userspace daemon that transfers data between Lustre and a HSM solution. Because different HSM solutions use different APIs, copytools can typically only work with a specific HSM. Only one copytool can be run by an agent node.\n\nThe following rule applies regarding copytool instances: a Lustre file system only supports a single copytool process, per ARCHIVE ID (see below), per client node. Due to a Lustre software limitation, this constraint is irrespective of the number of Lustre file systems mounted by the Agent.\n\nBundled with Lustre tools, the POSIX copytool can work with any HSM or external storage that exports a POSIX API.\n\n### Archive ID, multiple backends\n\nA Lustre file system can be bound to several different HSM solutions. Each bound HSM solution is identified by a number referred to as ARCHIVE ID. A unique value of ARCHIVE ID must be chosen for each bound HSM solution. ARCHIVE ID must be in the range 1 to 32.\n\nA Lustre file system supports an unlimited number of copytool instances. You need, at least, one copytool per ARCHIVE ID. When using the POSIX copytool, this ID is defined using `--archive` switch.\n\nFor example: if a single Lustre file system is bound to 2 different HSMs (A and B,) ARCHIVE ID \u201c1\u201d can be chosen for HSM A and ARCHIVE ID \u201c2\u201d for HSM B. If you start 3 copytool instances for ARCHIVE ID 1, all of them will use Archive ID \u201c1\u201d. The same rule applies for copytool instances dealing with the HSM B, using Archive ID \u201c2\u201d.\n\nWhen issuing HSM requests, you can use the `--archive` switch to choose the backend you want to use. In this example, file `foo` will be archived into backend ARCHIVE ID \u201c5\u201d:\n\n```\n$ lfs hsm_archive --archive=5 /mnt/lustre/foo\n```\n\nA default ARCHIVE ID can be defined which will be used when the `--archive` switch is not specified:\n\n```\n$ lctl set_param -P mdt.lustre-MDT0000.hsm.default_archive_id=5\n```\n\nThe ARCHIVE ID of archived files can be checked using `lfs hsm_state` command:\n\n```\n$ lfs hsm_state /mnt/lustre/foo\n/mnt/lustre/foo: (0x00000009) exists archived, archive_id:5\n```\n\n### Registered agents\n\nA Lustre file system allocates a unique UUID per client mount point, for each filesystem. Only one copytool can be registered for each Lustre mount point. As a consequence, the UUID uniquely identifies a copytool, per filesystem.\n\nThe currently registered copytool instances (agents UUID) can be retrieved by running the following command, per MDT, on MDS nodes:\n\n```\n$ lctl get_param -n mdt.$FSNAME-MDT0000.hsm.agents\nuuid=a19b2416-0930-fc1f-8c58-c985ba5127ad archive_id=1 requests=[current:0 ok:0 errors:0]\n```\n\nThe returned fields have the following meaning:\n\n- `uuid` the client mount used by the corresponding copytool.\n- `archive_id` comma-separated list of ARCHIVE IDs accessible by this copytool.\n- `requests` various statistics on the number of requests processed by this copytool.\n\n### Timeout\n\nOne or more copytool instances may experience conditions that cause them to become unresponsive. To avoid blocking access to the related files a timeout value is defined for request processing. A copytool must be able to fully complete a request within this time. The default is 3600 seconds.\n\n```\n$ lctl set_param -n mdt.lustre-MDT0000.hsm.active_request_timeout\n```\n\n## Requests\n\nData management between a Lustre file system and HSM solutions is driven by requests. There are five types:\n\n- `ARCHIVE` Copy data from a Lustre file system file into the HSM solution.\n- `RELEASE` Remove file data from the Lustre file system.\n- `RESTORE` Copy back data from the HSM solution into the corresponding Lustre file system file.", "mimetype": "text/plain", "start_char_idx": 3581, "end_char_idx": 7490, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8a9f542-64c3-4e43-8ed0-89b3db7c222b": {"__data__": {"id_": "b8a9f542-64c3-4e43-8ed0-89b3db7c222b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3445594814cd167537ba0d74b2637b6e324f1044231f3ef0572628ca0da292e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8f89da6-3dfc-4280-adb3-fec694077ca5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5897a6a4fc1f5f9aaabaa797b54c25a3fd2ebe48d2e4fce4a4adc9a9e7a0405a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a2a71f4-2a66-490a-9864-9ceed73b855d", "node_type": "1", "metadata": {}, "hash": "28dbea3860e1207080e403dcdcdbb486be72e1a2e2af3780cbe4be5f2a1ec08c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `requests` various statistics on the number of requests processed by this copytool.\n\n### Timeout\n\nOne or more copytool instances may experience conditions that cause them to become unresponsive. To avoid blocking access to the related files a timeout value is defined for request processing. A copytool must be able to fully complete a request within this time. The default is 3600 seconds.\n\n```\n$ lctl set_param -n mdt.lustre-MDT0000.hsm.active_request_timeout\n```\n\n## Requests\n\nData management between a Lustre file system and HSM solutions is driven by requests. There are five types:\n\n- `ARCHIVE` Copy data from a Lustre file system file into the HSM solution.\n- `RELEASE` Remove file data from the Lustre file system.\n- `RESTORE` Copy back data from the HSM solution into the corresponding Lustre file system file.\n- `REMOVE` Delete the copy of the data from the HSM solution.\n- `CANCEL` Cancel an in-progress or pending request.\n\nOnly the `RELEASE` is performed synchronously and does not involve the coordinator. Other requests are handled by Coordinators. Each MDT coordinator is resiliently managing them.\n\n### Commands\n\nRequests are submitted using `lfs` command:\n\n```\n$ lfs hsm_archive [--archive=ID] FILE1 [FILE2...]\n$ lfs hsm_release FILE1 [FILE2...]\n$ lfs hsm_restore FILE1 [FILE2...]\n$ lfs hsm_remove  FILE1 [FILE2...]\n```\n\nRequests are sent to the default ARCHIVE ID unless an ARCHIVE ID is specified with the `--archive` option (See *the section called \u201c Archive ID, multiple backends \u201d*).\n\n### Automatic restore\n\nReleased files are automatically restored when a process tries to read or modify them. The corresponding I/O will block waiting for the file to be restored. This is transparent to the process. For example, the following command automatically restores the file if released.\n\n```\n$ cat /mnt/lustre/released_file\n```\n\n### Request monitoring\n\nThe list of registered requests and their status can be monitored, per MDT, with the following command:\n\n```\n$ lctl get_param -n mdt.lustre-MDT0000.hsm.actions\n```\n\nThe list of requests currently being processed by a copytool is available with:\n\n```\n$ lctl get_param -n mdt.lustre-MDT0000.hsm.active_requests\n```\n\n## File states\n\nWhen files are archived or released, their state in the Lustre file system changes. This state can be read using the following `lfs` command:\n\n```\n$ lfs hsm_state FILE1 [FILE2...]\n```\n\nThere is also a list of specific policy flags which could be set to have a per-file specific policy:\n\n- `NOARCHIVE` This file will never be archived.\n- `NORELEASE` This file will never be released. This value cannot be set if the flag is currently set to `RELEASED`\n- `DIRTY` This file has been modified since a copy of it was made in the HSM solution. `DIRTY` files should be archived again. The `DIRTY` flag can only be set if `EXIST` is set.\n\nThe following options can only be set by the root user.\n\n- `LOST` This file was previously archived but the copy was lost on the HSM solution for some reason in the backend (for example, by a corrupted tape), and could not be restored. If the file is not in the `RELEASE` state it needs to be archived again. If the file is in the `RELEASE` state, the file data is lost.\n\nSome flags can be manually set or cleared using the following commands:\n\n```\n$ lfs hsm_set [FLAGS] FILE1 [FILE2...]\n$ lfs hsm_clear [FLAGS] FILE1 [FILE2...]\n```\n\n## Tuning\n\n### `hsm_controlpolicy`\n\n`hsm_control` controls coordinator activity and can also purge the action list.\n\n```\n$ lctl set_param mdt.$FSNAME-MDT0000.hsm_control=purge\n```\n\nPossible values are:\n\n- `enabled` Start coordinator thread. Requests are dispatched on available copytool instances.\n- `disabled` Pause coordinator activity. No new request will be scheduled. No timeout will be handled. New requests will be registered but will be handled only when the coordinator is enabled again.\n- `shutdown` Stop coordinator thread. No request can be submitted.\n- `purge` Clear all recorded requests. Do not change coordinator state.", "mimetype": "text/plain", "start_char_idx": 6669, "end_char_idx": 10671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a2a71f4-2a66-490a-9864-9ceed73b855d": {"__data__": {"id_": "6a2a71f4-2a66-490a-9864-9ceed73b855d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3445594814cd167537ba0d74b2637b6e324f1044231f3ef0572628ca0da292e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8a9f542-64c3-4e43-8ed0-89b3db7c222b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3735f335773101fce132a6df40dc53ed488c0108d96341906d7b37e0db72118f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0985e58-e9bd-406a-a937-97aa977d2707", "node_type": "1", "metadata": {}, "hash": "b1c7c7a87d5272fd1819c79771f577e993ad4bdbbca5af22772eba4eecff0bcc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some flags can be manually set or cleared using the following commands:\n\n```\n$ lfs hsm_set [FLAGS] FILE1 [FILE2...]\n$ lfs hsm_clear [FLAGS] FILE1 [FILE2...]\n```\n\n## Tuning\n\n### `hsm_controlpolicy`\n\n`hsm_control` controls coordinator activity and can also purge the action list.\n\n```\n$ lctl set_param mdt.$FSNAME-MDT0000.hsm_control=purge\n```\n\nPossible values are:\n\n- `enabled` Start coordinator thread. Requests are dispatched on available copytool instances.\n- `disabled` Pause coordinator activity. No new request will be scheduled. No timeout will be handled. New requests will be registered but will be handled only when the coordinator is enabled again.\n- `shutdown` Stop coordinator thread. No request can be submitted.\n- `purge` Clear all recorded requests. Do not change coordinator state.\n\n### `max_requests`\n\n`max_requests` is the maximum number of active requests at the same time. This is a per coordinator value, and independent of the number of agents.\n\nFor example, if 2 MDT and 4 agents are present, the agents will never have to handle more than 2 x `max_requests`.\n\n```\n$ lctl set_param mdt.$FSNAME-MDT0000.hsm.max_requests=10\n```\n\n### `policy`\n\nChange system behavior. Values can be added or removed by prefixing them with '+' or '-'.\n\n```\n$ lctl set_param mdt.$FSNAME-MDT0000.hsm.policy=+NRA\n```\n\nPossible values are a combination of:\n\n- `NRA` No Retry Action. If a restore fails, do not reschedule it automatically.\n- `NBR` Non Blocking Restore.  Restore is triggered but does not block clients. Access to a released file returns `ENODATA`.\n\n### `grace_delay`\n\n`grace_delay` is the delay, expressed in seconds, before a successful or failed request is cleared from the whole request list.\n\n```\n$ lctl set_param mdt.$FSNAME-MDT0000.hsm.grace_delay=10\n```\n\n## change logs\n\nA changelog record type \u201cHSM\u201c was added for Lustre file system logs that relate to HSM events.\n\n```\n16HSM   13:49:47.469433938 2013.10.01 0x280 t=[0x200000400:0x1:0x0]\n```\n\nTwo items of information are available for each HSM record: the FID of the modified file and a bit mask. The bit mask codes the following information (lowest bits first):\n\n- Error code, if any (7 bits)\n- HSM event (3 bits)\n  - `HE_ARCHIVE = 0` File has been archived.\n  - `HE_RESTORE = 1` File has been restored.\n  - `HE_CANCEL = 2` A request for this file has been canceled.\n  - `HE_RELEASE = 3` File has been released.\n  - `HE_REMOVE = 4` A remove request has been executed automatically.\n  - `HE_STATE = 5` File flags have changed.\n- HSM flags (3 bits)\n  - `CLF_HSM_DIRTY=0x1`\n\nIn the above example, `0x280` means the error code is 0 and the event is HE_STATE.\n\nWhen using `liblustreapi`, there is a list of helper functions to easily extract the different values from this bitmask, like: `hsm_get_cl_event()`, `hsm_get_cl_flags()`, and `hsm_get_cl_error()`\n\n## Policy engine\n\nA Lustre file system does not have an internal component responsible for automatically scheduling archive requests and release requests under any conditions (like low space). Automatically scheduling archive operations is the role of the policy engine.\n\nIt is recommended that the Policy Engine run on a dedicated client, similar to an agent node, with a Lustre version 2.5+.\n\nA policy engine is a userspace program using the Lustre file system HSM specific API to monitor the file system and schedule requests.\n\nRobinhood is the recommended policy engine.\n\n### Robinhood\n\nRobinhood is a Policy engine and reporting tool for large file systems. It maintains a replicate of file system metadata in a database that can be queried at will. Robinhood makes it possible to schedule mass action on file system entries by defining attribute-based policies, provides fast `find` and `du` enhanced clones, and provides administrators with an overall view of file system content through a web interface and command line tools.\n\nRobinhood can be used for various configurations.", "mimetype": "text/plain", "start_char_idx": 9874, "end_char_idx": 13786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0985e58-e9bd-406a-a937-97aa977d2707": {"__data__": {"id_": "a0985e58-e9bd-406a-a937-97aa977d2707", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f01c8240-32d1-4591-ac57-6a7a39cc34a8", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3445594814cd167537ba0d74b2637b6e324f1044231f3ef0572628ca0da292e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a2a71f4-2a66-490a-9864-9ceed73b855d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d359d024104548a296d34a7399768443cfa85f3f5641c27bf181021978417434", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Automatically scheduling archive operations is the role of the policy engine.\n\nIt is recommended that the Policy Engine run on a dedicated client, similar to an agent node, with a Lustre version 2.5+.\n\nA policy engine is a userspace program using the Lustre file system HSM specific API to monitor the file system and schedule requests.\n\nRobinhood is the recommended policy engine.\n\n### Robinhood\n\nRobinhood is a Policy engine and reporting tool for large file systems. It maintains a replicate of file system metadata in a database that can be queried at will. Robinhood makes it possible to schedule mass action on file system entries by defining attribute-based policies, provides fast `find` and `du` enhanced clones, and provides administrators with an overall view of file system content through a web interface and command line tools.\n\nRobinhood can be used for various configurations. Robinhood is an external project, and further information can be found on the project website: <https://sourceforge.net/apps/trac/robinhood/wiki/Doc>.", "mimetype": "text/plain", "start_char_idx": 12894, "end_char_idx": 13937, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd554e57-cbe8-43fd-8a3d-99e34ba1d830": {"__data__": {"id_": "cd554e57-cbe8-43fd-8a3d-99e34ba1d830", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8de845787a4ce46a25f25adc36b8ae865519e8ac501e0403174ee696ac285e80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b331903-ef04-46bf-ad5f-2c7cabe52273", "node_type": "1", "metadata": {}, "hash": "f1c6bdcdb587de8841ae1f8b41361eb64cf620da6be893ab2cf261b1d3e14b1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Persistent Client Cache (PCC)\n\n## 1. Introduction\n\nFlash-based SSDs help to (partly) close the ever-increasing performance gap between magnetic disks and CPUs. SSDs build a new level in the storage hierarchy, both in terms of price and performance. The large size of data sets stored in Lustre, ranging up to hundreds of PiB in the largest centers, makes it more cost-effective to store most of the data on HDDs and only an active subset of data on SSDs.\n\nThe PCC mechanism allows clients equipped with internal SSDs to deliver additional performance for both read and write intensive applications that have node-local I/O patterns without losing the benefits of the global Lustre namespace. PCC is combined with Lustre HSM and layout lock mechanisms to provide persistent caching services using the local SSD storage, while allowing migration of individual files between local and shared storage. This enables I/O intensive applications to read and write data on client nodes without losing the benefits of the global Lustre namespace. \n\nThe main advantages to use this cache on the Lustre clients is that the I/O stack is much simpler for the cached data, as there is no interference with I/Os from other clients, which enables performance optimizations. There are no special requirements on the hardware of the client nodes. Any Linux filesystem, such as ext4 on an NVMe device, can be used as PCC cache. Local file caching reduces the pressure on the object storage targets (OSTs), as small or random I/Os can be aggregated to large sequential I/Os and temporary files do not even need to be flushed to OSTs.\n\n## 2. Design\n\n### 2.1. Lustre Read-Write PCC Caching\n\n![overview of PCC-RW Architecture.png](figures/27-overview_of_PCC-RW Architecture.png)\n\nLustre typically uses its integrated HSM mechanism to interface with larger and slower archival storage using tapes or other media. PCC-RW, on the contrary, is actually an HSM backend storage system which provides a group of high-speed local caches on Lustre clients. Figure 27.1, \u201cOverview of PCC-RW Architecture\u201d shows the PCC-RW architecture. Each client uses its own local storage, usually in the form of NVMe, formatted as a local file system for the local cache. Cached I/Os are directed to files in the local file system, while normal I/ O are directed to OSTs.\n\nPCC-RW uses Lustre's HSM mechanism for data synchronization. Each PCC node is actually an HSM agent and has a copy tool instance running on it. The Lustre HSM copytool is used to restore files from the local cache to Lustre OSTs. Any remote access for a PCC cached file from another Lustre client triggers this data synchronization. If a PCC client goes offline, the cached data becomes temporarily inaccessible to other clients. The data will be accessible again after the PCC client reboots, mounts the Lustre filesystem, and restarts the copytool.\n\nCurrently, PCC clients cache entire files on their local filesystems. A file has to be attached to PCC before I/O can be directed to a client cache. The Lustre layout lock feature is used to ensure that the caching services are consistent with the global file system state. The file data can be written/read directly to/from the local PCC cache after a successful attach operation. If the attach has not been successful, the client will simply fall back to the normal I/O path and direct I/Os to OSTs. PCC-RW cached files are automatically restored to the global filesystem when a process on another client tries to read or modify them. The corresponding I/O will be blocked, waiting for the released file to be restored. This is transparent to the application.\n\nThe revocation of the layout lock can automatically detach the file from the PCC cache at any time. The PCC-RW cached file can be manually detached by the lfs pcc detach command. After the cached file is detached from the cache and restored to OSTs, it will be removed from the PCC filesystem.\n\nFailed PCC-RW operations usually return corresponding error codes. There is a special case when the space of the local PCC file system is exhausted. In this case, PCC-RW can fall back to the normal I/O path automatically since the capacity of the Lustre file system is much larger than the capacity of the PCC device.\n\n### 2.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b331903-ef04-46bf-ad5f-2c7cabe52273": {"__data__": {"id_": "7b331903-ef04-46bf-ad5f-2c7cabe52273", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8de845787a4ce46a25f25adc36b8ae865519e8ac501e0403174ee696ac285e80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd554e57-cbe8-43fd-8a3d-99e34ba1d830", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1b457d3d6ed8ab1d03eb73dc1a591c86b2c5d8eb82054077c4e407a7c21802b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7012fb65-3f50-4799-8bd5-f011dfb15c31", "node_type": "1", "metadata": {}, "hash": "69971c631dbdfc37025f2d6e51dfffe2b379079a163672973a078593ac474c67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PCC-RW cached files are automatically restored to the global filesystem when a process on another client tries to read or modify them. The corresponding I/O will be blocked, waiting for the released file to be restored. This is transparent to the application.\n\nThe revocation of the layout lock can automatically detach the file from the PCC cache at any time. The PCC-RW cached file can be manually detached by the lfs pcc detach command. After the cached file is detached from the cache and restored to OSTs, it will be removed from the PCC filesystem.\n\nFailed PCC-RW operations usually return corresponding error codes. There is a special case when the space of the local PCC file system is exhausted. In this case, PCC-RW can fall back to the normal I/O path automatically since the capacity of the Lustre file system is much larger than the capacity of the PCC device.\n\n### 2.2. Rule-based Persistent Client Cache\n\nPCC includes a rule-based, configurable caching infrastructure that enables it to achieve various objectives, such as customizing I/O caching and providing performance isolation and QoS guarantees. \n\nFor PCC-RW, when a file is being created, a rule-based policy is used to determine whether it will be cached. It supports rules for different users, groups, projects, or filenames extensions. \n\nRule-based PCC-RW caching of newly created files can determine which file can use a cache on PCC directly without administrator's intervention.\n\n## 3. PCC Command Line Tools\n\nLustre provides `lfs` and `lctl` command line tools for users to interact with PCC feature.\n\n### 3.1 Add a PCC backend on a client\n\nCommand:\n\n```\nclient# lctl pcc add mountpoint pccpath [--param|-p cfgparam]\n```\n\nThe above command will add a PCC backend to the Lustre client.\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| mountpoint | The Lustre client mount point.                               |\n| pccpath    | The directory path on local filesystem for PCC cache. The whole filesystem does not need to be exclusively dedicated to the PCC cache, but the directory should not be accessible to regular users. |\n| cfgparam   | A string in the form of name-value pairs to config the PCC backend such as read-write attach id (archive ID), and auto caching rule, etc. |\n\n**Note:** when a client node has more than one Lustre mount point or Lustre filesystem instance, the parameter `mountpoint` makes sure that only the PCC backend on specified Lustre filesystem instance or Lustre mount point is configured. This Lustre mount point must be the same as the HSM (lhsmtool_posix) configuration, if the PCC backend is used as PCC-RW caching. Also, the parameter `pccpath` should be the same as the HSM root parameter of the POSIX copytool (lhsmtool_posix).\n\nPCC-RW uses Lustre's HSM mechanism for data synchronization. Before using PCC-RW on a client, it is still necessary to setup HSM on the MDTs and the PCC client nodes.\n\nFirst, a coordinator must be activated on each of the filesystem MDTs. This can be achieved with the command:\n\n```\nmds# lctl set_param mdt.$FSNAME-MDT0000.hsm_control=enabled\nmdt.lustre-MDT0000.hsm_control=enabled\n```\n\nNext, launch the copytool on each agent node (PCC client node) to connect to your HSM storage. This command will be of the form:\n\n```\nclient# lhsmtool_posix --daemon --hsm-root $PCCPATH --archive=$ARCHIVE_ID $LUSTREPATH\n```\n\nExamples:\n\nThe following command adds a PCC backend on a client:\n\n```\nclient# lctl pcc add /mnt/lustre /mnt/pcc --param \"projid={500,1000}&fname={*.h5},uid=1001 rwid=2\"\n```\n\nThe first substring of the config parameter is the auto-cache rule, where \"&\" represents the logical AND operator while \",\" represents the logical OR operator.", "mimetype": "text/plain", "start_char_idx": 3382, "end_char_idx": 7180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7012fb65-3f50-4799-8bd5-f011dfb15c31": {"__data__": {"id_": "7012fb65-3f50-4799-8bd5-f011dfb15c31", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8de845787a4ce46a25f25adc36b8ae865519e8ac501e0403174ee696ac285e80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b331903-ef04-46bf-ad5f-2c7cabe52273", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4524d4ac13951fa799e57c768cc20d25c5ca2b5a566c29169f1a88a086886a43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0d96c8d-593b-4d86-add1-ef04496a7171", "node_type": "1", "metadata": {}, "hash": "fdaa059c2c3d3d1fc714b0a91e6d6ef9e1e8c6f71a4a28fd3e3f84500999e283", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This command will be of the form:\n\n```\nclient# lhsmtool_posix --daemon --hsm-root $PCCPATH --archive=$ARCHIVE_ID $LUSTREPATH\n```\n\nExamples:\n\nThe following command adds a PCC backend on a client:\n\n```\nclient# lctl pcc add /mnt/lustre /mnt/pcc --param \"projid={500,1000}&fname={*.h5},uid=1001 rwid=2\"\n```\n\nThe first substring of the config parameter is the auto-cache rule, where \"&\" represents the logical AND operator while \",\" represents the logical OR operator. The example rule means that new files are only auto cached if either of the following conditions are satisfied: \n\n\u2022 The project ID is either 500 or 1000 and the suffix of the file name is \"h5\"; \n\n\u2022 The user ID is 1001; \n\nThe currently supported name-value pairs for PCC backend configuration are listing as follows: \n\n\u2022 `rwid` PCC-RW attach ID which is same as the archive ID of the copytool agent running on this PCC node. \n\n\u2022 `auto_attach \"auto_attach=1\"` enables auto attach at the next open or during I/ O. Enabling this option should cause automatic attaching of valid PCC-cached files which were detached due to the manual `lfs pcc detach` command or revocation of layout lock (i.e. LRU lock shrinking). `\"auto_attach=0\"` means that auto file attach is disabled and is the default mode.\n\n### 3.2 Delete a PCC backend from a client\n\n**Command:**\n\n```\nlctl pcc del <mountpoint> <pccpath>\n```\n\nThe above command will delete a PCC backend from a Lustre client.\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| mountpoint | The Lustre client mount point.                               |\n| pccpath    | A PCC backend is specified by this path. Please refer to `lctl pcc` add for details. |\n\n**Examples:**\n\nThe following command will delete a PCC backend referenced by `\"/mnt/pcc\"` on a client with the mount point of `\"/mnt/lustre\"`.\n\n```\nclient# lctl pcc del /mnt/lustre /mnt/pcc\n```\n\n### 3.3. Remove all PCC backends on a client\n\n**Command:**\n\n```\nlctl pcc clear <mountpoint>\n```\n\nThe above command will remove all PCC backends on a Lustre client.\n\n| **Option** | **Description**                |\n| ---------- | ------------------------------ |\n| mountpoint | The Lustre client mount point. |\n\n**Examples:**\n\nThe following command will remove all PCC backends from a client with the mount point of `\"/mnt/lustre\"`.\n\n```\nclient# lctl pcc clear /mnt/lustre\n```\n\n### 3.4.  List all PCC backends on a client\n\n**Command:**\n\n```\nlctl pcc list <mountpoint>\n```\n\nThe above command will list all PCC backends on a Lustre client.\n\n| **Option** | **Description**                |\n| ---------- | ------------------------------ |\n| mountpoint | The Lustre client mount point. |\n\n**Examples:**\n\nThe following command will list all PCC backends on a client with the mount point of `\"/ mnt/lustre\"`.\n\n```\nclient# lctl pcc list /mnt/lustre\n```\n\n### 3.5.  Attach given files into PCC\n\n**Command:**\n\n```\nlfs pcc attach --id|-i <NUM> <file...>\n```\n\nThe above command will attach the given files onto PCC.\n\n| **Option**      | **Description**                               |\n| --------------- | --------------------------------------------- |\n| --id\\|-i \\<NUM> | Attach ID to select which PCC backend to use. |\n\n**Examples:**\n\nThe following command will attach the file referenced by `/mnt/lustre/test` onto the PCC backend with PCC-RW attach ID that equals 2.\n\n```\nclient# lfs pcc attach -i 2 /mnt/lustre/test\n```\n\n### 3.6.  Attach given files into PCC by FID(s)\n\n**Command:**\n\n```\nlfs pcc attach_fid --id|-i <NUM> --mnt|-m <mountpoint> <fid...>\n```\n\nThe above command will attach the given files referenced by their FIDs into PCC.", "mimetype": "text/plain", "start_char_idx": 6717, "end_char_idx": 10404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0d96c8d-593b-4d86-add1-ef04496a7171": {"__data__": {"id_": "a0d96c8d-593b-4d86-add1-ef04496a7171", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8de845787a4ce46a25f25adc36b8ae865519e8ac501e0403174ee696ac285e80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7012fb65-3f50-4799-8bd5-f011dfb15c31", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "918ffbc8fff59899936b38b094ac1c6482632e895726f0c3b499749be92eb43b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14493710-ca01-4a67-aae9-b89acc28ca0b", "node_type": "1", "metadata": {}, "hash": "779d834f20afd95b40f00bba42cc60e0ac0fa551ceab3d84d98fa6258f3c9e89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| **Option**      | **Description**                               |\n| --------------- | --------------------------------------------- |\n| --id\\|-i \\<NUM> | Attach ID to select which PCC backend to use. |\n\n**Examples:**\n\nThe following command will attach the file referenced by `/mnt/lustre/test` onto the PCC backend with PCC-RW attach ID that equals 2.\n\n```\nclient# lfs pcc attach -i 2 /mnt/lustre/test\n```\n\n### 3.6.  Attach given files into PCC by FID(s)\n\n**Command:**\n\n```\nlfs pcc attach_fid --id|-i <NUM> --mnt|-m <mountpoint> <fid...>\n```\n\nThe above command will attach the given files referenced by their FIDs into PCC.\n\n| **Option**             | **Description**                               |\n| ---------------------- | --------------------------------------------- |\n| --id\\|-i \\<NUM>        | Attach ID to select which PCC backend to use. |\n| --mnt\\|-m\\<mountpoint> | The Lustre mount point.                       |\n\n**Examples:**\n\nThe following command will attach the file referenced by FID `0x200000401:0x1:0x0` onto the PCC backend with PCC-RW attach ID that equals 2.\n\n```\nclient# lfs pcc attach_fid -i 2 -m /mnt/lustre 0x200000401:0x1:0x0\n```\n\n### 3.7.  Detach given files from PCC\n\n**Command:**\n\n```\nlfs pcc detach [--keep|-k] <file...>\n```\n\nThe above command will detach given files from PCC.\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| --keep\\|-k | By default, the detach command will detach the file from PCC permanently and remove the PCC copy after detach.<br />This option will only detach the file, but keep the PCC copy in cache. It allows the detached file to be attached automatically at the next open if the cached copy of the file is still valid. |\n\n**Examples:**\n\nThe following command will detach the file referenced by `/mnt/lustre/test` from PCC permanently and remove the corresponding cached file on PCC.\n\n```\nclient# lfs pcc detach /mnt/lustre/test\n```\n\nThe following command will detach the file referenced by `/mnt/lustre/test` from PCC, but allow the file to be attached automatically at the next open. \n\n```\nclient# lfs pcc detach -k /mnt/lustre/test\n```\n\n### 3.8.  Detach given files from PCC by FID(s)\n\n**Command:**\n\n```\nlfs pcc detach_fid [--keep|-k] <mountpoint> <fid...>\n```\n\nThe above command will detach the given files from PCC by FID(s).\n\n| **Option** | **Description**                                        |\n| ---------- | ------------------------------------------------------ |\n| --keep\\|-k | Please refer to the command lfs pcc detach for details |\n\n**Examples:**\n\nThe following command will detach the file referenced by FID `0x200000401:0x1:0x0` from PCC permanently and remove the corresponding cached file on PCC.\n\n```\nclient# lfs pcc detach_fid /mnt/lustre 0x200000401:0x1:0x0\n```\n\nThe following command will detach the file referenced by FID `0x200000401:0x1:0x0` from PCC, but allow the file to be attached automatically at the next open.\n\n```\nclient# lfs pcc detach_fid -k /mnt/lustre 0x200000401:0x1:0x0\n```\n\n### 3.9. Display the PCC state for given files\n\n**Command:**\n\n```\nlfs pcc state <file...>\n```\n\nThe above command will display the PCC state for given files.", "mimetype": "text/plain", "start_char_idx": 9779, "end_char_idx": 13024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14493710-ca01-4a67-aae9-b89acc28ca0b": {"__data__": {"id_": "14493710-ca01-4a67-aae9-b89acc28ca0b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8de845787a4ce46a25f25adc36b8ae865519e8ac501e0403174ee696ac285e80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0d96c8d-593b-4d86-add1-ef04496a7171", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1a8468661ca8237e3d1bee4ccb8d1efc9947aec786db1f0c57d4c614f0fac7ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\nclient# lfs pcc detach_fid /mnt/lustre 0x200000401:0x1:0x0\n```\n\nThe following command will detach the file referenced by FID `0x200000401:0x1:0x0` from PCC, but allow the file to be attached automatically at the next open.\n\n```\nclient# lfs pcc detach_fid -k /mnt/lustre 0x200000401:0x1:0x0\n```\n\n### 3.9. Display the PCC state for given files\n\n**Command:**\n\n```\nlfs pcc state <file...>\n```\n\nThe above command will display the PCC state for given files.\n\n**Examples:**\n\nThe following command will display the PCC state of the file referenced by `/mnt/lustre/`\n\n```\nclient# lfs pcc state /mnt/lustre/test \nfile: /mnt/lustre/test, type: readwrite, PCC file: /mnt/pcc/0004/0000/0bd1/0000/0002/0000/0x200000bd1:0x4:0x0, user number: 1, flags: 4\n```\n\n If the file \"/mnt/lustre/test\" is not cached on PCC, the output of its PCC state is as follow: \n\n```\nclient# lfs pcc state /mnt/lustre/test \nfile: /mnt/lustre/test, type: none\n```\n\n## 4. PCC Configuration Example\n\n1. Setup HSM on MDT\n\n   ```\n   mds# lctl set_param mdt.lustre-MDT0000.hsm_control=enabled\n   ```\n\n2. Setup PCC on the clients\n\n   ```\n   client1# lhsmtool_posix --daemon --hsm-root /mnt/pcc --archive=1 /mnt/lustre < /dev/null > /tmp/copytool_log 2>&1\n   client1# lctl pcc add /mnt/lustre /mnt/pcc \"projid={1000},uid={500} rwid=1\"\n   \n   client2# lhsmtool_posix --daemon --hsm-root /mnt/pcc --archive=2 /mnt/lustre < /dev/null > /tmp/copytool_log 2>&1\n   client2# lctl pcc add /mnt/lustre /mnt/pcc \"projid={1000}&gid={500} rwid\n   ```\n\n3. Execute PCC commands on the clients\n\n   ```\n   client1# echo \"QQQQQ\" > /mnt/lustre/test\n   \n   client2# lfs pcc attach -i 2 /mnt/lustre/test\n   \n   client2# lfs pcc state /mnt/lustre/test\n   file: /mnt/lustre/test, type: readwrite, PCC file: /mnt/pcc/0004/0000/0bd1/0000/0002/0000/0x200000bd1:0x4:0x0, user number: 1, flags: 6\n   \n   client2# lfs pcc detach /mnt/lustre/test\n   ```", "mimetype": "text/plain", "start_char_idx": 12569, "end_char_idx": 14451, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f93ab89e-cda3-4f5a-afde-101d5349ad25": {"__data__": {"id_": "f93ab89e-cda3-4f5a-afde-101d5349ad25", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "450e77dd6c8b606cffbe2f76c9ec0035deb2f5eeff9037b8c45aa014ea4a7d46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2053816-b9eb-4574-8b06-7bbf7faaa21e", "node_type": "1", "metadata": {}, "hash": "7c68b3438903d8c2834855133c3edf06890a118b764ab3bd069f88ceb3b7b298", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in Lustre 2.9\n\n# Mapping UIDs and GIDs with Nodemap\n\n- [Mapping UIDs and GIDs with Nodemap](#mapping-uids-and-gids-with-nodemap)\n  * [Setting a Mapping](#setting-a-mapping)\n    + [Defining Terms](#defining-terms)\n    + [Deciding on NID Ranges](#deciding-on-nid-ranges)\n    + [Describing and Deploying a Sample Mapping](#describing-and-deploying-a-sample-mapping)\n  * [Altering Properties](#altering-properties)\n    + [Managing the Properties](#managing-the-properties)\n    + [Mixing Properties](#mixing-properties)\n  * [Enabling the Feature](#enabling-the-feature)\n  * [`default` Nodemap](#default-nodemap)\n  * [Verifying Settings](#verifying-settings)\n  * [Ensuring Consistency](#ensuring-consistency)\n\n\nThis chapter describes how to map UID and GIDs across a Lustre file system using the nodemap feature, and includes the following sections:\n\n- [the section called \u201cSetting a Mapping\u201d](#setting-a-mapping)\n- [the section called \u201cAltering Properties\u201d](#altering-properties)\n- [the section called \u201cEnabling the Feature\u201d](#enabling-the-feature)\n- [the section called \u201cdefault Nodemap\u201d](#default-nodemap)\n- [the section called \u201cVerifying Settings\u201d](#verifying-settings)\n- [the section called \u201cEnsuring Consistency\u201d](#ensuring-consistency)\n\n## Setting a Mapping\n\nThe nodemap feature supported in Lustre 2.9 was first introduced in Lustre 2.7 as a technology preview. It allows UIDs and GIDs from remote systems to be mapped to local sets of UIDs and GIDs while retaining POSIX ownership, permissions and quota information. As a result, multiple sites with conflicting user and group identifiers can operate on a single Lustre file system without creating collisions in UID or GID space.\n\n### Defining Terms\n\nWhen the nodemap feature is enabled, client file system access to a Lustre system is filtered through the nodemap identity mapping policy engine. Lustre connectivity is governed by network identifiers, or *NIDs*, such as`192.168.7.121@tcp`. When an operation is made from a NID, Lustre decides if that NID is part of a *nodemap*, a policy group consisting of one or more NID ranges. If no policy group exists for that NID, access is squashed to user `nobody` by default. Each policy group also has several *properties*, such as `trusted` and `admin`, which determine access conditions. A collection of identity maps or *idmaps* are kept for each policy group. These idmaps determine how UIDs and GIDs on the client are translated into the canonical user space of the local Lustre file system.\n\nIn order for nodemap to function properly, the MGS, MDS, and OSS systems must all have a version of Lustre which supports nodemap. Clients operate transparently and do not require special configuration or knowledge of the nodemap setup.\n\n### Deciding on NID Ranges\n\nNIDs can be described as either a singleton address or a range of addresses. A single address is described in standard Lustre NID format, such as `10.10.6.120@tcp`. A range is described using a dash to separate the range, for example, `192.168.20.[0-255]@tcp`.\n\nThe range must be contiguous.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3067, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2053816-b9eb-4574-8b06-7bbf7faaa21e": {"__data__": {"id_": "a2053816-b9eb-4574-8b06-7bbf7faaa21e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "450e77dd6c8b606cffbe2f76c9ec0035deb2f5eeff9037b8c45aa014ea4a7d46", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f93ab89e-cda3-4f5a-afde-101d5349ad25", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0553f0d847c4d9906399ed150eaa7ed774e47aebc8c1006dcf711271aef27bfe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "374d567e-9228-4288-bc7d-9b9d5118cc3d", "node_type": "1", "metadata": {}, "hash": "a7b5614274df2d9f57007f21e7b349b88064bd300f9be808bacea5430a9c4135", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A collection of identity maps or *idmaps* are kept for each policy group. These idmaps determine how UIDs and GIDs on the client are translated into the canonical user space of the local Lustre file system.\n\nIn order for nodemap to function properly, the MGS, MDS, and OSS systems must all have a version of Lustre which supports nodemap. Clients operate transparently and do not require special configuration or knowledge of the nodemap setup.\n\n### Deciding on NID Ranges\n\nNIDs can be described as either a singleton address or a range of addresses. A single address is described in standard Lustre NID format, such as `10.10.6.120@tcp`. A range is described using a dash to separate the range, for example, `192.168.20.[0-255]@tcp`.\n\nThe range must be contiguous. The full LNet definition for a nidlist is as follows:\n\n```\n<nidlist>       :== <nidrange> [ ' ' <nidrange> ]\n<nidrange>      :== <addrrange> '@' <net>\n<addrrange>     :== '*' |\n                        <ipaddr_range> |\n                        <numaddr_range>\n<ipaddr_range>  :==\n        <numaddr_range>.<numaddr_range>.<numaddr_range>.<numaddr_range>\n<numaddr_range> :== <number> |\n                        <expr_list>\n<expr_list>     :== '[' <range_expr> [ ',' <range_expr>] ']'\n<range_expr>    :== <number> |\n                        <number> '-' <number> |\n                        <number> '-' <number> '/' <number>\n<net>           :== <netname> | <netname><number>\n<netname>       :== \"lo\" | \"tcp\" | \"o2ib\" | \"gni\"\n<number>        :== <nonnegative decimal> | <hexadecimal> \n```\n\n### Describing and Deploying a Sample Mapping\n\nDeploy nodemap by first considering which users need to be mapped, and what sets of network addresses or ranges are involved. Issues of visibility between users must be examined as well.\n\nConsider a deployment where researchers are working on data relating to birds. The researchers use a computing system which mounts Lustre from a single IPv4 address, `192.168.0.100`. Name this policy group `BirdResearchSite`. The IP address forms the NID `192.168.0.100@tcp`. Create the policy group and add the NID to that group on the MGS using the `lctl` command:\n\n```\nmgs# lctl nodemap_add BirdResearchSite\nmgs# lctl nodemap_add_range --name BirdResearchSite --range 192.168.0.100@tcp\n```\n\n**Note**\n\nA NID cannot be in more than one policy group. Assign a NID to a new policy group by first removing it from the existing group.\n\nThe researchers use the following identifiers on their host system:\n\n- `swan` (UID 530) member of group `wetlands` (GID 600)\n- `duck` (UID 531) member of group `wetlands` (GID 600)\n- `hawk` (UID 532) member of group `raptor` (GID 601)\n- `merlin` (UID 533) member of group `raptor` (GID 601)\n\nAssign a set of six idmaps to this policy group, with four for UIDs, and two for GIDs. Pick a starting point, e.g. UID 11000, with room for additional UIDs and GIDs to be added as the configuration grows.", "mimetype": "text/plain", "start_char_idx": 2302, "end_char_idx": 5212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "374d567e-9228-4288-bc7d-9b9d5118cc3d": {"__data__": {"id_": "374d567e-9228-4288-bc7d-9b9d5118cc3d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "450e77dd6c8b606cffbe2f76c9ec0035deb2f5eeff9037b8c45aa014ea4a7d46", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2053816-b9eb-4574-8b06-7bbf7faaa21e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a0c49ba418b9fc22617f8185303510b57ea5a6cbabdf134a8e814be3ff02623a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea62810b-4b18-4659-b0e5-c7d34e7ee8f4", "node_type": "1", "metadata": {}, "hash": "c8c4e9b6e98ab294fe7dbe60918fb775cfdb97a9afbb0f60988599c38e5cc167", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Assign a NID to a new policy group by first removing it from the existing group.\n\nThe researchers use the following identifiers on their host system:\n\n- `swan` (UID 530) member of group `wetlands` (GID 600)\n- `duck` (UID 531) member of group `wetlands` (GID 600)\n- `hawk` (UID 532) member of group `raptor` (GID 601)\n- `merlin` (UID 533) member of group `raptor` (GID 601)\n\nAssign a set of six idmaps to this policy group, with four for UIDs, and two for GIDs. Pick a starting point, e.g. UID 11000, with room for additional UIDs and GIDs to be added as the configuration grows. Use the `lctl` command to set up the idmaps:\n\n```\nmgs# lctl nodemap_add_idmap --name BirdResearchSite --idtype uid --idmap 530:11000\nmgs# lctl nodemap_add_idmap --name BirdResearchSite --idtype uid --idmap 531:11001\nmgs# lctl nodemap_add_idmap --name BirdResearchSite --idtype uid --idmap 532:11002\nmgs# lctl nodemap_add_idmap --name BirdResearchSite --idtype uid --idmap 533:11003\nmgs# lctl nodemap_add_idmap --name BirdResearchSite --idtype gid --idmap 600:11000\nmgs# lctl nodemap_add_idmap --name BirdResearchSite --idtype gid --idmap 601:11001\n```\n\nThe parameter `530:11000` assigns a client UID, for example UID 530, to a single canonical UID, such as UID 11000. Each assignment is made individually. There is no method to specify a range `530-533:11000-11003`. UID and GID idmaps are assigned separately. There is no implied relationship between the two.\n\nFiles created on the Lustre file system from the `192.168.0.100@tcp` NID using UID `duck` and GID `wetlands` are stored in the Lustre file system using the canonical identifiers, in this case UID 11001 and GID 11000. A different NID, if not part of the same policy group, sees its own view of the same file space.\n\nSuppose a previously created project directory exists owned by UID 11002/GID 11001, with mode 770. When users `hawk` and `merlin` at 192.168.0.100 place files named `hawk-file` and `merlin-file` into the directory, the contents from the 192.168.0.100 client appear as:\n\n```\n[merlin@192.168.0.100 projectsite]$ ls -la\ntotal 34520\ndrwxrwx--- 2 hawk   raptor     4096 Jul 23 09:06 .\ndrwxr-xr-x 3 nobody nobody     4096 Jul 23 09:02 ..\n-rw-r--r-- 1 hawk   raptor 10240000 Jul 23 09:05 hawk-file\n-rw-r--r-- 1 merlin raptor 25100288 Jul 23 09:06 merlin-file\n```\n\nFrom a privileged view, the canonical owners are displayed:\n\n```\n[root@trustedSite projectsite]# ls -la\ntotal 34520\ndrwxrwx--- 2 11002 11001     4096 Jul 23 09:06 .\ndrwxr-xr-x 3 root root     4096 Jul 23 09:02 ..\n-rw-r--r-- 1 11002 11001 10240000 Jul 23 09:05 hawk-file\n-rw-r--r-- 1 11003 11001 25100288 Jul 23 09:06 merlin-file\n```\n\nIf UID 11002 or GID 11001 do not exist on the Lustre MDS or MGS, create them in LDAP or other data sources, or trust clients by setting `identity_upcall` to `NONE`. For more information, see *the section called \u201cUser/Group Upcall\u201d*.\n\nBuilding a larger and more complex configuration is possible by iterating through the `lctl` commands above. In short:\n\n1. Create a name for the policy group.\n2. Create a set of NID ranges used by the group.\n3.", "mimetype": "text/plain", "start_char_idx": 4634, "end_char_idx": 7725, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea62810b-4b18-4659-b0e5-c7d34e7ee8f4": {"__data__": {"id_": "ea62810b-4b18-4659-b0e5-c7d34e7ee8f4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "450e77dd6c8b606cffbe2f76c9ec0035deb2f5eeff9037b8c45aa014ea4a7d46", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "374d567e-9228-4288-bc7d-9b9d5118cc3d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "54befaadd3f2232be94e88d846e50bf13071520e20ba25ca26053eca32b59d57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c91a1b06-ae3e-41f3-b7d2-b3d65353b4a3", "node_type": "1", "metadata": {}, "hash": "0499e529a768e42da1394f4ef3088b8283e1e0a994e29f405e00112676936618", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "drwxr-xr-x 3 root root     4096 Jul 23 09:02 ..\n-rw-r--r-- 1 11002 11001 10240000 Jul 23 09:05 hawk-file\n-rw-r--r-- 1 11003 11001 25100288 Jul 23 09:06 merlin-file\n```\n\nIf UID 11002 or GID 11001 do not exist on the Lustre MDS or MGS, create them in LDAP or other data sources, or trust clients by setting `identity_upcall` to `NONE`. For more information, see *the section called \u201cUser/Group Upcall\u201d*.\n\nBuilding a larger and more complex configuration is possible by iterating through the `lctl` commands above. In short:\n\n1. Create a name for the policy group.\n2. Create a set of NID ranges used by the group.\n3. Define which UID and GID translations need to occur for the group.\n\n## Altering Properties\n\nPrivileged users access mapped systems with rights dependent on certain properties, described below. By default, root access is squashed to user `nobody`, which interferes with most administrative actions.\n\n### Managing the Properties\n\nSeveral properties exist, off by default, which change client behavior: `admin`, `trusted`, `squash_uid`, `squash_gid`, and `deny_unknown`.\n\n- The `trusted` property permits members of a policy group to see the file system's canonical identifiers. In the above example, UID 11002 and GID 11001 will be seen without translation. This can be utilized when local UID and GID sets already map directly to the specified users.\n- The property `admin` defines whether root is squashed on the policy group. By default, it is squashed, unless this property is enabled. Coupled with the `trusted` property, this will allow unmapped access for backup nodes, transfer points, or other administrative mount points.\n- The property `deny_unknown` denies all access to users not mapped in a particular nodemap. This is useful if a site is concerned about unmapped users accessing the file system in order to satisfy security requirements.\n- The properties `squash_uid` and `squash_gid` define the default UID and GID that users will be squashed to if unmapped, unless the deny_unknown flag is set, in which case access will still be denied.\n\nAlter values to either true (1) or false (0) on the MGS:\n\n```\nmgs# lctl nodemap_modify --name BirdAdminSite --property trusted --value 1\nmgs# lctl nodemap_modify --name BirdAdminSite --property admin --value 1\nmgs# lctl nodemap_modify --name BirdAdminSite --property deny_unknown --value 1\n```\n\nChange values during system downtime to minimize the chance of any ownership or permissions problems if the policy group is active. Although changes can be made live, client caching of data may interfere with modification as there are a few seconds of lead time before the change is distributed. \n\n### Mixing Properties\n\nWith both `admin` and `trusted` properties set, the policy group has full access, as if nodemap was turned off, to the Lustre file system. The administrative site for the Lustre file system needs at least one group with both properties in order to perform maintenance or to perform administrative tasks.\n\n**Warning**\n\nMDS systems **must** be in a policy group with both these properties set to 1. It is recommended to put the MDS in a policy group labeled \u201cTrustedSystems\u201d or some identifier that makes the association clear.\n\nIf a policy group has the `admin` property set, but does not have the property `trusted` set, root is mapped directly to root, any explicitly specified UID and GID idmaps are honored, and other access is squashed. If root alters ownership to UIDs or GIDs which are locally known from that host but not part of an idmap, root effectively changes ownership of those files to the default squashed UID and GID.\n\nIf `trusted` is set but `admin` is not, the policy group has full access to the canonical UID and GID sets of the Lustre file system, and root is squashed.\n\nThe deny_unknown property, once enabled, prevents unmapped users from accessing the file system. Root access also is denied, if the `admin` property is off, and root is not part of any mapping.\n\nWhen nodemaps are modified, the change events are queued and distributed across the cluster. Under normal conditions, these changes can take around ten seconds to propagate.", "mimetype": "text/plain", "start_char_idx": 7112, "end_char_idx": 11256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c91a1b06-ae3e-41f3-b7d2-b3d65353b4a3": {"__data__": {"id_": "c91a1b06-ae3e-41f3-b7d2-b3d65353b4a3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "450e77dd6c8b606cffbe2f76c9ec0035deb2f5eeff9037b8c45aa014ea4a7d46", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea62810b-4b18-4659-b0e5-c7d34e7ee8f4", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b1b62ffe63587337e016c0df03db6e3dc7e305d62ba8e2c28756e234b119080b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47c61460-18e3-4c24-8063-e675db1a7b93", "node_type": "1", "metadata": {}, "hash": "d14e4bdf7464d00e28e1d871fa02f717203517b68b42bfd150f635989832a5cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If root alters ownership to UIDs or GIDs which are locally known from that host but not part of an idmap, root effectively changes ownership of those files to the default squashed UID and GID.\n\nIf `trusted` is set but `admin` is not, the policy group has full access to the canonical UID and GID sets of the Lustre file system, and root is squashed.\n\nThe deny_unknown property, once enabled, prevents unmapped users from accessing the file system. Root access also is denied, if the `admin` property is off, and root is not part of any mapping.\n\nWhen nodemaps are modified, the change events are queued and distributed across the cluster. Under normal conditions, these changes can take around ten seconds to propagate. During this distribution window, file access could be made via the old or new nodemap settings. Therefore, it is recommended to save changes for a maintenance window or to deploy them while the mapped nodes are not actively writing to the file system.\n\n## Enabling the Feature\n\nThe nodemap feature is simple to enable:\n\n```\nmgs# lctl nodemap_activate 1\n```\n\nPassing the parameter 0 instead of 1 disables the feature again. After deploying the feature, validate the mappings are intact before offering the file system to be mounted by clients.\n\nIntroduced in Lustre 2.8So far, changes have been made on the MGS. Prior to Lustre 2.9, changes must also be manually set on MDS systems as well. Also, changes must be manually deployed to OSS servers if quota is enforced, utilizing `lctl set_param` instead of `lctl`. Prior to 2.9, the configuration is not persistent, requiring a script which generates the mapping to be saved and deployed after every Lustre restart. As an example, use this style to deploy settings on the OSS:`oss# lctl set_param nodemap.add_nodemap=*SiteName* oss# lctl set_param nodemap.add_nodemap_range='*SiteName 192.168.0.15@tcp*' oss# lctl set_param nodemap.add_nodemap_idmap='*SiteName* uid *510:1700*' oss# lctl set_param nodemap.add_nodemap_idmap='*SiteName* gid *612:1702*'`In Lustre 2.9 and later, nodemap configuration is saved on the MGS and distributed automatically to MGS, MDS, and OSS nodes, a process which takes approximately ten seconds in normal circumstances.\n\n## `default` Nodemap\n\nThere is a special nodemap called `default`. As the name suggests, it is created by default and cannot be removed. It is like a fallback nodemap, setting the behaviour for Lustre clients that do not match any other nodemap.\n\nBecause of its special role, only some parameters can be set on the `default` nodemap:\n\n- `admin`\n- `trusted`\n- `squash_uid`\n- `squash_gid`\n- `fileset`\n- `audit_mode`\n\nIn particular, no UID/GID mapping can be defined on the `default` nodemap.\n\n**Note**\n\nBe careful when altering the `admin` and `trusted` properties of the `default` nodemap, especially if your Lustre servers fall into this nodemap.\n\n## Verifying Settings\n\nBy using `lctl nodemap_info all`, existing nodemap configuration is listed for easy export. This command acts as a shortcut into the configuration interface for nodemap. On the Lustre MGS, the `nodemap.active` parameter contains a `1` if nodemap is active on the system. Each policy group creates a directory containing the following parameters:\n\n- `admin` and `trusted` each contain a `1` if the values are set, and `0` otherwise.\n- `idmap` contains a list of the idmaps for the policy group, while `ranges` contains a list of NIDs for the group.\n- `squash_uid` and `squash_gid` determine what UID and GID users are squashed to if needed.", "mimetype": "text/plain", "start_char_idx": 10537, "end_char_idx": 14066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "47c61460-18e3-4c24-8063-e675db1a7b93": {"__data__": {"id_": "47c61460-18e3-4c24-8063-e675db1a7b93", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "450e77dd6c8b606cffbe2f76c9ec0035deb2f5eeff9037b8c45aa014ea4a7d46", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c91a1b06-ae3e-41f3-b7d2-b3d65353b4a3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "54856668c8afe3f073b2b352c6de73f08fbcb88c3aa8ab1c9143d237930ea474", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Note**\n\nBe careful when altering the `admin` and `trusted` properties of the `default` nodemap, especially if your Lustre servers fall into this nodemap.\n\n## Verifying Settings\n\nBy using `lctl nodemap_info all`, existing nodemap configuration is listed for easy export. This command acts as a shortcut into the configuration interface for nodemap. On the Lustre MGS, the `nodemap.active` parameter contains a `1` if nodemap is active on the system. Each policy group creates a directory containing the following parameters:\n\n- `admin` and `trusted` each contain a `1` if the values are set, and `0` otherwise.\n- `idmap` contains a list of the idmaps for the policy group, while `ranges` contains a list of NIDs for the group.\n- `squash_uid` and `squash_gid` determine what UID and GID users are squashed to if needed.\n\nThe expected outputs for the BirdResearchSite in the example above are:\n\n```\nmgs# lctl get_param nodemap.BirdResearchSite.idmap\n\n [\n  { idtype: uid, client_id: 530, fs_id: 11000 },\n  { idtype: uid, client_id: 531, fs_id: 11001 },\n  { idtype: uid, client_id: 532, fs_id: 11002 },\n  { idtype: uid, client_id: 533, fs_id: 11003 },\n  { idtype: gid, client_id: 600, fs_id: 11000 },\n  { idtype: gid, client_id: 601, fs_id: 11001 }\n ]\n\n mgs# lctl get_param nodemap.BirdResearchSite.ranges\n [\n  { id: 11, start_nid: 192.168.0.100@tcp, end_nid: 192.168.0.100@tcp }\n ]\n```\n\n## Ensuring Consistency\n\nConsistency issues may arise in a nodemap enabled configuration when Lustre clients mount from an unknown NID range, new UIDs and GIDs that were not part of a known map are added, or there are misconfigurations in the rules. Keep in mind the following when activating nodemap on a production system:\n\n- Creating new policy groups or idmaps on a production system is allowed, but reserve a maintenance window to alter the `trusted` property to avoid metadata problems.\n- To perform administrative tasks, access the Lustre file system via a policy group with `trusted` and `admin`properties set. This prevents the creation of orphaned and squashed files. Granting the `admin` property without the `trusted` property is dangerous. The root user on the client may know of UIDs and GIDs that are not present in any idmap. If root alters ownership to those identifiers, the ownership is squashed as a result. For example, tar file extracts may be flipped from an expected UID such as UID 500 to `nobody`, normally UID 99.\n- To map distinct UIDs at two or more sites onto a single UID or GID on the Lustre file system, create overlapping idmaps and place each site in its own policy group. Each distinct UID may have its own mapping onto the target UID or GID.\n- Introduced in Lustre 2.8In Lustre 2.8, changes must be manually kept in a script file to be re-applied after a Lustre reload, and changes must be made on each OSS, MDS, and MGS nodes, as there is no automatic synchronization between the nodes.\n- If `deny_unknown` is in effect, it is possible for unmapped users to see dentries which were viewed by a mapped user. This is a result of client caching, and unmapped users will not be able to view any file contents.\n- Nodemap activation status can be checked with `lctl nodemap_info`, but extra validation is possible. One way of ensuring valid deployment on a production system is to create a fingerprint of known files with specific UIDs and GIDs mapped to a test client. After bringing the Lustre system online after maintenance, the test client can validate the UIDs and GIDs map correctly before the system is mounted in user space.", "mimetype": "text/plain", "start_char_idx": 13247, "end_char_idx": 16797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "500cba4f-d799-4781-8e18-44990af993bf": {"__data__": {"id_": "500cba4f-d799-4781-8e18-44990af993bf", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97520c7d-6b47-4583-bdf0-c111eb12421b", "node_type": "1", "metadata": {}, "hash": "4fdaf818e643d29f9d6148d0b1862eb8b7f3c7594d20f1185bf0b09aaf6e982d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in Lustre 2.9\n\n# Configuring Shared-Secret Key (SSK) Security\n\n- [Configuring Shared-Secret Key (SSK) Security](#configuring-shared-secret-key-ssk-security)\n  * [SSK Security Overview](#ssk-security-overview)\n    + [Key features](#key-features)\n  * [SSK Security Flavors](#ssk-security-flavors)\n    + [Secure RPC Rules](#secure-rpc-rules)\n      - [Defining Rules](#defining-rules)\n      - [Listing Rules](#listing-rules)\n      - [Deleting Rules](#deleting-rules)\n  * [SSK Key Files](#ssk-key-files)\n    + [Key File Management](#key-file-management)\n      - [Writing Key Files](#writing-key-files)\n      - [Modifying Key Files](#modifying-key-files)\n      - [Reading Key Files](#reading-key-files)\n      - [Loading Key Files](#loading-key-files)\n  * [Lustre GSS Keyring](#lustre-gss-keyring)\n    + [Setup](#setup)\n    + [Server Setup](#server-setup)\n    + [Debugging GSS Keyring](#debugging-gss-keyring)\n    + [Revoking Keys](#revoking-keys)\n  * [Role of Nodemap in SSK](#role-of-nodemap-in-ssk)\n  * [SSK Examples](#ssk-examples)\n    + [Securing Client to Server Communications](#securing-client-to-server-communications)\n    + [Securing MGS Communications](#securing-mgs-communications)\n    + [Securing Server to Server Communications](#securing-server-to-server-communications)\n  * [Viewing Secure PtlRPC Contexts](#viewing-secure-ptlrpc-contexts)\n\nThis chapter describes how to configure Shared-Secret Key security and includes the following sections:\n\n- [the section called \u201cSSK Security Overview\u201d](#ssk-security-overview)\n- [the section called \u201cSSK Security Flavors\u201d](#ssk-security-flavors)\n- [the section called \u201cSSK Key Files\u201d](#ssk-key-files)\n- [the section called \u201cLustre GSS Keyring\u201d](#lustre-gss-keyring)\n- [the section called \u201cRole of Nodemap in SSK\u201d](#role-of-nodemap-in-ssk)\n- [the section called \u201cSSK Examples\u201d](#ssk-examples)\n- [the section called \u201cViewing Secure PtlRPC Contexts\u201d](#viewing-secure-ptlrpc-contexts)\n\n## SSK Security Overview\n\nThe SSK feature ensures integrity and data protection for Lustre PtlRPC traffic. Key files containing a shared secret and session-specific attributes are distributed to Lustre hosts. This authorizes Lustre hosts to mount the file system and optionally enables secure data transport, depending on which security flavor is configured. The administrator handles the generation, distribution, and installation of SSK key files, see\u00a0*the section called \u201cKey File Management\u201d*.\n\n### Key features\n\nSSK provides the following key features:\n\n- Host-based authentication\n- Data Transport Privacy\n  - Encrypts Lustre RPCs\n  - Prevents eavesdropping\n- Data Transport Integrity - Keyed-Hashing Message Authentication Code (HMAC)\n  - Prevents man-in-the-middle attacks\n  - Ensures RPCs cannot be altered undetected\n\n## SSK Security Flavors\n\nSSK is implemented as a Generic Security Services (GSS) mechanism through Lustre's support of the GSS Application Program Interface (GSSAPI). The SSK GSS mechanism supports five flavors that offer varying levels of protection.\n\nFlavors provided:\n\n- `skn` - SSK Null (Authentication)\n- `ska` - SSK Authentication and Integrity for non-bulk RPCs\n- `ski` - SSK Authentication and Integrity\n- `skpi` - SSK Authentication, Privacy, and Authentication\n- `gssnull` - Provides no protection. Used for testing purposes only\n\nThe table below describes the security characteristics of each flavor:\n\n**Table 9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97520c7d-6b47-4583-bdf0-c111eb12421b": {"__data__": {"id_": "97520c7d-6b47-4583-bdf0-c111eb12421b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "500cba4f-d799-4781-8e18-44990af993bf", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b3039226b181e77181e0b4a84d44e23530a8157474acad6d03d173d155835315", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e53743b-fa8b-4b25-9e82-67f189d4cfbe", "node_type": "1", "metadata": {}, "hash": "dddee7688837ff056fc3bc8d87f60f710eb2ea978be27b6f3f947f4af49470fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The SSK GSS mechanism supports five flavors that offer varying levels of protection.\n\nFlavors provided:\n\n- `skn` - SSK Null (Authentication)\n- `ska` - SSK Authentication and Integrity for non-bulk RPCs\n- `ski` - SSK Authentication and Integrity\n- `skpi` - SSK Authentication, Privacy, and Authentication\n- `gssnull` - Provides no protection. Used for testing purposes only\n\nThe table below describes the security characteristics of each flavor:\n\n**Table 9. SSK Security Flavor Protections**\n\n|                               | skn  | ska  | ski  | skpi |\n| ----------------------------- | ---- | ---- | ---- | ---- |\n| Required to mount file system | Yes  | Yes  | Yes  | Yes  |\n| Provides RPC Integrity        | No   | Yes  | Yes  | Yes  |\n| Provides RPC Privacy          | No   | No   | No   | Yes  |\n| Provides Bulk RPC Integrity   | No   | No   | Yes  | Yes  |\n| Provides Bulk RPC Privacy     | No   | No   | No   | Yes  |\n\n\n\nValid non-GSS flavors include:\n\n`null` - Provides no protection. This is the default flavor.\n\n`plain` - Plaintext with a hash on each RPC.\n\n### Secure RPC Rules\n\nSecure RPC configuration rules are written to the Lustre log (llog) with the `lctl` command. Rules are processed with the llog and dictate the security flavor that is used for a particular Lustre network and direction.\n\n**Note**\n\nRules take affect in a matter of seconds and impact both existing and new connections.\n\nRule format:\n\n*target*.srpc.flavor.*network*[.*direction*]=*flavor*\n\n- *target* - This could be the file system name or a specific MDT/OST device name.\n- *network* - LNet network name of the RPC initiator. For example `tcp1` or `o2ib0`. This can also be the keyword `default` that applies to all networks otherwise specified.\n- *direction* - Direction is optional. This could be one of `mdt2mdt`, `mdt2ost`,` cli2mdt`, or `cli2ost`.\n\n**Note**\n\nTo secure the connection to the MGS use the `mgssec=`*flavor* mount option. This is required because security rules are unknown to the initiator until after the MGS connection has been established.\n\nThe examples below are for a test Lustre file system named *testfs*.\n\n#### Defining Rules\n\nRules can be defined and deleted in any order. The rule with the greatest specificity for a given connection is applied. The *fsname*`.srpc.flavor.default` rule is the broadest rule as it applies to all non-MGS connections for the file system in the absence of a more specific rule. You may tailor SSK security to your needs by further specifying a specific `target`, `network`, and/or `direction`.\n\nThe following example illustrates an approach to configuring SSK security for an environment consisting of three LNet networks. The requirements for this example are:\n\n- All non-MGS connections must be authenticated.\n- PtlRPC traffic on LNet network `tcp0` must be encrypted.\n- LNet networks `tcp1` and `o2ib0` are local physically secure networks that require high performance. Do not encrypt PtlRPC traffic on these networks.\n\n1. Ensure that all non-MGS connections are authenticated and encrypted by default.\n\n   ```\n   mgs# lctl conf_param testfs.srpc.flavor.default=skpi\n   ```\n\n2. Override the file system default security flavor on LNet networks `tcp1` and `o2ib0` with `ska`. Security flavor `ska`provides authentication but without the performance impact of encryption and bulk RPC integrity.\n\n   ```\n   mgs# lctl conf_param testfs.srpc.flavor.tcp1=ska\n   mgs# lctl conf_param testfs.srpc.flavor.o2ib0=ska\n   ```\n\n**Note**\n\nCurrently the \"`lctl set_param -P`\" format does not work with sptlrpc.\n\n#### Listing Rules\n\nTo view the Secure RPC Config Rules, enter:\n\n```\nmgs# lctl get_param mgs.", "mimetype": "text/plain", "start_char_idx": 2937, "end_char_idx": 6578, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e53743b-fa8b-4b25-9e82-67f189d4cfbe": {"__data__": {"id_": "2e53743b-fa8b-4b25-9e82-67f189d4cfbe", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97520c7d-6b47-4583-bdf0-c111eb12421b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1a82f71c232289057ccbbc8a5aa1a114c31c2a769a354223abe463209cb726bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ecbf791-d6cf-4d54-8cd0-017baa7c34e3", "node_type": "1", "metadata": {}, "hash": "550352ebcb5f40aee255f2a96e8e7127d6640a79e5113ea22577f778de859d8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Ensure that all non-MGS connections are authenticated and encrypted by default.\n\n   ```\n   mgs# lctl conf_param testfs.srpc.flavor.default=skpi\n   ```\n\n2. Override the file system default security flavor on LNet networks `tcp1` and `o2ib0` with `ska`. Security flavor `ska`provides authentication but without the performance impact of encryption and bulk RPC integrity.\n\n   ```\n   mgs# lctl conf_param testfs.srpc.flavor.tcp1=ska\n   mgs# lctl conf_param testfs.srpc.flavor.o2ib0=ska\n   ```\n\n**Note**\n\nCurrently the \"`lctl set_param -P`\" format does not work with sptlrpc.\n\n#### Listing Rules\n\nTo view the Secure RPC Config Rules, enter:\n\n```\nmgs# lctl get_param mgs.*.live.testfs\n...\nSecure RPC Config Rules:\ntestfs.srpc.flavor.tcp.cli2mdt=skpi\ntestfs.srpc.flavor.tcp.cli2ost=skpi\ntestfs.srpc.flavor.o2ib=ski\n...\n```\n\n#### Deleting Rules\n\nTo delete a security flavor for an LNet network use the `conf_param -d` command to delete the flavor for that network:\n\nFor example, to delete the `testfs.srpc.flavor.o2ib1=ski` rule, enter:\n\n```\nmgs# lctl conf_param -d testfs.srpc.flavor.o2ib1\n```\n\n## SSK Key Files\n\nSSK key files are a collection of attributes formatted as fixed length values and stored in a file, which are distributed by the administrator to client and server nodes. Attributes include:\n\n- **Version** - Key file schema version number. Not user-defined.\n\n- **Type** - A mandatory attribute that denotes the Lustre role of the key file consumer. Valid key types are:\n\n  - **mgs** - for MGS when the `mgssec` `mount.lustre` option is used.\n  - **server** - for MDS and OSS servers\n  - **client** - for clients as well as servers who communicate with other servers in a client context (e.g. MDS communication with OSTs).\n\n  \n\n- **HMAC algorithm** - The Keyed-Hash Message Authentication Code algorithm used for integrity. Valid algorithms are (Default: SHA256):\n\n  - SHA256\n  - SHA512\n\n  \n\n- **Cryptographic algorithm** - Cipher for encryption. Valid algorithms are (Default: AES-256-CTR).\n\n  - AES-256-CTR\n\n- **Session security context expiration** - Seconds before session contexts generated from key expire and are regenerated (Default: 604800 seconds (7 days)).\n\n- **Shared key length** - Shared key length in bits (Default: 256).\n\n- **Prime length** - Length of prime (p) in bits used for the Diffie-Hellman Key Exchange (DHKE). (Default: 2048). This is generated only for client keys and can take a while to generate. This value also sets the minimum prime length that servers and MGS will accept from a client. Clients attempting to connect with a prime length less than the minimum will be rejected. In this way servers can guarantee the minimum encryption level that will be permitted.\n\n- **File system name** - Lustre File system name for key.\n\n- **MGS NIDs** - Comma-separated list of MGS NIDs. Only required when `mgssec` is used (Default: \"\").\n\n- **Nodemap name** - Nodemap name for key (Default: \"default\"). See *the section called \u201cRole of Nodemap in SSK\u201d*\n\n- **Shared key** - Shared secret used by all SSK flavors to provide authentication.\n\n- **Prime (p)** - Prime used for the DHKE. This is only used for keys with `Type=client`.\n\n**Note**\n\nKey files provide a means to authenticate Lustre connections; always store and transfer key files securely. Key files must not be world writable or they will fail to load.\n\n### Key File Management\n\nThe `lgss_sk` utility is used to write, modify, and read SSK key files. `lgss_sk` can be used to load key files singularly into the kernel keyring. `lgss_sk` options include:\n\n**Table 10. lgss_sk Parameters**\n\n| Parameter         | Value      | Description                                                  |\n| ----------------- | ---------- | ------------------------------------------------------------ |\n| `-l|--load`       | *filename* | Install key from file into user's session keyring.", "mimetype": "text/plain", "start_char_idx": 5909, "end_char_idx": 9768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ecbf791-d6cf-4d54-8cd0-017baa7c34e3": {"__data__": {"id_": "4ecbf791-d6cf-4d54-8cd0-017baa7c34e3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e53743b-fa8b-4b25-9e82-67f189d4cfbe", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2449da36f38259b4ddc4ef29deaf6111b609f9a339d2b419a5d05372a22f1ff6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e12e8e0f-3b9c-43ec-af99-5dd69231a90b", "node_type": "1", "metadata": {}, "hash": "26092cae167e406f7d580421727b07fcac838f74411305df2ab8f516217e5f87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Prime (p)** - Prime used for the DHKE. This is only used for keys with `Type=client`.\n\n**Note**\n\nKey files provide a means to authenticate Lustre connections; always store and transfer key files securely. Key files must not be world writable or they will fail to load.\n\n### Key File Management\n\nThe `lgss_sk` utility is used to write, modify, and read SSK key files. `lgss_sk` can be used to load key files singularly into the kernel keyring. `lgss_sk` options include:\n\n**Table 10. lgss_sk Parameters**\n\n| Parameter         | Value      | Description                                                  |\n| ----------------- | ---------- | ------------------------------------------------------------ |\n| `-l|--load`       | *filename* | Install key from file into user's session keyring. Must be executed by *root*. |\n| `-m|--modify`     | *filename* | Modify a file's key attributes                               |\n| `-r|--read`       | *filename* | Show file's key attributes                                   |\n| `-w|--write`      | *filename* | Generate key file                                            |\n| `-c|--crypt`      | *cipher*   | Cipher for encryption (Default: AES Counter mode)AES-256-CTR |\n| `-i|--hmac`       | *hash*     | Hash algorithm for intregrity (Default: SHA256)SHA256 or SHA512 |\n| `-e|--expire`     | *seconds*  | Seconds before contexts from key expire (Default: 604800 (7 days)) |\n| `-f|--fsname`     | *name*     | File system name for key                                     |\n| `-g|--mgsnids`    | *NID(s)*   | Comma separated list of MGS NID(s). Only required when mgssec is used (Default: \"\") |\n| `-n|--nodemap`    | *map*      | Nodemap name for key (Default: \"default\")                    |\n| `-p|--prime-bits` | *length*   | Prime length (p) for DHKE in bits (Default: 2048)            |\n| `-t|--type`       | *type*     | Key type (mgs, server, client)                               |\n| `-k|--key-bits`   | *length*   | Shared key length in bits (Default: 256)                     |\n| `-d|--data`       | *file*     | Shared key random data source (Default: /dev/random)         |\n| `-v|--verbose`    |            | Increase verbosity for errors                                |\n\n#### Writing Key Files\n\nKey files are generated by the `lgss_sk` utility. Parameters are specified on the command line followed by the `--write` parameter and the filename to write to. The `lgss_sk` utility will not overwrite files so the filename must be unique. Mandatory parameters for generating key files are `--type`, either `--fsname` or `--mgsnids`, and `--write`; all other parameters are optional.\n\n`lgss_sk` uses `/dev/random` as the default entropy data source; you may override this with the `--data` parameter. When no hardware random number generator is available on the system where `lgss_sk` is executing, you may need to press keys on the keyboard or move the mouse (if directly attached to the system) or cause disk IO (if system is remote), in order to generate entropy for the shared key. It is possible to use `/dev/urandom` for testing purposes but this may provide less security in some cases.\n\nExample:\n\nTo create a *server* type key file for the *testfs* Lustre file system for clients in the *biology* nodemap, enter:\n\n```\nserver# lgss_sk -t server -f testfs -n biology \\\n-w testfs.server.biology.key\n```\n\n#### Modifying Key Files\n\nLike writing key files you modify them by specifying the paramaters on the command line that you want to change. Only key file attributes associated with the parameters provided are changed; all other attributes remain unchanged.", "mimetype": "text/plain", "start_char_idx": 8978, "end_char_idx": 12592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e12e8e0f-3b9c-43ec-af99-5dd69231a90b": {"__data__": {"id_": "e12e8e0f-3b9c-43ec-af99-5dd69231a90b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ecbf791-d6cf-4d54-8cd0-017baa7c34e3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a8080a7e10e93e8a1ba66f934dc2900a6fa8b833102797af5a1a104ca19c1bd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4da52265-8c66-44ba-be49-47e16fcf8b71", "node_type": "1", "metadata": {}, "hash": "0bb80081514b528b318d9e900bb45180d6b094edd589ded9a833cf0f6b36ce1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When no hardware random number generator is available on the system where `lgss_sk` is executing, you may need to press keys on the keyboard or move the mouse (if directly attached to the system) or cause disk IO (if system is remote), in order to generate entropy for the shared key. It is possible to use `/dev/urandom` for testing purposes but this may provide less security in some cases.\n\nExample:\n\nTo create a *server* type key file for the *testfs* Lustre file system for clients in the *biology* nodemap, enter:\n\n```\nserver# lgss_sk -t server -f testfs -n biology \\\n-w testfs.server.biology.key\n```\n\n#### Modifying Key Files\n\nLike writing key files you modify them by specifying the paramaters on the command line that you want to change. Only key file attributes associated with the parameters provided are changed; all other attributes remain unchanged.\n\nTo modify a key file's *Type* to *client* and populate the *Prime (p)* key attribute, if it is missing, enter:\n\n```\nclient# lgss_sk -t client -m testfs.client.biology.key\n```\n\nTo add MGS NIDs `192.168.1.101@tcp,10.10.0.101@o2ib` to server key file `testfs.server.biology.key` and client key file `testfs.client.biology.key`, enter\n\n```\nserver# lgss_sk -g 192.168.1.101@tcp,10.10.0.101@o2ib \\\n-m testfs.server.biology.key\n\nclient# lgss_sk -g 192.168.1.101@tcp,10.10.0.101@o2ib \\\n-m testfs.client.biology.key\n```\n\nTo modify the `testfs.server.biology.key` on the MGS to support MGS connections from *biology* clients, modify the key file's *Type* to include *mgs* in addition to *server*, enter:\n\n```\nmgs# lgss_sk -t mgs,server -m testfs.server.biology.key\n```\n\n#### Reading Key Files\n\nRead key files with the `lgss_sk` utility and `--read` parameter. Read the keys modified in the previous examples:\n\n```\nmgs# lgss_sk -r testfs.server.biology.key\nVersion:        1\nType:           mgs server\nHMAC alg:       SHA256\nCrypt alg:      AES-256-CTR\nCtx Expiration: 604800 seconds\nShared keylen:  256 bits\nPrime length:   2048 bits\nFile system:    testfs\nMGS NIDs:       192.168.1.101@tcp 10.10.0.101@o2ib\nNodemap name:   biology\nShared key:\n  0000: 84d2 561f 37b0 4a58 de62 8387 217d c30a  ..V.7.JX.b..!}..\n  0010: 1caa d39c b89f ee6c 2885 92e7 0765 c917  .......l(....e..\n\nclient# lgss_sk -r testfs.client.biology.key\nVersion:        1\nType:           client\nHMAC alg:       SHA256\nCrypt alg:      AES-256-CTR\nCtx Expiration: 604800 seconds\nShared keylen:  256 bits\nPrime length:   2048 bits\nFile system:    testfs\nMGS NIDs:       192.168.1.101@tcp 10.10.0.101@o2ib\nNodemap name:   biology\nShared key:\n  0000: 84d2 561f 37b0 4a58 de62 8387 217d c30a  ..V.7.JX.b..!}..\n  0010: 1caa d39c b89f ee6c 2885 92e7 0765 c917  .......l(....e..\nPrime (p) :\n  0000: 8870 c3e3 09a5 7091 ae03 f877 f064 c7b5  .p....p....w.d..\n  0010: 14d9 bc54 75f8 80d3 22f9 2640 0215 6404  ...Tu...\".&@..d.", "mimetype": "text/plain", "start_char_idx": 11729, "end_char_idx": 14565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4da52265-8c66-44ba-be49-47e16fcf8b71": {"__data__": {"id_": "4da52265-8c66-44ba-be49-47e16fcf8b71", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e12e8e0f-3b9c-43ec-af99-5dd69231a90b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a0e1d3d0c072b4613f65d6b8d87cbd805fbe503f18b7094a49798607a3e68ba5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "955d252e-8d72-4be9-87eb-66d9f5f08f2e", "node_type": "1", "metadata": {}, "hash": "603343b40f03ef88ccf18890d2d4eee9ffb998914b1249fb8a6c29539913712d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0010: 1caa d39c b89f ee6c 2885 92e7 0765 c917  .......l(....e..\nPrime (p) :\n  0000: 8870 c3e3 09a5 7091 ae03 f877 f064 c7b5  .p....p....w.d..\n  0010: 14d9 bc54 75f8 80d3 22f9 2640 0215 6404  ...Tu...\".&@..d.\n  0020: 1c53 ba84 1267 bea2 fb05 37a4 ed2d 5d90  .S...g....7..-].\n  0030: 84e3 1a67 67f0 47c7 0c68 5635 f50e 9cf0  ...gg.G..hV5....\n  0040: e622 6f53 2627 6af6 9598 eeed 6290 9b1e  .\"oS&'j.....b...\n  0050: 2ec5 df04 884a ea12 9f24 cadc e4b6 e91d  .....J...$......\n  0060: 362f a239 0a6d 0141 b5e0 5c56 9145 6237  6/.9.m.A..\\V.Eb7\n  0070: 59ed 3463 90d7 1cbe 28d5 a15d 30f7 528b  Y.4c....(..]0.R.\n  0080: 76a3 2557 e585 a1be c741 2a81 0af0 2181  v.%W.....A*...!.\n  0090: 93cc a17a 7e27 6128 5ebd e0a4 3335 db63  ...z~'a(^...35.c\n  00a0: c086 8d0d 89c1 c203 3298 2336 59d8 d7e7  ........2.#6Y...\n  00b0: e52a b00c 088f 71c3 5109 ef14 3910 fcf6  .*....q.Q...9...\n  00c0: 0fa0 7db7 4637 bb95 75f4 eb59 b0cd 4077  ..}.F7..u..Y..@w\n  00d0: 8f6a 2ebd f815 a9eb 1b77 c197 5100 84c0  .j.......w..Q...\n  00e0: 3dc0 d75d 40b3 6be5 a843 751a b09c 1b20  =..]@.k..Cu....\n  00f0: 8126 4817 e657 b004 06b6 86fb 0e08 6a53  .&H..W........jS\n```\n\n#### Loading Key Files\n\nKey files can be loaded into the kernel keyring with the `lgss_sk` utility or at mount time with the `skpath` mount option. The `skpath` method has the advantage that it accepts a directory path and loads all key files within the directory into the keyring. The `lgss_sk` utility loads a single key file into the keyring with each invocation. Key files must not be world writable or they will fail to load.\n\nThird party tools can also load the keys if desired. The only caveat is that the key must be available when the request_key upcall to userspace is made and they use the correct key descriptions for a key so that it can be found during the upcall (see Key Descriptions).", "mimetype": "text/plain", "start_char_idx": 14358, "end_char_idx": 16195, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "955d252e-8d72-4be9-87eb-66d9f5f08f2e": {"__data__": {"id_": "955d252e-8d72-4be9-87eb-66d9f5f08f2e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4da52265-8c66-44ba-be49-47e16fcf8b71", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c378382368469f2509dceddc53830153393668224c54154341f013ed8c1df828", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f990239-3787-458e-a0ed-09b3d54d702c", "node_type": "1", "metadata": {}, "hash": "de1c1945648bca99c82d570ea576610f8e624ffc0248218b60ac2dd25696da20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The `skpath` method has the advantage that it accepts a directory path and loads all key files within the directory into the keyring. The `lgss_sk` utility loads a single key file into the keyring with each invocation. Key files must not be world writable or they will fail to load.\n\nThird party tools can also load the keys if desired. The only caveat is that the key must be available when the request_key upcall to userspace is made and they use the correct key descriptions for a key so that it can be found during the upcall (see Key Descriptions).\n\nExamples:\n\nLoad the `testfs.server.biology.key` key file using `lgss_sk`, enter:\n\n```\nserver# lgss_sk -l testfs.server.biology.key\n```\n\nUse the `skpath` mount option to load all of the key files in the `/secure_directory` directory when mounting a storage target, enter:\n\n```\nserver# mount -t lustre -o skpath=/secure_directory \\\n/storage/target /mount/point\n```\n\nUse the `skpath` mount option to load key files into the keyring on a client, enter:\n\n```\nclient# mount -t lustre -o skpath=/secure_directory \\\nmgsnode:/testfs /mnt/testfs\n```\n\n## Lustre GSS Keyring\n\nThe Lustre GSS Keyring binary\u00a0`lgss_keyring`\u00a0is used by SSK to handle the upcall from kernel space into user space via\u00a0`request-key`. The purpose of\u00a0`lgss_keyring`\u00a0is to create a token that is passed as part of the security context initialization RPC (SEC_CTX_INIT).\n\n### Setup\n\nThe Lustre GSS keyring types of flavors utilize the Linux kernel keyring infrastructure to maintain keys as well as to perform the upcall from kernel space to userspace for key negotiation/establishment. The GSS keyring establishes a key type (see \u201crequest-key(8)\u201d) named `lgssc` when the Lustre `ptlrpc_gss` kernel module is loaded. When a security context must be established it creates a key and uses the `request-key` binary in an upcall to establish the key. This key will look for the configuration file in `/etc/request-key.d` with the name *keytype*.conf, for Lustre this is `lgssc.conf`.\n\nEach node participating in SSK Security must have a `/etc/request-key.d/lgssc.conf` file that contains the following single line:\n\n`create lgssc * * /usr/sbin/lgss_keyring %o %k %t %d %c %u %g %T %P %S`\n\nThe `request-key` binary will call `lgss_keyring` with the arguments following it with their substituted values (see `request-key.conf(5)`).\n\n### Server Setup\n\nLustre servers do not use the Linux `request-key` mechanism as clients do. Instead servers run a daemon that uses a pipefs with the kernel to trigger events based on read/write to a file descriptor. The server-side binary is `lsvcgssd`. It can be executed in the foreground or as a daemon. Below are the parameters for the `lsvcgssd` binary which requires various security flavors (`gssnull, krb5, sk`) to be enabled explicitly. This ensures that only required functionality is enabled.\n\n**Table 11. lsvcgssd Parameters**\n\n| Parameter | Description                           |\n| --------- | ------------------------------------- |\n| `-f`      | Run in foreground                     |\n| `-n`      | Do not establish Kerberos credentials |\n| `-v`      | Verbosity                             |\n| `-m`      | Service MDS                           |\n| `-o`      | Service OSS                           |\n| `-g`      | Service MGS                           |\n| `-k`      | Enable Kerberos support               |\n| `-s`      | Enable Shared Key support             |\n| `-z`      | Enable `gssnull` support              |\n\n \n\n \n\nA SysV style init script is installed for starting and stopping the `lsvcgssd` daemon. The init script checks the`LSVCGSSARGS` variable in the `/etc/sysconfig/lsvcgss` configuration file for startup parameters.\n\nKeys during the upcall on the client and handling of an RPC on the server are found by using a specific key description for each key in the kernel keyring.\n\nFor each MGS NID there must be a separate key loaded. The format of the key description should be:\n\n**Table 12.", "mimetype": "text/plain", "start_char_idx": 15642, "end_char_idx": 19606, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f990239-3787-458e-a0ed-09b3d54d702c": {"__data__": {"id_": "4f990239-3787-458e-a0ed-09b3d54d702c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "955d252e-8d72-4be9-87eb-66d9f5f08f2e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d509745669029f6e3d27b00e2a66714af6edc788dd6c8dcf26ee4b8cedd9dca5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "727175eb-c8a0-421c-8c00-82eea3ed13bc", "node_type": "1", "metadata": {}, "hash": "ef868e5d6bdbdd4c5e7f6a09c6b91b4fe4b08f4eca5fb7fd58c339acf03b9f7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The init script checks the`LSVCGSSARGS` variable in the `/etc/sysconfig/lsvcgss` configuration file for startup parameters.\n\nKeys during the upcall on the client and handling of an RPC on the server are found by using a specific key description for each key in the kernel keyring.\n\nFor each MGS NID there must be a separate key loaded. The format of the key description should be:\n\n**Table 12. Key Descriptions**\n\n| Type            | Key Description               | Example                      |\n| --------------- | ----------------------------- | ---------------------------- |\n| MGC             | lustre:MGC*NID*               | `lustre:MGC192.168.1.10@tcp` |\n| MDC/OSC/OSP/LWP | lustre:*fsname*               | `lustre:testfs`              |\n| MDT             | lustre:*fsname*:*NodemapName* | `lustre:testfs:biology`      |\n| OST             | lustre:*fsname*:*NodemapName* | `lustre:testfs:biology`      |\n| MGS             | lustre:MGS                    | `lustre:MGS`                 |\n\n \n\n \n\nAll keys for Lustre use the `user` type for keys and are attached to the user\u2019s keyring. This is not configurable. Below is an example showing how to list the user\u2019s keyring, load a key file, read the key, and clear the key from the kernel keyring.\n\n```\nclient# keyctl show\nSession Keyring\n  17053352 --alswrv      0     0  keyring: _ses\n 773000099 --alswrv      0 65534   \\_ keyring: _uid.0\n\nclient# lgss_sk -l /secure_directory/testfs.client.key\n\nclient# keyctl show\nSession Keyring\n  17053352 --alswrv      0     0  keyring: _ses\n 773000099 --alswrv      0 65534   \\_ keyring: _uid.0\n1028795127 --alswrv      0     0       \\_ user: lustre:testfs\n\nclient# keyctl pipe 1028795127 | lgss_sk -r -\nVersion:        1\nType:           client\nHMAC alg:       SHA256\nCrypt alg:      AES-256-CTR\nCtx Expiration: 604800 seconds\nShared keylen:  256 bits\nPrime length:   2048 bits\nFile system:    testfs\nMGS NIDs:\nNodemap name:   default\nShared key:\n  0000: faaf 85da 93d0 6ffc f38c a5c6 f3a6 0408  ......o.........\n  0010: 1e94 9b69 cf82 d0b9 880b f173 c3ea 787a  ...i.......s..xz\nPrime (p) :\n  0000: 9c12 ed95 7b9d 275a 229e 8083 9280 94a0  ....{.'Z\".......\n  0010: 8593 16b2 a537 aa6f 8b16 5210 3dd5 4c0c  .....7.o..R.=.L.\n  0020: 6fae 2729 fcea 4979 9435 f989 5b6e 1b8a  o.')..Iy.5..[n..\n  0030: 5039 8db2 3a23 31f0 540c 33cb 3b8e 6136  P9..:#1.T.3.;.a6\n  0040: ac18 1eba f79f c8dd 883d b4d2 056c 0501  .........=...l..\n  0050: ac17 a4ab 9027 4930 1d19 7850 2401 7ac4  .....'I0..xP$.z.\n  0060: 92b4 2151 8837 ba23 94cf 22af 72b3 e567  ..!Q.7.#..\".r..g\n  0070: 30eb 0cd4 3525 8128 b0ff 935d 0ba3 0fc0  0...5%.(...]....", "mimetype": "text/plain", "start_char_idx": 19213, "end_char_idx": 21825, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "727175eb-c8a0-421c-8c00-82eea3ed13bc": {"__data__": {"id_": "727175eb-c8a0-421c-8c00-82eea3ed13bc", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f990239-3787-458e-a0ed-09b3d54d702c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cf0a270350fd464bc771356d049b705d07c3faec683504defe7a56046bd5fca2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f4d9103-e696-4ce0-8ad8-a0b921a123b3", "node_type": "1", "metadata": {}, "hash": "c9689cfe79d72d4d1df2f7cb8e55ec195de41f4e9f4c0afc31dcda74b17d4e21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ";.a6\n  0040: ac18 1eba f79f c8dd 883d b4d2 056c 0501  .........=...l..\n  0050: ac17 a4ab 9027 4930 1d19 7850 2401 7ac4  .....'I0..xP$.z.\n  0060: 92b4 2151 8837 ba23 94cf 22af 72b3 e567  ..!Q.7.#..\".r..g\n  0070: 30eb 0cd4 3525 8128 b0ff 935d 0ba3 0fc0  0...5%.(...]....\n  0080: 9afa 5da7 0329 3ce9 e636 8a7d c782 6203  ..]..)<..6.}..b.\n  0090: bb88 012e 61e7 5594 4512 4e37 e01d bdfc  ....a.U.E.N7....\n  00a0: cb1d 6bd2 6159 4c3a 1f4f 1167 0e26 9e5e  ..k.aYL:.O.g.&.^\n  00b0: 3cdc 4a93 63f6 24b1 e0f1 ed77 930b 9490  <.J.c.$....w....\n  00c0: 25ef 4718 bff5 033e 11ba e769 4969 8a73  %.G....>...iIi.s\n  00d0: 9f5f b7bb 9fa0 7671 79a4 0d28 8a80 1ea1  ._....vqy..(....\n  00e0: a4df 98d6 e20e fe10 8190 5680 0d95 7c83  ..........V...|.\n  00f0: 6e21 abb3 a303 ff55 0aa8 ad89 b8bf 7723  n!.....U......w#\n\nclient# keyctl clear @u\n\nclient# keyctl show\nSession Keyring\n  17053352 --alswrv      0     0  keyring: _ses\n 773000099 --alswrv      0 65534   \\_ keyring: _uid.0\n```\n\n### Debugging GSS Keyring\n\nLustre client and server support several debug levels, which can be seen below.\n\nDebug levels:\n\n- 0 - Error\n- 1 - Warn\n- 2 - Info\n- 3 - Debug\n- 4 - Trace\n\nTo set the debug level on the client use the Lustre parameter:\n\n`sptlrpc.gss.lgss_keyring.debug_level`\n\nFor example to set the debug level to trace, enter:\n\n```\nclient# lctl set_param sptlrpc.gss.lgss_keyring.debug_level=4\n```\n\nServer-side verbosity is increased by adding additional verbose flags (`-v`) to the command line arguments for the daemon. The following command runs the `lsvcgssd` daemon in the foreground with debug verbosity supporting gssnull and SSK\n\n```\nserver# lsvcgssd -f -vvv -z -s\n```\n\n`lgss_keyring` is called as part of the `request-key` upcall which has no standard output; therefore logging is done through syslog. The server-side logging with `lsvcgssd` is written to standard output when executing in the foreground and to syslog in daemon mode.\n\n### Revoking Keys\n\nThe keys discussed above with `lgss_sk` and the `skpath` mount options are not revoked. They are only used to create valid contexts for client connections. Instead of revoking them they can be invalidated in one of two ways.\n\n- Unloading the key from the user keyring on the server will cause new client connections to fail. If no longer necessary it can be deleted.\n- Changing the nodemap name for the clients on the servers. Since the nodemap is an integral part of the shared key context instantiation, renaming the nodemap a group of NIDs belongs to will prevent any new contexts.\n\nThere currently does not exist a mechanism to flush contexts from Lustre. Targets could be unmounted from the servers to purge contexts.", "mimetype": "text/plain", "start_char_idx": 21557, "end_char_idx": 24220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f4d9103-e696-4ce0-8ad8-a0b921a123b3": {"__data__": {"id_": "4f4d9103-e696-4ce0-8ad8-a0b921a123b3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "727175eb-c8a0-421c-8c00-82eea3ed13bc", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "557db07dfab26b6247db9ef862ea4c8fed16841da63e20fec5fb2f95594ac907", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d1f4446-4ad9-4743-9cdb-333265814353", "node_type": "1", "metadata": {}, "hash": "12db8314a0c637df489f4aa52d6e9f07726820a8a56ae4bf6f00126c6f6df8a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The server-side logging with `lsvcgssd` is written to standard output when executing in the foreground and to syslog in daemon mode.\n\n### Revoking Keys\n\nThe keys discussed above with `lgss_sk` and the `skpath` mount options are not revoked. They are only used to create valid contexts for client connections. Instead of revoking them they can be invalidated in one of two ways.\n\n- Unloading the key from the user keyring on the server will cause new client connections to fail. If no longer necessary it can be deleted.\n- Changing the nodemap name for the clients on the servers. Since the nodemap is an integral part of the shared key context instantiation, renaming the nodemap a group of NIDs belongs to will prevent any new contexts.\n\nThere currently does not exist a mechanism to flush contexts from Lustre. Targets could be unmounted from the servers to purge contexts. Alternatively shorter context expiration could be used when the key is created so that contexts need to be refreshed more frequently than the default. 3600 seconds could be reasonable depending on the use case so that contexts will have to be renegotiated every hour.\n\n## Role of Nodemap in SSK\n\nSSK uses Nodemap (See *Mapping UIDs and GIDs with Nodemap*) policy group names and their associated NID range(s) as a mechanism to prevent key file forgery, and to control the range of NIDs on which a given key file can be used.\n\nClients assume they are in the nodemap specified in the key file they use. When clients instantiate security contexts an upcall is triggered that specifies information about the context that triggers it. From this context information `request-key` calls `lgss_keyring`, which in turn looks up the key with description lustre:*fsname* or lustre:*target_name* for the MGC. Using the key found in the user keyring matching the description, the nodemap name is read from the key, hashed with SHA256, and sent to the server.\n\nServers look up the client\u2019s NID to determine which nodemap the NID is associated with and sends the nodemap name to `lsvcgssd`. The `lsvcgssd` daemon verifies whether the HMAC equals the nodemap value sent by the client. This prevents forgery and invalidates the key when a client\u2019s NID is not associated with the nodemap name defined on the servers.\n\nIt is not required to activate the Nodemap feature in order for SSK to perform client NID to nodemap name lookups.\n\n## SSK Examples\n\nThe examples in this section use 1 MGS/MDS (NID 172.16.0.1@tcp), 1 OSS (NID 172.16.0.3@tcp), and 2 clients. The Lustre file system name is\u00a0*testfs*.\n\n### Securing Client to Server Communications\n\nThis example illustrates how to configure SSK to apply Privacy and Integrity protections to client-to-server PtlRPC traffic on the `tcp` network. Rules that specify a direction, specifically `cli2mdt` and `cli2ost`, are used. This permits server-to-server communications to continue using `null` which is the *default* flavor for all Lustre connections. This arrangement provides no server-to-server protections, see *the section called \u201cSecuring Server to Server Communications\u201d*.\n\n1. Create secure directory for storing SSK key files.\n\n   ```\n   mds# mkdir /secure_directory\n   mds# chmod 600 /secure_directory\n   oss# mkdir /secure_directory\n   oss# chmod 600 /secure_directory\n   cli1# mkdir /secure_directory\n   cli1# chmod 600 /secure_directory\n   cli2# mkdir /secure_directory\n   cli2# chmod 600 /secure_directory\n   ```\n\n2. Generate a key file for the MDS and OSS servers. Run:\n\n   ```\n   mds# lgss_sk -t server -f testfs -w \\\n   /secure_directory/testfs.server.key\n   ```\n\n3. Securely copy the /secure_directory/testfs.server.key key file to the OSS.\n\n   ```\n   mds# scp /secure_directory/testfs.server.key \\\n   oss:/secure_directory/\n   ```\n\n4. Securely copy the `/secure_directory/testfs.server.key` key file to`/secure_directory/testfs.client.key` on *client1*.\n\n   ```\n   mds# scp /secure_directory/testfs.server.key \\\n   client1:/secure_directory/testfs.client.key\n   ```\n\n5. Modify the key file type to `client` on *client1*.", "mimetype": "text/plain", "start_char_idx": 23345, "end_char_idx": 27390, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d1f4446-4ad9-4743-9cdb-333265814353": {"__data__": {"id_": "6d1f4446-4ad9-4743-9cdb-333265814353", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f4d9103-e696-4ce0-8ad8-a0b921a123b3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d17f7c1c688882cc318dc12c53697292397948afccd9a81da2ee4e12801c99d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18d32423-84bc-46dc-9aa5-046a8cdb2759", "node_type": "1", "metadata": {}, "hash": "61a7e8d36f776ad07f4e2768101c9160929caa94ba3daa50444c15c7bec5a894", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Generate a key file for the MDS and OSS servers. Run:\n\n   ```\n   mds# lgss_sk -t server -f testfs -w \\\n   /secure_directory/testfs.server.key\n   ```\n\n3. Securely copy the /secure_directory/testfs.server.key key file to the OSS.\n\n   ```\n   mds# scp /secure_directory/testfs.server.key \\\n   oss:/secure_directory/\n   ```\n\n4. Securely copy the `/secure_directory/testfs.server.key` key file to`/secure_directory/testfs.client.key` on *client1*.\n\n   ```\n   mds# scp /secure_directory/testfs.server.key \\\n   client1:/secure_directory/testfs.client.key\n   ```\n\n5. Modify the key file type to `client` on *client1*. This operation also generates a prime number of `Prime length`to populate the `Prime (p)` attribute. Run:\n\n   ```\n   client1# lgss_sk -t client \\\n   -m /secure_directory/testfs.client.key\n   ```\n\n6. Create a `/etc/request-key.d/lgssc.conf` file on all nodes that contains this line '`create lgssc * * /usr/sbin/lgss_keyring %o %k %t %d %c %u %g %T %P %S`' without the single quotes. Run:\n\n   ```\n   mds# echo create lgssc \\* \\* /usr/sbin/lgss_keyring %o %k %t %d %c %u %g %T %P %S > /etc/request-key.d/lgssc.conf\n   oss# echo create lgssc \\* \\* /usr/sbin/lgss_keyring %o %k %t %d %c %u %g %T %P %S > /etc/request-key.d/lgssc.conf\n   client1# echo create lgssc \\* \\* /usr/sbin/lgss_keyring %o %k %t %d %c %u %g %T %P %S > /etc/request-key.d/lgssc.conf\n   client2# echo create lgssc \\* \\* /usr/sbin/lgss_keyring %o %k %t %d %c %u %g %T %P %S > /etc/request-key.d/lgssc.conf\n   ```\n\n7. Configure the `lsvcgss` daemon on the MDS and OSS. Set the `LSVCGSSDARGS` variable in`/etc/sysconfig/lsvcgss` on the MDS to `\u2018-s -m\u2019`. On the OSS, set the `LSVCGSSDARGS` variable in`/etc/sysconfig/lsvcgss` to `\u2018-s -o\u2019`\n\n8. Start the `lsvcgssd` daemon on the MDS and OSS. Run:\n\n   ```\n   mds# systemctl start lsvcgss.service\n   oss# systemctl start lsvcgss.service\n   ```\n\n9. Mount the MDT and OST with the `-o skpath=/secure_directory` mount option. The `skpath` option loads all SSK key files found in the directory into the kernel keyring.\n\n10. Set client to MDT and client to OST security flavor to SSK Privacy and Integrity, `skpi`:\n\n    ```\n    mds# lctl conf_param testfs.srpc.flavor.tcp.cli2mdt=skpi\n    mds# lctl conf_param testfs.srpc.flavor.tcp.cli2ost=skpi\n    ```\n\n11. Mount the testfs file system on client1 and client2:\n\n    ```\n    client1# mount -t lustre -o skpath=/secure_directory 172.16.0.1@tcp:/testfs /mnt/testfs\n    client2# mount -t lustre -o skpath=/secure_directory 172.16.0.1@tcp:/testfs /mnt/testfs\n    mount.lustre: mount 172.16.0.1@tcp:/testfs at /mnt/testfs failed: Connection refused\n    ```\n\n12. *client2* failed to authenticate because it does not have a valid key file. Repeat steps 4 and 5, substitute client1 for client2, then mount the testfs file system on client2:\n\n    ```\n    client2# mount -t lustre -o skpath=/secure_directory 172.16.0.1@tcp:/testfs /mnt/testfs\n    ```\n\n13.", "mimetype": "text/plain", "start_char_idx": 26782, "end_char_idx": 29692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18d32423-84bc-46dc-9aa5-046a8cdb2759": {"__data__": {"id_": "18d32423-84bc-46dc-9aa5-046a8cdb2759", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d1f4446-4ad9-4743-9cdb-333265814353", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5a350b8908b1245d318b7d3b1e438a96edcd21b2893271cee047cf0baeaab437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e67619a3-0fd3-416d-899f-58d0a2a848cf", "node_type": "1", "metadata": {}, "hash": "a3c5587932fcc8dff8653c96642afde366c1b34a3e0b0d3ff6faa5675b71835b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*client2* failed to authenticate because it does not have a valid key file. Repeat steps 4 and 5, substitute client1 for client2, then mount the testfs file system on client2:\n\n    ```\n    client2# mount -t lustre -o skpath=/secure_directory 172.16.0.1@tcp:/testfs /mnt/testfs\n    ```\n\n13. Verify that the `mdc` and `osc` connections are using the SSK mechanism and that `rpc` and `bulk` security flavors are `skpi`. See *the section called \u201cViewing Secure PtlRPC Contexts\u201d*.\n\n    Notice the `mgc` connection to the MGS has no secure PtlRPC security context. This is because `skpi` security was only specified for client-to-MDT and client-to-OST connections in step 10. The following example details the steps necessary to secure the connection to the MGS.\n\n### Securing MGS Communications\n\nThis example builds on the previous example.\n\n1. Enable `lsvcgss` MGS service support on MGS. Edit `/etc/sysconfig/lsvcgss` on the MGS and add the (`-g`) parameter to the `LSVCGSSDARGS` variable. Restart the `lsvcgss` service.\n\n2. Add *mgs* key type and *MGS NIDs* to `/secure_directory/testfs.server.key` on MDS.\n\n   ```\n   mgs# lgss_sk -t mgs,server -g 172.16.0.1@tcp,172.16.0.2@tcp -m /secure_directory/testfs.server.key\n   ```\n\n3. Load the modified key file on the MGS. Run:\n\n   ```\n   mgs# lgss_sk -l /secure_directory/testfs.server.key\n   ```\n\n4. Add *MGS NIDs* to `/secure_directory/testfs.client.key` on client, client1.\n\n   ```\n   client1# lgss_sk -g 172.16.0.1@tcp,172.16.0.2@tcp -m /secure_directory/testfs.client.key\n   ```\n\n5. Unmount the testfs file system on client1, then mount with the `mgssec=skpi` mount option:\n\n   ```\n   cli1# mount -t lustre -o mgssec=skpi,skpath=/secure_directory 172.16.0.1@tcp:/testfs /mnt/testfs\n   ```\n\n6. Verify that client1\u2019s MGC connection is using the SSK mechanism and `skpi` security flavor. See *the section called \u201cViewing Secure PtlRPC Contexts\u201d*.\n\n### Securing Server to Server Communications\n\nThis example illustrates how to configure SSK to apply *Integrity* protection, `ski` flavor, to MDT-to-OST PtlRPC traffic on the `tcp` network.\n\nThis example builds on the previous example.\n\n1. Create a Nodemap policy group named `LustreServers` on the MGS for the Lustre Servers, enter:\n\n   ```\n   mgs# lctl nodemap_add LustreServers\n   ```\n\n2. Add MDS and OSS NIDs to the LustreServers nodemap, enter:\n\n   ```\n   mgs# lctl nodemap_add_range --name LustreServers --range 172.16.0.[1-3]@tcp\n   ```\n\n3. Create key file of type `mgs,server` for use with nodes in the *LustreServers* Nodemap range.\n\n   ```\n   mds# lgss_sk -t mgs,server -f testfs -g \\\n   172.16.0.1@tcp,172.16.0.2@tcp -n LustreServers -w \\\n   /secure_directory/testfs.LustreServers.key\n   ```\n\n4. Securely copy the `/secure_directory/testfs.LustreServers.key` key file to the OSS.\n\n   ```\n   mds# scp /secure_directory/testfs.LustreServers.key oss:/secure_directory/\n   ```\n\n5. On the MDS and OSS, copy `/secure_directory/testfs.LustreServers.key` to`/secure_directory/testfs.LustreServers.client.key`.\n\n6. On each server modify the key file type of `/secure_directory/testfs.LustreServers.client.key` to be of type client. This operation also generates a prime number of *Prime length* to populate the *Prime (p)* attribute.", "mimetype": "text/plain", "start_char_idx": 29403, "end_char_idx": 32631, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e67619a3-0fd3-416d-899f-58d0a2a848cf": {"__data__": {"id_": "e67619a3-0fd3-416d-899f-58d0a2a848cf", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18d32423-84bc-46dc-9aa5-046a8cdb2759", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "18c426843c26adea0d268be27280fc8e758488ffa88557f19d85106d42484873", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e68caf36-73a2-4ceb-8f72-07334ca8152e", "node_type": "1", "metadata": {}, "hash": "206576e86be279d94b39e2d39aba357ce65e2387272f7a1c98791bde0987521e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Securely copy the `/secure_directory/testfs.LustreServers.key` key file to the OSS.\n\n   ```\n   mds# scp /secure_directory/testfs.LustreServers.key oss:/secure_directory/\n   ```\n\n5. On the MDS and OSS, copy `/secure_directory/testfs.LustreServers.key` to`/secure_directory/testfs.LustreServers.client.key`.\n\n6. On each server modify the key file type of `/secure_directory/testfs.LustreServers.client.key` to be of type client. This operation also generates a prime number of *Prime length* to populate the *Prime (p)* attribute. Run:\n\n   ```\n   mds# lgss_sk -t client -m \\\n   /secure_directory/testfs.LustreServers.client.key\n   oss# lgss_sk -t client -m \\\n   /secure_directory/testfs.LustreServers.client.key\n   ```\n\n7. Load the `/secure_directory/testfs.LustreServers.key` and`/secure_directory/testfs.LustreServers.client.key` key files into the keyring on the MDS and OSS, enter:\n\n   ```\n   mds# lgss_sk -l /secure_directory/testfs.LustreServers.key\n   mds# lgss_sk -l /secure_directory/testfs.LustreServers.client.key\n   oss# lgss_sk -l /secure_directory/testfs.LustreServers.key\n   oss# lgss_sk -l /secure_directory/testfs.LustreServers.client.key\n   ```\n\n8. Set MDT to OST security flavor to SSK Integrity, `ski`:\n\n   ```\n   mds# lctl conf_param testfs.srpc.flavor.tcp.mdt2ost=ski\n   ```\n\n9. Verify that the `osc` and `osp` connections to the OST have a secure `ski` security context. See *the section called \u201cViewing Secure PtlRPC Contexts\u201d*.\n\n## Viewing Secure PtlRPC Contexts\n\nFrom the client (or servers which have mgc, osc, mdc contexts) you can view info regarding all users\u2019 contexts and the flavor in use for an import. For user\u2019s contexts (srpc_context), SSK and gssnull only support a single root UID so there should only be one context. The other file in the import (srpc_info) has additional sptlrpc details. The `rpc`and `bulk` flavors allow you to verify which security flavor is in use.\n\n```\nclient1# lctl get_param *.", "mimetype": "text/plain", "start_char_idx": 32103, "end_char_idx": 34043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e68caf36-73a2-4ceb-8f72-07334ca8152e": {"__data__": {"id_": "e68caf36-73a2-4ceb-8f72-07334ca8152e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fe9bbcf-6085-4523-a8c0-b662c60ba81d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5638a08aef2f545547f5449ea81f125e4e51cbb90d54b33e0b307b1edbd3739a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e67619a3-0fd3-416d-899f-58d0a2a848cf", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4bce95640446a7a300197b0da3f71e0c51981b1264518f084b857791247bf07e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Verify that the `osc` and `osp` connections to the OST have a secure `ski` security context. See *the section called \u201cViewing Secure PtlRPC Contexts\u201d*.\n\n## Viewing Secure PtlRPC Contexts\n\nFrom the client (or servers which have mgc, osc, mdc contexts) you can view info regarding all users\u2019 contexts and the flavor in use for an import. For user\u2019s contexts (srpc_context), SSK and gssnull only support a single root UID so there should only be one context. The other file in the import (srpc_info) has additional sptlrpc details. The `rpc`and `bulk` flavors allow you to verify which security flavor is in use.\n\n```\nclient1# lctl get_param *.*.srpc_*\nmdc.testfs-MDT0000-mdc-ffff8800da9f0800.srpc_contexts=\nffff8800da9600c0: uid 0, ref 2, expire 1478531769(+604695), fl uptodate,cached,, seq 7, win 2048, key 27a24430(ref 1), hdl 0xf2020f47cbffa93d:0xc23f4df4bcfb7be7, mech: sk\nmdc.testfs-MDT0000-mdc-ffff8800da9f0800.srpc_info=\nrpc flavor:     skpi\nbulk flavor:    skpi\nflags:          rootonly,udesc,\nid:             3\nrefcount:       3\nnctx:   1\ngc internal     3600\ngc next 3505\nmgc.MGC172.16.0.1@tcp.srpc_contexts=\nffff8800dbb09b40: uid 0, ref 2, expire 1478531769(+604695), fl uptodate,cached,, seq 18, win 2048, key 3e3f709f(ref 1), hdl 0xf2020f47cbffa93b:0xc23f4df4bcfb7be6, mech: sk\nmgc.MGC172.16.0.1@tcp.srpc_info=\nrpc flavor:     skpi\nbulk flavor:    skpi\nflags:          -,\nid:             2\nrefcount:       3\nnctx:   1\ngc internal     3600\ngc next 3505\nosc.testfs-OST0000-osc-ffff8800da9f0800.srpc_contexts=\nffff8800db9e5600: uid 0, ref 2, expire 1478531770(+604696), fl uptodate,cached,, seq 3, win 2048, key 3f7c1d70(ref 1), hdl 0xf93e61c64b6b415d:0xc23f4df4bcfb7bea, mech: sk\nosc.testfs-OST0000-osc-ffff8800da9f0800.srpc_info=\nrpc flavor:     skpi\nbulk flavor:    skpi\nflags:          rootonly,bulk,\nid:             6\nrefcount:       3\nnctx:   1\ngc internal     3600\ngc next 3505\n```", "mimetype": "text/plain", "start_char_idx": 33402, "end_char_idx": 35299, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "45af6caf-a2a2-43c7-b8ec-1d951d70e6b1": {"__data__": {"id_": "45af6caf-a2a2-43c7-b8ec-1d951d70e6b1", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3232d1da-0ba6-40fc-905c-67797e14dedd", "node_type": "1", "metadata": {}, "hash": "69d79dcefb3412b48e520b28741c4e0be0282a5bf851363bb992839dcceb1f55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Managing Security in a Lustre File System\n\n- [Managing Security in a Lustre File System](#managing-security-in-a-lustre-file-system)\n  * [Using ACLs](#using-acls)\n    + [How ACLs Work](#how-acls-work)\n    + [Using ACLs with the Lustre Software](#using-acls-with-the-lustre-software)\n    + [Examples](#examples)\n  * [Using Root Squash](#using-root-squash)\n    + [Configuring Root Squash](#configuring-root-squash)\n    + [Enabling and Tuning Root Squash](#enabling-and-tuning-root-squash)\n    + [Tips on Using Root Squash](#tips-on-using-root-squash)\n  * [Isolating Clients to a Sub-directory Tree](#isolating-clients-to-a-sub-directory-tree)\n    + [Identifying Clients](#identifying-clients)\n    + [Configuring Isolation](#configuring-isolation)\n    + [Making Isolation Permanent](#making-isolation-permanent)\n  * [Checking SELinux Policy Enforced by Lustre Clients](#checking-selinux-policy-enforced-by-lustre-clients)\n    + [Determining SELinux Policy Info](#determining-selinux-policy-info)\n    + [Enforcing SELinux Policy Check](#enforcing-selinux-policy-check)\n    + [Making SELinux Policy Check Permanent](#making-selinux-policy-check-permanent)\n    + [Sending SELinux Status Info from Clients](#sending-selinux-status-info-from-clients)\n\nThis chapter describes security features of the Lustre file system and includes the following sections:\n\n- [the section called \u201cUsing ACLs\u201d](#using-acls)\n- [the section called \u201cUsing Root Squash\u201d](#using-root-squash)\n- [the section called \u201c Isolating Clients to a Sub-directory Tree\u201d](#isolating-clients-to-a-sub-directory-tree)\n- [the section called \u201c Checking SELinux Policy Enforced by Lustre Clients\u201d](#checking-selinux-policy-enforced-by-lustre-clients)\n- [the section called \u201c Encrypting files and directories\u201d ](#Encrypting files and directories)\n\n## Using ACLs\n\nAn access control list (ACL), is a set of data that informs an operating system about permissions or access rights that each user or group has to specific system objects, such as directories or files. Each object has a unique security attribute that identifies users who have access to it. The ACL lists each object and user access privileges such as read, write or execute.\n\n### How ACLs Work\n\nImplementing ACLs varies between operating systems. Systems that support the Portable Operating System Interface (POSIX) family of standards share a simple yet powerful file system permission model, which should be well-known to the Linux/UNIX administrator. ACLs add finer-grained permissions to this model, allowing for more complicated permission schemes. For a detailed explanation of ACLs on a Linux operating system, refer to the SUSE Labs article [Posix Access Control Lists on Linux](https://wiki.lustre.org/images/5/57/PosixAccessControlInLinux.pdf).\n\nWe have implemented ACLs according to this model. The Lustre software works with the standard Linux ACL tools, setfacl, getfacl, and the historical chacl, normally installed with the ACL package.\n\n**Note**\n\nACL support is a system-range feature, meaning that all clients have ACL enabled or not. You cannot specify which clients should enable ACL.\n\n### Using ACLs with the Lustre Software\n\nPOSIX Access Control Lists (ACLs) can be used with the Lustre software. An ACL consists of file entries representing permissions based on standard POSIX file system object permissions that define three classes of user (owner, group and other). Each class is associated with a set of permissions [read (r), write (w) and execute (x)].\n\n- Owner class permissions define access privileges of the file owner.\n- Group class permissions define access privileges of the owning group.\n- Other class permissions define access privileges of all users not in the owner or group class.\n\nThe `ls -l` command displays the owner, group, and other class permissions in the first column of its output (for example, `-rw-r- --` for a regular file with read and write access for the owner class, read access for the group class, and no access for others).\n\nMinimal ACLs have three entries. Extended ACLs have more than the three entries. Extended ACLs also contain a mask entry and may contain any number of named user and named group entries.\n\nThe MDS needs to be configured to enable ACLs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4234, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3232d1da-0ba6-40fc-905c-67797e14dedd": {"__data__": {"id_": "3232d1da-0ba6-40fc-905c-67797e14dedd", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45af6caf-a2a2-43c7-b8ec-1d951d70e6b1", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "66fae22cb539a1fcaded02d0512edd445b1cecc1ae4b7b33eb358299641f585c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b17e48e7-1e41-4576-a65a-b2141bca4d26", "node_type": "1", "metadata": {}, "hash": "b5f7bb6afe7cb62b9ea4626cd557315517f181d58d92fea44be241cb086fd792", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each class is associated with a set of permissions [read (r), write (w) and execute (x)].\n\n- Owner class permissions define access privileges of the file owner.\n- Group class permissions define access privileges of the owning group.\n- Other class permissions define access privileges of all users not in the owner or group class.\n\nThe `ls -l` command displays the owner, group, and other class permissions in the first column of its output (for example, `-rw-r- --` for a regular file with read and write access for the owner class, read access for the group class, and no access for others).\n\nMinimal ACLs have three entries. Extended ACLs have more than the three entries. Extended ACLs also contain a mask entry and may contain any number of named user and named group entries.\n\nThe MDS needs to be configured to enable ACLs. Use `--mountfsoptions` to enable ACLs when creating your configuration:\n\n```\n$ mkfs.lustre --fsname spfs --mountfsoptions=acl --mdt -mgs /dev/sda\n```\n\nAlternately, you can enable ACLs at run time by using the `--acl` option with `mkfs.lustre`:\n\n```\n$ mount -t lustre -o acl /dev/sda /mnt/mdt\n```\n\nTo check ACLs on the MDS:\n\n```\n$ lctl get_param -n mdc.home-MDT0000-mdc-*.connect_flags | grep acl acl\n```\n\nTo mount the client with no ACLs:\n\n```\n$ mount -t lustre -o noacl ibmds2@o2ib:/home /home\n```\n\nACLs are enabled in a Lustre file system on a system-wide basis; either all clients enable ACLs or none do. Activating ACLs is controlled by MDS mount options `acl` / `noacl` (enable/disable ACLs). Client-side mount options acl/noacl are ignored. You do not need to change the client configuration, and the 'acl' string will not appear in the client /etc/mtab. The client acl mount option is no longer needed. If a client is mounted with that option, then this message appears in the MDS syslog:\n\n```\n...MDS requires ACL support but client does not\n```\n\nThe message is harmless but indicates a configuration issue, which should be corrected.\n\nIf ACLs are not enabled on the MDS, then any attempts to reference an ACL on a client return an Operation not supported error.\n\n### Examples\n\nThese examples are taken directly from the POSIX paper referenced above. ACLs on a Lustre file system work exactly like ACLs on any Linux file system. They are manipulated with the standard tools in the standard manner. Below, we create a directory and allow a specific user access.\n\n```\n[root@client lustre]# umask 027\n[root@client lustre]# mkdir rain\n[root@client lustre]# ls -ld rain\ndrwxr-x---  2 root root 4096 Feb 20 06:50 rain\n[root@client lustre]# getfacl rain\n# file: rain\n# owner: root\n# group: root\nuser::rwx\ngroup::r-x\nother::---\n \n[root@client lustre]# setfacl -m user:chirag:rwx rain\n[root@client lustre]# ls -ld rain\ndrwxrwx---+ 2 root root 4096 Feb 20 06:50 rain\n[root@client lustre]# getfacl --omit-header rain\nuser::rwx\nuser:chirag:rwx\ngroup::r-x\nmask::rwx\nother::---\n```\n\n## Using Root Squash\n\nRoot squash is a security feature which restricts super-user access rights to a Lustre file system. Without the root squash feature enabled, Lustre file system users on untrusted clients could access or modify files owned by root on the file system, including deleting them. Using the root squash feature restricts file access/modifications as the root user to only the specified clients. Note, however, that this does *not* prevent users on insecure clients from accessing files owned by *other* users.\n\nThe root squash feature works by re-mapping the user ID (UID) and group ID (GID) of the root user to a UID and GID specified by the system administrator, via the Lustre configuration management server (MGS). The root squash feature also enables the Lustre file system administrator to specify a set of client for which UID/GID re-mapping does not apply.", "mimetype": "text/plain", "start_char_idx": 3406, "end_char_idx": 7191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b17e48e7-1e41-4576-a65a-b2141bca4d26": {"__data__": {"id_": "b17e48e7-1e41-4576-a65a-b2141bca4d26", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3232d1da-0ba6-40fc-905c-67797e14dedd", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "07e28ce0e8097476437807aeade1ebfc4544e0f689f6588994544fae654647d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b0b8101-feaf-4b95-8b18-3bfbcb7c7576", "node_type": "1", "metadata": {}, "hash": "a1a32fd6c7ac4b76bf9956e7729b671344a4101097708dec570fce9a650a9d67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Without the root squash feature enabled, Lustre file system users on untrusted clients could access or modify files owned by root on the file system, including deleting them. Using the root squash feature restricts file access/modifications as the root user to only the specified clients. Note, however, that this does *not* prevent users on insecure clients from accessing files owned by *other* users.\n\nThe root squash feature works by re-mapping the user ID (UID) and group ID (GID) of the root user to a UID and GID specified by the system administrator, via the Lustre configuration management server (MGS). The root squash feature also enables the Lustre file system administrator to specify a set of client for which UID/GID re-mapping does not apply.\n\n**Note**\n\nNodemaps (*Mapping UIDs and GIDs with Nodemap*) are an alternative to root squash, since it also allows root squash on a per-client basis. With UID maps, the clients can even have a local root UID without actually having root access to the filesystem itself.\n\n### Configuring Root Squash\n\nRoot squash functionality is managed by two configuration parameters, `root_squash` and `nosquash_nids`.\n\n- The `root_squash` parameter specifies the UID and GID with which the root user accesses the Lustre file system.\n- The `nosquash_nids` parameter specifies the set of clients to which root squash does not apply. LNet NID range syntax is used for this parameter (see the NID range syntax rules described in *the section called \u201cUsing Root Squash\u201d*). For example:\n\n```\nnosquash_nids=172.16.245.[0-255/2]@tcp\n```\n\nIn this example, root squash does not apply to TCP clients on subnet 172.16.245.0 that have an even number as the last component of their IP address.\n\n### Enabling and Tuning Root Squash\n\nThe default value for `nosquash_nids` is NULL, which means that root squashing applies to all clients. Setting the root squash UID and GID to 0 turns root squash off.\n\nRoot squash parameters can be set when the MDT is created (`mkfs.lustre --mdt`). For example:\n\n```\nmds# mkfs.lustre --reformat --fsname=testfs --mdt --mgs \\\n       --param \"mdt.root_squash=500:501\" \\\n       --param \"mdt.nosquash_nids='0@elan1 192.168.1.[10,11]'\" /dev/sda1\n```\n\nRoot squash parameters can also be changed on an unmounted device with `tunefs.lustre`. For example:\n\n```\ntunefs.lustre --param \"mdt.root_squash=65534:65534\"  \\\n--param \"mdt.nosquash_nids=192.168.0.13@tcp0\" /dev/sda1\n```\n\nRoot squash parameters can also be changed with the `lctl conf_param` command. For example:\n\n```\nmgs# lctl conf_param testfs.mdt.root_squash=\"1000:101\"\nmgs# lctl conf_param testfs.mdt.nosquash_nids=\"*@tcp\"\n```\n\nTo retrieve the current root squash parameter settings, the following `lctl get_param` commands can be used:\n\n```\nmgs# lctl get_param mdt.*.root_squash\nmgs# lctl get_param mdt.*.nosquash_nids\n```\n\n**Note**\n\nWhen using the lctl conf_param command, keep in mind:\n\n- `lctl conf_param` must be run on a live MGS\n- `lctl conf_param` causes the parameter to change on all MDSs\n- `lctl conf_param` is to be used once per a parameter\n\nThe root squash settings can also be changed temporarily with `lctl set_param` or persistently with `lctl set_param -P`.", "mimetype": "text/plain", "start_char_idx": 6433, "end_char_idx": 9623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b0b8101-feaf-4b95-8b18-3bfbcb7c7576": {"__data__": {"id_": "1b0b8101-feaf-4b95-8b18-3bfbcb7c7576", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b17e48e7-1e41-4576-a65a-b2141bca4d26", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c15fa60db69d0c86ec6d4eef51c9d53696898ed096a12fdd8d33d43885562363", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "260f57d6-fdf5-4982-aeae-0088bc3a6fe9", "node_type": "1", "metadata": {}, "hash": "2e80a277149434e6f6e105ecfcba9039a3a402c369486f88a46925487bd874f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*.root_squash\nmgs# lctl get_param mdt.*.nosquash_nids\n```\n\n**Note**\n\nWhen using the lctl conf_param command, keep in mind:\n\n- `lctl conf_param` must be run on a live MGS\n- `lctl conf_param` causes the parameter to change on all MDSs\n- `lctl conf_param` is to be used once per a parameter\n\nThe root squash settings can also be changed temporarily with `lctl set_param` or persistently with `lctl set_param -P`. For example:\n\n```\nmgs# lctl set_param mdt.testfs-MDT0000.root_squash=\"1:0\"\nmgs# lctl set_param -P mdt.testfs-MDT0000.root_squash=\"1:0\"\n```\n\nThe `nosquash_nids` list can be cleared with:\n\n```\nmgs# lctl conf_param testfs.mdt.nosquash_nids=\"NONE\"\n```\n\n\\- OR -\n\n```\nmgs# lctl conf_param testfs.mdt.nosquash_nids=\"clear\"\n```\n\nIf the `nosquash_nids` value consists of several NID ranges (e.g. `0@elan`, `1@elan1`), the list of NID ranges must be quoted with single (') or double ('') quotation marks. List elements must be separated with a space. For example:\n\n```\nmds# mkfs.lustre ... --param \"mdt.nosquash_nids='0@elan1 1@elan2'\" /dev/sda1\nlctl conf_param testfs.mdt.nosquash_nids=\"24@elan 15@elan1\"\n```\n\nThese are examples of incorrect syntax:\n\n```\nmds# mkfs.lustre ... --param \"mdt.nosquash_nids=0@elan1 1@elan2\" /dev/sda1\nlctl conf_param testfs.mdt.nosquash_nids=24@elan 15@elan1\n```\n\nTo check root squash parameters, use the lctl get_param command:\n\n```\nmds# lctl get_param mdt.testfs-MDT0000.root_squash\nlctl get_param mdt.*.nosquash_nids\n```\n\n**Note**\n\nAn empty nosquash_nids list is reported as NONE.\n\n### Tips on Using Root Squash\n\nLustre configuration management limits root squash in several ways.\n\n- The `lctl conf_param` value overwrites the parameter's previous value. If the new value uses an incorrect syntax, then the system continues with the old parameters and the previously-correct value is lost on remount. That is, be careful doing root squash tuning.\n- `mkfs.lustre` and `tunefs.lustre` do not perform parameter syntax checking. If the root squash parameters are incorrect, they are ignored on mount and the default values are used instead.\n- Root squash parameters are parsed with rigorous syntax checking. The root_squash parameter should be specified as `<decnum>:<decnum>`. The `nosquash_nids` parameter should follow LNet NID range list syntax.\n\nLNet NID range syntax:\n\n```\n<nidlist>     :== <nidrange> [ ' ' <nidrange> ]\n<nidrange>   :== <addrrange> '@' <net>\n<addrrange>  :== '*' |\n           <ipaddr_range> |\n           <numaddr_range>\n<ipaddr_range>       :==\n<numaddr_range>.<numaddr_range>.<numaddr_range>.<numaddr_range>\n<numaddr_range>      :== <number> |\n                   <expr_list>\n<expr_list>  :== '[' <range_expr> [ ',' <range_expr>] ']'\n<range_expr> :== <number> |\n           <number> '-' <number> |\n           <number> '-' <number> '/' <number>\n<net>        :== <netname> | <netname><number>\n<netname>    :== \"lo\" | \"tcp\" | \"o2ib\"\n           | \"ra\" | \"elan\"\n<number>     :== <nonnegative decimal> | <hexadecimal>\n```\n\n**Note**\n\nFor networks using numeric addresses (e.g. elan), the address range must be specified in the`<numaddr_range>` syntax. For networks using IP addresses, the address range must be in the`<ipaddr_range>`. For example, if elan is using numeric addresses, `1.2.3.4@elan` is incorrect.", "mimetype": "text/plain", "start_char_idx": 9214, "end_char_idx": 12471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "260f57d6-fdf5-4982-aeae-0088bc3a6fe9": {"__data__": {"id_": "260f57d6-fdf5-4982-aeae-0088bc3a6fe9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b0b8101-feaf-4b95-8b18-3bfbcb7c7576", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d7db1a30206d4387162809637ab913e42b5db07139a7b95fbfd547728ee39aa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e05ad97-7c80-4f0e-9930-928a5a937f64", "node_type": "1", "metadata": {}, "hash": "4cc903c467f1ffced51c1a81c7734bda1b911da7201fda7755b9a1dcf05007dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "elan), the address range must be specified in the`<numaddr_range>` syntax. For networks using IP addresses, the address range must be in the`<ipaddr_range>`. For example, if elan is using numeric addresses, `1.2.3.4@elan` is incorrect.\n\n## Isolating Clients to a Sub-directory Tree\n\nIsolation is the Lustre implementation of the generic concept of multi-tenancy, which aims at providing separated namespaces from a single filesystem. Lustre Isolation enables different populations of users on the same file system beyond normal Unix permissions/ACLs, even when users on the clients may have root access. Those tenants share the same file system, but they are isolated from each other: they cannot access or even see each other\u2019s files, and are not aware that they are sharing common file system resources.\n\nLustre Isolation leverages the Fileset feature (*the section called \u201cFileset Feature\u201d*) to mount only a subdirectory of the filesystem rather than the root directory. In order to achieve isolation, the subdirectory mount, which presents to tenants only their own fileset, has to be imposed to the clients. To that extent, we make use of the nodemap feature (*Mapping UIDs and GIDs with Nodemap*). We group all clients used by a tenant under a common nodemap entry, and we assign to this nodemap entry the fileset to which the tenant is restricted.\n\n### Identifying Clients\n\nEnforcing multi-tenancy on Lustre relies on the ability to properly identify the client nodes used by a tenant, and trust those identities. This can be achieved by having physical hardware and/or network security, so that client nodes have well-known NIDs. It is also possible to make use of strong authentication with Kerberos or Shared-Secret Key (see *Configuring Shared-Secret Key (SSK) Security*). Kerberos prevents NID spoofing, as every client needs its own credentials, based on its NID, in order to connect to the servers. Shared-Secret Key also prevents tenant impersonation, because keys can be linked to a specific nodemap. See *the section called \u201cRole of Nodemap in SSK\u201d* for detailed explanations.\n\n### Configuring Isolation\n\nIsolation on Lustre can be achieved by setting the `fileset` parameter on a nodemap entry. All clients belonging to this nodemap entry will automatically mount this fileset instead of the root directory. For example:\n\n```\nmgs# lctl nodemap_set_fileset --name tenant1 --fileset '/dir1'\n```\n\nSo all clients matching the `tenant1` nodemap will be automatically presented the fileset `/dir1` when mounting. This means these clients are doing an implicit subdirectory mount on the subdirectory `/dir1`.\n\n**Note**\n\nIf subdirectory defined as fileset does not exist on the file system, it will prevent any client belonging to the nodemap from mounting Lustre.\n\nTo delete the fileset parameter, just set it to an empty string:\n\n```\nmgs# lctl nodemap_set_fileset --name tenant1 --fileset ''\n```\n### Making Isolation Permanent\n\nIn order to make isolation permanent, the fileset parameter on the nodemap has to be set with `lctl set_param`with the `-P` option.\n\n```\nmgs# lctl set_param nodemap.tenant1.fileset=/dir1\nmgs# lctl set_param -P nodemap.tenant1.fileset=/dir1\n```\n\nThis way the fileset parameter will be stored in the Lustre config logs, letting the servers retrieve the information after a restart.\n\nIntroduced in Lustre 2.13\n\n## Encrypting files and directoriesChecking SELinux Policy Enforced by Lustre Clients\n\nSELinux provides a mechanism in Linux for supporting Mandatory Access Control (MAC) policies. When a MAC policy is enforced, the operating system\u2019s (OS) kernel defines application rights, firewalling applications from compromising the entire system. Regular users do not have the ability to override the policy.\n\nOne purpose of SELinux is to protect the **OS** from privilege escalation. To that extent, SELinux defines confined and unconfined domains for processes and users. Each process, user, file is assigned a security context, and rules define the allowed operations by processes and users on files.\n\nAnother purpose of SELinux can be to protect **data** sensitivity, thanks to Multi-Level Security (MLS). MLS works on top of SELinux, by defining the concept of security levels in addition to domains.", "mimetype": "text/plain", "start_char_idx": 12236, "end_char_idx": 16479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e05ad97-7c80-4f0e-9930-928a5a937f64": {"__data__": {"id_": "7e05ad97-7c80-4f0e-9930-928a5a937f64", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "260f57d6-fdf5-4982-aeae-0088bc3a6fe9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cfaf7498726bc06903627af3373fc8231cdbd68340dde5496b35ca3a3f8a65b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81e0d73d-72fd-4060-a924-6b0f880dcec1", "node_type": "1", "metadata": {}, "hash": "3b813dacda000e69813a15bae1b9f9e42b1493efdf4c29e8d8a633fbe7523536", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in Lustre 2.13\n\n## Encrypting files and directoriesChecking SELinux Policy Enforced by Lustre Clients\n\nSELinux provides a mechanism in Linux for supporting Mandatory Access Control (MAC) policies. When a MAC policy is enforced, the operating system\u2019s (OS) kernel defines application rights, firewalling applications from compromising the entire system. Regular users do not have the ability to override the policy.\n\nOne purpose of SELinux is to protect the **OS** from privilege escalation. To that extent, SELinux defines confined and unconfined domains for processes and users. Each process, user, file is assigned a security context, and rules define the allowed operations by processes and users on files.\n\nAnother purpose of SELinux can be to protect **data** sensitivity, thanks to Multi-Level Security (MLS). MLS works on top of SELinux, by defining the concept of security levels in addition to domains. Each process, user and file is assigned a security level, and the model states that processes and users can read the same or lower security level, but can only write to their own or higher security level.\n\nFrom a file system perspective, the security context of files must be stored permanently. Lustre makes use of the`security.selinux` extended attributes on files to hold this information. Lustre supports SELinux on the client side. All you have to do to have MAC and MLS on Lustre is to enforce the appropriate SELinux policy (as provided by the Linux distribution) on all Lustre clients. No SELinux is required on Lustre servers.\n\nBecause Lustre is a distributed file system, the specificity when using MLS is that Lustre really needs to make sure data is always accessed by nodes with the SELinux MLS policy properly enforced. Otherwise, data is not protected. This means Lustre has to check that SELinux is properly enforced on client side, with the right, unaltered policy. And if SELinux is not enforced as expected on a client, the server denies its access to Lustre.\n\n### Determining SELinux Policy Info\n\nA string that represents the SELinux Status info will be used by servers as a reference, to check if clients are enforcing SELinux properly. This reference string can be obtained on a client node known to enforce the right SELinux policy, by calling the `l_getsepol` command line utility:\n\n```\nclient# l_getsepol\nSELinux status info: 1:mls:31:40afb76d077c441b69af58cccaaa2ca63641ed6e21b0a887dc21a684f508b78f\n```\n\nThe string describing the SELinux policy has the following syntax:\n\n`mode:name:version:hash`\n\nwhere:\n\n- `mode` is a digit telling if SELinux is in Permissive mode (0) or Enforcing mode (1)\n- `name` is the name of the SELinux policy\n- `version` is the version of the SELinux policy\n- `hash` is the computed hash of the binary representation of the policy, as exported in /etc/selinux/`name`/policy/policy. `version`\n\n### Enforcing SELinux Policy Check\n\nSELinux policy check can be enforced by setting the `sepol` parameter on a nodemap entry. All clients belonging to this nodemap entry must enforce the SELinux policy described by this parameter, otherwise they are denied access to the Lustre file system. For example:\n\n```\nmgs# lctl nodemap_set_sepol --name restricted\n     --sepol '1:mls:31:40afb76d077c441b69af58cccaaa2ca63641ed6e21b0a887dc21a684f508b78f'\n```\n\nSo all clients matching the `restricted` nodemap must enforce the SELinux policy which description matches`1:mls:31:40afb76d077c441b69af58cccaaa2ca63641ed6e21b0a887dc21a684f508b78f`. If not, they will get Permission Denied when trying to mount or access files on the Lustre file system.\n\nTo delete the `sepol` parameter, just set it to an empty string:\n\n```\nmgs# lctl nodemap_set_sepol --name restricted --sepol ''\n```\n\nSee *Mapping UIDs and GIDs with Nodemap* for more details about the Nodemap feature.\n\n### Making SELinux Policy Check Permanent\n\nIn order to make SELinux Policy check permanent, the sepol parameter on the nodemap has to be set with `lctl set_param` with the `-P` option.", "mimetype": "text/plain", "start_char_idx": 15557, "end_char_idx": 19565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81e0d73d-72fd-4060-a924-6b0f880dcec1": {"__data__": {"id_": "81e0d73d-72fd-4060-a924-6b0f880dcec1", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e05ad97-7c80-4f0e-9930-928a5a937f64", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a916da0db983334c2fe764b6a7ee3a772fa98819e9c70b51179283399a272d51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07494a65-fa94-43c6-bb5d-505af52af2d4", "node_type": "1", "metadata": {}, "hash": "0a92c7c4f19280d36f870b92dbf04aac0f97052109d0abdaf268a6042e204697", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If not, they will get Permission Denied when trying to mount or access files on the Lustre file system.\n\nTo delete the `sepol` parameter, just set it to an empty string:\n\n```\nmgs# lctl nodemap_set_sepol --name restricted --sepol ''\n```\n\nSee *Mapping UIDs and GIDs with Nodemap* for more details about the Nodemap feature.\n\n### Making SELinux Policy Check Permanent\n\nIn order to make SELinux Policy check permanent, the sepol parameter on the nodemap has to be set with `lctl set_param` with the `-P` option.\n\n```\nmgs# lctl set_param nodemap.restricted.sepol=1:mls:31:40afb76d077c441b69af58cccaaa2ca63641ed6e21b0a887dc21a684f508b78f\nmgs# lctl set_param -P nodemap.restricted.sepol=1:mls:31:40afb76d077c441b69af58cccaaa2ca63641ed6e21b0a887dc21a684f508b78f\n```\n\nThis way the sepol parameter will be stored in the Lustre config logs, letting the servers retrieve the information after a restart.\n\n### Sending SELinux Status Info from Clients\n\nIn order for Lustre clients to send their SELinux status information, in\tcase SELinux is enabled locally, the`send_sepol` ptlrpc kernel module's parameter has to be set to a non-zero value. `send_sepol` accepts various values:\n\n- 0: do not send SELinux policy info;\n- -1: fetch SELinux policy info for every request;\n- N > 0: only fetch SELinux policy info every N seconds. Use `N = 2^31-1` to have SELinux policy info fetched only at mount time.\n\nClients that are part of a nodemap on which `sepol` is defined must send SELinux status info. And the SELinux policy they enforce must match the representation stored into the nodemap. Otherwise they will be denied access to the Lustre file system.\n\n ## Encrypting files and directories\n\nThe purpose that client-side encryption wants to serve is to be able to provide a special directory for each user, to safely store sensitive files. The goals are to protect data in transit between clients and servers, and protect data at rest.\n\nThis feature is implemented directly at the Lustre client level. Lustre client-side encryption relies on kernel `fscrypt. fscrypt` is a library which filesystems can hook into to support transparent encryption of files and directories. As a consequence, the key points described below are extracted from `fscrypt` documentation.\n\nFor full details, please refer to documentation available with the Lustre sources, under the `Documentation/client_side_encryption` directory.\n\n**Note**\n\nThe client-side encryption feature is available natively on Lustre clients running a Linux distribution with at least kernel 5.4. It is also available thanks to an additional kernel library provided by Lustre, on clients that run a Linux distribution with basic support for encryption, including:\n\n* CentOS/RHEL 8.1 and later;\n* Ubuntu 18.04 and later;\n* SLES 15 SP2 and later.\n\n### Client-side encryption access semantics\n\nOnly Lustre clients need access to encryption master keys. Keys are added to the filesystem-level encryption keyring on the Lustre client.\n\n* **With the key**\n\n  With the encryption key, encrypted regular files, directories, and symlinks behave very similarly to their unencrypted counterparts --- after all, the encryption is intended to be transparent. However, astute users may notice some differences in behavior:\n\n  * Unencrypted files, or files encrypted with a different encryption policy (i.e. different key, modes, or flags), cannot be renamed or linked into an encrypted directory. However, encrypted files can be renamed within an encrypted directory, or into an unencrypted directory.\n\n    **Note**\n\n    \"moving\" an unencrypted file into an encrypted directory, e.g. with the mv program, is implemented in userspace by a copy followed by a delete. Be aware the original unencrypted data may remain recoverable from free space on the disk; it is best to keep all files encrypted from the very beginning.\n\n  * On Lustre, Direct I/O is supported for encrypted files.\n\n  * The fallocate() operations FALLOC_FL_COLLAPSE_RANGE,\n    FALLOC_FL_INSERT_RANGE, and FALLOC_FL_ZERO_RANGE are not supported on encrypted\n    files and will fail with EOPNOTSUPP.\n\n  * DAX (Direct Access) is not supported on encrypted files.", "mimetype": "text/plain", "start_char_idx": 19058, "end_char_idx": 23206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07494a65-fa94-43c6-bb5d-505af52af2d4": {"__data__": {"id_": "07494a65-fa94-43c6-bb5d-505af52af2d4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81e0d73d-72fd-4060-a924-6b0f880dcec1", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7400f99657d984d76ab574c442f134eaa26807831981faa066253be18f70bfb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79e428fe-eceb-4dcc-8653-142fadd62c84", "node_type": "1", "metadata": {}, "hash": "ed1d4104f2eeedb92d22de129a968e8ee199f60036316c92d30557100d16c52a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "different key, modes, or flags), cannot be renamed or linked into an encrypted directory. However, encrypted files can be renamed within an encrypted directory, or into an unencrypted directory.\n\n    **Note**\n\n    \"moving\" an unencrypted file into an encrypted directory, e.g. with the mv program, is implemented in userspace by a copy followed by a delete. Be aware the original unencrypted data may remain recoverable from free space on the disk; it is best to keep all files encrypted from the very beginning.\n\n  * On Lustre, Direct I/O is supported for encrypted files.\n\n  * The fallocate() operations FALLOC_FL_COLLAPSE_RANGE,\n    FALLOC_FL_INSERT_RANGE, and FALLOC_FL_ZERO_RANGE are not supported on encrypted\n    files and will fail with EOPNOTSUPP.\n\n  * DAX (Direct Access) is not supported on encrypted files.\n\n  * mmap is supported. This is possible because the pagecache for an encrypted file contains the plaintext, not the ciphertext.\n\n* **Without the key**\n\n  Some filesystem operations may be performed on encrypted regular files, directories, and symlinks even before their encryption key has been added, or after their encryption key has been removed: \n  \n  \u2022 File metadata may be read, e.g. using stat(). \n  \n  \u2022 Directories may be listed, and the whole namespace tree may be walked through. \n  \n  \u2022 Files may be deleted. That is, nondirectory files may be deleted with unlink() as usual, and empty directories may be deleted with rmdir() as usual. Therefore, rm and rm -r will work as expected. \n  \n  \u2022 Symlink targets may be read and followed, but they will be presented in encrypted form, similar to filenames in directories. Hence, they are unlikely to point to anywhere useful. \n  \n  Without the key, regular files cannot be opened or truncated. Attempts to do so will fail with ENOKEY. This implies that any regular file operations that require a file descriptor, such as read(), write(), mmap(), fallocate(), and ioctl(), are also forbidden. \n  \n  Also without the key, files of any type (including directories) cannot be created or linked into an encrypted directory, nor can a name in an encrypted directory be the source or target of a rename, nor can an O_TMPFILE temporary file be created in an encrypted directory. All such operations will fail with ENOKEY. \n  \n  It is not currently possible to backup and restore encrypted files without the encryption key. This would require special APIs which have not yet been implemented\n  \n* **Encryption policy enforcement**\n\n  After an encryption policy has been set on a directory, all regular files, directories, and symbolic links created in that directory (recursively) will inherit that encryption policy. Special files --- that is, named pipes, device nodes, and UNIX domain sockets --- will not be encrypted. \n  \n  Except for those special files, it is forbidden to have unencrypted files, or files encrypted with a different encryption policy, in an encrypted directory tree.\n\n### Client-side encryption key hierarchy\n\nEach encrypted directory tree is protected by a master key. \n\nTo \"unlock\" an encrypted directory tree, userspace must provide the appropriate master key. There can be any number of master keys, each of which protects any number of directory trees on any number of filesystems.\n\n### Client-side encryption modes and usage\n\n`fscrypt` allows one encryption mode to be specified for file contents and one encryption mode to be specified for filenames. Different directory trees are permitted to use different encryption modes. Currently, the following pairs of encryption modes are supported:\n\n\u2022 AES-256-XTS for contents and AES-256-CTS-CBC for filenames \n\n\u2022 AES-128-CBC for contents and AES-128-CTS-CBC for filenames \n\nIf unsure, you should use the (AES-256-XTS, AES-256-CTS-CBC) pair.\n\n**Warning** \n\nIn Lustre 2.14, client-side encryption only supports content encryption, and not filename encryption. As a consequence, only content encryption mode will be taken into account, and filename encryption mode will be ignored to leave filenames in clear text.\n\n### Client-side encryption threat model\n\n* **Offline attacks**\n\n  For the Lustre case, block devices are Lustre targets attached to the Lustre servers. Manipulating the filesystem offline means accessing the filesystem on these targets while Lustre is offline. \n\n  Provided that a strong encryption key is chosen, fscrypt protects the confidentiality of file contents in the event of a single point-in-time permanent offline compromise of the block device content. Lustre client-side encryption does not protect the confidentiality of metadata, e.g.", "mimetype": "text/plain", "start_char_idx": 22388, "end_char_idx": 26992, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79e428fe-eceb-4dcc-8653-142fadd62c84": {"__data__": {"id_": "79e428fe-eceb-4dcc-8653-142fadd62c84", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07494a65-fa94-43c6-bb5d-505af52af2d4", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb0e7a5ef4ff382f875779915dfef328ff3e81efa00c040cf8a491236dacf9b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07914103-c94a-439a-95d2-f2f9081034d8", "node_type": "1", "metadata": {}, "hash": "2473d0967db1ce03ae6a6a8bd1514b93a50723d035e7a2afb8335c1726cbfe7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Warning** \n\nIn Lustre 2.14, client-side encryption only supports content encryption, and not filename encryption. As a consequence, only content encryption mode will be taken into account, and filename encryption mode will be ignored to leave filenames in clear text.\n\n### Client-side encryption threat model\n\n* **Offline attacks**\n\n  For the Lustre case, block devices are Lustre targets attached to the Lustre servers. Manipulating the filesystem offline means accessing the filesystem on these targets while Lustre is offline. \n\n  Provided that a strong encryption key is chosen, fscrypt protects the confidentiality of file contents in the event of a single point-in-time permanent offline compromise of the block device content. Lustre client-side encryption does not protect the confidentiality of metadata, e.g. file names, file sizes, file permissions, file timestamps, and extended attributes. Also, the existence and location of holes (unallocated blocks which logically contain all zeroes) in files is not protected.\n\n* **Online attacks**\n\n  * On Lustre client\n\n    After an encryption key has been added, fscrypt does not hide the plaintext file contents or filenames from other users on the same node. Instead, existing access control mechanisms such as file mode bits, POSIX ACLs, LSMs, or namespaces should be used for this purpose. \n\n    For the Lustre case, it means plaintext file contents or filenames are not hidden from other users on the same Lustre client. \n\n    An attacker who compromises the system enough to read from arbitrary memory, e.g. by exploiting a kernel security vulnerability, can compromise all encryption keys that are currently in use. However, fscrypt allows encryption keys to be removed from the kernel, which may protect them from later compromise. Key removal can be carried out by non-root users. In more detail, the key removal will wipe the master encryption key from kernel memory. Moreover, it will try to evict all cached inodes which had been \"unlocked\" using the key, thereby wiping their per-file keys and making them once again appear \"locked\", i.e. in ciphertext or encrypted form.\n\n  * On Lustre server\n\n    An attacker on a Lustre server who compromises the system enough to read arbitrary memory, e.g. by exploiting a kernel security vulnerability, cannot compromise Lustre files content. Indeed, encryption keys are not forwarded to the Lustre servers, and servers do not carry out decryption or encryption. Moreover, bulk RPCs received by servers contain encrypted data, which is written as-is to the underlying filesystem.\n\n###  Manage encryption on directories\n\nBy default, Lustre client-side encryption is enabled, letting users define encryption policies on a perdirectory basis.\n\n**Note** \n\nAdministrators can decide to prevent a Lustre client mount-point from using encryption by specifying the `noencrypt` client mount option. This can be also enforced from server side thanks to the `forbid_encryption` property on nodemaps. See Section \u201cAltering Properties\u201d for how to manage nodemaps.\n\n`fscrypt` userspace tool can be used to manage encryption policies. See https://github.com/google/fscrypt for comprehensive explanations. Below are examples on how to use this tool with Lustre. If not told otherwise, commands must be run on Lustre client side.\n\n* Two preliminary steps are required before actually deciding which directories to encrypt, and this is the only functionality which requires root privileges. Administrator has to run:\n\n  ```\n  # fscrypt setup\n  Customizing passphrase hashing difficulty for this system...\n  Created global config file at \"/etc/fscrypt.conf\".\n  Metadata directories created at \"/.fscrypt\".\n  ```\n\n  This first command has to be run on all clients that want to use encryption, as it sets up global fscrypt parameters outside of Lustre.\n\n  ```\n  # fscrypt setup /mnt/lustre\n  Metadata directories created at \"/mnt/lustre/.fscrypt\"\n  ```\n\n  This second command has to be run on just one Lustre client.\n\n  **Note**\n\n  The file `/etc/fscrypt.conf` can be edited. It is strongly recommended to set policy_version to 2, so that fscrypt wipes files from memory when the encryption key is removed.", "mimetype": "text/plain", "start_char_idx": 26172, "end_char_idx": 30365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07914103-c94a-439a-95d2-f2f9081034d8": {"__data__": {"id_": "07914103-c94a-439a-95d2-f2f9081034d8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79e428fe-eceb-4dcc-8653-142fadd62c84", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4631fbd683aac06449fca154b79502231d1577d6ecbad84ad3946a1597028e53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "325566b4-d0f8-4a1a-a101-523b573da843", "node_type": "1", "metadata": {}, "hash": "95adad64f110eec0c1f804ca36a6589730e33058e460889615aecf07b971ef17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Administrator has to run:\n\n  ```\n  # fscrypt setup\n  Customizing passphrase hashing difficulty for this system...\n  Created global config file at \"/etc/fscrypt.conf\".\n  Metadata directories created at \"/.fscrypt\".\n  ```\n\n  This first command has to be run on all clients that want to use encryption, as it sets up global fscrypt parameters outside of Lustre.\n\n  ```\n  # fscrypt setup /mnt/lustre\n  Metadata directories created at \"/mnt/lustre/.fscrypt\"\n  ```\n\n  This second command has to be run on just one Lustre client.\n\n  **Note**\n\n  The file `/etc/fscrypt.conf` can be edited. It is strongly recommended to set policy_version to 2, so that fscrypt wipes files from memory when the encryption key is removed.\n\n* Now a regular user is able to select a directory to encrypt:\n\n  ```\n  $ fscrypt encrypt /mnt/lustre/vault\n  The following protector sources are available:\n  1 - Your login passphrase (pam_passphrase)\n  2 - A custom passphrase (custom_passphrase)\n  3 - A raw 256-bit key (raw_key)\n  Enter the source number for the new protector [2 - custom_passphrase]: 2\n  Enter a name for the new protector: shield\n  Enter custom passphrase for protector \"shield\":\n  Confirm passphrase:\n  \"/mnt/lustre/vault\" is now encrypted, unlocked, and ready for use.\n  ```\n\n  Starting from here, all files and directories created under /mnt/lustre/vault will be encrypted, according to the policy defined at the previsous step.\n\n  **Note** \n\n  The encryption policy is inherited by all subdirectories. It is not possible to change the policy for a subdirectory.\n\n* Another user can decide to encrypt a different directory with its own protector:\n\n  ```\n  $ fscrypt encrypt /mnt/lustre/private\n  Should we create a new protector? [y/N] Y\n  The following protector sources are available:\n  1 - Your login passphrase (pam_passphrase)\n  2 - A custom passphrase (custom_passphrase)\n  3 - A raw 256-bit key (raw_key)\n  Enter the source number for the new protector [2 - custom_passphrase]: 2\n  Enter a name for the new protector: armor\n  Enter custom passphrase for protector \"armor\":\n  Confirm passphrase:\n  \"/mnt/lustre/private\" is now encrypted, unlocked, and ready for use.\n  ```\n\n* Users can decide to lock an encrypted directory at any time:\n\n  ```\n  $ fscrypt lock /mnt/lustre/vault\n  \"/mnt/lustre/vault\" is now locked.\n  ```\n\n* Users regain access to the encrypted directory with the command:\n\n  ```\n  $ fscrypt unlock /mnt/lustre/vault\n  Enter custom passphrase for protector \"shield\":\n  \"/mnt/lustre/vault\" is now unlocked and ready for use.\n  ```\n\n* Actually, fscrypt does not give direct access to master keys, but to protectors that are used to encrypt them. This mechanism gives the ability to change a passphrase:\n\n  ```\n  $ fscrypt status /mnt/lustre\n  lustre filesystem \"/mnt/lustre\" has 2 protectors and 2 policies\n  PROTECTOR LINKED DESCRIPTION\n  deacab807bf0e788 No custom protector \"shield\"\n  e691ae7a1990fc2a No custom protector \"armor\"\n  POLICY UNLOCKED PROTECTORS\n  52b2b5aff0e59d8e0d58f962e715862e No deacab807bf0e788\n  374e8944e4294b527e50363d86fc9411 No e691ae7a1990fc2a\n  $ fscrypt metadata change-passphrase --protector=/mnt/lustre:deacab807bf0e788\n  Enter old custom passphrase for protector \"shield\":\n  Enter new custom passphrase for protector \"shield\":\n  Confirm passphrase:\n  Passphrase for protector deacab807bf0e788 successfully changed.\n  ```\n\n  It makes also possible to have multiple protectors for the same policy. This is really useful when several users share an encrypted directory, because it avoids the need to share any secret between them.\n\n  ```\n  $ fscrypt status /mnt/lustre/vault\n  \"/mnt/lustre/vault\" is encrypted with fscrypt.", "mimetype": "text/plain", "start_char_idx": 29653, "end_char_idx": 33316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "325566b4-d0f8-4a1a-a101-523b573da843": {"__data__": {"id_": "325566b4-d0f8-4a1a-a101-523b573da843", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07914103-c94a-439a-95d2-f2f9081034d8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "043744294bf1f681992d896b92884f1e050cbdeb6d271323d1c930f8e48167c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2497dfb-87e4-469e-b917-5938bf600c1e", "node_type": "1", "metadata": {}, "hash": "2ff47fe2808d610a51b52c34ee2ce039ab8010310e4f704c08024c0fd139cb4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\n\n  It makes also possible to have multiple protectors for the same policy. This is really useful when several users share an encrypted directory, because it avoids the need to share any secret between them.\n\n  ```\n  $ fscrypt status /mnt/lustre/vault\n  \"/mnt/lustre/vault\" is encrypted with fscrypt.\n  Policy: 52b2b5aff0e59d8e0d58f962e715862e\n  Options: padding:32 contents:AES_256_XTS filenames:AES_256_CTS policy_version:2\n  Unlocked: No\n  Protected with 1 protector:\n  PROTECTOR LINKED DESCRIPTION\n  deacab807bf0e788 No custom protector \"shield\"\n  $ fscrypt metadata create protector /mnt/lustre\n  Create new protector on \"/mnt/lustre\" [Y/n] Y\n  The following protector sources are available:\n  1 - Your login passphrase (pam_passphrase)\n  2 - A custom passphrase (custom_passphrase)\n  3 - A raw 256-bit key (raw_key)\n  Enter the source number for the new protector [2 - custom_passphrase]: 2\n  Enter a name for the new protector: bunker\n  Enter custom passphrase for protector \"bunker\":\n  Confirm passphrase:\n  Protector f3cc1b5cf9b8f41c created on filesystem \"/mnt/lustre\".\n  $ fscrypt metadata add-protector-to-policy\n   --protector=/mnt/lustre:f3cc1b5cf9b8f41c\n   --policy=/mnt/lustre:52b2b5aff0e59d8e0d58f962e715862e\n  WARNING: All files using this policy will be accessible with this protector!!\n  Protect policy 52b2b5aff0e59d8e0d58f962e715862e with protector f3cc1b5cf9b8f41c? [Y/n] Y\n  Enter custom passphrase for protector \"bunker\":\n  Enter custom passphrase for protector \"shield\":\n  Protector f3cc1b5cf9b8f41c now protecting policy 52b2b5aff0e59d8e0d58f962e715862e.\n  $ fscrypt status /mnt/lustre/vault\n  \"/mnt/lustre/vault\" is encrypted with fscrypt.\n  Policy: 52b2b5aff0e59d8e0d58f962e715862e\n  Options: padding:32 contents:AES_256_XTS filenames:AES_256_CTS policy_version:2\n  Unlocked: No\n  Protected with 2 protectors:\n  PROTECTOR LINKED DESCRIPTION\n  deacab807bf0e788 No custom protector \"shield\"\n  f3cc1b5cf9b8f41c No custom protector \"bunker\"\n  ```\n\n## Configuring Kerberos (KRB) Security\n\nThis chapter describes how to use Kerberos with Lustre.\n\n###  What Is Kerberos?\n\nKerberos is a mechanism for authenticating all entities (such as users and servers) on an \"unsafe\" network. Each of these entities, known as \"principals\", negotiate a runtime key with the Kerberos server. This key enables principals to verify that messages from the Kerberos server are authentic. By trusting the Kerberos server, users and services can authenticate one another. \n\nSetting up Lustre with Kerberos can provide advanced security protections for the Lustre network. Broadly, Kerberos offers three types of benefit:\n\n\u2022 Allows Lustre connection peers (MDS, OSS and clients) to authenticate one another. \n\u2022 Protects the integrity of PTLRPC messages from being modified during network transfer. \n\u2022 Protects the privacy of the PTLRPC message from being eavesdropped during network transfer.\n\nKerberos uses the \u201ckernel keyring\u201d client upcall mechanism.\n\n### Security Flavor\n\nA security flavor is a string to describe what kind authentication and data transformation be performed upon a PTLRPC connection. It covers both RPC message and BULK data.\n\nThe supported flavors are described in following table:\n\n| Base Flavor | Authentication | RPC Message Protection   | Bulk Data Protection | Notes                                                        |\n| ----------- | -------------- | ------------------------ | -------------------- | ------------------------------------------------------------ |\n| null        | N/A            | N/A                      | N/A                  |                                                              |\n| krb5n       | GSS/Kerberos5  | null                     | checksum             | No protection of RPC message, checksum protection of bulk data, light performance overhead.", "mimetype": "text/plain", "start_char_idx": 33013, "end_char_idx": 36834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2497dfb-87e4-469e-b917-5938bf600c1e": {"__data__": {"id_": "d2497dfb-87e4-469e-b917-5938bf600c1e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "325566b4-d0f8-4a1a-a101-523b573da843", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "292347a676da6c492f9e211fc326c94e18518cf6961c10bb4d2f749125b6f7bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e04316a3-9431-4a00-815f-eea66d935b72", "node_type": "1", "metadata": {}, "hash": "6b8c6abda84fae6ae7d075271395e8816ec063021f601f9a1cde52e0d7f1cca9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Protects the integrity of PTLRPC messages from being modified during network transfer. \n\u2022 Protects the privacy of the PTLRPC message from being eavesdropped during network transfer.\n\nKerberos uses the \u201ckernel keyring\u201d client upcall mechanism.\n\n### Security Flavor\n\nA security flavor is a string to describe what kind authentication and data transformation be performed upon a PTLRPC connection. It covers both RPC message and BULK data.\n\nThe supported flavors are described in following table:\n\n| Base Flavor | Authentication | RPC Message Protection   | Bulk Data Protection | Notes                                                        |\n| ----------- | -------------- | ------------------------ | -------------------- | ------------------------------------------------------------ |\n| null        | N/A            | N/A                      | N/A                  |                                                              |\n| krb5n       | GSS/Kerberos5  | null                     | checksum             | No protection of RPC message, checksum protection of bulk data, light performance overhead. |\n| krb5a       | GSS/Kerberos5  | partial integrity (krb5) | checksum             | Only header of RPC message is integrity protected, and checksum protection of bulk data, more performance overhead compare to krb5n. |\n| krb5i       | GSS/Kerberos5  | integrity (krb5)         | integrity (krb5)     | transformation algorithm is determined by actual Kerberos algorithms enforced by KDC and principals; heavy performance penalty. |\n| krb5p       | GSS/Kerberos5  | privacy (krb5)           | privacy (krb5)       | transformation privacy protection algorithm is determined by actual Kerberos algorithms enforced by KDC and principals; the heaviest performance penalty. |\n\n### Kerberos Setup\n\n#### Distribution\n\nWe only support MIT Kerberos 5, from version 1.3. \n\nFor environmental requirements in general, and clock synchronization in particular, please refer to section Section 8.1.2, \u201cEnvironmental Requirements\u201d.\n\n#### Principals Configuration\n\n* Configure client nodes:\n\n  * For each client node, create a lustre_root principal and generate keytab.\n\n    ```\n    kadmin> addprinc -randkey lustre_root/client_host.domain@REALM\n    kadmin> ktadd lustre_root/client_host.domain@REALM\n    ```\n\n  * Install the keytab on the client node.\n  \n* Configure MGS nodes:\n  \n  * For each MGS node, create a lustre_mgs principal and generate keytab.\n  \n    ```\n    kadmin> addprinc -randkey lustre_mgs/mgs_host.domain@REALM\n    kadmin> ktadd lustre_mds/mgs_host.domain@REALM\n    ```\n  \n  * Install the keytab on the MGS nodes.\n  \n* Configure MDS nodes:\n\n  * For each MDS node, create a lustre_mds principal and generate keytab.\n\n    ```\n    kadmin> addprinc -randkey lustre_mds/mds_host.domain@REALM\n    kadmin> ktadd lustre_mds/mds_host.domain@REALM\n    ```\n\n  * Install the keytab on the MDS nodes.\n\n* Configure OSS nodes:\n\n  * For each OSS node, create a lustre_oss principal and generate keytab.\n\n    ```\n    kadmin> addprinc -randkey lustre_oss/oss_host.domain@REALM\n    kadmin> ktadd lustre_oss/oss_host.domain@REALM\n    ```\n\n  * Install the keytab on the client node.\n\n    **Note**\n\n  * The host.domain should be the FQDN in your network, otherwise server might not recognize any GSS request.\n\n  * As an alternative for the client keytab, if you want to save the trouble of assigning unique keytab for each client node, you can create a general lustre_root principal and its keytab, and install the same keytab on as many client nodes as you want. **Be aware that in this way one compromised client means all clients are insecure.**\n\n    ```\n    kadmin> addprinc -randkey lustre_root@REALM\n    kadmin> ktadd lustre_root@REALM\n    ```\n\n  * Lustre support following enctypes for MIT Kerberos 5 version 1.3 or higher\n\n    * aes128-cts \n    * aes256-cts\n\n### Networking\n\nOn networks for which name resolution to IP address is possible, like TCP or InfiniBand, the names used in the principals must be the ones that resolve to the IP addresses used by the Lustre NIDs. \n\nIf you are using a network which is **NOT** TCP or InfiniBand (e.g.", "mimetype": "text/plain", "start_char_idx": 35725, "end_char_idx": 39865, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e04316a3-9431-4a00-815f-eea66d935b72": {"__data__": {"id_": "e04316a3-9431-4a00-815f-eea66d935b72", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2497dfb-87e4-469e-b917-5938bf600c1e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a2f85587e185bc590e0fe300b5d7c9aee9ed242ff78c283fb0e792c08cff7dc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aca219f7-5935-40fe-9e19-1336f1cc7867", "node_type": "1", "metadata": {}, "hash": "9834aca7e2ae64ec9ccf60e20ba54caa768e2d0ca99b4c593bc726aceeb4ab94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Be aware that in this way one compromised client means all clients are insecure.**\n\n    ```\n    kadmin> addprinc -randkey lustre_root@REALM\n    kadmin> ktadd lustre_root@REALM\n    ```\n\n  * Lustre support following enctypes for MIT Kerberos 5 version 1.3 or higher\n\n    * aes128-cts \n    * aes256-cts\n\n### Networking\n\nOn networks for which name resolution to IP address is possible, like TCP or InfiniBand, the names used in the principals must be the ones that resolve to the IP addresses used by the Lustre NIDs. \n\nIf you are using a network which is **NOT** TCP or InfiniBand (e.g. PTL4LND), you need to have a / etc/lustre/nid2hostname script on **each** node, which purpose is to translate NID into hostname. Following is a possible example for PTL4LND:\n\n```\n#!/bin/bash\nset -x\n# convert a NID for a LND to a hostname\n# called with thre arguments: lnd netid nid\n# $lnd is the string \"PTL4LND\", etc.\n# $netid is the network identifier in hex string format\n# $nid is the NID in hex format\n# output the corresponding hostname,\n# or error message leaded by a '@' for error logging.\nlnd=$1\nnetid=$2\n# convert hex NID number to decimal\nnid=$((0x$3))\ncase $lnd in\n PTL4LND) # simply add 'node' at the beginning\n echo \"node$nid\"\n ;;\n *)\n echo \"@unknown LND: $lnd\"\n ;;\nesac\n```\n\n### Required packages\n\nEvery node should have following packages installed:\n\n\u2022 krb5-workstation \n\n\u2022 krb5-libs \n\n\u2022 keyutils \n\n\u2022 keyutils-libs\n\nOn the node used to build Lustre with GSS support, following packages should be installed:\n\n\u2022 krb5-devel \n\n\u2022 keyutils-libs-devel\n\n### Build Lustre\n\nEnable GSS at configuration time:\n\n```\n./configure --enable-gss --other-options\n```\n\n### Running\n\n#### GSS Daemons\n\nMake sure to start the daemon process lsvcgssd on each server node (MGS, MDS and OSS) before starting Lustre. The command syntax is:\n\n```\nlsvcgssd [-f] [-v] [-g] [-m] [-o] -k\n```\n\n\u2022 -f: run in foreground, instead of as daemon \n\n\u2022 -v: increase verbosity by 1. For example, to set the verbose level to 3, run 'lsvcgssd -vvv'. Verbose logging can help you make sure Kerberos is set up correctly. \n\n\u2022 -g: service MGS \n\n\u2022 -m: service MDS \n\n\u2022 -o: service OSS \n\n\u2022 -k: enable kerberos support\n\n#### Setting Security Flavors\n\nSecurity flavors can be set by defining sptlrpc rules on the MGS. These rules are persistent, and are in the form: ` <spec>=<flavor>`\n\n* To add a rule:\n\n  ```\n  mgs> lctl conf_param <spec>=<flavor>\n  ```\n\n  If there is an existing rule on `<spec>`, it will be overwritten.\n\n* To delete a rule:\n\n  ```\n  mgs> lctl conf_param -d <spec>\n  ```\n\n* To list existing rules:\n\n  ```\n  msg> lctl get_param mgs.MGS.live.<fs-name> | grep \"srpc.flavor\"\n  ```\n\n  **Note**\n\n  * If nothing is specified, by default all RPC connections will use null flavor, which means no security.\n  * After you change a rule, it usually takes a few minutes to apply the new rule to all nodes, depending on global system load.\n  * Before you change a rule, make sure affected nodes are ready for the new security flavor. E.g. if you change flavor from null to krb5p but GSS/Kerberos environment is not properly configured on affected nodes, those nodes might be evicted because they cannot communicate with each other.\n\n####  Rules Syntax & Examples\n\nThe general syntax is: `<target>.srpc.flavor.<network <direction>]=flavor`\n* `<target>` can be filesystem name, or specific MDT/OST device name. For example `testfs, testfs-MDT0000, testfs-OST0001`.", "mimetype": "text/plain", "start_char_idx": 39280, "end_char_idx": 42696, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aca219f7-5935-40fe-9e19-1336f1cc7867": {"__data__": {"id_": "aca219f7-5935-40fe-9e19-1336f1cc7867", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b986ca8-7311-4014-8513-80f735259c79", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f05b7c77a69bf87e1af4db38d4fb2de4d006baa8b0f12c0ddbfc2264bcd6e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e04316a3-9431-4a00-815f-eea66d935b72", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "667678192d2101c74c0113d19caa5d7a14135a9cadcdbf317548d3aa0ffff1c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "* After you change a rule, it usually takes a few minutes to apply the new rule to all nodes, depending on global system load.\n  * Before you change a rule, make sure affected nodes are ready for the new security flavor. E.g. if you change flavor from null to krb5p but GSS/Kerberos environment is not properly configured on affected nodes, those nodes might be evicted because they cannot communicate with each other.\n\n####  Rules Syntax & Examples\n\nThe general syntax is: `<target>.srpc.flavor.<network <direction>]=flavor`\n* `<target>` can be filesystem name, or specific MDT/OST device name. For example `testfs, testfs-MDT0000, testfs-OST0001`.\n\n* `<network>` is the LNet network name, for example tcp0, o2ib0, or default to not filter on LNet network.\n* `<direction>` can be one of cli2mdt, cli2ost, mdt2mdt, mdt2ost. Direction is optional.\n\nExamples:\n\n* Apply krb5i on ALL connections for file system testfs:\n```\nmgs> lctl conf_param testfs.srpc.flavor.default=krb5i\n```\n* Nodes in network tcp0 use krb5p; all other nodes use null.\n```\nmgs> lctl conf_param testfs.srpc.flavor.tcp0=krb5p\nmgs> lctl conf_param testfs.srpc.flavor.default=null\n```\n* Nodes in network tcp0 use krb5p; nodes in o2ib0 use krb5n; among other nodes, clients use krb5i to MDT/OST, MDTs use null to other MDTs, MDTs use krb5a to OSTs.\n```\nmgs> lctl conf_param testfs.srpc.flavor.tcp0=krb5p\nmgs> lctl conf_param testfs.srpc.flavor.o2ib0=krb5n\nmgs> lctl conf_param testfs.srpc.flavor.default.cli2mdt=krb5i\nmgs> lctl conf_param testfs.srpc.flavor.default.cli2ost=krb5i\nmgs> lctl conf_param testfs.srpc.flavor.default.mdt2mdt=null\nmgs> lctl conf_param testfs.srpc.flavor.default.mdt2ost=krb5a\n```\n#### Regular Users Authentication\n\nOn client nodes, non-root users need to issue kinit before accessing Lustre, just like other Kerberized applications.\n\u2022 Required by kerberos, the user's principal (username@REALM) should be added to the KDC.\n\u2022 Client and MDT nodes should have the same user database used for name and uid/gid translation. \n\nRegular users can destroy the established security contexts before logging out, by issuing:\n```\nlfs flushctx -k -r <mount point>\n```\nHere -k is to destroy the on-disk Kerberos credential cache, similar to kdestroy, and -r is to reap the revoked keys from the keyring when flushing the GSS context. Otherwise it only destroys established contexts in kernel memory.\n\n### Secure MGS connection\n\nEach node can specify which flavor to use to connect to the MGS, by using the mgssec=flavor mount option. Once a flavor is chosen, it cannot be changed until re-mount. \n\nBecause a Lustre node only has one connection to the MGS, if there is more than one target or client on the node, they necessarily use the same security flavor to the MGS, being the one enforced when the first connection to the MGS was established. \n\nBy default, the MGS accepts RPCs with any flavor. But it is possible to configure the MGS to only accept a given flavor. The syntax is identical to what is explained in paragraph Section 30.6.7.3, \u201cRules Syntax & Examples\u201d, but with special target _mgs:\n\n```\nmgs> lctl conf_param _mgs.srpc.flavor.<network>=<flavor>\n```", "mimetype": "text/plain", "start_char_idx": 42047, "end_char_idx": 45193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20bc7b11-c15c-4eb1-91db-fd05353848af": {"__data__": {"id_": "20bc7b11-c15c-4eb1-91db-fd05353848af", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47974e4509ee5bb6084613eb6f7bd301008642bfc91aef3fb6d74aed452ce87e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3072836d-b160-46b4-87dc-275d9713e2a6", "node_type": "1", "metadata": {}, "hash": "2b2925a843f0fc7654dc9082e03600e0278aba26389b4a5f3ff7568b63ba621f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Lustre ZFS Snapshots\n\n- [Lustre ZFS Snapshots](#lustre-zfs-snapshots)\n  * [Introduction](#introduction)\n    + [Requirements](#requirements)\n  * [Configuration](#configuration)\n  * [Snapshot Operations](#snapshot-operations)\n    + [Creating a Snapshot](#creating-a-snapshot)\n    + [Delete a Snapshot](#delete-a-snapshot)\n    + [Mounting a Snapshot](#mounting-a-snapshot)\n    + [Unmounting a Snapshot](#unmounting-a-snapshot)\n    + [List Snapshots](#list-snapshots)\n    + [Modify Snapshot Attributes](#modify-snapshot-attributes)\n  * [Global Write Barriers](#global-write-barriers)\n    + [Impose Barrier](#impose-barrier)\n    + [Remove Barrier](#remove-barrier)\n    + [Query Barrier](#query-barrier)\n    + [Rescan Barrier](#rescan-barrier)\n  * [Snapshot Logs](#snapshot-logs)\n  * [Lustre Configuration Logs](#lustre-configuration-logs)\n\n\nThis chapter describes the ZFS Snapshot feature support in Lustre and contains following sections:\n\n- [the section called \u201cIntroduction\u201d](#introduction)\n- [the section called \u201cConfiguration\u201d](#introduction)\n- [the section called \u201cSnapshot Operations\u201d](#snapshot-operations)\n- [the section called \u201cGlobal Write Barriers\u201d](#global-write-barriers)\n- [the section called \u201cSnapshot Logs\u201d](#snapshot-logs)\n- [the section called \u201cLustre Configuration Logs\u201d](#lustre-configuration-logs)\n\n## Introduction\n\nSnapshots provide fast recovery of files from a previously created checkpoint without recourse to an offline backup or remote replica. Snapshots also provide a means to version-control storage, and can be used to recover lost files or previous versions of files.\n\nFilesystem snapshots are intended to be mounted on user-accessible nodes, such as login nodes, so that users can restore files (e.g. after accidental delete or overwrite) without administrator intervention. It would be possible to mount the snapshot filesystem(s) via automount when users access them, rather than mounting all snapshots, to reduce overhead on login nodes when the snapshots are not in use.\n\nRecovery of lost files from a snapshot is usually considerably faster than from any offline backup or remote replica. However, note that snapshots do not improve storage reliability and are just as exposed to hardware failure as any other storage volume.\n\n### Requirements\n\nAll Lustre server targets must be ZFS file systems running Lustre version 2.10 or later. In addition, the MGS must be able to communicate via ssh or another remote access protocol, without password authentication, to all other servers.\n\nThe feature is enabled by default and cannot be disabled. The management of snapshots is done through `lctl`commands on the MGS.\n\nLustre snapshot is based on Copy-On-Write; the snapshot and file system may share a single copy of the data until a file is changed on the file system. The snapshot will prevent the space of deleted or overwritten files from being released until the snapshot(s) referencing those files is deleted. The file system administrator needs to establish a snapshot create/backup/remove policy according to their system\u2019s actual size and usage.\n\n## Configuration\n\nThe snapshot tool loads system configuration from the `/etc/ldev.conf` file on the MGS and calls related ZFS commands to maintian the Lustre snapshot pieces on all targets (MGS/MDT/OST). Please note that the `/etc/ldev.conf` file is used for other purposes as well.\n\nThe format of the file is:\n\n```\n<host> foreign/- <label> <device> [journal-path]/- [raidtab]\n```\n\nThe format of `<label>` is:\n\n```\nfsname-<role><index> or <role><index>\n```\n\nThe format of <device> is:\n\n```\n[md|zfs:][pool_dir/]<pool>/<filesystem>\n```\n\nSnapshot only uses the fields <host>, <label> and <device>.\n\nExample:\n\n```\nmgs# cat /etc/ldev.conf\nhost-mdt1 - myfs-MDT0000 zfs:/tmp/myfs-mdt1/mdt1\nhost-mdt2 - myfs-MDT0001 zfs:myfs-mdt2/mdt2\nhost-ost1 - OST0000 zfs:/tmp/myfs-ost1/ost1\nhost-ost2 - OST0001 zfs:myfs-ost2/ost2\n```\n\nThe configuration file is edited manually.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3944, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3072836d-b160-46b4-87dc-275d9713e2a6": {"__data__": {"id_": "3072836d-b160-46b4-87dc-275d9713e2a6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47974e4509ee5bb6084613eb6f7bd301008642bfc91aef3fb6d74aed452ce87e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20bc7b11-c15c-4eb1-91db-fd05353848af", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f8917d13cde9ecd52c4c54f3205ff32e99db42e2414aea9fad214feba795fd14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6162b0e9-1cf7-451b-86db-b807f7a0d50a", "node_type": "1", "metadata": {}, "hash": "c05b218d6b57a45fa5df48888c915a7a2e17aa49bef66e06ea442976ef6fd070", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Example:\n\n```\nmgs# cat /etc/ldev.conf\nhost-mdt1 - myfs-MDT0000 zfs:/tmp/myfs-mdt1/mdt1\nhost-mdt2 - myfs-MDT0001 zfs:myfs-mdt2/mdt2\nhost-ost1 - OST0000 zfs:/tmp/myfs-ost1/ost1\nhost-ost2 - OST0001 zfs:myfs-ost2/ost2\n```\n\nThe configuration file is edited manually.\n\nOnce the configuration file is updated to reflect the current file system setup, you are ready to create a file system snapshot.\n\n## Snapshot Operations\n\n### Creating a Snapshot\n\nTo create a snapshot of an existing Lustre file system, run the following `lctl` command on the MGS:\n\n```\nlctl snapshot_create [-b | --barrier [on | off]] [-c | --comment\ncomment] -F | --fsname fsname> [-h | --help] -n | --name ssname>\n[-r | --rsh remote_shell][-t | --timeout timeout]\n```\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `-b`       | set write barrier before creating snapshot. The default value is 'on'. |\n| `-c`       | a description for the purpose of the snapshot                |\n| `-F`       | the filesystem name                                          |\n| `-h`       | help information                                             |\n| `-n`       | the name of the snapshot                                     |\n| `-r`       | the remote shell used for communication with remote target. The default value is 'ssh' |\n| `-t`       | the lifetime (seconds) for write barrier. The default value is 30 seconds |\n\n### Delete a Snapshot\n\nTo delete an existing snapshot, run the following `lctl` command on the MGS:\n\n```\nlctl snapshot_destroy [-f | --force] <-F | --fsname fsname>\n<-n | --name ssname> [-r | --rsh remote_shell]\n```\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `-f`       | destroy the snapshot by force                                |\n| `-F`       | the filesystem name                                          |\n| `-h`       | help information                                             |\n| `-n`       | the name of the snapshot                                     |\n| `-r`       | the remote shell used for communication with remote target. The default value is 'ssh' |\n\n### Mounting a Snapshot\n\nSnapshots are treated as separate file systems and can be mounted on Lustre clients. The snapshot file system must be mounted as a read-only file system with the `-o ro` option. If the `mount` command does not include the read-only option, the mount will fail.\n\n**Note**\n\nBefore a snapshot can be mounted on the client, the snapshot must first be mounted on the servers using the `lctl` utility.\n\nTo mount a snapshot on the server, run the following lctl command on the MGS:\n\n```\nlctl snapshot_mount <-F | --fsname fsname> [-h | --help]\n<-n | --name ssname> [-r | --rsh remote_shell]\n```\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `-F`       | the filesystem name                                          |\n| `-h`       | help information                                             |\n| `-n`       | the name of the snapshot                                     |\n| `-r`       | the remote shell used for communication with remote target. The default value is 'ssh' |\n\nAfter the successful mounting of the snapshot on the server, clients can now mount the snapshot as a read-only filesystem. For example, to mount a snapshot named *snapshot_20170602* for a filesystem named *myfs*, the following mount command would be used:\n\n```\nmgs# lctl snapshot_mount -F myfs -n snapshot_20170602\n```\n\nAfter mounting on the server, use `lctl snapshot_list` to get the fsname for the snapshot itself as follows:\n\n```\nss_fsname=$(lctl snapshot_list -F myfs -n snapshot_20170602 |\n          awk '/^snapshot_fsname/ { print $2 }')\n```\n\nFinally, mount the snapshot on the client:\n\n```\nmount -t lustre -o ro $MGS_nid:/$ss_fsname $local_mount_point\n```\n\n### Unmounting a Snapshot\n\nTo unmount a snapshot from the servers, first unmount the snapshot file system from all clients, using the standard `umount` command on each client.", "mimetype": "text/plain", "start_char_idx": 3683, "end_char_idx": 7907, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6162b0e9-1cf7-451b-86db-b807f7a0d50a": {"__data__": {"id_": "6162b0e9-1cf7-451b-86db-b807f7a0d50a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47974e4509ee5bb6084613eb6f7bd301008642bfc91aef3fb6d74aed452ce87e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3072836d-b160-46b4-87dc-275d9713e2a6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e3c887cad7ea96866e87de05e127cb1dd55f71f48764a6a7e8d1ed66736058a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c17f761a-5422-43e5-8923-77324d6f9c98", "node_type": "1", "metadata": {}, "hash": "821b98ed7a24bc658fc08345564820c43613fe1125ac103f380e6ed7eee4056b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, to mount a snapshot named *snapshot_20170602* for a filesystem named *myfs*, the following mount command would be used:\n\n```\nmgs# lctl snapshot_mount -F myfs -n snapshot_20170602\n```\n\nAfter mounting on the server, use `lctl snapshot_list` to get the fsname for the snapshot itself as follows:\n\n```\nss_fsname=$(lctl snapshot_list -F myfs -n snapshot_20170602 |\n          awk '/^snapshot_fsname/ { print $2 }')\n```\n\nFinally, mount the snapshot on the client:\n\n```\nmount -t lustre -o ro $MGS_nid:/$ss_fsname $local_mount_point\n```\n\n### Unmounting a Snapshot\n\nTo unmount a snapshot from the servers, first unmount the snapshot file system from all clients, using the standard `umount` command on each client. For example, to unmount the snapshot file system named *snapshot_20170602* run the following command on each client that has it mounted:\n\n```\nclient# umount $local_mount_point\n```\n\nAfter all clients have unmounted the snapshot file system, run the following `lctl`command on a server node where the snapshot is mounted:\n\n```\nlctl snapshot_umount [-F | --fsname fsname] [-h | --help]\n<-n | -- name ssname> [-r | --rsh remote_shell]\n```\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `-F`       | the filesystem name                                          |\n| `-h`       | help information                                             |\n| `-n`       | the name of the snapshot                                     |\n| `-r`       | the remote shell used for communication with remote target. The default value is 'ssh' |\n\nFor example:\n\n```\nlctl snapshot_umount -F myfs -n snapshot_20170602\n```\n### List Snapshots\n\nTo list the available snapshots for a given file system, use the following `lctl` command on the MGS:\n\n```\nlctl snapshot_list [-d | --detail] <-F | --fsname fsname>\n[-h | -- help] [-n | --name ssname] [-r | --rsh remote_shell]\n```\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `-d`       | list every piece for the specified snapshot                  |\n| `-F`       | the filesystem name                                          |\n| `-h`       | help information                                             |\n| `-n`       | the snapshot's name. If the snapshot name is not supplied, all snapshots for this file system will be displayed |\n| `-r`       | the remote shell used for communication with remote target. The default value is 'ssh' |\n\n### Modify Snapshot Attributes\n\nCurrently, Lustre snapshot has five user visible attributes; snapshot name, snapshot comment, create time, modification time, and snapshot file system name. Among them, the former two attributes can be modified. Renaming follows the general ZFS snapshot name rules, such as the maximum length is 256 bytes, cannot conflict with the reserved names, and so on.\n\nTo modify a snapshot\u2019s attributes, use the following `lctl` command on the MGS:\n\n```\nlctl snapshot_modify [-c | --comment comment]\n<-F | --fsname fsname> [-h | --help] <-n | --name ssname>\n[-N | --new new_ssname] [-r | --rsh remote_shell]\n```\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `-c`       | update the snapshot's comment                                |\n| `-F`       | the filesystem name                                          |\n| `-h`       | help information                                             |\n| `-n`       | the snapshot's name                                          |\n| `-N`       | rename the snapshot's name as *new_ssname*                   |\n| `-r`       | the remote shell used for communication with remote target. The default value is 'ssh' |\n\n## Global Write Barriers\n\nSnapshots are non-atomic across multiple MDTs and OSTs, which means that if there is activity on the file system while a snapshot is being taken, there may be user-visible namespace inconsistencies with files created or destroyed in the interval between the MDT and OST snapshots. In order to create a consistent snapshot of the file system, we are able to set a global write barrier, or \u201cfreeze\u201d the system. Once set, all metadata modifications will be blocked until the write barrier is actively removed (\u201cthawed\u201d) or expired. The user can set a timeout parameter on a global barrier or the barrier can be explicitly removed. The default timeout period is 30 seconds.", "mimetype": "text/plain", "start_char_idx": 7190, "end_char_idx": 11769, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c17f761a-5422-43e5-8923-77324d6f9c98": {"__data__": {"id_": "c17f761a-5422-43e5-8923-77324d6f9c98", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47974e4509ee5bb6084613eb6f7bd301008642bfc91aef3fb6d74aed452ce87e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6162b0e9-1cf7-451b-86db-b807f7a0d50a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ed6f4ef5b6d464fbc24882b900556d383f759bf9aaaa6d4060c371fe7700a611", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea23b2d1-7e3c-4af0-a085-16cbfa1cce2a", "node_type": "1", "metadata": {}, "hash": "30aa5d2908d715ac242fd8f1dfec50597efd854386240e2b5661d6cc680e2acc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The default value is 'ssh' |\n\n## Global Write Barriers\n\nSnapshots are non-atomic across multiple MDTs and OSTs, which means that if there is activity on the file system while a snapshot is being taken, there may be user-visible namespace inconsistencies with files created or destroyed in the interval between the MDT and OST snapshots. In order to create a consistent snapshot of the file system, we are able to set a global write barrier, or \u201cfreeze\u201d the system. Once set, all metadata modifications will be blocked until the write barrier is actively removed (\u201cthawed\u201d) or expired. The user can set a timeout parameter on a global barrier or the barrier can be explicitly removed. The default timeout period is 30 seconds.\n\nIt is important to note that snapshots are usable without the global barrier. Only files that are currently being modified by clients (write, create, unlink) may be inconsistent as noted above if the barrier is not used. Other files not curently being modified would be usable even without the barrier.\n\nThe snapshot create command will call the write barrier internally when requested using the `-b` option to `lctl snapshot_create`. So, explicit use of the barrier is not required when using snapshots but included here as an option to quiet the file system before a snapshot is created.\n\n### Impose Barrier\n\nTo impose a global write barrier, run the `lctl barrier_freeze` command on the MGS:\n\n```\nlctl barrier_freeze <fsname> [timeout (in seconds)]\nwhere timeout default is 30.\n```\n\nFor example, to freeze the filesystem *testfs* for `15` seconds:\n\n```\nmgs# lctl barrier_freeze testfs 15\n```\n\nIf the command is successful, there will be no output from the command. Otherwise, an error message will be printed.\n\n### Remove Barrier\n\nTo remove a global write barrier, run the `lctl barrier_thaw` command on the MGS:\n\n```\nlctl barrier_thaw <fsname>\n```\n\nFor example, to thaw the write barrier for the filesystem *testfs*:\n\n```\nmgs# lctl barrier_thaw testfs\n```\n\nIf the command is successful, there will be no output from the command. Otherwise, an error message will be printed.\n\n### Query Barrier\n\nTo see how much time is left on a global write barrier, run the `lctl barrier_stat` command on the MGS:\n\n```\n# lctl barrier_stat <fsname>\n```\n\nFor example, to stat the write barrier for the filesystem *testfs*:\n\n```\nmgs# lctl barrier_stat testfs\nThe barrier for testfs is in 'frozen'\nThe barrier will be expired after 7 seconds\n```\n\nIf the command is successful, a status from the table below will be printed. Otherwise, an error message will be printed.\n\nThe possible status and related meanings for the write barrier are as follows:\n\n**Table 13. Write Barrier Status**\n\n| **Status**    | **Meaning**                                                  |\n| ------------- | ------------------------------------------------------------ |\n| `init`        | barrier has never been set on the system                     |\n| `freezing_p1` | In the first stage of setting the write barrier              |\n| `freezing_p2` | the second stage of setting the write barrier                |\n| `frozen`      | the write barrier has been set successfully                  |\n| `thawing`     | In thawing the write barrier                                 |\n| `thawed`      | The write barrier has been thawed                            |\n| `failed`      | Failed to set write barrier                                  |\n| `expired`     | The write barrier is expired                                 |\n| `rescan`      | In scanning the MDTs status, see the command `barrier_rescan` |\n| `unknown`     | Other cases                                                  |\n\nIf the barrier is in \u2019freezing_p1\u2019, \u2019freezing_p2\u2019 or \u2019frozen\u2019 status, then the remaining lifetime will be returned also.\n\n### Rescan Barrier\n\nTo rescan a global write barrier to check which MDTs are active, run the `lctl barrier_rescan` command on the MGS:\n\n```\nlctl barrier_rescan <fsname> [timeout (in seconds)],\nwhere the default timeout is 30 seconds.\n```\n\nFor example, to rescan the barrier for filesystem *testfs*:\n\n```\nmgs# lctl barrier_rescan testfs\n1 of 4 MDT(s) in the filesystem testfs are inactive\n```\n\nIf the command is successful, the number of MDTs that are unavailable against the total MDTs will be reported. Otherwise, an error message will be printed.", "mimetype": "text/plain", "start_char_idx": 11044, "end_char_idx": 15385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea23b2d1-7e3c-4af0-a085-16cbfa1cce2a": {"__data__": {"id_": "ea23b2d1-7e3c-4af0-a085-16cbfa1cce2a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b88264eb-e9c4-4f08-9be9-eb423dd36b19", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "47974e4509ee5bb6084613eb6f7bd301008642bfc91aef3fb6d74aed452ce87e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c17f761a-5422-43e5-8923-77324d6f9c98", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "00f195610b92411279b7ce9231596f1a256e3fd457af6dfb770ce4c70cd60907", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Rescan Barrier\n\nTo rescan a global write barrier to check which MDTs are active, run the `lctl barrier_rescan` command on the MGS:\n\n```\nlctl barrier_rescan <fsname> [timeout (in seconds)],\nwhere the default timeout is 30 seconds.\n```\n\nFor example, to rescan the barrier for filesystem *testfs*:\n\n```\nmgs# lctl barrier_rescan testfs\n1 of 4 MDT(s) in the filesystem testfs are inactive\n```\n\nIf the command is successful, the number of MDTs that are unavailable against the total MDTs will be reported. Otherwise, an error message will be printed.\n\n## Snapshot Logs\n\nA log of all snapshot activity can be found in the following file: `/var/log/lsnapshot.log`. This file contains information on when a snapshot was created, an attribute was changed, when it was mounted, and other snapshot information.\n\nThe following is a sample `/var/log/lsnapshot` file:\n\n```\nMon Mar 21 19:43:06 2016\n(15826:jt_snapshot_create:1138:scratch:ssh): Create snapshot lss_0_0\nsuccessfully with comment <(null)>, barrier <enable>, timeout <30>\nMon Mar 21 19:43:11 2016(13030:jt_snapshot_create:1138:scratch:ssh):\nCreate snapshot lss_0_1 successfully with comment <(null)>, barrier\n<disable>, timeout <-1>\nMon Mar 21 19:44:38 2016 (17161:jt_snapshot_mount:2013:scratch:ssh):\nThe snapshot lss_1a_0 is mounted\nMon Mar 21 19:44:46 2016\n(17662:jt_snapshot_umount:2167:scratch:ssh): the snapshot lss_1a_0\nhave been umounted\nMon Mar 21 19:47:12 2016\n(20897:jt_snapshot_destroy:1312:scratch:ssh): Destroy snapshot\nlss_2_0 successfully with force <disable>\n```\n\n## Lustre Configuration Logs\n\nA snapshot is independent from the original file system that it is derived from and is treated as a new file system name that can be mounted by Lustre client nodes. The file system name is part of the configuration log names and exists in configuration log entries. Two commands exist to manipulate configuration logs: `lctl fork_lcfg` and `lctl erase_lcfg`.\n\nThe snapshot commands will use configuration log functionality internally when needed. So, use of the barrier is not required to use snapshots but included here as an option. The following configuration log commands are independent of snapshots and can be used independent of snapshot use.\n\nTo fork a configuration log, run the following `lctl` command on the MGS:\n\n```\nlctl fork_lcfg\n```\n\nUsage: fork_lcfg <fsname> <newname>\n\nTo erase a configuration log, run the following `lctl` command on the MGS:\n\n```\nlctl erase_lcfg\n```\n\nUsage: erase_lcfg <fsname>", "mimetype": "text/plain", "start_char_idx": 14837, "end_char_idx": 17313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b73853a3-4cab-444d-9984-5ebe0d6ebbc4": {"__data__": {"id_": "b73853a3-4cab-444d-9984-5ebe0d6ebbc4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "035e36cf-d71e-413b-9e80-0285118eb183", "node_type": "1", "metadata": {}, "hash": "bffe0fecbde19592603c6353b8336f6aea83db1eef88a03b7d78bb426f9ae97f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Testing Lustre Network Performance (LNet Self-Test)\n\n- [Testing Lustre Network Performance (LNet Self-Test)](#testing-lustre-network-performance-lnet-self-test)\n  * [LNet Self-Test Overview](#lnet-self-test-overview)\n    + [Prerequisites](#prerequisites)\n  * [Using LNet Self-Test](#using-lnet-self-test)\n    + [Creating a Session](#creating-a-session)\n    + [Setting Up Groups](#setting-up-groups)\n    + [Defining and Running the Tests](#defining-and-running-the-tests)\n    + [Sample Script](#sample-script)\n  * [LNet Self-Test Command Reference](#lnet-self-test-command-reference)\n    + [Session Commands](#session-commands)\n    + [Group Commands](#group-commands)\n    + [Batch and Test Commands](#batch-and-test-commands)\n    + [Other Commands](#other-commands)\n\nThis chapter describes the LNet self-test, which is used by site administrators to confirm that Lustre Networking (LNet) has been properly installed and configured, and that underlying network software and hardware are performing according to expectations. The chapter includes:\n\n- [the section called \u201c LNet Self-Test Overview\u201d](#lnet-self-test-overview)\n- [the section called \u201cUsing LNet Self-Test\u201d](#using-lnet-self-test)\n- [the section called \u201cLNet Self-Test Command Reference\u201d](#lnet-self-test-command-reference)\n\n## LNet Self-Test Overview\n\nLNet self-test is a kernel module that runs over LNet and the Lustre network drivers (LNDs). It is designed to:\n\n- Test the connection ability of the Lustre network\n- Run regression tests of the Lustre network\n- Test performance of the Lustre network\n\nAfter you have obtained performance results for your Lustre network, refer to [*Tuning a Lustre File System*](04.03-Tuning%20a%20Lustre%20File%20System.md) for information about parameters that can be used to tune LNet for optimum performance.\n\n**Note**\n\nApart from the performance impact, LNet self-test is invisible to the Lustre file system.\n\nAn LNet self-test cluster includes two types of nodes:\n\n- **Console node** - A node used to control and monitor an LNet self-test cluster. The console node serves as the user interface of the LNet self-test system and can be any node in the test cluster. All self-test commands are entered from the console node. From the console node, a user can control and monitor the status of the entire LNet self-test cluster (session). The console node is exclusive in that a user cannot control two different sessions from one console node.\n- **Test nodes** - The nodes on which the tests are run. Test nodes are controlled by the user from the console node; the user does not need to log into them directly.\n\nLNet self-test has two user utilities:\n\n- **lst** - The user interface for the self-test console (run on the *console node*). It provides a list of commands to control the entire test system, including commands to create a session, create test groups, etc.\n- **lstclient** - The userspace LNet self-test program (run on a *test node*). The `lstclient` utility is linked with userspace LNDs and LNet. This utility is not needed if only kernel space LNet and LNDs are used.\n\n**Note**\n\n*Test nodes* can be in either kernel or userspace. A *console node* can invite a kernel *test node* to join the session by running `lst add_group NID`, but the *console node* cannot actively add a userspace *test node* to the session. A *console node* can passively accept a *test node* to the session while the *test node* is running `lstclient` to connect to the *console node*.\n\n### Prerequisites\n\nTo run LNet self-test, these modules must be loaded on both *console nodes* and *test nodes*:\n\n- `libcfs`\n- `net`\n- `lnet_selftest`\n- `klnds`: A kernel Lustre network driver (LND) (i.e, `ksocklnd`, `ko2iblnd`...) as needed by your network configuration.\n\nTo load the required modules, run:\n\n```\nmodprobe lnet_selftest \n```\n\nThis command recursively loads the modules on which LNet self-test depends.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "035e36cf-d71e-413b-9e80-0285118eb183": {"__data__": {"id_": "035e36cf-d71e-413b-9e80-0285118eb183", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b73853a3-4cab-444d-9984-5ebe0d6ebbc4", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9a633cd07a583d1c06b857a56e8b1cdf6f6b3dd858171998b7f7259bc2d6c0f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c613bba-1dde-460e-b307-9f46b7238ef5", "node_type": "1", "metadata": {}, "hash": "da4631e7cefa2047045ea8d516c6dc4b7cc7e849891a71e69da4209f691f5af4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A *console node* can passively accept a *test node* to the session while the *test node* is running `lstclient` to connect to the *console node*.\n\n### Prerequisites\n\nTo run LNet self-test, these modules must be loaded on both *console nodes* and *test nodes*:\n\n- `libcfs`\n- `net`\n- `lnet_selftest`\n- `klnds`: A kernel Lustre network driver (LND) (i.e, `ksocklnd`, `ko2iblnd`...) as needed by your network configuration.\n\nTo load the required modules, run:\n\n```\nmodprobe lnet_selftest \n```\n\nThis command recursively loads the modules on which LNet self-test depends.\n\n**Note**\n\nWhile the *console node* and *test nodes* require all the prerequisite modules to be loaded, userspace test nodes do not require these modules.\n\n## Using LNet Self-Test\n\nThis section describes how to create and run an LNet self-test. The examples shown are for a test that simulates the traffic pattern of a set of Lustre servers on a TCP network accessed by Lustre clients on an InfiniBand network connected via LNet routers. In this example, half the clients are reading and half the clients are writing.\n\n### Creating a Session\n\nA *session* is a set of processes that run on a *test node*. Only one session can be run at a time on a test node to ensure that the session has exclusive use of the node. The console node is used to create, change or destroy a session (`new_session`, `end_session`, `show_session`). For more about session parameters, see [*the section called \u201cSession Commands\u201d*](#session-commands).\n\nAlmost all operations should be performed within the context of a session. From the *console node*, a user can only operate nodes in his own session. If a session ends, the session context in all test nodes is stopped.\n\nThe following commands set the `LST_SESSION` environment variable to identify the session on the console node and create a session called `read_write`:\n\n```\nexport LST_SESSION=$$\nlst new_session read_write\n```\n\n### Setting Up Groups\n\nA *group* is a named collection of nodes. Any number of groups can exist in a single LNet self-test session. Group membership is not restricted in that a *test node* can be included in any number of groups.\n\nEach node in a group has a rank, determined by the order in which it was added to the group. The rank is used to establish test traffic patterns.\n\nA user can only control nodes in his/her session. To allocate nodes to the session, the user needs to add nodes to a group (of the session). All nodes in a group can be referenced by the group name. A node can be allocated to multiple groups of a session.\n\nIn the following example, three groups are established on a console node:\n\n```\nlst add_group servers 192.168.10.[8,10,12-16]@tcp\nlst add_group readers 192.168.1.[1-253/2]@o2ib\nlst add_group writers 192.168.1.[2-254/2]@o2ib\n```\n\nThese three groups include:\n\n- Nodes that will function as 'servers' to be accessed by 'clients' during the LNet self-test session\n- Nodes that will function as 'clients' that will simulate *reading* data from the 'servers'\n- Nodes that will function as 'clients' that will simulate *writing* data to the 'servers'\n\n**Note**\n\nA *console node* can associate kernel space *test nodes* with the session by running `lst add_group *NIDs*`, but a userspace test node cannot be actively added to the session. A console node can passively \"accept\" a test node to associate with a test session while the test node running `lstclient`connects to the console node, i.e: `lstclient --sesid *CONSOLE_NID* --group *NAME*`).\n\n### Defining and Running the Tests\n\nA *test* generates a network load between two groups of nodes, a source group identified using the `--from`parameter and a target group identified using the `--to` parameter. When a test is running, each node in the `--from *group*` simulates a client by sending requests to nodes in the `--to *group*`, which are simulating a set of servers, and then receives responses in return. This activity is designed to mimic Lustre file system RPC traffic.", "mimetype": "text/plain", "start_char_idx": 3332, "end_char_idx": 7316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c613bba-1dde-460e-b307-9f46b7238ef5": {"__data__": {"id_": "0c613bba-1dde-460e-b307-9f46b7238ef5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "035e36cf-d71e-413b-9e80-0285118eb183", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "67d1390b8b9a4cbb9ca1060f2afae8eaa543fb34043de544aef37538314d79ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69f23c0a-4913-42a8-98dc-e6dc594da9db", "node_type": "1", "metadata": {}, "hash": "c0a082ff0970ea134ddad76458831c89a15bb888733463c210813cb6d324754d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A console node can passively \"accept\" a test node to associate with a test session while the test node running `lstclient`connects to the console node, i.e: `lstclient --sesid *CONSOLE_NID* --group *NAME*`).\n\n### Defining and Running the Tests\n\nA *test* generates a network load between two groups of nodes, a source group identified using the `--from`parameter and a target group identified using the `--to` parameter. When a test is running, each node in the `--from *group*` simulates a client by sending requests to nodes in the `--to *group*`, which are simulating a set of servers, and then receives responses in return. This activity is designed to mimic Lustre file system RPC traffic.\n\nA *batch* is a collection of tests that are started and stopped together and run in parallel. A test must always be run as part of a batch, even if it is just a single test. Users can only run or stop a test batch, not individual tests.\n\nTests in a batch are non-destructive to the file system, and can be run in a normal Lustre file system environment (provided the performance impact is acceptable).\n\nA simple batch might contain a single test, for example, to determine whether the network bandwidth presents an I/O bottleneck. In this example, the `--to *group*` could be comprised of Lustre OSSs and `--from *group*` the compute nodes. A second test could be added to perform pings from a login node to the MDS to see how checkpointing affects the `ls -l` process.\n\nTwo types of tests are available:\n\n- **ping -** A `ping` generates a short request message, which results in a short response. Pings are useful to determine latency and small message overhead and to simulate Lustre metadata traffic.\n- **brw -** In a `brw` ('bulk read write') test, data is transferred from the target to the source (`brwread`) or data is transferred from the source to the target (`brwwrite`). The size of the bulk transfer is set using the `size`parameter. A brw test is useful to determine network bandwidth and to simulate Lustre I/O traffic.\n\nIn the example below, a batch is created called `bulk_rw`. Then two `brw` tests are added. In the first test, 1M of data is sent from the servers to the clients as a simulated read operation with a simple data validation check. In the second test, 4K of data is sent from the clients to the servers as a simulated write operation with a full data validation check.\n\n```\nlst add_batch bulk_rw\nlst add_test --batch bulk_rw --from readers --to servers \\\n  brw read check=simple size=1M\nlst add_test --batch bulk_rw --from writers --to servers \\\n  brw write check=full size=4K\n```\n\nThe traffic pattern and test intensity is determined by several properties such as test type, distribution of test nodes, concurrency of test, and RDMA operation type. For more details, see [*the section called \u201cBatch and Test Commands\u201d*](#batch-and-test-commands).\n\n### Sample Script\n\nThis sample LNet self-test script simulates the traffic pattern of a set of Lustre servers on a TCP network, accessed by Lustre clients on an InfiniBand network (connected via LNet routers). In this example, half the clients are reading and half the clients are writing.\n\nRun this script on the console node:\n\n```\n#!/bin/bash\nexport LST_SESSION=$$\nlst new_session read/write\nlst add_group servers 192.168.10.[8,10,12-16]@tcp\nlst add_group readers 192.168.1.[1-253/2]@o2ib\nlst add_group writers 192.168.1.[2-254/2]@o2ib\nlst add_batch bulk_rw\nlst add_test --batch bulk_rw --from readers --to servers \\\nbrw read check=simple size=1M\nlst add_test --batch bulk_rw --from writers --to servers \\\nbrw write check=full size=4K\n# start running\nlst run bulk_rw\n# display server stats for 30 seconds\nlst stat servers & sleep 30; kill $!\n# tear down\nlst end_session\n```\n\n**Note**\n\nThis script can be easily adapted to pass the group NIDs by shell variables or command line arguments (making it good for general-purpose use).", "mimetype": "text/plain", "start_char_idx": 6623, "end_char_idx": 10528, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69f23c0a-4913-42a8-98dc-e6dc594da9db": {"__data__": {"id_": "69f23c0a-4913-42a8-98dc-e6dc594da9db", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c613bba-1dde-460e-b307-9f46b7238ef5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4a50a04a78fa8ce31f575a96d5a1fb04cd5a4742eb7925c9accdc98d52911711", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29be9a0a-7c85-4056-becb-ff538025c8eb", "node_type": "1", "metadata": {}, "hash": "ab0962aad9e346f151dec00d99e7aa87b0a2504f20535118f4a306e26a5d2365", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[8,10,12-16]@tcp\nlst add_group readers 192.168.1.[1-253/2]@o2ib\nlst add_group writers 192.168.1.[2-254/2]@o2ib\nlst add_batch bulk_rw\nlst add_test --batch bulk_rw --from readers --to servers \\\nbrw read check=simple size=1M\nlst add_test --batch bulk_rw --from writers --to servers \\\nbrw write check=full size=4K\n# start running\nlst run bulk_rw\n# display server stats for 30 seconds\nlst stat servers & sleep 30; kill $!\n# tear down\nlst end_session\n```\n\n**Note**\n\nThis script can be easily adapted to pass the group NIDs by shell variables or command line arguments (making it good for general-purpose use).\n\n## LNet Self-Test Command Reference\n\nThe LNet self-test (`lst`) utility is used to issue LNet self-test commands. The\u00a0`lst`\u00a0utility takes a number of command line arguments. The first argument is the command name and subsequent arguments are command-specific.\n\n### Session Commands\n\nThis section describes `lst` session commands.\n\n**LST_FEATURES**\n\nThe `lst` utility uses the `LST_FEATURES` environmental variable to determine what optional features should be enabled. All features are disabled by default. The supported values for `LST_FEATURES` are:\n\n- **1 -** Enable the Variable Page Size feature for LNet Selftest.\n\nExample:\n\n```\nexport LST_FEATURES=1\n```\n\n**LST_SESSION**\n\nThe `lst` utility uses the `LST_SESSION` environmental variable to identify the session locally on the self-test console node. This should be a numeric value that uniquely identifies all session processes on the node. It is convenient to set this to the process ID of the shell both for interactive use and in shell scripts. Almost all `lst` commands require `LST_SESSION` to be set.\n\nExample:\n\n```\nexport LST_SESSION=$$\n```\n\n**new_session [ --timeout SECONDS ]** **[ -- force ] SESSNAME**\n\nCreates a new session session named *SESSNAME*.\n\n| **Parameter**         | **Description**                                              |\n| --------------------- | ------------------------------------------------------------ |\n| `--timeout *seconds*` | Console timeout value of the session. The session ends automatically if it remains idle (i.e., no commands are issued) for this period. |\n| `--force`             | Ends conflicting sessions. This determines who 'wins' when one session conflicts with another. For example, if there is already an active session on this node, then the attempt to create a new session fails unless the `--force` flag is specified. If the `--force` flag is specified, then the active session is ended. Similarly, if a session attempts to add a node that is already 'owned' by another session, the `--force` flag allows this session to 'steal' the node. |\n| `*name*`              | A human-readable string to print when listing sessions or reporting session conflicts. |\n\n**Example:**\n\n```\n$ lst new_session --force read_write\n```\n\n`end_session`\n\nStops all operations and tests in the current session and clears the session's status.\n\n```\n$ lst end_session\n```\n\n`show_session`\n\nShows the session information. This command prints information about the current session. It does not require LST_SESSION to be defined in the process environment.\n\n```\n$ lst show_session \n```\n\n### Group Commands\n\nThis section describes `lst` group commands.\n\n`add_group *name* *NIDS* [*NIDs*...]`\n\nCreates the group and adds a list of test nodes to the group.\n\n| **Parameter** | **Description**                                              |\n| ------------- | ------------------------------------------------------------ |\n| `*name*`      | Name of the group.                                           |\n| `*NIDs*`      | A string that may be expanded to include one or more LNet NIDs. |\n\n**Example:**\n\n```\n$ lst add_group servers 192.168.10.[35,40-45]@tcp\n$ lst add_group clients 192.168.1.[10-100]@tcp 192.168.[2,4].\\\n  [10-20]@tcp\n```\n\n`update_group *name* [--refresh] [--clean *status*] [--remove *NIDs*]`\n\nUpdates the state of nodes in a group or adjusts a group's membership. This command is useful if some nodes have crashed and should be excluded from the group.", "mimetype": "text/plain", "start_char_idx": 9925, "end_char_idx": 13980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29be9a0a-7c85-4056-becb-ff538025c8eb": {"__data__": {"id_": "29be9a0a-7c85-4056-becb-ff538025c8eb", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69f23c0a-4913-42a8-98dc-e6dc594da9db", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3a7b31f49e2192eddc7f5d442a9800841ccbe94b22b4c1e656fe1ef0c60e27b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af4d7fa8-d26a-4baa-9ed0-be56a752cfa7", "node_type": "1", "metadata": {}, "hash": "2de0fe24c8376795fb6cd9a745452217ee3b0dc52aa84758c4936c6d16bb0297", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| **Parameter** | **Description**                                              |\n| ------------- | ------------------------------------------------------------ |\n| `*name*`      | Name of the group.                                           |\n| `*NIDs*`      | A string that may be expanded to include one or more LNet NIDs. |\n\n**Example:**\n\n```\n$ lst add_group servers 192.168.10.[35,40-45]@tcp\n$ lst add_group clients 192.168.1.[10-100]@tcp 192.168.[2,4].\\\n  [10-20]@tcp\n```\n\n`update_group *name* [--refresh] [--clean *status*] [--remove *NIDs*]`\n\nUpdates the state of nodes in a group or adjusts a group's membership. This command is useful if some nodes have crashed and should be excluded from the group.\n\n| **Parameter**      | **Description**                                              |                                             |\n| ------------------ | ------------------------------------------------------------ | ------------------------------------------- |\n| `--refresh`        | Refreshes the state of all inactive nodes in the group.      |                                             |\n| `--clean *status*` | Removes nodes with a specified status from the group. Status may be: |                                             |\n|                    | active                                                       | The node is in the current session.         |\n|                    | busy                                                         | The node is now owned by another session.   |\n|                    | down                                                         | The node has been marked down.              |\n|                    | unknown                                                      | The node's status has yet to be determined. |\n|                    | invalid                                                      | Any state but active.                       |\n| `--remove *NIDs*`  | Removes specified nodes from the group.                      |                                             |\n\n**Example:**\n\n```\n$ lst update_group clients --refresh\n$ lst update_group clients --clean busy\n$ lst update_group clients --clean invalid // \\\n  invalid == busy || down || unknown\n$ lst update_group clients --remove \\192.168.1.[10-20]@tcp\n```\n\n`list_group [*name*] [--active] [--busy] [--down] [--unknown] [--all]`\n\nPrints information about a group or lists all groups in the current session if no group is specified.\n\n| **Parameter** | **Description**         |\n| ------------- | ----------------------- |\n| `*name*`      | The name of the group.  |\n| `--active`    | Lists the active nodes. |\n| `--busy`      | Lists the busy nodes.   |\n| `--down`      | Lists the down nodes.   |\n| `--unknown`   | Lists unknown nodes.    |\n| `--all`       | Lists all nodes.        |\n\nExample:\n\n```\n$ lst list_group\n1) clients\n2) servers\nTotal 2 groups\n$ lst list_group clients\nACTIVE BUSY DOWN UNKNOWN TOTAL\n3 1 2 0 6\n$ lst list_group clients --all\n192.168.1.10@tcp Active\n192.168.1.11@tcp Active\n192.168.1.12@tcp Busy\n192.168.1.13@tcp Active\n192.168.1.14@tcp DOWN\n192.168.1.15@tcp DOWN\nTotal 6 nodes\n$ lst list_group clients --busy\n192.168.1.12@tcp Busy\nTotal 1 node\n```\n\n`del_group *name*`\n\nRemoves a group from the session. If the group is referred to by any test, then the operation fails. If nodes in the group are referred to only by this group, then they are kicked out from the current session; otherwise, they are still in the current session.\n\n```\n$ lst del_group clients\n```\n\n`lstclient --sesid *NID* --group *name* [--server_mode]`\n\nUse `lstclient` to run the userland self-test client. The `lstclient` command should be executed after creating a session on the console. There are only two mandatory options for `lstclient`:\n\n| **Parameter**    | **Description**                                              |\n| ---------------- | ------------------------------------------------------------ |\n| `--sesid *NID*`  | The first console's NID.                                     |\n| `--group *name*` | The test group to join.                                      |\n| `--server_mode`  | When included, forces LNet to behave as a server, such as starting an acceptor if the underlying NID needs it or using privileged ports. Only root is allowed to use the `--server_mode` option.", "mimetype": "text/plain", "start_char_idx": 13271, "end_char_idx": 17586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af4d7fa8-d26a-4baa-9ed0-be56a752cfa7": {"__data__": {"id_": "af4d7fa8-d26a-4baa-9ed0-be56a752cfa7", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29be9a0a-7c85-4056-becb-ff538025c8eb", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da6e5cdaad33d7a78940b7dc9aa6dc23edd926429413647a3213c831095e8027", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a8d8bc9-fe55-4333-b713-daa2a965a34e", "node_type": "1", "metadata": {}, "hash": "961cf252554fdac00896fe7e2c746de7c5166a9a720ed67f885e501c3bdbbb64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\n$ lst del_group clients\n```\n\n`lstclient --sesid *NID* --group *name* [--server_mode]`\n\nUse `lstclient` to run the userland self-test client. The `lstclient` command should be executed after creating a session on the console. There are only two mandatory options for `lstclient`:\n\n| **Parameter**    | **Description**                                              |\n| ---------------- | ------------------------------------------------------------ |\n| `--sesid *NID*`  | The first console's NID.                                     |\n| `--group *name*` | The test group to join.                                      |\n| `--server_mode`  | When included, forces LNet to behave as a server, such as starting an acceptor if the underlying NID needs it or using privileged ports. Only root is allowed to use the `--server_mode` option. |\n\n**Example:**\n\n```\nConsole $ lst new_session testsession\nClient1 $ lstclient --sesid 192.168.1.52@tcp --group clients\n```\n\n**Example:**\n\n```\nClient1 $ lstclient --sesid 192.168.1.52@tcp |--group clients --server_mode\n```\n\n### Batch and Test Commands\n\nThis section describes `lst` batch and test commands.\n\n`add_batch *name*`\n\nA default batch test set named batch is created when the session is started. You can specify a batch name by using `add_batch`:\n\n```\n$ lst add_batch bulkperf\n```\n\nCreates a batch test called `bulkperf`.\n\n```\nadd_test --batch batchname [--loop loop_count] [--concurrency active_count] [--distribute source_count:sink_count] \\\n         --from group --to group brw|ping test_options\n        \n```\n\nAdds a test to a batch. The parameters are described below.\n\n| **Parameter**                              | **Description**                                              |\n| ------------------------------------------ | ------------------------------------------------------------ |\n| `--batch *batchname*`                      | Names a group of tests for later execution.                  |\n| `--loop *loop_count*`                      | Number of times to run the test.                             |\n| `--concurrency *active_count*`             | The number of requests that are active at one time.          |\n| `--distribute *source_count*:*sink_count*` | Determines the ratio of client nodes to server nodes for the specified test. This allows you to specify a wide range of topologies, including one-to-one and all-to-all. Distribution divides the source group into subsets, which are paired with equivalent subsets from the target group so only nodes in matching subsets communicate. |\n| `--from *group*`                           | The source group (test client).                              |\n| `--to *group*`                             | The target group (test server).                              |\n| `ping`                                     | Sends a small request message, resulting in a small reply message. For more details, see [*the section called \u201cDefining and Running the Tests\u201d*](#defining-and-running-the-tests). `ping` does not have any additional options. |\n| `brw`                                      | Sends a small request message followed by a bulk data transfer, resulting in a small reply message. [*the section called \u201cDefining and Running the Tests\u201d*](#defining-and-running-the-tests). Options are: |\n| `read | write`                             | Read or write. The default is read.                          |\n| `size=*bytes[KM]*`                         | I/O size in bytes, kilobytes, or Megabytes (i.e., `size=1024`, `size=4K`, `size=1M`). The default is 4 kilobytes. |\n| `check=full|simple`                        | A data validation check (checksum of data). The default is that no check is done.", "mimetype": "text/plain", "start_char_idx": 16753, "end_char_idx": 20446, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a8d8bc9-fe55-4333-b713-daa2a965a34e": {"__data__": {"id_": "2a8d8bc9-fe55-4333-b713-daa2a965a34e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af4d7fa8-d26a-4baa-9ed0-be56a752cfa7", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "88b6eed89e67dc6f8c980dd2ec5d134070881db34f5db2ff6cbe2fee8d0f5228", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "492b0191-9870-4e04-9a6d-ef25b9b490d5", "node_type": "1", "metadata": {}, "hash": "24ee7390304c50d1711884ef56f95335718c561bec5a96910956f8c615e94951", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "`ping` does not have any additional options. |\n| `brw`                                      | Sends a small request message followed by a bulk data transfer, resulting in a small reply message. [*the section called \u201cDefining and Running the Tests\u201d*](#defining-and-running-the-tests). Options are: |\n| `read | write`                             | Read or write. The default is read.                          |\n| `size=*bytes[KM]*`                         | I/O size in bytes, kilobytes, or Megabytes (i.e., `size=1024`, `size=4K`, `size=1M`). The default is 4 kilobytes. |\n| `check=full|simple`                        | A data validation check (checksum of data). The default is that no check is done. |\n\n**Examples showing use of the distribute parameter:**\n\n```\nClients: (C1, C2, C3, C4, C5, C6)\nServer: (S1, S2, S3)\n--distribute 1:1 (C1->S1), (C2->S2), (C3->S3), (C4->S1), (C5->S2),\n\\(C6->S3) /* -> means test conversation */ --distribute 2:1 (C1,C2->S1), (C3,C4->S2), (C5,C6->S3)\n--distribute 3:1 (C1,C2,C3->S1), (C4,C5,C6->S2), (NULL->S3)\n--distribute 3:2 (C1,C2,C3->S1,S2), (C4,C5,C6->S3,S1)\n--distribute 4:1 (C1,C2,C3,C4->S1), (C5,C6->S2), (NULL->S3)\n--distribute 4:2 (C1,C2,C3,C4->S1,S2), (C5, C6->S3, S1)\n--distribute 6:3 (C1,C2,C3,C4,C5,C6->S1,S2,S3)\n```\n\nThe setting `--distribute 1:1` is the default setting where each source node communicates with one target node.\n\nWhen the setting `--distribute 1: *n*` (where `*n*` is the size of the target group) is used, each source node communicates with every node in the target group.\n\nNote that if there are more source nodes than target nodes, some source nodes may share the same target nodes. Also, if there are more target nodes than source nodes, some higher-ranked target nodes will be idle.\n\n**Example showing a brw test:**\n\n```\n$ lst add_group clients 192.168.1.[10-17]@tcp\n$ lst add_group servers 192.168.10.[100-103]@tcp\n$ lst add_batch bulkperf\n$ lst add_test --batch bulkperf --loop 100 --concurrency 4 \\\n  --distribute 4:2 --from clients brw WRITE size=16K\n```\n\nIn the example above, a batch test called bulkperf that will do a 16 kbyte bulk write request. In this test, two groups of four clients (sources) write to each of four servers (targets) as shown below:\n\n- `192.168.1.[10-13]` will write to `192.168.10.[100,101]`\n- `192.168.1.[14-17]` will write to `192.168.10.[102,103]`\n\n**list_batch [name]** **[--test index]** **[--active]** **[--invalid]** **[--server|client]**\n\nLists batches in the current session or lists client and server nodes in a batch or a test.\n\n| **Parameter**     | **Description**                                              |\n| ----------------- | ------------------------------------------------------------ |\n| `--test *index*`  | Lists tests in a batch. If no option is used, all tests in the batch are listed. If one of these options are used, only specified tests in the batch are listed: |\n| `active`          | Lists only active batch tests.                               |\n| `invalid`         | Lists only invalid batch tests.                              |\n| `server | client` | Lists client and server nodes in a batch test.", "mimetype": "text/plain", "start_char_idx": 19746, "end_char_idx": 22879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "492b0191-9870-4e04-9a6d-ef25b9b490d5": {"__data__": {"id_": "492b0191-9870-4e04-9a6d-ef25b9b490d5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a8d8bc9-fe55-4333-b713-daa2a965a34e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2965385614d79ef27494371718015fc5a1623a87e50114547b321a250c2f09c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f01f6609-2e15-4b2b-90cd-ac68bbd9c800", "node_type": "1", "metadata": {}, "hash": "7b69924d917c560fb6f23fda4ab47cc58b004d4d7c59dabbfefee12950fcdf26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[100,101]`\n- `192.168.1.[14-17]` will write to `192.168.10.[102,103]`\n\n**list_batch [name]** **[--test index]** **[--active]** **[--invalid]** **[--server|client]**\n\nLists batches in the current session or lists client and server nodes in a batch or a test.\n\n| **Parameter**     | **Description**                                              |\n| ----------------- | ------------------------------------------------------------ |\n| `--test *index*`  | Lists tests in a batch. If no option is used, all tests in the batch are listed. If one of these options are used, only specified tests in the batch are listed: |\n| `active`          | Lists only active batch tests.                               |\n| `invalid`         | Lists only invalid batch tests.                              |\n| `server | client` | Lists client and server nodes in a batch test.               |\n\n**Example:**\n\n```\n$ lst list_batchbulkperf\n$ lst list_batch bulkperf\nBatch: bulkperf Tests: 1 State: Idle\nACTIVE BUSY DOWN UNKNOWN TOTAL\nclient 8 0 0 0 8\nserver 4 0 0 0 4\nTest 1(brw) (loop: 100, concurrency: 4)\nACTIVE BUSY DOWN UNKNOWN TOTAL\nclient 8 0 0 0 8\nserver 4 0 0 0 4\n$ lst list_batch bulkperf --server --active\n192.168.10.100@tcp Active\n192.168.10.101@tcp Active\n192.168.10.102@tcp Active\n192.168.10.103@tcp Active\n```\n\n`run *name*`\n\nRuns the batch.\n\n```\n$ lst run bulkperf\n\n```\n\n`stop *name*`\n\nStops the batch.\n\n```\n$ lst stop bulkperf\n\n```\n\n**query name [--test index]** **[--timeout seconds]** **[--loop loopcount]** **[--delay seconds]** **[--all]**\n\nQueries the batch status.\n\n| **Parameter**         | **Description**                                              |\n| --------------------- | ------------------------------------------------------------ |\n| `--test *index*`      | Only queries the specified test. The test index starts from 1. |\n| `--timeout *seconds*` | The timeout value to wait for RPC. The default is 5 seconds. |\n| `--loop *#*`          | The loop count of the query.                                 |\n| `--delay *seconds*`   | The interval of each query. The default is 5 seconds.        |\n| `--all`               | The list status of all nodes in a batch or a test.           |\n\n**Example:**\n\n```\n$ lst run bulkperf\n$ lst query bulkperf --loop 5 --delay 3\nBatch is running\nBatch is running\nBatch is running\nBatch is running\nBatch is running\n$ lst query bulkperf --all\n192.168.1.10@tcp Running\n192.168.1.11@tcp Running\n192.168.1.12@tcp Running\n192.168.1.13@tcp Running\n192.168.1.14@tcp Running\n192.168.1.15@tcp Running\n192.168.1.16@tcp Running\n192.168.1.17@tcp Running\n$ lst stop bulkperf\n$ lst query bulkperf\nBatch is idle\n\n```\n\n### Other Commands\n\nThis section describes other `lst` commands.\n\n`ping [-session] [--group *name*] [--nodes *NIDs*] [--batch *name*] [--server] [--timeout *seconds*]`\n\nSends a 'hello' query to the nodes.\n\n| **Parameter**         | **Description**                                              |\n| --------------------- | ------------------------------------------------------------ |\n| `--session`           | Pings all nodes in the current session.                      |\n| `--group *name*`      | Pings all nodes in a specified group.                        |\n| `--nodes *NIDs*`      | Pings all specified nodes.                                   |\n| `--batch *name*`      | Pings all client nodes in a batch.                           |\n| `--server`            | Sends RPC to all server nodes instead of client nodes. This option is only used with `--batch *name*`. |\n| `--timeout *seconds*` | The RPC timeout value.                                       |\n\n**Example:**\n\n```\n# lst ping 192.168.10.", "mimetype": "text/plain", "start_char_idx": 22027, "end_char_idx": 25665, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f01f6609-2e15-4b2b-90cd-ac68bbd9c800": {"__data__": {"id_": "f01f6609-2e15-4b2b-90cd-ac68bbd9c800", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "492b0191-9870-4e04-9a6d-ef25b9b490d5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0daae90ae677b1b6b60c1cb84b3db6d23d28022129c1cbba6aad52db190c2178", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c781917-1bea-42f6-ba8a-619a13dcd7ad", "node_type": "1", "metadata": {}, "hash": "e5f198802fb81921ac98ed7ca5736275cfb1eeb9ab1f373ef78d23a2f474339e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| **Parameter**         | **Description**                                              |\n| --------------------- | ------------------------------------------------------------ |\n| `--session`           | Pings all nodes in the current session.                      |\n| `--group *name*`      | Pings all nodes in a specified group.                        |\n| `--nodes *NIDs*`      | Pings all specified nodes.                                   |\n| `--batch *name*`      | Pings all client nodes in a batch.                           |\n| `--server`            | Sends RPC to all server nodes instead of client nodes. This option is only used with `--batch *name*`. |\n| `--timeout *seconds*` | The RPC timeout value.                                       |\n\n**Example:**\n\n```\n# lst ping 192.168.10.[15-20]@tcp\n192.168.1.15@tcp Active [session: liang id: 192.168.1.3@tcp]\n192.168.1.16@tcp Active [session: liang id: 192.168.1.3@tcp]\n192.168.1.17@tcp Active [session: liang id: 192.168.1.3@tcp]\n192.168.1.18@tcp Busy [session: Isaac id: 192.168.10.10@tcp]\n192.168.1.19@tcp Down [session: <NULL> id: LNET_NID_ANY]\n192.168.1.20@tcp Down [session: <NULL> id: LNET_NID_ANY]\n\n```\n\n`stat [--bw] [--rate] [--read] [--write] [--max] [--min] [--avg] \" \" [--timeout *seconds*] [--delay *seconds*] *group|NIDs* [*group|NIDs*]`\n\nThe collection performance and RPC statistics of one or more nodes.\n\n| **Parameter**         | **Description**                                              |\n| --------------------- | ------------------------------------------------------------ |\n| `--bw`                | Displays the bandwidth of the specified group/nodes.         |\n| `--rate`              | Displays the rate of RPCs of the specified group/nodes.      |\n| `--read`              | Displays the read statistics of the specified group/nodes.   |\n| `--write`             | Displays the write statistics of the specified group/nodes.  |\n| `--max`               | Displays the maximum value of the statistics.                |\n| `--min`               | Displays the minimum value of the statistics.                |\n| `--avg`               | Displays the average of the statistics.                      |\n| `--timeout *seconds*` | The timeout of the statistics RPC. The default is 5 seconds. |\n| `--delay *seconds*`   | The interval of the statistics (in seconds).                 |\n\n**Example:**\n\n```\n$ lst run bulkperf\n$ lst stat clients\n[LNet Rates of clients]\n[W] Avg: 1108 RPC/s Min: 1060 RPC/s Max: 1155 RPC/s\n[R] Avg: 2215 RPC/s Min: 2121 RPC/s Max: 2310 RPC/s\n[LNet Bandwidth of clients]\n[W] Avg: 16.60 MB/s Min: 16.10 MB/s Max: 17.1 MB/s\n[R] Avg: 40.49 MB/s Min: 40.30 MB/s Max: 40.68 MB/s\n\n```\n\nSpecifying a group name ( *group* ) causes statistics to be gathered for all nodes in a test group. For example:\n\n```\n$ lst stat servers\n\n```\n\nwhere servers is the name of a test group created by `lst add_group`\n\nSpecifying a `*NID*` range (`*NIDs*`) causes statistics to be gathered for selected nodes. For example:\n\n```\n$ lst stat 192.168.0.[1-100/2]@tcp\n\n```\n\nOnly LNet performance statistics are available. By default, all statistics information is displayed. Users can specify additional information with these options.\n\n`show_error [--session] [*group*|*NIDs*]...`\n\nLists the number of failed RPCs on test nodes.\n\n| **Parameter** | **Description**                                              |\n| ------------- | ------------------------------------------------------------ |\n| `--session`   | Lists errors in the current test session. With this option, historical RPC errors are not listed.", "mimetype": "text/plain", "start_char_idx": 24870, "end_char_idx": 28449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c781917-1bea-42f6-ba8a-619a13dcd7ad": {"__data__": {"id_": "9c781917-1bea-42f6-ba8a-619a13dcd7ad", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d617e61-6c89-4bb9-a949-0006b8fd89f5", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4de2cfc4f3fbb1f971ef14f28aa6e5bd6ead2deb741763d0f7969980f2b07f42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f01f6609-2e15-4b2b-90cd-ac68bbd9c800", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b0e95506ad991ddf9885fb6259bfac2f1e91edd2bdedb512143a2fbcf9172234", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example:\n\n```\n$ lst stat servers\n\n```\n\nwhere servers is the name of a test group created by `lst add_group`\n\nSpecifying a `*NID*` range (`*NIDs*`) causes statistics to be gathered for selected nodes. For example:\n\n```\n$ lst stat 192.168.0.[1-100/2]@tcp\n\n```\n\nOnly LNet performance statistics are available. By default, all statistics information is displayed. Users can specify additional information with these options.\n\n`show_error [--session] [*group*|*NIDs*]...`\n\nLists the number of failed RPCs on test nodes.\n\n| **Parameter** | **Description**                                              |\n| ------------- | ------------------------------------------------------------ |\n| `--session`   | Lists errors in the current test session. With this option, historical RPC errors are not listed. |\n\n**Example:**\n\n```\n$ lst show_error client\nsclients\n12345-192.168.1.15@tcp: [Session: 1 brw errors, 0 ping errors] \\\n  [RPC: 20 errors, 0 dropped,\n12345-192.168.1.16@tcp: [Session: 0 brw errors, 0 ping errors] \\\n  [RPC: 1 errors, 0 dropped, Total 2 error nodes in clients\n$ lst show_error --session clients\nclients\n12345-192.168.1.15@tcp: [Session: 1 brw errors, 0 ping errors]\nTotal 1 error nodes in clients\n\n```", "mimetype": "text/plain", "start_char_idx": 27652, "end_char_idx": 28865, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "464a55e4-48af-405c-a8d6-ec93289db24f": {"__data__": {"id_": "464a55e4-48af-405c-a8d6-ec93289db24f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb42bff2-8e2c-4867-9311-c53e6ffa5aed", "node_type": "1", "metadata": {}, "hash": "93288c1c32ec2d35a0ee7c89d55d8a5359ba681bbcaca08480d01f232038232d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Benchmarking Lustre File System Performance (Lustre I/O Kit)\n\n- [Benchmarking Lustre File System Performance (Lustre I/O Kit)](#benchmarking-lustre-file-system-performance-lustre-io-kit)\n  * [Using Lustre I/O Kit Tools](#using-lustre-io-kit-tools)\n    + [Contents of the Lustre I/O Kit](#contents-of-the-lustre-io-kit)\n    + [Preparing to Use the Lustre I/O Kit](#preparing-to-use-the-lustre-io-kit)\n  * [Testing I/O Performance of Raw Hardware (`sgpdd-survey`)](#testing-io-performance-of-raw-hardware-sgpdd-survey)\n    + [Tuning Linux Storage Devices](#tuning-linux-storage-devices)\n    + [Running sgpdd-survey](#running-sgpdd-survey)\n  * [Testing OST Performance (`obdfilter-survey`)](#testing-ost-performance-obdfilter-survey)\n    + [Testing Local Disk Performance](#testing-local-disk-performance)\n    + [Testing Network Performance](#testing-network-performance)\n    + [Testing Remote Disk Performance](#testing-remote-disk-performance)\n    + [Output Files](#output-files)\n      - [Script Output](#script-output)\n      - [Visualizing Results](#visualizing-results)\n  * [Testing OST I/O Performance (`ost-survey`)](#testing-ost-io-performance-ost-survey)\n  * [Testing MDS Performance (`mds-survey`)](#testing-mds-performance-mds-survey)\n    + [Output Files](#output-files-1)\n    + [Script Output](#script-output-1)\n  * [Collecting Application Profiling Information (`stats-collect`)](#collecting-application-profiling-information-stats-collect)\n    + [Using `stats-collect`](#using-stats-collect)\n\nThis chapter describes the Lustre I/O kit, a collection of I/O benchmarking tools for a Lustre cluster. It includes:\n\n- [the section called \u201c Using Lustre I/O Kit Tools\u201d](#using-lustre-io-kit-tools)\n- [the section called \u201cTesting I/O Performance of Raw Hardware (sgpdd-survey)\u201d](#testing-io-performance-of-raw-hardware-sgpdd-survey)\n- [the section called \u201cTesting OST Performance (obdfilter-survey)\u201d](#testing-ost-performance-obdfilter-survey)\n- [the section called \u201cTesting OST I/O Performance (ost-survey)\u201d](#testing-ost-io-performance-ost-survey)\n- [the section called \u201cTesting MDS Performance (mds-survey)\u201d](#testing-mds-performance-mds-survey)\n- [the section called \u201cCollecting Application Profiling Information (stats-collect)\u201d](#collecting-application-profiling-information-stats-collect)\n\n## Using Lustre I/O Kit Tools\n\nThe tools in the Lustre I/O Kit are used to benchmark Lustre file system hardware and validate that it is working as expected before you install the Lustre software. It can also be used to to validate the performance of the various hardware and software layers in the cluster and also to find and troubleshoot I/O issues.\n\nTypically, performance is measured starting with single raw devices and then proceeding to groups of devices. Once raw performance has been established, other software layers are then added incrementally and tested.\n\n### Contents of the Lustre I/O Kit\n\nThe I/O kit contains three tests, each of which tests a progressively higher layer in the Lustre software stack:\n\n- `sgpdd-survey` - Measure basic 'bare metal' performance of devices while bypassing the kernel block device layers, buffer cache, and file system.\n- `obdfilter-survey` - Measure the performance of one or more OSTs directly on the OSS node or alternately over the network from a Lustre client.\n- `ost-survey` - Performs I/O against OSTs individually to allow performance comparisons to detect if an OST is performing sub-optimally due to hardware issues.\n\nTypically with these tests, a Lustre file system should deliver 85-90% of the raw device performance.\n\nA utility `stats-collect` is also provided to collect application profiling information from Lustre clients and servers. See [*the section called \u201cCollecting Application Profiling Information (`stats-collect`)\u201d*](#collecting-application-profiling-information-stats-collect) for more information.\n\n### Preparing to Use the Lustre I/O Kit\n\nThe following prerequisites must be met to use the tests in the Lustre I/O kit:\n\n- Password-free remote access to nodes in the system (provided by `ssh` or `rsh`).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4083, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb42bff2-8e2c-4867-9311-c53e6ffa5aed": {"__data__": {"id_": "bb42bff2-8e2c-4867-9311-c53e6ffa5aed", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "464a55e4-48af-405c-a8d6-ec93289db24f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0a866dc2b9d784b70a438f3ca3477b09ce4d58b4d845306a44d44283c97d481b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e31f2ba0-fdb2-4dbb-9e16-a62dc367f0cc", "node_type": "1", "metadata": {}, "hash": "2a83776a8bf0207d598a2792ac915555c25c91c39c208249e5820ae6ce9c8b02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `ost-survey` - Performs I/O against OSTs individually to allow performance comparisons to detect if an OST is performing sub-optimally due to hardware issues.\n\nTypically with these tests, a Lustre file system should deliver 85-90% of the raw device performance.\n\nA utility `stats-collect` is also provided to collect application profiling information from Lustre clients and servers. See [*the section called \u201cCollecting Application Profiling Information (`stats-collect`)\u201d*](#collecting-application-profiling-information-stats-collect) for more information.\n\n### Preparing to Use the Lustre I/O Kit\n\nThe following prerequisites must be met to use the tests in the Lustre I/O kit:\n\n- Password-free remote access to nodes in the system (provided by `ssh` or `rsh`).\n- LNet self-test completed to test that Lustre networking has been properly installed and configured. See [*Testing Lustre Network Performance (LNet Self-Test)*](04.01-Testing%20Lustre%20Network%20Performance%20(LNet%20Self-Test).md).\n- Lustre file system software installed.\n- `sg3_utils` package providing the `sgp_dd` tool (`sg3_utils` is a separate RPM package available online using YUM).\n\nDownload the Lustre I/O kit (`lustre-iokit`)from:\n\n<http://downloads.whamcloud.com/>\n\n## Testing I/O Performance of Raw Hardware (`sgpdd-survey`)\n\nThe `sgpdd-survey` tool is used to test bare metal I/O performance of the raw hardware, while bypassing as much of the kernel as possible. This survey may be used to characterize the performance of a SCSI device by simulating an OST serving multiple stripe files. The data gathered by this survey can help set expectations for the performance of a Lustre OST using this device.\n\nThe script uses `sgp_dd` to carry out raw sequential disk I/O. It runs with variable numbers of sgp_dd threads to show how performance varies with different request queue depths.\n\nThe script spawns variable numbers of `sgp_dd` instances, each reading or writing a separate area of the disk to demonstrate performance variance within a number of concurrent stripe files.\n\nSeveral tips and insights for disk performance measurement are described below. Some of this information is specific to RAID arrays and/or the Linux RAID implementation.\n\n- *Performance is limited by the slowest disk.*\n\n  Before creating a RAID array, benchmark all disks individually. We have frequently encountered situations where drive performance was not consistent for all devices in the array. Replace any disks that are significantly slower than the rest.\n\n- *Disks and arrays are very sensitive to request size.*\n\n  To identify the optimal request size for a given disk, benchmark the disk with different record sizes ranging from 4 KB to 1 to 2 MB.\n\n**Caution**\n\nThe `sgpdd-survey` script overwrites the device being tested, which results in the **\\*LOSS OF ALL DATA*** on that device. Exercise caution when selecting the device to be tested.\n\n**Note**\n\nArray performance with all LUNs loaded does not always match the performance of a single LUN when tested in isolation.\n\n**Prerequisites:**\n\n- `sgp_dd` tool in the `sg3_utils` package\n- Lustre software is *NOT* required\n\nThe device(s) being tested must meet one of these two requirements:\n\n- If the device is a SCSI device, it must appear in the output of `sg_map` (make sure the kernel module `sg` is loaded).\n- If the device is a raw device, it must appear in the output of `raw -qa`.\n\nRaw and SCSI devices cannot be mixed in the test specification.\n\n**Note**\n\nIf you need to create raw devices to use the `sgpdd-survey` tool, note that raw device 0 cannot be used due to a bug in certain versions of the \"raw\" utility (including the version shipped with Red Hat Enterprise Linux 4U4.)\n\n### Tuning Linux Storage Devices\n\nTo get large I/O transfers (1 MB) to disk, it may be necessary to tune several kernel parameters as specified:\n\n```\n/sys/block/sdN/queue/max_sectors_kb = 4096\n/sys/block/sdN/queue/max_phys_segments = 256\n/proc/scsi/sg/allow_dio = 1\n/sys/module/ib_srp/parameters/srp_sg_tablesize = 255\n/sys/block/sdN/queue/scheduler\n```\n\n**Note**\n\nRecommended schedulers are **deadline** and **noop**. The scheduler is set by default to **deadline**, unless it has already been set to **noop**.", "mimetype": "text/plain", "start_char_idx": 3317, "end_char_idx": 7539, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e31f2ba0-fdb2-4dbb-9e16-a62dc367f0cc": {"__data__": {"id_": "e31f2ba0-fdb2-4dbb-9e16-a62dc367f0cc", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb42bff2-8e2c-4867-9311-c53e6ffa5aed", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "96cebdb481ae49d8b5a8673a00b832c151e174a67d34e6a521f0efd891696ddc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c41aa1c-442c-4a2a-84cf-8cd0800036c8", "node_type": "1", "metadata": {}, "hash": "774c96890bd97c87bd8c0da7d3f3a800957cf6895b0905d91a138647ab8cc4bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Tuning Linux Storage Devices\n\nTo get large I/O transfers (1 MB) to disk, it may be necessary to tune several kernel parameters as specified:\n\n```\n/sys/block/sdN/queue/max_sectors_kb = 4096\n/sys/block/sdN/queue/max_phys_segments = 256\n/proc/scsi/sg/allow_dio = 1\n/sys/module/ib_srp/parameters/srp_sg_tablesize = 255\n/sys/block/sdN/queue/scheduler\n```\n\n**Note**\n\nRecommended schedulers are **deadline** and **noop**. The scheduler is set by default to **deadline**, unless it has already been set to **noop**.\n\n### Running sgpdd-survey\n\nThe `sgpdd-survey` script must be customized for the particular device being tested and for the location where the script saves its working and result files (by specifying the `${rslt}` variable). Customization variables are described at the beginning of the script.\n\nWhen the `sgpdd-survey` script runs, it creates a number of working files and a pair of result files. The names of all the files created start with the prefix defined in the variable `${rslt}`. (The default value is `/tmp`.) The files include:\n\n- File containing standard output data (same as `stdout`)\n\n  ```\n  rslt_date_time.summary\n  ```\n\n- Temporary (tmp) files\n\n  ```\n  rslt_date_time_*\n  ```\n\n- Collected tmp files for post-mortem\n\n  ```\n  rslt_date_time.detail\n  ```\n\nThe `stdout` and the `.summary` file will contain lines like this:\n\n```\ntotal_size 8388608K rsz 1024 thr 1 crg 1 180.45 MB/s 1 x 180.50 \\\n        = 180.50 MB/s\n```\n\nEach line corresponds to a run of the test. Each test run will have a different number of threads, record size, or number of regions.\n\n- `total_size` - Size of file being tested in KBs (8 GB in above example).\n- `rsz` - Record size in KBs (1 MB in above example).\n- `thr` - Number of threads generating I/O (1 thread in above example).\n- `crg` - Current regions, the number of disjoint areas on the disk to which I/O is being sent (1 region in above example, indicating that no seeking is done).\n- `MB/s` - Aggregate bandwidth measured by dividing the total amount of data by the elapsed time (180.45 MB/s in the above example).\n- `MB/s` - The remaining numbers show the number of regions X performance of the slowest disk as a sanity check on the aggregate bandwidth.\n\nIf there are so many threads that the `sgp_dd` script is unlikely to be able to allocate I/O buffers, then `ENOMEM` is printed in place of the aggregate bandwidth result.\n\nIf one or more `sgp_dd` instances do not successfully report a bandwidth number, then `FAILED` is printed in place of the aggregate bandwidth result.\n\n## Testing OST Performance (`obdfilter-survey`)\n\nThe `obdfilter-survey` script generates sequential I/O from varying numbers of threads and objects (files) to simulate the I/O patterns of a Lustre client.\n\nThe `obdfilter-survey` script can be run directly on the OSS node to measure the OST storage performance without any intervening network, or it can be run remotely on a Lustre client to measure the OST performance including network overhead.\n\nThe `obdfilter-survey` is used to characterize the performance of the following:\n\n- **Local file system** - In this mode, the `obdfilter-survey` script exercises one or more instances of the obdfilter directly. The script may run on one or more OSS nodes, for example, when the OSSs are all attached to the same multi-ported disk subsystem.\n\n  Run the script using the `case=disk` parameter to run the test against all the local OSTs. The script automatically detects all local OSTs and includes them in the survey.\n\n  To run the test against only specific OSTs, run the script using the `targets=parameter` to list the OSTs to be tested explicitly. If some OSTs are on remote nodes, specify their hostnames in addition to the OST name (for example, `oss2:lustre-OST0004`).\n\n  All `obdfilter` instances are driven directly.", "mimetype": "text/plain", "start_char_idx": 7028, "end_char_idx": 10840, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c41aa1c-442c-4a2a-84cf-8cd0800036c8": {"__data__": {"id_": "2c41aa1c-442c-4a2a-84cf-8cd0800036c8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e31f2ba0-fdb2-4dbb-9e16-a62dc367f0cc", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fad0fa3ad80e5ffe2419409a7e1d02c8c2214f4c98ec5a2cd825c2ce572bdd04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3376098-ec3d-4c5b-9f7e-10a1b51a3d52", "node_type": "1", "metadata": {}, "hash": "a9d0a9ebed0a7638ff52b1c676f3ddf26cd707884dda5bffaf83d2ad99fc5edd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The `obdfilter-survey` is used to characterize the performance of the following:\n\n- **Local file system** - In this mode, the `obdfilter-survey` script exercises one or more instances of the obdfilter directly. The script may run on one or more OSS nodes, for example, when the OSSs are all attached to the same multi-ported disk subsystem.\n\n  Run the script using the `case=disk` parameter to run the test against all the local OSTs. The script automatically detects all local OSTs and includes them in the survey.\n\n  To run the test against only specific OSTs, run the script using the `targets=parameter` to list the OSTs to be tested explicitly. If some OSTs are on remote nodes, specify their hostnames in addition to the OST name (for example, `oss2:lustre-OST0004`).\n\n  All `obdfilter` instances are driven directly. The script automatically loads the `obdecho` module (if required) and creates one instance of `echo_client` for each `obdfilter` instance in order to generate I/O requests directly to the OST.\n\n  For more details, see [*the section called \u201cTesting Local Disk Performance\u201d*](#testing-local-disk-performance).\n\n- **Network** - In this mode, the Lustre client generates I/O requests over the network but these requests are not sent to the OST file system. The OSS node runs the obdecho server to receive the requests but discards them before they are sent to the disk.\n\n  Pass the parameters `case=network` and `targets=*hostname|IP_of_server*` to the script. For each network case, the script does the required setup.\n\n  For more details, see [*the section called \u201cTesting Network Performance\u201d*](#testing-network-performance).\n\n- **Remote file system over the network** - In this mode the `obdfilter-survey` script generates I/O from a Lustre client to a remote OSS to write the data to the file system.\n\n  To run the test against all the local OSCs, pass the parameter `case=netdisk` to the script. Alternately you can pass the target= parameter with one or more OSC devices (e.g., `lustre-OST0000-osc-ffff88007754bc00`) against which the tests are to be run.\n\n  For more details, see [*the section called \u201cTesting Remote Disk Performance\u201d*](#testing-remote-disk-performance).\n\n**Caution**\n\nThe `obdfilter-survey` script is potentially destructive and there is a small risk data may be lost. To reduce this risk, `obdfilter-survey` should not be run on devices that contain data that needs to be preserved. Thus, the best time to run `obdfilter-survey` is before the Lustre file system is put into production. The reason `obdfilter-survey` may be safe to run on a production file system is because it creates objects with object sequence 2. Normal file system objects are typically created with object sequence 0.\n\n**Note**\n\nIf the `obdfilter-survey` test is terminated before it completes, some small amount of space is leaked. you can either ignore it or reformat the file system.\n\n**Note**\n\nThe `obdfilter-survey` script is *NOT* scalable beyond tens of OSTs since it is only intended to measure the I/O performance of individual storage subsystems, not the scalability of the entire system.\n\n**Note**\n\nThe `obdfilter-survey` script must be customized, depending on the components under test and where the script's working files should be kept. Customization variables are described at the beginning of the `obdfilter-survey` script. In particular, pay attention to the listed maximum values listed for each parameter in the script.\n\n### Testing Local Disk Performance\n\nThe `obdfilter-survey` script can be run automatically or manually against a local disk. This script profiles the overall throughput of storage hardware, including the file system and RAID layers managing the storage, by sending workloads to the OSTs that vary in thread count, object count, and I/O size.\n\nWhen the `obdfilter-survey` script is run, it provides information about the performance abilities of the storage hardware and shows the saturation points.\n\nThe `plot-obdfilter` script generates from the output of the `obdfilter-survey` a CSV file and parameters for importing into a spreadsheet or gnuplot to visualize the data.\n\nTo run the `obdfilter-survey` script, create a standard Lustre file system configuration; no special setup is needed.", "mimetype": "text/plain", "start_char_idx": 10017, "end_char_idx": 14268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3376098-ec3d-4c5b-9f7e-10a1b51a3d52": {"__data__": {"id_": "f3376098-ec3d-4c5b-9f7e-10a1b51a3d52", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c41aa1c-442c-4a2a-84cf-8cd0800036c8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6610fdb5dd5275ed78d751971c6e366151eac5cb108673a5097f99cac6e70acc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61c1da22-0d52-4893-a2de-6ee03d524084", "node_type": "1", "metadata": {}, "hash": "4e9b754ce314dc1462f1e39f017517fa48a8b64c253f7ba1b6e726cc580fe126", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, pay attention to the listed maximum values listed for each parameter in the script.\n\n### Testing Local Disk Performance\n\nThe `obdfilter-survey` script can be run automatically or manually against a local disk. This script profiles the overall throughput of storage hardware, including the file system and RAID layers managing the storage, by sending workloads to the OSTs that vary in thread count, object count, and I/O size.\n\nWhen the `obdfilter-survey` script is run, it provides information about the performance abilities of the storage hardware and shows the saturation points.\n\nThe `plot-obdfilter` script generates from the output of the `obdfilter-survey` a CSV file and parameters for importing into a spreadsheet or gnuplot to visualize the data.\n\nTo run the `obdfilter-survey` script, create a standard Lustre file system configuration; no special setup is needed.\n\n**To perform an automatic run:**\n\n1. Start the Lustre OSTs.\n\n   The Lustre OSTs should be mounted on the OSS node(s) to be tested. The Lustre client is not required to be mounted at this time.\n\n2. Verify that the obdecho module is loaded. Run:\n\n   ```\n   modprobe obdecho\n   ```\n\n3. Run the `obdfilter-survey` script with the parameter `case=disk`.\n\n   For example, to run a local test with up to two objects (nobjhi), up to two threads (thrhi), and 1024 MB transfer size (size):\n\n   ```\n   $ nobjhi=2 thrhi=2 size=1024 case=disk sh obdfilter-survey\n   ```\n\n4. Performance measurements for write, rewrite, read etc are provided below:\n\n   ```\n   # example output\n   Fri Sep 25 11:14:03 EDT 2015 Obdfilter-survey for case=disk from hds1fnb6123\n   ost 10 sz 167772160K rsz 1024K obj   10 thr   10 write 10982.73 [ 601.97,2912.91] rewrite 15696.54 [1160.92,3450.85] read 12358.60 [ 938.96,2634.87] \n   ...\n   ```\n\n   The file `./lustre-iokit/obdfilter-survey/README.obdfilter-survey` provides an explaination for the output as follows:\n\n   ```\n   ost 10          is the total number of OSTs under test.\n   sz 167772160K   is the total amount of data read or written (in bytes).\n   rsz 1024K       is the record size (size of each echo_client I/O, in bytes).\n   obj    10       is the total number of objects over all OSTs\n   thr    10       is the total number of threads over all OSTs and objects\n   write           is the test name.  If more tests have been specified they\n              all appear on the same line.\n   10982.73        is the aggregate bandwidth over all OSTs measured by\n              dividing the total number of MB by the elapsed time.\n   [601.97,2912.91] are the minimum and maximum instantaneous bandwidths seen on\n              any individual OST.\n   Note that although the numbers of threads and objects are specifed per-OST\n   in the customization section of the script, results are reported aggregated\n   over all OSTs.\n   ```\n\nTo perform a manual run:\n\n1. Start the Lustre OSTs.\n\n   The Lustre OSTs should be mounted on the OSS node(s) to be tested. The Lustre client is not required to be mounted at this time.\n\n2. Verify that the `obdecho` module is loaded. Run:\n\n   ```\n   modprobe obdecho\n   ```\n\n3. Determine the OST names.\n\n   On the OSS nodes to be tested, run the `lctl dl` command. The OST device names are listed in the fourth column of the output. For example:\n\n   ```\n   $ lctl dl |grep obdfilter\n   0 UP obdfilter lustre-OST0001 lustre-OST0001_UUID 1159\n   2 UP obdfilter lustre-OST0002 lustre-OST0002_UUID 1159\n   ...\n   ```\n\n4. List all OSTs you want to test.\n\n   Use the `targets=parameter` to list the OSTs separated by spaces.", "mimetype": "text/plain", "start_char_idx": 13377, "end_char_idx": 16939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61c1da22-0d52-4893-a2de-6ee03d524084": {"__data__": {"id_": "61c1da22-0d52-4893-a2de-6ee03d524084", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3376098-ec3d-4c5b-9f7e-10a1b51a3d52", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8f03a26a9ee2993b0886b69b93551f64050e49d5a0db595d72a9776c3a1920e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ca26635-e8e4-488b-bf27-eae09c16899d", "node_type": "1", "metadata": {}, "hash": "4bb1d6539459a17e8733c9fd6bc274e3e8f8e6dfedd56128e2b8d0dabe95813e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Lustre client is not required to be mounted at this time.\n\n2. Verify that the `obdecho` module is loaded. Run:\n\n   ```\n   modprobe obdecho\n   ```\n\n3. Determine the OST names.\n\n   On the OSS nodes to be tested, run the `lctl dl` command. The OST device names are listed in the fourth column of the output. For example:\n\n   ```\n   $ lctl dl |grep obdfilter\n   0 UP obdfilter lustre-OST0001 lustre-OST0001_UUID 1159\n   2 UP obdfilter lustre-OST0002 lustre-OST0002_UUID 1159\n   ...\n   ```\n\n4. List all OSTs you want to test.\n\n   Use the `targets=parameter` to list the OSTs separated by spaces. List the individual OSTs by name using the format `*fsname*-*OSTnumber*` (for example, `lustre-OST0001`). You do not have to specify an MDS or LOV.\n\n5. Run the `obdfilter-survey` script with the `targets=parameter`.\n\n   For example, to run a local test with up to two objects (`nobjhi`), up to two threads (`thrhi`), and 1024 Mb (size) transfer size:\n\n   ```\n   $ nobjhi=2 thrhi=2 size=1024 targets=\"lustre-OST0001 \\\n   \t   lustre-OST0002\" sh obdfilter-survey\n   ```\n\n### Testing Network Performance\n\nThe `obdfilter-survey` script can only be run automatically against a network; no manual test is provided.\n\nTo run the network test, a specific Lustre file system setup is needed. Make sure that these configuration requirements have been met.\n\n**To perform an automatic run:**\n\n1. Start the Lustre OSTs.\n\n   The Lustre OSTs should be mounted on the OSS node(s) to be tested. The Lustre client is not required to be mounted at this time.\n\n2. Verify that the `obdecho` module is loaded. Run:\n\n   ```\n   modprobe obdecho\n   ```\n\n3. Start `lctl` and check the device list, which must be empty. Run:\n\n   ```\n   lctl dl\n   ```\n\n4. Run the `obdfilter-survey` script with the parameters `case=network` and `targets=*hostname|ip_of_server*`. For example:\n\n   ```\n   $ nobjhi=2 thrhi=2 size=1024 targets=\"oss0 oss1\" \\\n   \t   case=network sh obdfilter-survey\n   ```\n\n5. On the server side, view the statistics at:\n\n   ```\n   lctl get_param obdecho.echo_srv.stats\n   ```\n\n   where `*echo_srv*` is the `obdecho` server created by the script.\n\n### Testing Remote Disk Performance\n\nThe `obdfilter-survey` script can be run automatically or manually against a network disk. To run the network disk test, start with a standard Lustre configuration. No special setup is needed.\n\n**To perform an automatic run:**\n\n1. Start the Lustre OSTs.\n\n   The Lustre OSTs should be mounted on the OSS node(s) to be tested. The Lustre client is not required to be mounted at this time.\n\n2. Verify that the `obdecho` module is loaded. Run:\n\n   ```\n   modprobe obdecho\n   ```\n\n3. Run the `obdfilter-survey` script with the parameter `case=netdisk`. For example:\n\n   ```\n   $ nobjhi=2 thrhi=2 size=1024 case=netdisk sh obdfilter-survey\n   ```\n\n**To perform a manual run:**\n\n1. Start the Lustre OSTs.\n\n   The Lustre OSTs should be mounted on the OSS node(s) to be tested. The Lustre client is not required to be mounted at this time.\n\n2. Verify that the `obdecho` module is loaded. Run:\n\n   modprobe obdecho\n\n3. Determine the OSC names.\n\n   On the OSS nodes to be tested, run the `lctl dl` command. The OSC device names are listed in the fourth column of the output.", "mimetype": "text/plain", "start_char_idx": 16345, "end_char_idx": 19571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ca26635-e8e4-488b-bf27-eae09c16899d": {"__data__": {"id_": "0ca26635-e8e4-488b-bf27-eae09c16899d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61c1da22-0d52-4893-a2de-6ee03d524084", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bc8ca08d94fcf3b8a388b888f5b923aa991b28bd2833ff41ba7b2b2e4e231ce3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9686f81e-bf5a-4a13-9a37-f9e79a87bcb9", "node_type": "1", "metadata": {}, "hash": "78c0c43da05f23c0aa0ea1e219cd1b7c33aa3ca487e1d4e2c7c259570509613c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Run:\n\n   ```\n   modprobe obdecho\n   ```\n\n3. Run the `obdfilter-survey` script with the parameter `case=netdisk`. For example:\n\n   ```\n   $ nobjhi=2 thrhi=2 size=1024 case=netdisk sh obdfilter-survey\n   ```\n\n**To perform a manual run:**\n\n1. Start the Lustre OSTs.\n\n   The Lustre OSTs should be mounted on the OSS node(s) to be tested. The Lustre client is not required to be mounted at this time.\n\n2. Verify that the `obdecho` module is loaded. Run:\n\n   modprobe obdecho\n\n3. Determine the OSC names.\n\n   On the OSS nodes to be tested, run the `lctl dl` command. The OSC device names are listed in the fourth column of the output. For example:\n\n   ```\n   $ lctl dl |grep obdfilter\n   3 UP osc lustre-OST0000-osc-ffff88007754bc00 \\\n              54b91eab-0ea9-1516-b571-5e6df349592e 5\n   4 UP osc lustre-OST0001-osc-ffff88007754bc00 \\\n              54b91eab-0ea9-1516-b571-5e6df349592e 5\n   ...\n   ```\n\n4. List all OSCs you want to test.\n\n   Use the `targets=parameter` to list the OSCs separated by spaces. List the individual OSCs by name separated by spaces using the format `*fsname*-*OST_name*-osc-*instance*` (for example, `lustre-OST0000-osc-ffff88007754bc00`). You *do not have to specify an MDS or LOV.*\n\n5. Run the `obdfilter-survey` script with the `targets=*osc*` and `case=netdisk`.\n\n   An example of a local test run with up to two objects (`nobjhi`), up to two threads (`thrhi`), and 1024 Mb (size) transfer size is shown below:\n\n   ```\n   $ nobjhi=2 thrhi=2 size=1024 \\\n              targets=\"lustre-OST0000-osc-ffff88007754bc00 \\\n              lustre-OST0001-osc-ffff88007754bc00\" sh obdfilter-survey\n   ```\n\n### Output Files\n\nWhen the `obdfilter-survey` script runs, it creates a number of working files and a pair of result files. All files start with the prefix defined in the variable `${rslt}`.\n\n| **File**              | **Description**                        |\n| --------------------- | -------------------------------------- |\n| `${rslt}.summary`     | Same as stdout                         |\n| `${rslt}.script_*`    | Per-host test script files             |\n| `${rslt}.detail_tmp*` | Per-OST result files                   |\n| `${rslt}.detail`      | Collected result files for post-mortem |\n\nThe `obdfilter-survey` script iterates over the given number of threads and objects performing the specified tests and checks that all test processes have completed successfully.\n\n**Note**\n\nThe `obdfilter-survey` script may not clean up properly if it is aborted or if it encounters an unrecoverable error. In this case, a manual cleanup may be required, possibly including killing any running instances of `lctl` (local or remote), removing `echo_client` instances created by the script and unloading `obdecho`.\n\n#### Script Output\n\nThe `.summary` file and `stdout` of the `obdfilter-survey` script contain lines like:\n\n```\nost 8 sz 67108864K rsz 1024 obj 8 thr 8 write 613.54 [ 64.00, 82.00]\n```\n\nWhere:\n\n| **Parameter and value** | **Description**                                              |\n| ----------------------- | ------------------------------------------------------------ |\n| ost 8                   | Total number of OSTs being tested.                           |\n| sz 67108864K            | Total amount of data read or written (in KB).                |\n| rsz 1024                | Record size (size of each echo_client I/O, in KB).           |\n| obj 8                   | Total number of objects over all OSTs.                       |\n| thr 8                   | Total number of threads over all OSTs and objects.           |\n| write                   | Test name. If more tests have been specified, they all appear on the same line.", "mimetype": "text/plain", "start_char_idx": 18943, "end_char_idx": 22612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9686f81e-bf5a-4a13-9a37-f9e79a87bcb9": {"__data__": {"id_": "9686f81e-bf5a-4a13-9a37-f9e79a87bcb9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ca26635-e8e4-488b-bf27-eae09c16899d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8b00ac56f89ea96ab122621eddfb341c15198dc5249625287ab5783770f4e705", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de46db37-e19d-4f14-89a4-5bb1253f3241", "node_type": "1", "metadata": {}, "hash": "17bffdee0e7ef4e3fa3acb28e3be0e7bc6d9435f4cfa913519c1d00c2b99ace5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| sz 67108864K            | Total amount of data read or written (in KB).                |\n| rsz 1024                | Record size (size of each echo_client I/O, in KB).           |\n| obj 8                   | Total number of objects over all OSTs.                       |\n| thr 8                   | Total number of threads over all OSTs and objects.           |\n| write                   | Test name. If more tests have been specified, they all appear on the same line. |\n| 613.54                  | Aggregate bandwidth over all OSTs (measured by dividing the total number of MB by the elapsed time). |\n| [64, 82.00]             | Minimum and maximum instantaneous bandwidths on an individual OST. |\n\n**Note**\n\nAlthough the numbers of threads and objects are specified per-OST in the customization section of the script, the reported results are aggregated over all OSTs. \n\n#### Visualizing Results\n\nIt is useful to import the `obdfilter-survey` script summary data (it is fixed width) into Excel (or any graphing package) and graph the bandwidth versus the number of threads for varying numbers of concurrent regions. This shows how the OSS performs for a given number of concurrently-accessed objects (files) with varying numbers of I/Os in flight.\n\nIt is also useful to monitor and record average disk I/O sizes during each test using the 'disk io size' histogram in the file `lctl get_param obdfilter.*.brw_stats` (see [*the section called \u201cMonitoring the OST Block I/O Stream\u201d*](06.02-Lustre%20Parameters.md#monitoring-the-ost-block-io-stream) for details). These numbers help identify problems in the system when full-sized I/Os are not submitted to the underlying disk. This may be caused by problems in the device driver or Linux block layer.\n\nThe `plot-obdfilter` script included in the I/O toolkit is an example of processing output files to a .csv format and plotting a graph using `gnuplot`.\n\n## Testing OST I/O Performance (`ost-survey`)\n\nThe `ost-survey` tool is a shell script that uses `lfs setstripe` to perform I/O against a single OST. The script writes a file (currently using `dd`) to each OST in the Lustre file system, and compares read and write speeds. The `ost-survey` tool is used to detect anomalies between otherwise identical disk subsystems.\n\n**Note**\n\nWe have frequently discovered wide performance variations across all LUNs in a cluster. This may be caused by faulty disks, RAID parity reconstruction during the test, or faulty network hardware.\n\nTo run the `ost-survey` script, supply a file size (in KB) and the Lustre file system mount point. For example, run:\n\n```\n$ ./ost-survey.sh -s 10 /mnt/lustre\n```\n\nTypical output is:\n\n```\nNumber of Active OST devices : 4\nWorst  Read OST indx: 2 speed: 2835.272725\nBest   Read OST indx: 3 speed: 2872.889668\nRead Average: 2852.508999 +/- 16.444792 MB/s\nWorst  Write OST indx: 3 speed: 17.705545\nBest   Write OST indx: 2 speed: 128.172576\nWrite Average: 95.437735 +/- 45.518117 MB/s\nOst#  Read(MB/s)  Write(MB/s)  Read-time  Write-time\n----------------------------------------------------\n0     2837.440       126.918        0.035      0.788\n1     2864.433       108.954        0.035      0.918\n2     2835.273       128.173        0.035      0.780\n3     2872.890       17.706        0.035      5.648\n```\n## Testing MDS Performance (`mds-survey`)\n\nThe `mds-survey` script tests the local metadata performance using the `echo_client` to drive the MDD layers of the MDS stack. It can be used with the following classes of operations:\n\n- `Open-create/mkdir/create`\n- `Lookup/getattr/setxattr`\n- `Delete/destroy`\n- `Unlink/rmdir`\n\nThese operations will be run by a variable number of concurrent threads and will test with the number of directories specified by the user.", "mimetype": "text/plain", "start_char_idx": 22139, "end_char_idx": 25886, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de46db37-e19d-4f14-89a4-5bb1253f3241": {"__data__": {"id_": "de46db37-e19d-4f14-89a4-5bb1253f3241", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9686f81e-bf5a-4a13-9a37-f9e79a87bcb9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9ea41210f0c04dd2fb2959fd96f05a82474b4c28c821079ea336110591baf270", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a613ba78-a4bc-4cf6-9c1a-659ad2d7dcc3", "node_type": "1", "metadata": {}, "hash": "b009953f1750dfcf1965c21e18a02940586f3457f558440ed9b7bb76fe7b3877", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It can be used with the following classes of operations:\n\n- `Open-create/mkdir/create`\n- `Lookup/getattr/setxattr`\n- `Delete/destroy`\n- `Unlink/rmdir`\n\nThese operations will be run by a variable number of concurrent threads and will test with the number of directories specified by the user. The run can be executed such that all threads operate in a single directory (dir_count=1) or in private/unique directory (dir_count=x thrlo=x thrhi=x).\n\nThe mdd instance is driven directly. The script automatically loads the obdecho module if required and creates instance of echo_client.\n\nThis script can also create OST objects by providing stripe_count greater than zero.\n\n**To perform a run:**\n\n1. Start the Lustre MDT.\n\n   The Lustre MDT should be mounted on the MDS node to be tested.\n\n2. Start the Lustre OSTs (optional, only required when test with OST objects)\n\n   The Lustre OSTs should be mounted on the OSS node(s).\n\n3. Run the `mds-survey` script as explain below\n\n   The script must be customized according to the components under test and where it should keep its working files. Customization variables are described as followed:\n\n   - `thrlo` - threads to start testing. skipped if less than `dir_count`\n   - `thrhi` - maximum number of threads to test\n   - `targets` - MDT instance\n   - `file_count` - number of files per thread to test\n   - `dir_count` - total number of directories to test. Must be less than or equal to `thrhi`\n   - `stripe_count `- number stripe on OST objects\n   - `tests_str` - test operations. Must have at least \"create\" and \"destroy\"\n   - `start_number` - base number for each thread to prevent name collisions\n   - `layer` - MDS stack's layer to be tested\n\n   Run without OST objects creation:\n\n   Setup the Lustre MDS without OST mounted. Then invoke the `mds-survey` script\n\n   ```\n   $ thrhi=64 file_count=200000 sh mds-survey\n   ```\n\n   Run with OST objects creation:\n\n   Setup the Lustre MDS with at least one OST mounted. Then invoke the `mds-survey` script with `stripe_count`parameter\n\n   ```\n   $ thrhi=64 file_count=200000 stripe_count=2 sh mds-survey\n   ```\n\n   Note: a specific MDT instance can be specified using targets variable.\n\n   ```\n   $ targets=lustre-MDT0000 thrhi=64 file_count=200000 stripe_count=2 sh mds-survey\n   ```\n\n### Output Files\n\nWhen the `mds-survey` script runs, it creates a number of working files and a pair of result files. All files start with the prefix defined in the variable `${rslt}`.\n\n| **File**              | **Description**                        |\n| --------------------- | -------------------------------------- |\n| `${rslt}.summary`     | Same as stdout                         |\n| `${rslt}.script_*`    | Per-host test script files             |\n| `${rslt}.detail_tmp*` | Per-mdt result files                   |\n| `${rslt}.detail`      | Collected result files for post-mortem |\n\nThe `mds-survey` script iterates over the given number of threads performing the specified tests and checks that all test processes have completed successfully.\n\n**Note**\n\nThe `mds-survey` script may not clean up properly if it is aborted or if it encounters an unrecoverable error. In this case, a manual cleanup may be required, possibly including killing any running instances of `lctl`, removing `echo_client` instances created by the script and unloading `obdecho`.\n\n### Script Output\n\nThe `.summary` file and `stdout` of the `mds-survey` script contain lines like:\n\n```\nmdt 1 file 100000 dir 4 thr 4 create 5652.05 [ 999.01,46940.48] destroy 5797.79 [ 0.00,52951.55] \n```\n\nWhere:\n\n| **Parameter and value** | **Description**                                              |\n| ----------------------- | ------------------------------------------------------------ |\n| mdt 1                   | Total number of MDT under test                               |\n| file 100000             | Total number of files per thread to operate                  |\n| dir 4                   | Total number of directories to operate                       |\n| thr 4                   | Total number of threads operate over all directories         |\n| create, destroy         | Tests name.", "mimetype": "text/plain", "start_char_idx": 25595, "end_char_idx": 29736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a613ba78-a4bc-4cf6-9c1a-659ad2d7dcc3": {"__data__": {"id_": "a613ba78-a4bc-4cf6-9c1a-659ad2d7dcc3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea0950f-ca76-4df0-9c7c-70e604fd5f14", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4f75f20f0934def0e6a3ebcae1eba8d70e1ade7672c5e4bfe226ddc08c0d3f07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de46db37-e19d-4f14-89a4-5bb1253f3241", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "698944e2a698a18c405748ff27b3be2f83b56e5a96433cf9fe717dbd4bb093ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Script Output\n\nThe `.summary` file and `stdout` of the `mds-survey` script contain lines like:\n\n```\nmdt 1 file 100000 dir 4 thr 4 create 5652.05 [ 999.01,46940.48] destroy 5797.79 [ 0.00,52951.55] \n```\n\nWhere:\n\n| **Parameter and value** | **Description**                                              |\n| ----------------------- | ------------------------------------------------------------ |\n| mdt 1                   | Total number of MDT under test                               |\n| file 100000             | Total number of files per thread to operate                  |\n| dir 4                   | Total number of directories to operate                       |\n| thr 4                   | Total number of threads operate over all directories         |\n| create, destroy         | Tests name. More tests will be displayed on the same line.   |\n| 565.05                  | Aggregate operations over MDT measured by dividing the total number of operations by the elapsed time. |\n| [999.01,46940.48]       | Minimum and maximum instantaneous operation seen on any individual MDT |\n\n**Note**\n\nIf script output has \"ERROR\", this usually means there is issue during the run such as running out of space on the MDT and/or OST. More detailed debug information is available in the ${rslt}.detail file\n\n## Collecting Application Profiling Information (`stats-collect`)\n\nThe `stats-collect` utility contains the following scripts used to collect application profiling information from Lustre clients and servers:\n\n- `lstat.sh` - Script for a single node that is run on each profile node.\n- `gather_stats_everywhere.sh` - Script that collect statistics.\n- `config.sh` - Script that contains customized configuration descriptions.\n\nThe `stats-collect` utility requires:\n\n- Lustre software to be installed and set up on your cluster\n- SSH and SCP access to these nodes without requiring a password\n\n### Using `stats-collect`\n\nThe stats-collect utility is configured by including profiling configuration variables in the config.sh script. Each configuration variable takes the following form, where 0 indicates statistics are to be collected only when the script starts and stops and *n* indicates the interval in seconds at which statistics are to be collected:\n\n```\nstatistic_INTERVAL=0|n\n```\n\nStatistics that can be collected include:\n\n- `VMSTAT` - Memory and CPU usage and aggregate read/write operations\n- `SERVICE` - Lustre OST and MDT RPC service statistics\n- `BRW` - OST bulk read/write statistics (brw_stats)\n- `SDIO` - SCSI disk IO statistics (sd_iostats)\n- `MBALLOC` - `ldiskfs` block allocation statistics\n- `IO` - Lustre target operations statistics\n- `JBD` - ldiskfs journal statistics\n- `CLIENT` - Lustre OSC request statistics\n\nTo collect profile information:\n\nBegin collecting statistics on each node specified in the config.sh script.\n\n1. Starting the collect profile daemon on each node by entering:\n\n   ```\n   sh gather_stats_everywhere.sh config.sh start \n   ```\n\n2. Run the test.\n\n3. Stop collecting statistics on each node, clean up the temporary file, and create a profiling tarball.\n\n   Enter:\n\n   ```\n   sh gather_stats_everywhere.sh config.sh stop log_name.tgz\n   ```\n\n   When `*log_name*.tgz` is specified, a profile tarball `*/tmp/log_name*.tgz` is created.\n\n4. Analyze the collected statistics and create a csv tarball for the specified profiling data.\n\n   ```\n   sh gather_stats_everywhere.sh config.sh analyse log_tarball.tgz csv\n   ```", "mimetype": "text/plain", "start_char_idx": 28936, "end_char_idx": 32397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2a6219e-cedd-431e-bd8c-250bb45f0f18": {"__data__": {"id_": "a2a6219e-cedd-431e-bd8c-250bb45f0f18", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec13075d-7b85-4b22-a524-49e499df6cde", "node_type": "1", "metadata": {}, "hash": "d27e08b34d76124c32f10cc77b2ed911e3d2ae1a418cc488c3147df42d83d0c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Tuning a Lustre File System\n\n- [Tuning a Lustre File System](#tuning-a-lustre-file-system)\n  * [Optimizing the Number of Service Threads](#optimizing-the-number-of-service-threads)\n    + [Specifying the OSS Service Thread Count](#specifying-the-oss-service-thread-count)\n    + [Specifying the MDS Service Thread Count](#specifying-the-mds-service-thread-count)\n  * [Binding MDS Service Thread to CPU Partitions](#binding-mds-service-thread-to-cpu-partitions)\n  * [Tuning LNet Parameters](#tuning-lnet-parameters)\n    + [Transmit and Receive Buffer Size](#transmit-and-receive-buffer-size)\n    + [Hardware Interrupts ( `enable_irq_affinity`)](#hardware-interrupts--enable_irq_affinity)\n    + [Binding Network Interface Against CPU Partitions](#binding-network-interface-against-cpu-partitions)\n    + [Network Interface Credits](#network-interface-credits)\n    + [Router Buffers](#router-buffers)\n    + [Portal Round-Robin](#portal-round-robin)\n    + [LNet Peer Health](#lnet-peer-health)\n  * [libcfs Tuning](#libcfs-tuning)\n    + [CPU Partition String Patterns](#cpu-partition-string-patterns)\n  * [LND Tuning](#lnd-tuning)\n    + [ko2iblnd Tuning](#ko2iblnd-tuning)\n  * [Network Request Scheduler (NRS) Tuning](#network-request-scheduler-nrs-tuning)L 2.4\n    + [First In, First Out (FIFO) policy](#first-in-first-out-fifo-policy)\n    + [Client Round-Robin over NIDs (CRR-N) policy](#client-round-robin-over-nids-crr-n-policy)\n    + [Object-based Round-Robin (ORR) policy](#object-based-round-robin-orr-policy)\n    + [Target-based Round-Robin (TRR) policy](#target-based-round-robin-trr-policy)\n    + [Token Bucket Filter (TBF) policy](#token-bucket-filter-tbf-policy)L 2.6\n      - [Enable TBF policy](#enable-tbf-policy)\n      - [Start a TBF rule](#start-a-tbf-rule)\n      - [Change a TBF rule](#change-a-tbf-rule)\n      - [Stop a TBF rule](#stop-a-tbf-rule)\n      - [Rule options](#rule-options)\n    + [Delay policy](#delay-policy)L 2.10\n  * [Lockless I/O Tunables](#lockless-io-tunables)\n  * [Server-Side Advice and Hinting](#server-side-advice-and-hinting)L 2.9\n    + [Overview](#overview)\n    + [Examples](#examples)\n  * [Large Bulk IO (16MB RPC)](#large-bulk-io-16mb-rpc)\n    + [Overview](#overview-1)\n    + [Usage](#usage)\n  * [Improving Lustre I/O Performance for Small Files](#improving-lustre-io-performance-for-small-files)\n  * [Understanding Why Write Performance is Better Than Read Performance](#understanding-why-write-performance-is-better-than-read-performance)\n\n\nThis chapter contains information about tuning a Lustre file system for better performance.\n\n**Note**\nMany options in the Lustre software are set by means of kernel module parameters. These parameters are contained in the /etc/modprobe.d/lustre.conf file.\n\n## Optimizing the Number of Service Threads\n\nAn OSS can have a minimum of two service threads and a maximum of 512 service threads. The number of service threads is a function of how much RAM and how many CPUs are on each OSS node (1 thread / 128MB * num_cpus). If the load on the OSS node is high, new service threads will be started in order to process more requests concurrently, up to 4x the initial number of threads (subject to the maximum of 512). For a 2GB 2-CPU system, the default thread count is 32 and the maximum thread count is 128.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec13075d-7b85-4b22-a524-49e499df6cde": {"__data__": {"id_": "ec13075d-7b85-4b22-a524-49e499df6cde", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2a6219e-cedd-431e-bd8c-250bb45f0f18", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "15e282559d9e46cf16a5ab9c63e3a3557036368c3e7952b5f0b10761ad65e5db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53b652f1-4066-4ad9-84dd-830c5689f4e5", "node_type": "1", "metadata": {}, "hash": "60b0298fc2da15b45e0354390b4bcdba21c37cfacca82da5c8f5c60d9a318c7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This chapter contains information about tuning a Lustre file system for better performance.\n\n**Note**\nMany options in the Lustre software are set by means of kernel module parameters. These parameters are contained in the /etc/modprobe.d/lustre.conf file.\n\n## Optimizing the Number of Service Threads\n\nAn OSS can have a minimum of two service threads and a maximum of 512 service threads. The number of service threads is a function of how much RAM and how many CPUs are on each OSS node (1 thread / 128MB * num_cpus). If the load on the OSS node is high, new service threads will be started in order to process more requests concurrently, up to 4x the initial number of threads (subject to the maximum of 512). For a 2GB 2-CPU system, the default thread count is 32 and the maximum thread count is 128.\n\nIncreasing the size of the thread pool may help when:\n\n- Several OSTs are exported from a single OSS\n- Back-end storage is running synchronously\n- I/O completions take excessive time due to slow storage\n\nDecreasing the size of the thread pool may help if:\n\n- Clients are overwhelming the storage capacity\n- There are lots of \"slow I/O\" or similar messages\n\nIncreasing the number of I/O threads allows the kernel and storage to aggregate many writes together for more efficient disk I/O. The OSS thread pool is shared--each thread allocates approximately 1.5 MB (maximum RPC size + 0.5 MB) for internal I/O buffers.\n\nIt is very important to consider memory consumption when increasing the thread pool size. Drives are only able to sustain a certain amount of parallel I/O activity before performance is degraded, due to the high number of seeks and the OST threads just waiting for I/O. In this situation, it may be advisable to decrease the load by decreasing the number of OST threads.\n\nDetermining the optimum number of OSS threads is a process of trial and error, and varies for each particular configuration. Variables include the number of OSTs on each OSS, number and speed of disks, RAID configuration, and available RAM. You may want to start with a number of OST threads equal to the number of actual disk spindles on the node. If you use RAID, subtract any dead spindles not used for actual data (e.g., 1 of N of spindles for RAID5, 2 of N spindles for RAID6), and monitor the performance of clients during usual workloads. If performance is degraded, increase the thread count and see how that works until performance is degraded again or you reach satisfactory performance.\n\n**Note**\n\nIf there are too many threads, the latency for individual I/O requests can become very high and should be avoided. Set the desired maximum thread count permanently using the method described above.\n\n### Specifying the OSS Service Thread Count\n\nThe `oss_num_threads` parameter enables the number of OST service threads to be specified at module load time on the OSS nodes:\n\n```\noptions ost oss_num_threads={N}\n```\n\nAfter startup, the minimum and maximum number of OSS thread counts can be set via the`{service}.thread_{min,max,started}` tunable. To change the tunable at runtime, run:\n\n```\nlctl {get,set}_param {service}.thread_{min,max,started}\n```\n\nThis works in a similar fashion to binding of threads on MDS. MDS thread tuning is covered in [*the section called \u201c Binding MDS Service Thread to CPU Partitions\u201d*](#binding-mds-service-thread-to-cpu-partitions).\n\n- `oss_cpts=[EXPRESSION]` binds the default OSS service on CPTs defined by `[EXPRESSION]`.\n- `oss_io_cpts=[EXPRESSION]` binds the IO OSS service on CPTs defined by `[EXPRESSION]`.\n\nFor further details, see [*the section called \u201cSetting MDS and OSS Thread Counts\u201d*](06.02-Lustre%20Parameters.md#setting-mds-and-oss-thread-counts).\n\n### Specifying the MDS Service Thread Count\n\nThe `mds_num_threads` parameter enables the number of MDS service threads to be specified at module load time on the MDS node:\n\n```\noptions mds mds_num_threads={N}\n```\n\nAfter startup, the minimum and maximum number of MDS thread counts can be set via the`{service}.thread_{min,max,started}` tunable.", "mimetype": "text/plain", "start_char_idx": 2481, "end_char_idx": 6522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53b652f1-4066-4ad9-84dd-830c5689f4e5": {"__data__": {"id_": "53b652f1-4066-4ad9-84dd-830c5689f4e5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec13075d-7b85-4b22-a524-49e499df6cde", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "51fc0b624481c3d644292c23c29154b01bfedc69aa1bb6e04e80ed8dc5e9bc8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77bc8d10-ae76-446f-becf-c5068e71c580", "node_type": "1", "metadata": {}, "hash": "e349cc2b8a7aed3cfc30568584141fbc9ccd065f18367b460ec5ba1fdbd78b23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `oss_cpts=[EXPRESSION]` binds the default OSS service on CPTs defined by `[EXPRESSION]`.\n- `oss_io_cpts=[EXPRESSION]` binds the IO OSS service on CPTs defined by `[EXPRESSION]`.\n\nFor further details, see [*the section called \u201cSetting MDS and OSS Thread Counts\u201d*](06.02-Lustre%20Parameters.md#setting-mds-and-oss-thread-counts).\n\n### Specifying the MDS Service Thread Count\n\nThe `mds_num_threads` parameter enables the number of MDS service threads to be specified at module load time on the MDS node:\n\n```\noptions mds mds_num_threads={N}\n```\n\nAfter startup, the minimum and maximum number of MDS thread counts can be set via the`{service}.thread_{min,max,started}` tunable. To change the tunable at runtime, run:\n\n```\nlctl {get,set}_param {service}.thread_{min,max,started}\n```\n\nFor details, see [*the section called \u201cSetting MDS and OSS Thread Counts\u201d*](06.02-Lustre%20Parameters.md#setting-mds-and-oss-thread-counts).\n\nThe number of MDS service threads started depends on system size and the load on the server, and has a default maximum of 64. The maximum potential number of threads (`MDS_MAX_THREADS`) is 1024.\n\n**Note**\n\nThe OSS and MDS start two threads per service per CPT at mount time, and dynamically increase the number of running service threads in response to server load. Setting the `*_num_threads` module parameter starts the specified number of threads for that service immediately and disables automatic thread creation behavior.\n\nParameters are available to provide administrators control over the number of service threads.\n\n- `mds_rdpg_num_threads` controls the number of threads in providing the read page service. The read page service handles file close and readdir operations.\n\n## Binding MDS Service Thread to CPU Partitions\n\nWith the Node Affinity ([*Node affinity*](07-Glossary.md#n)) feature, MDS threads can be bound to particular CPU partitions (CPTs) to improve CPU cache usage and memory locality. Default values for CPT counts and CPU core bindings are selected automatically to provide good overall performance for a given CPU count. However, an administrator can deviate from these setting if they choose. For details on specifying the mapping of CPU cores to CPTs see [*the section called \u201c libcfs Tuning\u201d*](#libcfs-tuning).\n\n- `mds_num_cpts=[EXPRESSION]` binds the default MDS service threads to CPTs defined by `EXPRESSION`. For example `mds_num_cpts=[0-3]` will bind the MDS service threads to `CPT[0,1,2,3]`.\n- `mds_rdpg_num_cpts=[EXPRESSION]` binds the read page service threads to CPTs defined by `EXPRESSION`. The read page service handles file close and readdir requests. For example `mds_rdpg_num_cpts=[4]` will bind the read page threads to `CPT4`.\n\nParameters must be set before module load in the file `/etc/modprobe.d/lustre.conf`. For example:\n\n**Example 1. lustre.conf**\n\n```\noptions lnet networks=tcp0(eth0)\noptions mdt mds_num_cpts=[0]\n```\n## Tuning LNet Parameters\n\nThis section describes LNet tunables, the use of which may be necessary on some systems to improve performance. To test the performance of your Lustre network, see\u00a0[*Testing Lustre Network Performance (LNet Self-Test)*](04.01-Testing%20Lustre%20Network%20Performance%20(LNet%20Self-Test).md).\n\n### Transmit and Receive Buffer Size\n\nThe kernel allocates buffers for sending and receiving messages on a network.\n\n`ksocklnd` has separate parameters for the transmit and receive buffers.\n\n```\noptions ksocklnd tx_buffer_size=0 rx_buffer_size=0\n```\n\nIf these parameters are left at the default value (0), the system automatically tunes the transmit and receive buffer size. In almost every case, this default produces the best performance. Do not attempt to tune these parameters unless you are a network expert.\n\n### Hardware Interrupts ( `enable_irq_affinity`)\n\nThe hardware interrupts that are generated by network adapters may be handled by any CPU in the system. In some cases, we would like network traffic to remain local to a single CPU to help keep the processor cache warm and minimize the impact of context switches.", "mimetype": "text/plain", "start_char_idx": 5847, "end_char_idx": 9893, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77bc8d10-ae76-446f-becf-c5068e71c580": {"__data__": {"id_": "77bc8d10-ae76-446f-becf-c5068e71c580", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53b652f1-4066-4ad9-84dd-830c5689f4e5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a09006998a0750801a0dc3f33054e88a603da12a9146b8b55bba368289f384c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec42e894-1708-49ed-a740-fb33442aaa14", "node_type": "1", "metadata": {}, "hash": "319d7c2fddef22221deb9a5de0499b8d020717da1df266c5ab5260289f406440", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Transmit and Receive Buffer Size\n\nThe kernel allocates buffers for sending and receiving messages on a network.\n\n`ksocklnd` has separate parameters for the transmit and receive buffers.\n\n```\noptions ksocklnd tx_buffer_size=0 rx_buffer_size=0\n```\n\nIf these parameters are left at the default value (0), the system automatically tunes the transmit and receive buffer size. In almost every case, this default produces the best performance. Do not attempt to tune these parameters unless you are a network expert.\n\n### Hardware Interrupts ( `enable_irq_affinity`)\n\nThe hardware interrupts that are generated by network adapters may be handled by any CPU in the system. In some cases, we would like network traffic to remain local to a single CPU to help keep the processor cache warm and minimize the impact of context switches. This is helpful when an SMP system has more than one network interface and ideal when the number of interfaces equals the number of CPUs. To enable the `enable_irq_affinity`parameter, enter:\n\n```\noptions ksocklnd enable_irq_affinity=1\n```\n\nIn other cases, if you have an SMP platform with a single fast interface such as 10 Gb Ethernet and more than two CPUs, you may see performance improve by turning this parameter off.\n\n```\noptions ksocklnd enable_irq_affinity=0\n```\n\nBy default, this parameter is off. As always, you should test the performance to compare the impact of changing this parameter.\n\n### Binding Network Interface Against CPU Partitions\n\nLustre allows enhanced network interface control. This means that an administrator can bind an interface to one or more CPU partitions. Bindings are specified as options to the LNet modules. For more information on specifying module options, see [*the section called \u201c Introduction\u201d*](06.06-Configuration%20Files%20and%20Module%20Parameters.md#introduction).\n\nFor example, `o2ib0(ib0)[0,1]` will ensure that all messages for `o2ib0` will be handled by LND threads executing on`CPT0` and `CPT1`. An additional example might be: `tcp1(eth0)[0]`. Messages for `tcp1` are handled by threads on`CPT0`.\n\n### Network Interface Credits\n\nNetwork interface (NI) credits are shared across all CPU partitions (CPT). For example, if a machine has four CPTs and the number of NI credits is 512, then each partition has 128 credits. If a large number of CPTs exist on the system, LNet checks and validates the NI credits for each CPT to ensure each CPT has a workable number of credits. For example, if a machine has 16 CPTs and the number of NI credits is 256, then each partition only has 16 credits. 16 NI credits is low and could negatively impact performance. As a result, LNet automatically adjusts the credits to 8* `peer_credits`( `peer_credits` is 8 by default), so each partition has 64 credits.\n\nIncreasing the number of `credits`/ `peer_credits` can improve the performance of high latency networks (at the cost of consuming more memory) by enabling LNet to send more inflight messages to a specific network/peer and keep the pipeline saturated.\n\nAn administrator can modify the NI credit count using `ksoclnd` or `ko2iblnd`. In the example below, 256 credits are applied to TCP connections.\n\n```\nksocklnd credits=256\n```\n\nApplying 256 credits to IB connections can be achieved with:\n\n```\nko2iblnd credits=256\n```\n\n**Note**\n\nLNet may revalidate the NI credits, so the administrator's request may not persist.\n\n### Router Buffers\n\nWhen a node is set up as an LNet router, three pools of buffers are allocated: tiny, small and large. These pools are allocated per CPU partition and are used to buffer messages that arrive at the router to be forwarded to the next hop. The three different buffer sizes accommodate different size messages.\n\nIf a message arrives that can fit in a tiny buffer then a tiny buffer is used, if a message doesn\u2019t fit in a tiny buffer, but fits in a small buffer, then a small buffer is used. Finally if a message does not fit in either a tiny buffer or a small buffer, a large buffer is used.\n\nRouter buffers are shared by all CPU partitions. For a machine with a large number of CPTs, the router buffer number may need to be specified manually for best performance. A low number of router buffers risks starving the CPU partitions of resources.", "mimetype": "text/plain", "start_char_idx": 9065, "end_char_idx": 13316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec42e894-1708-49ed-a740-fb33442aaa14": {"__data__": {"id_": "ec42e894-1708-49ed-a740-fb33442aaa14", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77bc8d10-ae76-446f-becf-c5068e71c580", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a51889154ee07334e3ef2129c6bf6c9ae0eb1bcfb1135728ed617b016f1f2dca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae54a7f7-6482-480d-bba9-74d9a63a8d5b", "node_type": "1", "metadata": {}, "hash": "45a15582772a47784071a394c376febc1b36bcf98d8c55f5a1988c8c9152a68b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Router Buffers\n\nWhen a node is set up as an LNet router, three pools of buffers are allocated: tiny, small and large. These pools are allocated per CPU partition and are used to buffer messages that arrive at the router to be forwarded to the next hop. The three different buffer sizes accommodate different size messages.\n\nIf a message arrives that can fit in a tiny buffer then a tiny buffer is used, if a message doesn\u2019t fit in a tiny buffer, but fits in a small buffer, then a small buffer is used. Finally if a message does not fit in either a tiny buffer or a small buffer, a large buffer is used.\n\nRouter buffers are shared by all CPU partitions. For a machine with a large number of CPTs, the router buffer number may need to be specified manually for best performance. A low number of router buffers risks starving the CPU partitions of resources.\n\n- `tiny_router_buffers`: Zero payload buffers used for signals and acknowledgements.\n- `small_router_buffers`: 4 KB payload buffers for small messages\n- `large_router_buffers`: 1 MB maximum payload buffers, corresponding to the recommended RPC size of 1 MB.\n\nThe default setting for router buffers typically results in acceptable performance. LNet automatically sets a default value to reduce the likelihood of resource starvation. The size of a router buffer can be modified as shown in the example below. In this example, the size of the large buffer is modified using the `large_router_buffers`parameter.\n\n```\nlnet large_router_buffers=8192\n```\n\n**Note**\n\nLNet may revalidate the router buffer setting, so the administrator's request may not persist.\n\n### Portal Round-Robin\n\nPortal round-robin defines the policy LNet applies to deliver events and messages to the upper layers. The upper layers are PLRPC service or LNet selftest.\n\nIf portal round-robin is disabled, LNet will deliver messages to CPTs based on a hash of the source NID. Hence, all messages from a specific peer will be handled by the same CPT. This can reduce data traffic between CPUs. However, for some workloads, this behavior may result in poorly balancing loads across the CPU.\n\nIf portal round-robin is enabled, LNet will round-robin incoming events across all CPTs. This may balance load better across the CPU but can incur a cross CPU overhead.\n\nThe current policy can be changed by an administrator with `echo *value*> /proc/sys/lnet/portal_rotor`. There are four options for `*value* `:\n\n- `OFF`\n\n  Disable portal round-robin on all incoming requests.\n\n- `ON`\n\n  Enable portal round-robin on all incoming requests.\n\n- `RR_RT`\n\n  Enable portal round-robin only for routed messages.\n\n- `HASH_RT`\n\n  Routed messages will be delivered to the upper layer by hash of source NID (instead of NID of router.) This is the default value.\n\n### LNet Peer Health\n\nTwo options are available to help determine peer health:\n\n- `peer_timeout`- The timeout (in seconds) before an aliveness query is sent to a peer. For example, if`peer_timeout` is set to `180sec`, an aliveness query is sent to the peer every 180 seconds. This feature only takes effect if the node is configured as an LNet router.\n\n  In a routed environment, the `peer_timeout` feature should always be on (set to a value in seconds) on routers. If the router checker has been enabled, the feature should be turned off by setting it to 0 on clients and servers.\n\n  For a non-routed scenario, enabling the `peer_timeout` option provides health information such as whether a peer is alive or not. For example, a client is able to determine if an MGS or OST is up when it sends it a message. If a response is received, the peer is alive; otherwise a timeout occurs when the request is made.\n\n  In general, `peer_timeout` should be set to no less than the LND timeout setting. For more information about LND timeouts, see [*the section called \u201cSetting Static Timeouts\u201d*](06.02-Lustre%20Parameters.md#setting-static-timeouts).\n\n  When the `o2iblnd`(IB) driver is used, `peer_timeout` should be at least twice the value of the `ko2iblnd`keepalive option. for more information about keepalive options, see [*the section called \u201c `SOCKLND` Kernel TCP/IP LND\u201d*](06.06-Configuration%20Files%20and%20Module%20Parameters.md#socklnd-kernel-tcpip-lnd).", "mimetype": "text/plain", "start_char_idx": 12456, "end_char_idx": 16686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae54a7f7-6482-480d-bba9-74d9a63a8d5b": {"__data__": {"id_": "ae54a7f7-6482-480d-bba9-74d9a63a8d5b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec42e894-1708-49ed-a740-fb33442aaa14", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3fe34427f9a32335818af99dd02515f54802a27d9528246458f8b742a60e1988", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4be836c2-5785-4d22-b85e-ee403b72b180", "node_type": "1", "metadata": {}, "hash": "b3cb46043717997e1279f967cd20c6e2b53d2e67f1f1c62cb4516941c49e3020", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If a response is received, the peer is alive; otherwise a timeout occurs when the request is made.\n\n  In general, `peer_timeout` should be set to no less than the LND timeout setting. For more information about LND timeouts, see [*the section called \u201cSetting Static Timeouts\u201d*](06.02-Lustre%20Parameters.md#setting-static-timeouts).\n\n  When the `o2iblnd`(IB) driver is used, `peer_timeout` should be at least twice the value of the `ko2iblnd`keepalive option. for more information about keepalive options, see [*the section called \u201c `SOCKLND` Kernel TCP/IP LND\u201d*](06.06-Configuration%20Files%20and%20Module%20Parameters.md#socklnd-kernel-tcpip-lnd).\n\n- `avoid_asym_router_failure`\u2013 When set to 1, the router checker running on the client or a server periodically pings all the routers corresponding to the NIDs identified in the routes parameter setting on the node to determine the status of each router interface. The default setting is 1. (For more information about the LNet routes parameter, see [*the section called \u201cSetting the LNet Module routes Parameter\u201d*](02.06-Configuring%20Lustre%20Networking%20(LNet).md#setting-the-lnet-module-routes-parameter).\n\n  A router is considered down if any of its NIDs are down. For example, router X has three NIDs: `Xnid1`, `Xnid2`, and `Xnid3`. A client is connected to the router via `Xnid1`. The client has router checker enabled. The router checker periodically sends a ping to the router via `Xnid1`. The router responds to the ping with the status of each of its NIDs. In this case, it responds with `Xnid1=up`, `Xnid2=up`, `Xnid3=down`. If`avoid_asym_router_failure==1`, the router is considered down if any of its NIDs are down, so router X is considered down and will not be used for routing messages. If `avoid_asym_router_failure==0`, router X will continue to be used for routing messages.\n\nThe following router checker parameters must be set to the maximum value of the corresponding setting for this option on any client or server:\n\n- `dead_router_check_interval`\n- `live_router_check_interval`\n- `router_ping_timeout`\n\nFor example, the `dead_router_check_interval` parameter on any router must be MAX.\n\n## libcfs Tuning\n\nLustre allows binding service threads via CPU Partition Tables (CPTs). This allows the system administrator to fine-tune on which CPU cores the Lustre service threads are run, for both OSS and MDS services, as well as on the client.\n\nCPTs are useful to reserve some cores on the OSS or MDS nodes for system functions such as system monitoring, HA heartbeat, or similar tasks. On the client it may be useful to restrict Lustre RPC service threads to a small subset of cores so that they do not interfere with computation, or because these cores are directly attached to the network interfaces.\n\nBy default, the Lustre software will automatically generate CPU partitions (CPT) based on the number of CPUs in the system. The CPT count can be explicitly set on the libcfs module using `cpu_npartitions=*NUMBER*`. The value of `cpu_npartitions` must be an integer between 1 and the number of online CPUs.\n\nIntroduced in Lustre 2.9In Lustre 2.9 and later the default is to use one CPT per NUMA node. In earlier versions of Lustre, by default there was a single CPT if the online CPU core count was four or fewer, and additional CPTs would be created depending on the number of CPU cores, typically with 4-8 cores per CPT.\n\n**Tip**\n\nSetting `cpu_npartitions=1` will disable most of the SMP Node Affinity functionality.\n\n### CPU Partition String Patterns\n\nCPU partitions can be described using string pattern notation. If `cpu_pattern=N` is used, then there will be one CPT for each NUMA node in the system, with each CPT mapping all of the CPU cores for that NUMA node.", "mimetype": "text/plain", "start_char_idx": 16037, "end_char_idx": 19780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4be836c2-5785-4d22-b85e-ee403b72b180": {"__data__": {"id_": "4be836c2-5785-4d22-b85e-ee403b72b180", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae54a7f7-6482-480d-bba9-74d9a63a8d5b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "08b5a3d8d0d49e2302e64adcef1b668e8362aed1b9227b2954b54db5b8e399d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3db5f6a8-287f-4a01-9514-4cd65c4fe0e3", "node_type": "1", "metadata": {}, "hash": "42db84fde3820abe60f02e9ff3c8bd182ac0f3305faf7a249b2affcf767be959", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The value of `cpu_npartitions` must be an integer between 1 and the number of online CPUs.\n\nIntroduced in Lustre 2.9In Lustre 2.9 and later the default is to use one CPT per NUMA node. In earlier versions of Lustre, by default there was a single CPT if the online CPU core count was four or fewer, and additional CPTs would be created depending on the number of CPU cores, typically with 4-8 cores per CPT.\n\n**Tip**\n\nSetting `cpu_npartitions=1` will disable most of the SMP Node Affinity functionality.\n\n### CPU Partition String Patterns\n\nCPU partitions can be described using string pattern notation. If `cpu_pattern=N` is used, then there will be one CPT for each NUMA node in the system, with each CPT mapping all of the CPU cores for that NUMA node.\n\nIt is also possible to explicitly specify the mapping between CPU cores and CPTs, for example:\n\n- `cpu_pattern=\"0[2,4,6] 1[3,5,7]`\n\n  Create two CPTs, CPT0 contains cores 2, 4, and 6, while CPT1 contains cores 3, 5, 7. CPU cores 0 and 1 will not be used by Lustre service threads, and could be used for node services such as system monitoring, HA heartbeat threads, etc. The binding of non-Lustre services to those CPU cores may be done in userspace using `numactl(8)` or other application-specific methods, but is beyond the scope of this document.\n\n- `cpu_pattern=\"N 0[0-3] 1[4-7]`\n\n  Create two CPTs, with CPT0 containing all CPUs in NUMA node[0-3], while CPT1 contains all CPUs in NUMA node [4-7].\n\nThe current configuration of the CPU partition can be read via `lctl get_parm cpu_partition_table`. For example, a simple 4-core system has a single CPT with all four CPU cores:\n\n```\n$ lctl get_param cpu_partition_table\ncpu_partition_table=0\t: 0 1 2 3\n```\n\nwhile a larger NUMA system with four 12-core CPUs may have four CPTs:\n\n```\n$ lctl get_param cpu_partition_table\ncpu_partition_table=\n0\t: 0 1 2 3 4 5 6 7 8 9 10 11\n1\t: 12 13 14 15 16 17 18 19 20 21 22 23\n2\t: 24 25 26 27 28 29 30 31 32 33 34 35\n3\t: 36 37 38 39 40 41 42 43 44 45 46 47\n```\n\n## LND Tuning\n\nLND tuning allows the number of threads per CPU partition to be specified. An administrator can set the threads for both `ko2iblnd` and `ksocklnd` using the `nscheds` parameter. This adjusts the number of threads for each partition, not the overall number of threads on the LND.\n\n**Note**\n\nThe default number of threads for `ko2iblnd` and `ksocklnd` are automatically set and are chosen to work well across a number of typical scenarios, for systems with both high and low core counts.\n\n### ko2iblnd Tuning\n\nThe following table outlines the ko2iblnd module parameters to be used for tuning:\n\n| **Module Parameter**      | **Default Value**                                      | **Description**                                              |\n| ------------------------- | ------------------------------------------------------ | ------------------------------------------------------------ |\n| `service`                 | `987`                                                  | Service number (within RDMA_PS_TCP).                         |\n| `cksum`                   | `0`                                                    | Set non-zero to enable message (not RDMA) checksums.         |\n| `timeout`                 | `50`                                                   | Timeout in seconds.                                          |\n| `nscheds`                 | `0`                                                    | Number of threads in each scheduler pool (per CPT). Value of zero means we derive the number from the number of cores. |\n| `conns_per_peer`          | `4 (OmniPath), 1 (Everything else)`                    | Introduced in 2.10. Number of connections to each peer. Messages are sent round-robin over the connection pool. Provides significant improvement with OmniPath. |\n| `ntx`                     | `512`                                                  | Number of message descriptors allocated for each pool at startup.", "mimetype": "text/plain", "start_char_idx": 19027, "end_char_idx": 22996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3db5f6a8-287f-4a01-9514-4cd65c4fe0e3": {"__data__": {"id_": "3db5f6a8-287f-4a01-9514-4cd65c4fe0e3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4be836c2-5785-4d22-b85e-ee403b72b180", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6160c0b68ff0d22ff0ae7cd76464d79b172b3a1c7d0ff4e03a8d61d9970e7df1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98cdaeb3-bb01-4f21-9979-b891f090eb0e", "node_type": "1", "metadata": {}, "hash": "d6fef31bb8245f2a54444fa06644d6835087e493076f30bdea15144b01edb884", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `cksum`                   | `0`                                                    | Set non-zero to enable message (not RDMA) checksums.         |\n| `timeout`                 | `50`                                                   | Timeout in seconds.                                          |\n| `nscheds`                 | `0`                                                    | Number of threads in each scheduler pool (per CPT). Value of zero means we derive the number from the number of cores. |\n| `conns_per_peer`          | `4 (OmniPath), 1 (Everything else)`                    | Introduced in 2.10. Number of connections to each peer. Messages are sent round-robin over the connection pool. Provides significant improvement with OmniPath. |\n| `ntx`                     | `512`                                                  | Number of message descriptors allocated for each pool at startup. Grows at runtime. Shared by all CPTs. |\n| `credits`                 | `256`                                                  | Number of concurrent sends on network.                       |\n| `peer_credits`            | `8`                                                    | Number of concurrent sends to 1 peer. Related/limited by IB queue size. |\n| `peer_credits_hiw`        | `0`                                                    | When eagerly to return credits.                              |\n| `peer_buffer_credits`     | `0`                                                    | Number per-peer router buffer credits.                       |\n| `peer_timeout`            | `180`                                                  | Seconds without aliveness news to declare peer dead (less than or equal to 0 to disable). |\n| `ipif_name`               | `ib0`                                                  | IPoIB interface name.                                        |\n| `retry_count`             | `5`                                                    | Retransmissions when no ACK received.                        |\n| `rnr_retry_count`         | `6`                                                    | RNR retransmissions.                                         |\n| `keepalive`               | `100`                                                  | Idle time in seconds before sending a keepalive.             |\n| `ib_mtu`                  | `0`                                                    | IB MTU 256/512/1024/2048/4096.                               |\n| `concurrent_sends`        | `0`                                                    | Send work-queue sizing. If zero, derived from `map_on_demand`and `peer_credits`. |\n| `map_on_demand`           | `0 (pre-4.8 Linux) 1 (4.8 Linux onward) 32 (OmniPath)` | Number of fragments reserved for connection. If zero, use global memory region (found to be security issue). If non-zero, use FMR or FastReg for memory registration. Value needs to agree between both peers of connection. |\n| `fmr_pool_size`           | `512`                                                  | Size of fmr pool on each CPT (>= ntx / 4). Grows at runtime. |\n| `fmr_flush_trigger`       | `384`                                                  | Number dirty FMRs that triggers pool flush.                  |\n| `fmr_cache`               | `1`                                                    | Non-zero to enable FMR caching.                              |\n| `dev_failover`            | `0`                                                    | HCA failover for bonding (0 OFF, 1 ON, other values reserved). |\n| `require_privileged_port` | `0`                                                    | Require privileged port when accepting connection.           |\n| `use_privileged_port`     | `1`                                                    | Use privileged port when initiating connection.              |\n| `wrq_sge`                 | `2`                                                    | Introduced in 2.10. Number scatter/gather element groups per work request. Used to deal with fragmentations which can consume double the number of work requests. |\n\nIntroduced in Lustre 2.4\n\n## Network Request Scheduler (NRS) Tuning\n\nThe Network Request Scheduler (NRS) allows the administrator to influence the order in which RPCs are handled at servers, on a per-PTLRPC service basis, by providing different policies that can be activated and tuned in order to influence the RPC ordering. The aim of this is to provide for better performance, and possibly discrete performance characteristics using future policies.\n\nThe NRS policy state of a PTLRPC service can be read and set via the `{service}.nrs_policies` tunable.", "mimetype": "text/plain", "start_char_idx": 22085, "end_char_idx": 26767, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98cdaeb3-bb01-4f21-9979-b891f090eb0e": {"__data__": {"id_": "98cdaeb3-bb01-4f21-9979-b891f090eb0e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3db5f6a8-287f-4a01-9514-4cd65c4fe0e3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bdbc371cef6fab43743e03e712f1d2731b94a37fda9d5caaf87b5727cd9bd095", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1e9a8b8-f0c1-4d86-a7c6-350eaca359b3", "node_type": "1", "metadata": {}, "hash": "baa1421b604d87321b0f1ba2eb7c0701ddba5ed72522fc4e55b66a0d79e586c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `use_privileged_port`     | `1`                                                    | Use privileged port when initiating connection.              |\n| `wrq_sge`                 | `2`                                                    | Introduced in 2.10. Number scatter/gather element groups per work request. Used to deal with fragmentations which can consume double the number of work requests. |\n\nIntroduced in Lustre 2.4\n\n## Network Request Scheduler (NRS) Tuning\n\nThe Network Request Scheduler (NRS) allows the administrator to influence the order in which RPCs are handled at servers, on a per-PTLRPC service basis, by providing different policies that can be activated and tuned in order to influence the RPC ordering. The aim of this is to provide for better performance, and possibly discrete performance characteristics using future policies.\n\nThe NRS policy state of a PTLRPC service can be read and set via the `{service}.nrs_policies` tunable. To read a PTLRPC service's NRS policy state, run:\n\n```\nlctl get_param {service}.nrs_policies\n```\n\nFor example, to read the NRS policy state of the `ost_io` service, run:\n\n```\n$ lctl get_param ost.OSS.ost_io.nrs_policies\nost.OSS.ost_io.nrs_policies=\n\nregular_requests:\n  - name: fifo\n    state: started\n    fallback: yes\n    queued: 0\n    active: 0\n\n  - name: crrn\n    state: stopped\n    fallback: no\n    queued: 0\n    active: 0\n\n  - name: orr\n    state: stopped\n    fallback: no\n    queued: 0\n    active: 0\n\n  - name: trr\n    state: started\n    fallback: no\n    queued: 2420\n    active: 268\n\n  - name: tbf\n    state: stopped\n    fallback: no\n    queued: 0\n    active: 0\n\n  - name: delay\n    state: stopped\n    fallback: no\n    queued: 0\n    active: 0\n\nhigh_priority_requests:\n  - name: fifo\n    state: started\n    fallback: yes\n    queued: 0\n    active: 0\n\n  - name: crrn\n    state: stopped\n    fallback: no\n    queued: 0\n    active: 0\n\n  - name: orr\n    state: stopped\n    fallback: no\n    queued: 0\n    active: 0\n\n  - name: trr\n    state: stopped\n    fallback: no\n    queued: 0\n    active: 0\n\n  - name: tbf\n    state: stopped\n    fallback: no\n    queued: 0\n    active: 0\n\n  - name: delay\n    state: stopped\n    fallback: no\n    queued: 0\n    active: 0\n```\n\nNRS policy state is shown in either one or two sections, depending on the PTLRPC service being queried. The first section is named `regular_requests` and is available for all PTLRPC services, optionally followed by a second section which is named `high_priority_requests`. This is because some PTLRPC services are able to treat some types of RPCs as higher priority ones, such that they are handled by the server with higher priority compared to other, regular RPC traffic. For PTLRPC services that do not support high-priority RPCs, you will only see the`regular_requests` section.\n\nThere is a separate instance of each NRS policy on each PTLRPC service for handling regular and high-priority RPCs (if the service supports high-priority RPCs). For each policy instance, the following fields are shown:\n\n| **Field**  | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `name`     | The name of the policy.                                      |\n| `state`    | The state of the policy; this can be any of `invalid, stopping, stopped, starting, started`. A fully enabled policy is in the `started` state. |\n| `fallback` | Whether the policy is acting as a fallback policy or not. A fallback policy is used to handle RPCs that other enabled policies fail to handle, or do not support the handling of. The possible values are `no, yes`. Currently, only the FIFO policy can act as a fallback policy. |\n| `queued`   | The number of RPCs that the policy has waiting to be serviced. |\n| `active`   | The number of RPCs that the policy is currently handling.", "mimetype": "text/plain", "start_char_idx": 25807, "end_char_idx": 29669, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1e9a8b8-f0c1-4d86-a7c6-350eaca359b3": {"__data__": {"id_": "b1e9a8b8-f0c1-4d86-a7c6-350eaca359b3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98cdaeb3-bb01-4f21-9979-b891f090eb0e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "53060c71bd0522c52ffab7ab8394c82bf1f4f8dc93e10d78f37d0c1aff16a898", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1190fe6-b130-40a3-a14e-d54ec696349c", "node_type": "1", "metadata": {}, "hash": "e4c71cd3264c175ce91036f7dee8ade7744b753e78a9af1f3e3bf609ac822fb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each policy instance, the following fields are shown:\n\n| **Field**  | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `name`     | The name of the policy.                                      |\n| `state`    | The state of the policy; this can be any of `invalid, stopping, stopped, starting, started`. A fully enabled policy is in the `started` state. |\n| `fallback` | Whether the policy is acting as a fallback policy or not. A fallback policy is used to handle RPCs that other enabled policies fail to handle, or do not support the handling of. The possible values are `no, yes`. Currently, only the FIFO policy can act as a fallback policy. |\n| `queued`   | The number of RPCs that the policy has waiting to be serviced. |\n| `active`   | The number of RPCs that the policy is currently handling.    |\n\nTo enable an NRS policy on a PTLRPC service run:\n\n```\nlctl set_param {service}.nrs_policies=\npolicy_name\n```\n\nThis will enable the policy *policy_name*for both regular and high-priority RPCs (if the PLRPC service supports high-priority RPCs) on the given service. For example, to enable the CRR-N NRS policy for the ldlm_cbd service, run:\n\n```\n$ lctl set_param ldlm.services.ldlm_cbd.nrs_policies=crrn\nldlm.services.ldlm_cbd.nrs_policies=crrn\n      \n```\n\nFor PTLRPC services that support high-priority RPCs, you can also supply an optional *reg|hp*token, in order to enable an NRS policy for handling only regular or high-priority RPCs on a given PTLRPC service, by running:\n\n```\nlctl set_param {service}.nrs_policies=\"\npolicy_name \nreg|hp\"\n```\n\nFor example, to enable the TRR policy for handling only regular, but not high-priority RPCs on the `ost_io` service, run:\n\n```\n$ lctl set_param ost.OSS.ost_io.nrs_policies=\"trr reg\"\nost.OSS.ost_io.nrs_policies=\"trr reg\"\n      \n```\n\n**Note**\n\nWhen enabling an NRS policy, the policy name must be given in lower-case characters, otherwise the operation will fail with an error message.\n\n### First In, First Out (FIFO) policy\n\nThe first in, first out (FIFO) policy handles RPCs in a service in the same order as they arrive from the LNet layer, so no special processing takes place to modify the RPC handling stream. FIFO is the default policy for all types of RPCs on all PTLRPC services, and is always enabled irrespective of the state of other policies, so that it can be used as a backup policy, in case a more elaborate policy that has been enabled fails to handle an RPC, or does not support handling a given type of RPC.\n\nThe FIFO policy has no tunables that adjust its behaviour.\n\n### Client Round-Robin over NIDs (CRR-N) policy      \n\nThe client round-robin over NIDs (CRR-N) policy performs batched round-robin scheduling of all types of RPCs, with each batch consisting of RPCs originating from the same client node, as identified by its NID. CRR-N aims to provide for better resource utilization across the cluster,       and to help shorten completion times of jobs in some cases, by distributing available bandwidth more evenly across all clients.\n\nThe CRR-N policy can be enabled on all types of PTLRPC services, and has the following tunable that can be used to adjust its behavior:\n\n-  `{service}.nrs_crrn_quantum`           \n\n  The  `{service}.nrs_crrn_quantum` tunable determines the maximum allowed size of each batch of RPCs; the unit of measure is in number of RPCs.", "mimetype": "text/plain", "start_char_idx": 28772, "end_char_idx": 32207, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1190fe6-b130-40a3-a14e-d54ec696349c": {"__data__": {"id_": "e1190fe6-b130-40a3-a14e-d54ec696349c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1e9a8b8-f0c1-4d86-a7c6-350eaca359b3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d034e23274cdc641ac307cf1a74abdabc889647a1d31889d759b13d114147ad4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75157706-cebc-48b7-b47a-4033eaa07997", "node_type": "1", "metadata": {}, "hash": "fdb7e7cbdc3bcbb2f400c928375eb0fceadb6d772aa63a064caaf414bf20f95b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Client Round-Robin over NIDs (CRR-N) policy      \n\nThe client round-robin over NIDs (CRR-N) policy performs batched round-robin scheduling of all types of RPCs, with each batch consisting of RPCs originating from the same client node, as identified by its NID. CRR-N aims to provide for better resource utilization across the cluster,       and to help shorten completion times of jobs in some cases, by distributing available bandwidth more evenly across all clients.\n\nThe CRR-N policy can be enabled on all types of PTLRPC services, and has the following tunable that can be used to adjust its behavior:\n\n-  `{service}.nrs_crrn_quantum`           \n\n  The  `{service}.nrs_crrn_quantum` tunable determines the maximum allowed size of each batch of RPCs; the unit of measure is in number of RPCs. To read the maximum allowed batch size of a CRR-N policy, run:\n\n  ```\n  lctl get_param {service}.nrs_crrn_quantum\n  ```\n\n  For example, to read the maximum allowed batch size of a CRR-N policy on the ost_io service, run:\n\n  ```\n  $ lctl get_param ost.OSS.ost_io.nrs_crrn_quantum\n  ost.OSS.ost_io.nrs_crrn_quantum=reg_quantum:16\n  hp_quantum:8\n  ```\n\n  You can see that there is a separate maximum allowed batch size value for regular (           `reg_quantum`) and high-priority ( `hp_quantum`) RPCs (if the PTLRPC service supports           high-priority RPCs).\n\n  To set the maximum allowed batch size of a CRR-N policy on a given service, run:\n\n  ```\n  lctl set_param {service}.nrs_crrn_quantum=\n  1-65535\n  ```\n\n  This will set the maximum allowed batch size on a given service, for both regular and high-priority RPCs (if the PLRPC  service supports high-priority RPCs), to the indicated value.\n\n  For example, to set the maximum allowed batch size on the ldlm_canceld service to 16 RPCs, run:\n\n  ```\n  $ lctl set_param ldlm.services.ldlm_canceld.nrs_crrn_quantum=16\n  ldlm.services.ldlm_canceld.nrs_crrn_quantum=16     \n  ```\n\n  For PTLRPC services that support high-priority RPCs, you can also specify a different maximum allowed batch size for regular and high-priority RPCs, by running:\n\n  ```\n  $ lctl set_param {service}.nrs_crrn_quantum=\n  reg_quantum|hp_quantum:\n  1-65535\"\n  ```\n\n  For example, to set the maximum allowed batch size on the ldlm_canceld service, for high-priority RPCs to 32, run:\n\n  ```\n  $ lctl set_param ldlm.services.ldlm_canceld.nrs_crrn_quantum=\"hp_quantum:32\"\n  ldlm.services.ldlm_canceld.nrs_crrn_quantum=hp_quantum:32    \n  ```\n\n  By using the last method, you can also set the maximum regular and high-priority RPC batch sizes to different values, in a single command invocation.\n\n### Object-based Round-Robin (ORR) policy\n\nThe object-based round-robin (ORR) policy performs batched round-robin scheduling of bulk read write (brw) RPCs, with each batch consisting of RPCs that pertain to the same backend-file system object, as identified by its OST FID.\n\nThe ORR policy is only available for use on the ost_io service. The RPC batches it forms can potentially consist of mixed bulk read and bulk write RPCs. The RPCs in each batch are ordered in an ascending manner, based on either the file offsets, or the physical disk offsets of each RPC (only applicable to bulk read RPCs).\n\nThe aim of the ORR policy is to provide for increased bulk read throughput in some cases, by ordering bulk read RPCs (and potentially bulk write RPCs), and thus minimizing costly disk seek operations. Performance may also benefit from any resulting improvement in resource utilization, or by taking advantage of better locality of reference between RPCs.", "mimetype": "text/plain", "start_char_idx": 31408, "end_char_idx": 34983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75157706-cebc-48b7-b47a-4033eaa07997": {"__data__": {"id_": "75157706-cebc-48b7-b47a-4033eaa07997", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1190fe6-b130-40a3-a14e-d54ec696349c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f4cf31845550a0c1a7c24e29ec918f1be5ff7841363392ba4b4ee69ac1ad7c96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7f8500a-8f5b-46d6-98f8-0f7b5f35b110", "node_type": "1", "metadata": {}, "hash": "2b370f2e683d6cccd8afecb785db4eb48d250b202adeb958c73b7cad9abc8cba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The ORR policy is only available for use on the ost_io service. The RPC batches it forms can potentially consist of mixed bulk read and bulk write RPCs. The RPCs in each batch are ordered in an ascending manner, based on either the file offsets, or the physical disk offsets of each RPC (only applicable to bulk read RPCs).\n\nThe aim of the ORR policy is to provide for increased bulk read throughput in some cases, by ordering bulk read RPCs (and potentially bulk write RPCs), and thus minimizing costly disk seek operations. Performance may also benefit from any resulting improvement in resource utilization, or by taking advantage of better locality of reference between RPCs.\n\nThe ORR policy has the following tunables that can be used to adjust its behaviour:\n\n- `ost.OSS.ost_io.nrs_orr_quantum`\n\n  The `ost.OSS.ost_io.nrs_orr_quantum` tunable determines the maximum allowed size of each batch of RPCs; the unit of measure is in number of RPCs. To read the maximum allowed batch size of the ORR policy, run:\n\n  ```\n  $ lctl get_param ost.OSS.ost_io.nrs_orr_quantum\n  ost.OSS.ost_io.nrs_orr_quantum=reg_quantum:256\n  hp_quantum:16\n            \n  ```\n\n  You can see that there is a separate maximum allowed batch size value for regular ( `reg_quantum`) and high-priority ( `hp_quantum`) RPCs (if the PTLRPC service supports high-priority RPCs).\n\n  To set the maximum allowed batch size for the ORR policy, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_quantum=\n  1-65535\n  ```\n\n  This will set the maximum allowed batch size for both regular and high-priority RPCs, to the indicated value.\n\n  You can also specify a different maximum allowed batch size for regular and high-priority RPCs, by running:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_quantum=\n  reg_quantum|hp_quantum:\n  1-65535\n  ```\n\n  For example, to set the maximum allowed batch size for regular RPCs to 128, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_quantum=reg_quantum:128\n  ost.OSS.ost_io.nrs_orr_quantum=reg_quantum:128\n            \n  ```\n\n  By using the last method, you can also set the maximum regular and high-priority RPC batch sizes to different values, in a single command invocation.\n\n- `ost.OSS.ost_io.nrs_orr_offset_type`\n\n  The `ost.OSS.ost_io.nrs_orr_offset_type` tunable determines whether the ORR policy orders RPCs within each batch based on logical file offsets or physical disk offsets. To read the offset type value for the ORR policy, run:\n\n  ```\n  $ lctl get_param ost.OSS.ost_io.nrs_orr_offset_type\n  ost.OSS.ost_io.nrs_orr_offset_type=reg_offset_type:physical\n  hp_offset_type:logical\n            \n  ```\n\n  You can see that there is a separate offset type value for regular ( `reg_offset_type`) and high-priority (`hp_offset_type`) RPCs.\n\n  To set the ordering type for the ORR policy, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_offset_type=\n  physical|logical\n  ```\n\n  This will set the offset type for both regular and high-priority RPCs, to the indicated value.\n\n  You can also specify a different offset type for regular and high-priority RPCs, by running:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_offset_type=\n  reg_offset_type|hp_offset_type:\n  physical|logical\n  ```\n\n  For example, to set the offset type for high-priority RPCs to physical disk offsets, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_offset_type=hp_offset_type:physical\n  ost.OSS.ost_io.nrs_orr_offset_type=hp_offset_type:physical\n  ```\n\n  By using the last method, you can also set offset type for regular and high-priority RPCs to different values, in a single command invocation.\n\n  **Note**\n\n  Irrespective of the value of this tunable, only logical offsets can, and are used for ordering bulk write RPCs.", "mimetype": "text/plain", "start_char_idx": 34304, "end_char_idx": 38034, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7f8500a-8f5b-46d6-98f8-0f7b5f35b110": {"__data__": {"id_": "a7f8500a-8f5b-46d6-98f8-0f7b5f35b110", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75157706-cebc-48b7-b47a-4033eaa07997", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a01cb25a46040d1e39ad58e48c8e3ae789176b37606aab9b5846f6e48e89fc0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbc4d4d3-4a74-4974-a6b1-35f212b59ddc", "node_type": "1", "metadata": {}, "hash": "863581cb486413d439750631d820a4c3f7e3c0740f40f398f6ac80f3f7ba24b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can also specify a different offset type for regular and high-priority RPCs, by running:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_offset_type=\n  reg_offset_type|hp_offset_type:\n  physical|logical\n  ```\n\n  For example, to set the offset type for high-priority RPCs to physical disk offsets, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_offset_type=hp_offset_type:physical\n  ost.OSS.ost_io.nrs_orr_offset_type=hp_offset_type:physical\n  ```\n\n  By using the last method, you can also set offset type for regular and high-priority RPCs to different values, in a single command invocation.\n\n  **Note**\n\n  Irrespective of the value of this tunable, only logical offsets can, and are used for ordering bulk write RPCs.\n\n- `ost.OSS.ost_io.nrs_orr_supported`\n\n  The `ost.OSS.ost_io.nrs_orr_supported` tunable determines the type of RPCs that the ORR policy will handle. To read the types of supported RPCs by the ORR policy, run:\n\n  ```\n  $ lctl get_param ost.OSS.ost_io.nrs_orr_supported\n  ost.OSS.ost_io.nrs_orr_supported=reg_supported:reads\n  hp_supported=reads_and_writes\n            \n  ```\n\n  You can see that there is a separate supported 'RPC types' value for regular ( `reg_supported`) and high-priority ( `hp_supported`) RPCs.\n\n  To set the supported RPC types for the ORR policy, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_supported=\n  reads|writes|reads_and_writes\n  ```\n\n  This will set the supported RPC types for both regular and high-priority RPCs, to the indicated value.\n\n  You can also specify a different supported 'RPC types' value for regular and high-priority RPCs, by running:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_orr_supported=\n  reg_supported|hp_supported:\n  reads|writes|reads_and_writes\n  ```\n\n  For example, to set the supported RPC types to bulk read and bulk write RPCs for regular requests, run:\n\n  ```\n  $ lctl set_param\n  ost.OSS.ost_io.nrs_orr_supported=reg_supported:reads_and_writes\n  ost.OSS.ost_io.nrs_orr_supported=reg_supported:reads_and_writes\n            \n  ```\n\n  By using the last method, you can also set the supported RPC types for regular and high-priority RPC to different values, in a single command invocation.\n\n### Target-based Round-Robin (TRR) policy\n\nThe target-based round-robin (TRR) policy performs batched round-robin scheduling of brw RPCs, with each batch consisting of RPCs that pertain to the same OST, as identified by its OST index.\n\nThe TRR policy is identical to the object-based round-robin (ORR) policy, apart from using the brw RPC's target OST index instead of the backend-fs object's OST FID, for determining the RPC scheduling order. The goals of TRR are effectively the same as for ORR, and it uses the following tunables to adjust its behaviour:\n\n  - `ost.OSS.ost_io.nrs_trr_quantum`\n\n    The purpose of this tunable is exactly the same as for the `ost.OSS.ost_io.nrs_orr_quantum` tunable for the ORR policy, and you can use it in exactly the same way.\n\n  - `ost.OSS.ost_io.nrs_trr_offset_type`\n\n    The purpose of this tunable is exactly the same as for the `ost.OSS.ost_io.nrs_orr_offset_type` tunable for the ORR policy, and you can use it in exactly the same way.\n\n  - `ost.OSS.ost_io.nrs_trr_supported`\n\n    The purpose of this tunable is exactly the same as for the `ost.OSS.ost_io.nrs_orr_supported` tunable for the ORR policy, and you can use it in exactly the sme way.\n\n    \n\n  Introduced in Lustre 2.6\n\n  ### Token Bucket Filter (TBF) policy\n\nThe TBF (Token Bucket Filter) is a Lustre NRS policy which enables Lustre services to enforce the RPC rate limit on clients/jobs for QoS (Quality of Service) purposes.\n\n  **Figure 20. The internal structure of TBF policy**\n\n  !", "mimetype": "text/plain", "start_char_idx": 37303, "end_char_idx": 40987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbc4d4d3-4a74-4974-a6b1-35f212b59ddc": {"__data__": {"id_": "dbc4d4d3-4a74-4974-a6b1-35f212b59ddc", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7f8500a-8f5b-46d6-98f8-0f7b5f35b110", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "670ffbf16b61c5ab18651959ebda1cf4e6162bd3e7e59dc4fb40c1feeb5abe55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7aea255-df11-43fe-9485-423bea33c358", "node_type": "1", "metadata": {}, "hash": "2a8a371eaff06768cae4c6ce45d83815db8a5f93bea5e95cdb38f3ca60163539", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `ost.OSS.ost_io.nrs_trr_supported`\n\n    The purpose of this tunable is exactly the same as for the `ost.OSS.ost_io.nrs_orr_supported` tunable for the ORR policy, and you can use it in exactly the sme way.\n\n    \n\n  Introduced in Lustre 2.6\n\n  ### Token Bucket Filter (TBF) policy\n\nThe TBF (Token Bucket Filter) is a Lustre NRS policy which enables Lustre services to enforce the RPC rate limit on clients/jobs for QoS (Quality of Service) purposes.\n\n  **Figure 20. The internal structure of TBF policy**\n\n  ![The internal structure of TBF policy](figures/TBF_policy.svg)When a RPC request arrives, TBF policy puts it to a waiting queue according to its classification. The classification of RPC requests is based on either NID or JobID of the RPC according to the configure of TBF. TBF policy maintains multiple queues in the system, one queue for each category in the classification of RPC requests. The requests waits for tokens in the FIFO queue before they have been handled so as to keep the RPC rates under the limits.\n\nWhen Lustre services are too busy to handle all of the requests in time, all of the specified rates of the queues will not be satisfied. Nothing bad will happen except some of the RPC rates are slower than configured. In this case, the queue with higher rate will have an advantage over the queues with lower rates, but none of them will be starved.\n\nTo manage the RPC rate of queues, we don't need to set the rate of each queue manually. Instead, we define rules which TBF policy matches to determine RPC rate limits. All of the defined rules are organized as an ordered list. Whenever a queue is newly created, it goes though the rule list and takes the first matched rule as its rule, so that the queue knows its RPC token rate. A rule can be added to or removed from the list at run time. Whenever the list of rules is changed, the queues will update their matched rules.\n\n#### Enable TBF policy\n\nCommand:\n\n```\nlctl set_param ost.OSS.ost_io.nrs_policies=\"tbf <policy>\"\n\t\n```\n\nFor now, the RPCs can be classified into the different types according to their NID, JOBID, OPCode and UID/GID. When enabling TBF policy, you can specify one of the types, or just use \"tbf\" to enable all of them to do a fine-grained RPC requests classification.\n\nExample:\n\n```\n$ lctl set_param ost.OSS.ost_io.nrs_policies=\"tbf\"\n$ lctl set_param ost.OSS.ost_io.nrs_policies=\"tbf nid\"\n$ lctl set_param ost.OSS.ost_io.nrs_policies=\"tbf jobid\"\n$ lctl set_param ost.OSS.ost_io.nrs_policies=\"tbf opcode\"\n$ lctl set_param ost.OSS.ost_io.nrs_policies=\"tbf uid\"\n$ lctl set_param ost.OSS.ost_io.nrs_policies=\"tbf gid\"\n```\n\n#### Start a TBF rule\n\nThe TBF rule is defined in the parameter `ost.OSS.ost_io.nrs_tbf_rule`.\n\nCommand:\n\n```\nlctl set_param x.x.x.nrs_tbf_rule=\n\"[reg|hp] start rule_name arguments...\"\n```\n\n'*rule_name*' is a string up to 15 characters which identifies the TBF policy rule's name. Alphanumeric characters and underscores are accepted (e.g: \"test_rule_A1\").\n\n'arguments' is a string to specify the detailed rule according to the different types.\n\nNext, the different types of TBF policies will be described.\n\n- **NID based TBF policy**\n\n  Command:\n\n  ```\n  lctl set_param x.x.x.nrs_tbf_rule=\n  \"[reg|hp] start rule_name nid={nidlist} rate=rate\"\n  \t    \n  ```\n\n  '*nidlist*' uses the same format as configuring LNET route. '*rate*' is the (upper limit) RPC rate of the rule.\n\n  Example:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start other_clients nid={192.168.*.*@tcp} rate=50\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start computes nid={192.168.1.", "mimetype": "text/plain", "start_char_idx": 40478, "end_char_idx": 44075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7aea255-df11-43fe-9485-423bea33c358": {"__data__": {"id_": "c7aea255-df11-43fe-9485-423bea33c358", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbc4d4d3-4a74-4974-a6b1-35f212b59ddc", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "eaee0bac4b6c3371d603c89ce80e3b7bbc8a0ee984e4112ae1a68a39ffcfa993", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75128155-3564-440e-b39f-e13abf77c8e6", "node_type": "1", "metadata": {}, "hash": "16f9d17f7f8518362eeb1bc93f47cef3ddf2c0cca072d2242b912fe6ff1296a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "'arguments' is a string to specify the detailed rule according to the different types.\n\nNext, the different types of TBF policies will be described.\n\n- **NID based TBF policy**\n\n  Command:\n\n  ```\n  lctl set_param x.x.x.nrs_tbf_rule=\n  \"[reg|hp] start rule_name nid={nidlist} rate=rate\"\n  \t    \n  ```\n\n  '*nidlist*' uses the same format as configuring LNET route. '*rate*' is the (upper limit) RPC rate of the rule.\n\n  Example:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start other_clients nid={192.168.*.*@tcp} rate=50\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start computes nid={192.168.1.[2-128]@tcp} rate=500\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start loginnode nid={192.168.1.1@tcp} rate=100\"\n  ```\n\n  In this example, the rate of processing RPC requests from compute nodes is at most 5x as fast as those from login nodes. The output of `ost.OSS.ost_io.nrs_tbf_rule` is like:\n\n  ```\n  lctl get_param ost.OSS.ost_io.nrs_tbf_rule\n  ost.OSS.ost_io.nrs_tbf_rule=\n  regular_requests:\n  CPT 0:\n  loginnode {192.168.1.1@tcp} 100, ref 0\n  computes {192.168.1.[2-128]@tcp} 500, ref 0\n  other_clients {192.168.*.*@tcp} 50, ref 0\n  default {*} 10000, ref 0\n  high_priority_requests:\n  CPT 0:\n  loginnode {192.168.1.1@tcp} 100, ref 0\n  computes {192.168.1.[2-128]@tcp} 500, ref 0\n  other_clients {192.168.*.*@tcp} 50, ref 0\n  default {*} 10000, ref 0\n  ```\n\n  Also, the rule can be written in `reg` and `hp` formats:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"reg start loginnode nid={192.168.1.1@tcp} rate=100\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"hp start loginnode nid={192.168.1.1@tcp} rate=100\"\n  ```\n\n- **JobID based TBF policy**\n\n  For the JobID, please see [*the section called \u201c Lustre Jobstats\u201d*](03.01-Monitoring%20a%20Lustre%20File%20System.md#lustre-jobstats) for more details.\n\n  Command:\n\n  ```\n  lctl set_param x.x.x.nrs_tbf_rule=\n  \"[reg|hp] start rule_name jobid={jobid_list} rate=rate\"    \n  ```\n  \n\nWildcard is supported in {*jobid_list*}.\n\nExample:\n\n```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start iozone_user jobid={iozone.500} rate=100\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start dd_user jobid={dd.", "mimetype": "text/plain", "start_char_idx": 43456, "end_char_idx": 45674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75128155-3564-440e-b39f-e13abf77c8e6": {"__data__": {"id_": "75128155-3564-440e-b39f-e13abf77c8e6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7aea255-df11-43fe-9485-423bea33c358", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "911e20ee685cc311d3ccd4eaaa684b105c6cdacbf290c1a3840e3658085bcdf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "769ef2a2-22b4-41ed-bb8d-6482c4c95574", "node_type": "1", "metadata": {}, "hash": "8a431009cc5cd79ab2cfd566456f9e02e81dbba524b482dc35abbed230cd71ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Command:\n\n  ```\n  lctl set_param x.x.x.nrs_tbf_rule=\n  \"[reg|hp] start rule_name jobid={jobid_list} rate=rate\"    \n  ```\n  \n\nWildcard is supported in {*jobid_list*}.\n\nExample:\n\n```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start iozone_user jobid={iozone.500} rate=100\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start dd_user jobid={dd.*} rate=50\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start user1 jobid={*.600} rate=10\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start user2 jobid={io*.10* *.500} rate=200\"\n```\n\nAlso, the rule can be written in `reg` and `hp` formats:\n\n```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"hp start iozone_user1 jobid={iozone.500} rate=100\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"reg start iozone_user1 jobid={iozone.500} rate=100\"\n```\n\n- **Opcode based TBF policy**\n\n  Command:\n\n  ```\n  $ lctl set_param x.x.x.nrs_tbf_rule=\n  \"[reg|hp] start rule_name opcode={opcode_list} rate=rate\"\n  \t    \n  ```\n\n  Example:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start user1 opcode={ost_read} rate=100\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start iozone_user1 opcode={ost_read ost_write} rate=200\"\n  ```\n\n  Also, the rule can be written in `reg` and `hp` formats:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"hp start iozone_user1 opcode={ost_read} rate=100\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"reg start iozone_user1 opcode={ost_read} rate=100\"\n  ```\n\n- **UID/GID based TBF policy**\n\n  Command:\n\n  ```\n  $ lctl set_param ost.OSS.*.nrs_tbf_rule=\\\n  \"[reg][hp] start rule_name uid={uid} rate=rate\"\n  $ lctl set_param ost.OSS.*.nrs_tbf_rule=\\\n  \"[reg][hp] start rule_name gid={gid} rate=rate\"\n  ```\n\n  Exapmle:\n\n  Limit the rate of RPC requests of the uid 500\n\n  ```\n  $ lctl set_param ost.OSS.*.nrs_tbf_rule=\\\n  \"start tbf_name uid={500} rate=100\"\n  ```\n\n  Limit the rate of RPC requests of the gid 500\n\n  ```\n  $ lctl set_param ost.OSS.*.nrs_tbf_rule=\\\n  \"start tbf_name gid={500} rate=100\"\n  ```\n\n  Also, you can use the following rule to control all reqs to mds:\n\n  Start the tbf uid QoS on MDS:\n\n  ```\n  $ lctl set_param mds.MDS.*.nrs_policies=\"tbf uid\"\n  ```\n\n  Limit the rate of RPC requests of the uid 500\n\n  ```\n  $ lctl set_param mds.MDS.*.nrs_tbf_rule=\\\n  \"start tbf_name uid={500} rate=100\"\n  ```\n\n- **Policy combination**\n\n  To support TBF rules with complex expressions of conditions, TBF classifier is extented to classify RPC in a more fine-grained way. This feature supports logical conditional conjunction and disjunction operations among different types. In the rule: \"&\" represents the conditional conjunction and \",\" represents the conditional disjunction.\n\n  Example:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start comp_rule opcode={ost_write}&jobid={dd.0},\\\n  nid={192.168.1.", "mimetype": "text/plain", "start_char_idx": 45318, "end_char_idx": 48176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "769ef2a2-22b4-41ed-bb8d-6482c4c95574": {"__data__": {"id_": "769ef2a2-22b4-41ed-bb8d-6482c4c95574", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75128155-3564-440e-b39f-e13abf77c8e6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5ce2e37f34c8daef41c377c98d28d70c3a5493552e6c533e338a13f708892653", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fa1029f-9ae1-45c1-82d9-196f4786204f", "node_type": "1", "metadata": {}, "hash": "1b9dbc2a97269a2ca6a13c472c4f370d8ee3580120c13dfc59e8ce84dfde8ddc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*.nrs_policies=\"tbf uid\"\n  ```\n\n  Limit the rate of RPC requests of the uid 500\n\n  ```\n  $ lctl set_param mds.MDS.*.nrs_tbf_rule=\\\n  \"start tbf_name uid={500} rate=100\"\n  ```\n\n- **Policy combination**\n\n  To support TBF rules with complex expressions of conditions, TBF classifier is extented to classify RPC in a more fine-grained way. This feature supports logical conditional conjunction and disjunction operations among different types. In the rule: \"&\" represents the conditional conjunction and \",\" represents the conditional disjunction.\n\n  Example:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start comp_rule opcode={ost_write}&jobid={dd.0},\\\n  nid={192.168.1.[1-128]@tcp 0@lo} rate=100\"\n  ```\n\n  In this example, those RPCs whose `opcode` is ost_write and `jobid` is dd.0, or `nid` satisfies the condition of {192.168.1.[1-128]@tcp 0@lo} will be processed at the rate of 100 req/sec. The output of `ost.OSS.ost_io.nrs_tbf_rule`is like:\n\n  ```\n  $ lctl get_param ost.OSS.ost_io.nrs_tbf_rule\n  ost.OSS.ost_io.nrs_tbf_rule=\n  regular_requests:\n  CPT 0:\n  comp_rule opcode={ost_write}&jobid={dd.0},nid={192.168.1.[1-128]@tcp 0@lo} 100, ref 0\n  default * 10000, ref 0\n  CPT 1:\n  comp_rule opcode={ost_write}&jobid={dd.0},nid={192.168.1.[1-128]@tcp 0@lo} 100, ref 0\n  default * 10000, ref 0\n  high_priority_requests:\n  CPT 0:\n  comp_rule opcode={ost_write}&jobid={dd.0},nid={192.168.1.[1-128]@tcp 0@lo} 100, ref 0\n  default * 10000, ref 0\n  CPT 1:\n  comp_rule opcode={ost_write}&jobid={dd.0},nid={192.168.1.[1-128]@tcp 0@lo} 100, ref 0\n  default * 10000, ref 0\n  ```\n\n  Example:\n\n  ```\n  $ lctl set_param ost.OSS.*.nrs_tbf_rule=\\\n  \"start tbf_name uid={500}&gid={500} rate=100\"\n  ```\n\n  In this example, those RPC requests whose uid is 500 and gid is 500 will be processed at the rate of 100 req/sec.\n\n#### Change a TBF rule\n\nCommand:\n\n```\nlctl set_param x.x.x.nrs_tbf_rule=\n\"[reg|hp] change rule_name rate=rate\"\n          \n```\n\nExample:\n\n```\n$ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n\"change loginnode rate=200\"\n$ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n\"reg change loginnode rate=200\"\n$ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n\"hp change loginnode rate=200\"\n```\n\n#### Stop a TBF rule\n\nCommand:\n\n```\nlctl set_param x.x.x.nrs_tbf_rule=\"[reg|hp] stop\nrule_name\"\n```\n\nExample:\n\n```\n$ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\"stop loginnode\"\n$ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\"reg stop loginnode\"\n$ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\"hp stop loginnode\"\n```\n\n#### Rule options\n\nTo support more flexible rule conditions, the following options are added.\n\n- **Reordering of TBF rules**\n\n  By default, a newly started rule is prior to the old ones, but by specifying the argument '`rank=`' when inserting a new rule with \"`start`\" command, the rank of the rule can be changed. Also, it can be changed by \"`change`\" command.", "mimetype": "text/plain", "start_char_idx": 47494, "end_char_idx": 50365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4fa1029f-9ae1-45c1-82d9-196f4786204f": {"__data__": {"id_": "4fa1029f-9ae1-45c1-82d9-196f4786204f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "769ef2a2-22b4-41ed-bb8d-6482c4c95574", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "68d45b84c6056ebe4c29ac6fcd3a917903662a24c035065365292fb5a23bf363", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f158caa-8625-4ee3-96ba-4789d5ee53e4", "node_type": "1", "metadata": {}, "hash": "b989d3d26ec640cb5f9e39759cf0a30a00dc9fa5bf909d8a5b1b4b15cdfd05d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Reordering of TBF rules**\n\n  By default, a newly started rule is prior to the old ones, but by specifying the argument '`rank=`' when inserting a new rule with \"`start`\" command, the rank of the rule can be changed. Also, it can be changed by \"`change`\" command.\n\n  Command:\n\n  ```\n  lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\n  \"start rule_name arguments... rank=obj_rule_name\"\n  lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\n  \"change rule_name rate=rate rank=obj_rule_name\"\n  ```\n\n  By specifying the existing rule '*obj_rule_name*', the new rule '*rule_name*' will be moved to the front of '*obj_rule_name*'.\n\n  Example:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start computes nid={192.168.1.[2-128]@tcp} rate=500\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start user1 jobid={iozone.500 dd.500} rate=100\"\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\\\n  \"start iozone_user1 opcode={ost_read ost_write} rate=200 rank=computes\"\n  ```\n\n  In this example, rule \"iozone_user1\" is added to the front of rule \"computes\". We can see the order by the following command:\n\n  ```\n  $ lctl get_param ost.OSS.ost_io.nrs_tbf_rule\n  ost.OSS.ost_io.nrs_tbf_rule=\n  regular_requests:\n  CPT 0:\n  user1 jobid={iozone.500 dd.500} 100, ref 0\n  iozone_user1 opcode={ost_read ost_write} 200, ref 0\n  computes nid={192.168.1.[2-128]@tcp} 500, ref 0\n  default * 10000, ref 0\n  CPT 1:\n  user1 jobid={iozone.500 dd.500} 100, ref 0\n  iozone_user1 opcode={ost_read ost_write} 200, ref 0\n  computes nid={192.168.1.[2-128]@tcp} 500, ref 0\n  default * 10000, ref 0\n  high_priority_requests:\n  CPT 0:\n  user1 jobid={iozone.500 dd.500} 100, ref 0\n  iozone_user1 opcode={ost_read ost_write} 200, ref 0\n  computes nid={192.168.1.[2-128]@tcp} 500, ref 0\n  default * 10000, ref 0\n  CPT 1:\n  user1 jobid={iozone.500 dd.500} 100, ref 0\n  iozone_user1 opcode={ost_read ost_write} 200, ref 0\n  computes nid={192.168.1.[2-128]@tcp} 500, ref 0\n  default * 10000, ref 0\n  ```\n\n- **TBF realtime policies under congestion**\n\n  During TBF evaluation, we find that when the sum of I/O bandwidth requirements for all classes exceeds the system capacity, the classes with the same rate limits get less bandwidth than if preconfigured evenly. The reason for this is the heavy load on a congested server will result in some missed deadlines for some classes. The number of the calculated tokens may be larger than 1 during dequeuing. In the original implementation, all classes are equally handled to simply discard exceeding tokens.\n\n  Thus, a Hard Token Compensation (HTC) strategy has been implemented. A class can be configured with the HTC feature by the rule it matches. This feature means that requests in this kind of class queues have high real-time requirements and that the bandwidth assignment must be satisfied as good as possible. When deadline misses happen, the class keeps the deadline unchanged and the time residue(the remainder of elapsed time divided by 1/r) is compensated to the next round. This ensures that the next idle I/O thread will always select this class to serve until all accumulated exceeding tokens are handled or there are no pending requests in the class queue.", "mimetype": "text/plain", "start_char_idx": 50099, "end_char_idx": 53281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f158caa-8625-4ee3-96ba-4789d5ee53e4": {"__data__": {"id_": "2f158caa-8625-4ee3-96ba-4789d5ee53e4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fa1029f-9ae1-45c1-82d9-196f4786204f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8b782acca29f92381a638b06869c41cd7298a0fbd00066c6d800a9433c53fcb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a59ad66-2d74-48ad-ae9c-2a458860adb8", "node_type": "1", "metadata": {}, "hash": "ec395cee31282918b16462fd310d4196c8e12595499dad230fcabb612484e594", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The reason for this is the heavy load on a congested server will result in some missed deadlines for some classes. The number of the calculated tokens may be larger than 1 during dequeuing. In the original implementation, all classes are equally handled to simply discard exceeding tokens.\n\n  Thus, a Hard Token Compensation (HTC) strategy has been implemented. A class can be configured with the HTC feature by the rule it matches. This feature means that requests in this kind of class queues have high real-time requirements and that the bandwidth assignment must be satisfied as good as possible. When deadline misses happen, the class keeps the deadline unchanged and the time residue(the remainder of elapsed time divided by 1/r) is compensated to the next round. This ensures that the next idle I/O thread will always select this class to serve until all accumulated exceeding tokens are handled or there are no pending requests in the class queue.\n\n  Command:\n\n  A new command format is added to enable the realtime feature for a rule:\n\n  ```\n  lctl set_param x.x.x.nrs_tbf_rule=\\\n  \"start rule_name arguments... realtime=1\n  ```\n\n  Example:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_tbf_rule=\n  \"start realjob jobid={dd.0} rate=100 realtime=1\n  ```\n\n  This example rule means the RPC requests whose JobID is dd.0 will be processed at the rate of 100req/sec in realtime.\n\n \n\nIntroduced in Lustre 2.10\n\n### Delay policy\n\nThe NRS Delay policy seeks to perturb the timing of request processing at the PtlRPC layer, with the goal of simulating high server load, and finding and exposing timing related problems. When this policy is active, upon arrival of a request the policy will calculate an offset, within a defined, user-configurable range, from the request arrival time, to determine a time after which the request should be handled. The request is then stored using the cfs_binheap implementation, which sorts the request according to the assigned start time. Requests are removed from the binheap for handling once their start time has been passed.\n\nThe Delay policy can be enabled on all types of PtlRPC services, and has the following tunables that can be used to adjust its behavior:\n\n- `{service}.nrs_delay_min`\n\n  The `{service}.nrs_delay_min` tunable controls the minimum amount of time, in seconds, that a request will be delayed by this policy. The default is 5 seconds. To read this value run:\n\n  ```\n  lctl get_param {service}.nrs_delay_min\n  ```\n\n  For example, to read the minimum delay set on the ost_io service, run:\n\n  ```\n  $ lctl get_param ost.OSS.ost_io.nrs_delay_min\n  ost.OSS.ost_io.nrs_delay_min=reg_delay_min:5\n  hp_delay_min:5\n  ```\n\n  To set the minimum delay in RPC processing, run:\n\n  ```\n  lctl set_param {service}.nrs_delay_min=0-65535\n  ```\n\n  This will set the minimum delay time on a given service, for both regular and high-priority RPCs (if the PtlRPC service supports high-priority RPCs), to the indicated value.\n\n  For example, to set the minimum delay time on the ost_io service to 10, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_delay_min=10\n  ost.OSS.ost_io.nrs_delay_min=10\n  ```\n\n  For PtlRPC services that support high-priority RPCs, to set a different minimum delay time for regular and high-priority RPCs, run:\n\n  ```\n  lctl set_param {service}.nrs_delay_min=reg_delay_min|hp_delay_min:0-65535\n          \n  ```\n\n  For example, to set the minimum delay time on the ost_io service for high-priority RPCs to 3, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_delay_min=hp_delay_min:3\n  ost.OSS.ost_io.nrs_delay_min=hp_delay_min:3\n  ```\n\n  Note, in all cases the minimum delay time cannot exceed the maximum delay time.\n\n- `{service}.nrs_delay_max`\n\n  The `{service}.nrs_delay_max` tunable controls the maximum amount of time, in seconds, that a request will be delayed by this policy. The default is 300 seconds.", "mimetype": "text/plain", "start_char_idx": 52326, "end_char_idx": 56201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a59ad66-2d74-48ad-ae9c-2a458860adb8": {"__data__": {"id_": "6a59ad66-2d74-48ad-ae9c-2a458860adb8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f158caa-8625-4ee3-96ba-4789d5ee53e4", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d840be493f1b084bffd41de02a797bae97ddbe2aedcd214f66d3ec9ee887c472", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "459f8772-f83c-49fb-863e-ebd29a3bdb03", "node_type": "1", "metadata": {}, "hash": "da0f82d94510676157138ad3e043cf891faca439794c4cebcabab08933a57d16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `{service}.nrs_delay_max`\n\n  The `{service}.nrs_delay_max` tunable controls the maximum amount of time, in seconds, that a request will be delayed by this policy. The default is 300 seconds. To read this value run:\n\n  ```\n  lctl get_param {service}.nrs_delay_max\n  ```\n\n  For example, to read the maximum delay set on the ost_io service, run:\n\n  ```\n  $ lctl get_param ost.OSS.ost_io.nrs_delay_max\n  ost.OSS.ost_io.nrs_delay_max=reg_delay_max:300\n  hp_delay_max:300\n  ```\n\n  To set the maximum delay in RPC processing, run:\n\n  ```\n  lctl set_param {service}.nrs_delay_max=0-65535\n  ```\n\n  This will set the maximum delay time on a given service, for both regular and high-priority RPCs (if the PtlRPC service supports high-priority RPCs), to the indicated value.\n\n  For example, to set the maximum delay time on the ost_io service to 60, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_delay_max=60\n  ost.OSS.ost_io.nrs_delay_max=60\n  ```\n\n  For PtlRPC services that support high-priority RPCs, to set a different maximum delay time for regular and high-priority RPCs, run:\n\n  ```\n  lctl set_param {service}.nrs_delay_max=reg_delay_max|hp_delay_max:0-65535\n  ```\n\n  For example, to set the maximum delay time on the ost_io service for high-priority RPCs to 30, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_delay_max=hp_delay_max:30\n  ost.OSS.ost_io.nrs_delay_max=hp_delay_max:30\n  ```\n\n  Note, in all cases the maximum delay time cannot be less than the minimum delay time.\n\n- `{service}.nrs_delay_pct`\n\n  The `{service}.nrs_delay_pct` tunable controls the percentage of requests that will be delayed by this policy. The default is 100. Note, when a request is not selected for handling by the delay policy due to this variable then the request will be handled by whatever fallback policy is defined for that service. If no other fallback policy is defined then the request will be handled by the FIFO policy. To read this value run:\n\n  ```\n  lctl get_param {service}.nrs_delay_pct\n  ```\n\n  For example, to read the percentage of requests being delayed on the ost_io service, run:\n\n  ```\n  $ lctl get_param ost.OSS.ost_io.nrs_delay_pct\n  ost.OSS.ost_io.nrs_delay_pct=reg_delay_pct:100\n  hp_delay_pct:100\n  ```\n\n  To set the percentage of delayed requests, run:\n\n  ```\n  lctl set_param {service}.nrs_delay_pct=0-100\n  ```\n\n  This will set the percentage of requests delayed on a given service, for both regular and high-priority RPCs (if the PtlRPC service supports high-priority RPCs), to the indicated value.\n\n  For example, to set the percentage of delayed requests on the ost_io service to 50, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_delay_pct=50\n  ost.OSS.ost_io.nrs_delay_pct=50\n  ```\n\n  For PtlRPC services that support high-priority RPCs, to set a different delay percentage for regular and high-priority RPCs, run:\n\n  ```\n  lctl set_param {service}.nrs_delay_pct=reg_delay_pct|hp_delay_pct:0-100\n  ```\n\n  For example, to set the percentage of delayed requests on the ost_io service for high-priority RPCs to 5, run:\n\n  ```\n  $ lctl set_param ost.OSS.ost_io.nrs_delay_pct=hp_delay_pct:5\n  ost.OSS.ost_io.nrs_delay_pct=hp_delay_pct:5\n  ```\n\n## Lockless I/O Tunables\n\nThe lockless I/O tunable feature allows servers to ask clients to do lockless I/O (the server does the locking on behalf of clients) for contended files to avoid lock ping-pong.\n\nThe lockless I/O patch introduces these tunables:\n\n- **OST-side:**\n\n  ```\n  ldlm.namespaces.filter-fsname-*.", "mimetype": "text/plain", "start_char_idx": 56009, "end_char_idx": 59488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "459f8772-f83c-49fb-863e-ebd29a3bdb03": {"__data__": {"id_": "459f8772-f83c-49fb-863e-ebd29a3bdb03", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a59ad66-2d74-48ad-ae9c-2a458860adb8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "25f11a3206bb3ae8d5f559c0a25b831132bb03d74f9097ab71c14223094d5fa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9d963b6-9a11-4bd3-adcf-86a510dc2ac8", "node_type": "1", "metadata": {}, "hash": "cd463fa281a92789e1c71579a6ef9c99d96b745893473b4e41c75c76c915fc85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The lockless I/O patch introduces these tunables:\n\n- **OST-side:**\n\n  ```\n  ldlm.namespaces.filter-fsname-*.\n  ```\n\n  `contended_locks`- If the number of lock conflicts in the scan of granted and waiting queues at contended_locks is exceeded, the resource is considered to be contended.\n\n  `contention_seconds`- The resource keeps itself in a contended state as set in the parameter.\n\n  `max_nolock_bytes`- Server-side locking set only for requests less than the blocks set in the`max_nolock_bytes` parameter. If this tunable is set to zero (0), it disables server-side locking for read/write requests.\n\n- **Client-side:**\n\n  ```\n  /proc/fs/lustre/llite/lustre-*\n  ```\n\n  `contention_seconds`- `llite` inode remembers its contended state for the time specified in this parameter.\n\n- **Client-side statistics:**\n\n  The `/proc/fs/lustre/llite/lustre-*/stats` file has new rows for lockless I/O statistics.\n\n  `lockless_read_bytes` and `lockless_write_bytes`- To count the total bytes read or written, the client makes its own decisions based on the request size. The client does not communicate with the server if the request size is smaller than the `min_nolock_size`, without acquiring locks by the client.\n\n  \n\nIntroduced in Lustre 2.9 \n\n## Server-Side Advice and Hinting\n\n### Overview\n\nUse the `lfs ladvise` command to give file access advices or hints to servers.\n\n```\nlfs ladvise [--advice|-a ADVICE ] [--background|-b]\n[--start|-s START[kMGT]]\n{[--end|-e END[kMGT]] | [--length|-l LENGTH[kMGT]]}\nfile ...\n      \n```\n\n| **Option**                     | **Description**                                              |\n| ------------------------------ | ------------------------------------------------------------ |\n| `-a`, `--advice=``ADVICE`      | Give advice or hint of type `ADVICE`. Advice types are:`willread` to prefetch data into server cache`dontneed` to cleanup data cache on server`lockahead` Request an LDLM extent lock of the given mode on the given byte range`noexpand` Disable extent lock expansion behavior for I/O to this file descriptor |\n| `-b`, `--background`           | Enable the advices to be sent and handled asynchronously.    |\n| `-s`, `--start=``START_OFFSET` | File range starts from `START_OFFSET`                        |\n| `-e`, `--end=``END_OFFSET`     | File range ends at (not including) `END_OFFSET`. This option may not be specified at the same time as the `-l` option. |\n| `-l`, `--length=``LENGTH`      | File range has length of `LENGTH`. This option may not be specified at the same time as the`-e` option. |\n| `-m`, `--mode=` `MODE`         | Lockahead request mode `{READ,WRITE}`. Request a lock with this mode. |\n\nTypically, `lfs ladvise` forwards the advice to Lustre servers without guaranteeing when and what servers will react to the advice. Actions may or may not triggered when the advices are recieved, depending on the type of the advice, as well as the real-time decision of the affected server-side components.\n\nA typical usage of ladvise is to enable applications and users with external knowledge to intervene in server-side cache management. For example, if a bunch of different clients are doing small random reads of a file, prefetching pages into OSS cache with big linear reads before the random IO is a net benefit. Fetching that data into each client cache with fadvise() may not be, due to much more data being sent to the client.\n\n`ladvise lockahead` is different in that it attempts to control LDLM locking behavior by explicitly requesting LDLM locks in advance of use. This does not directly affect caching behavior, instead it is used in special cases to avoid pathological results (lock exchange) from the normal LDLM locking behavior.\n\nNote that the `noexpand` advice works on a specific file descriptor, so using it via lfs has no effect. It must be used on a particular file descriptor which is used for i/o to have any effect.", "mimetype": "text/plain", "start_char_idx": 59380, "end_char_idx": 63283, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9d963b6-9a11-4bd3-adcf-86a510dc2ac8": {"__data__": {"id_": "b9d963b6-9a11-4bd3-adcf-86a510dc2ac8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "459f8772-f83c-49fb-863e-ebd29a3bdb03", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "48367c488ffc4420a9e11952c8ba2614ea36b3eb3395bb0d5e57cbf67c03a0ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "653444f8-2f28-4c86-84be-fa5755f9ad79", "node_type": "1", "metadata": {}, "hash": "41fdc856813bf1ce0677126747b009ca8b4020fa607e91a431acebcd2eb33de2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A typical usage of ladvise is to enable applications and users with external knowledge to intervene in server-side cache management. For example, if a bunch of different clients are doing small random reads of a file, prefetching pages into OSS cache with big linear reads before the random IO is a net benefit. Fetching that data into each client cache with fadvise() may not be, due to much more data being sent to the client.\n\n`ladvise lockahead` is different in that it attempts to control LDLM locking behavior by explicitly requesting LDLM locks in advance of use. This does not directly affect caching behavior, instead it is used in special cases to avoid pathological results (lock exchange) from the normal LDLM locking behavior.\n\nNote that the `noexpand` advice works on a specific file descriptor, so using it via lfs has no effect. It must be used on a particular file descriptor which is used for i/o to have any effect.\n\nThe main difference between the Linux `fadvise()` system call and `lfs ladvise` is that `fadvise()` is only a client side mechanism that does not pass the advice to the filesystem, while `ladvise` can send advices or hints to the Lustre server side.\n\n### Examples\n\nThe following example gives the OST(s) holding the first 1GB of `/mnt/lustre/file1`a hint that the first 1GB of the file will be read soon.\n\n```\nclient1$ lfs ladvise -a willread -s 0 -e 1048576000 /mnt/lustre/file1\n        \n```\n\nThe following example gives the OST(s) holding the first 1GB of `/mnt/lustre/file1` a hint that the first 1GB of file will not be read in the near future, thus the OST(s) could clear the cache of the file in the memory.\n\n```\nclient1$ lfs ladvise -a dontneed -s 0 -e 1048576000 /mnt/lustre/file1\n        \n```\n\nThe following example requests an LDLM read lock on the first 1 MiB of `/mnt/lustre/file1`. This will attempt to request a lock from the OST holding that region of the file.\n\n```\nclient1$ lfs ladvise -a lockahead -m READ -s 0 -e 1M /mnt/lustre/file1\n        \n```\n\nThe following example requests an LDLM write lock on [3 MiB, 10 MiB] of `/mnt/lustre/file1`. This will attempt to request a lock from the OST holding that region of the file.\n\n```\nclient1$ lfs ladvise -a lockahead -m WRITE -s 3M -e 10M /mnt/lustre/file1\n```\n\n\n\nIntroduced in Lustre 2.9\n\n## Large Bulk IO (16MB RPC)\n\n### Overview\n\nBeginning with Lustre 2.9, Lustre is extended to support RPCs up to 16MB in size. By enabling a larger RPC size, fewer RPCs will be required to transfer the same amount of data between clients and servers. With a larger RPC size, the OSS can submit more data to the underlying disks at once, therefore it can produce larger disk I/Os to fully utilize the increasing bandwidth of disks.\n\nAt client connection time, clients will negotiate with servers what the maximum RPC size it is possible to use, but the client can always send RPCs smaller than this maximum.\n\nThe parameter `brw_size` is used on the OST to tell the client the maximum (preferred) IO size. All clients that talk to this target should never send an RPC greater than this size. Clients can individually set a smaller RPC size limit via the `osc.*.max_pages_per_rpc` tunable.\n\n**Note**\n\nThe smallest `brw_size` that can be set for ZFS OSTs is the `recordsize` of that dataset. This ensures that the client can always write a full ZFS file block if it has enough dirty data, and does not otherwise force it to do read- modify-write operations for every RPC.\n\n### Usage\n\nIn order to enable a larger RPC size, `brw_size` must be changed to an IO size value up to 16MB.", "mimetype": "text/plain", "start_char_idx": 62349, "end_char_idx": 65913, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "653444f8-2f28-4c86-84be-fa5755f9ad79": {"__data__": {"id_": "653444f8-2f28-4c86-84be-fa5755f9ad79", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9d963b6-9a11-4bd3-adcf-86a510dc2ac8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ee96e7810a74bc672d42e3b5c426a180fab86688ceda7f63bec7d714bd3e36b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24b29f16-da27-4188-8e59-c27e3184fb61", "node_type": "1", "metadata": {}, "hash": "fa73dd06a8f8eb6a43042771e6d7d527c7149f745d6101a7ef63b7337b45123a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At client connection time, clients will negotiate with servers what the maximum RPC size it is possible to use, but the client can always send RPCs smaller than this maximum.\n\nThe parameter `brw_size` is used on the OST to tell the client the maximum (preferred) IO size. All clients that talk to this target should never send an RPC greater than this size. Clients can individually set a smaller RPC size limit via the `osc.*.max_pages_per_rpc` tunable.\n\n**Note**\n\nThe smallest `brw_size` that can be set for ZFS OSTs is the `recordsize` of that dataset. This ensures that the client can always write a full ZFS file block if it has enough dirty data, and does not otherwise force it to do read- modify-write operations for every RPC.\n\n### Usage\n\nIn order to enable a larger RPC size, `brw_size` must be changed to an IO size value up to 16MB. To temporarily change `brw_size`, the following command should be run on the OSS:\n\n```\noss# lctl set_param obdfilter.fsname-OST*.brw_size=16\n```\n\nTo persistently change `brw_size`, the following command should be run:\n\n```\noss# lctl set_param -P obdfilter.fsname-OST*.brw_size=16\n```\n\nWhen a client connects to an OST target, it will fetch `brw_size` from the target and pick the maximum value of `brw_size` and its local setting for `max_pages_per_rpc` as the actual RPC size. Therefore, the `max_pages_per_rpc` on the client side would have to be set to 16M, or 4096 if the PAGESIZE is 4KB, to enable a 16MB RPC. To temporarily make the change, the following command should be run on the client to set `max_pages_per_rpc`:\n\n```\nclient$ lctl set_param osc.fsname-OST*.max_pages_per_rpc=16M\n```\n\nTo persistently make this change, the following command should be run:\n\n```\nclient$ lctl set_param -P obdfilter.fsname-OST*.osc.max_pages_per_rpc=16M\n```\n\n**Caution**\n\nThe `brw_size` of an OST can be changed on the fly. However, clients have to be remounted to renegotiate the new maximum RPC size.\n\n## Improving Lustre I/O Performance for Small Files\n\nAn environment where an application writes small file chunks from many clients to a single file can result in poor I/O performance. To improve the performance of the Lustre file system with small files:\n\n- Have the application aggregate writes some amount before submitting them to the Lustre file system. By default, the Lustre software enforces POSIX coherency semantics, so it results in lock ping-pong between client nodes if they are all writing to the same file at one time.\n\n  Using MPI-IO Collective Write functionality in the Lustre ADIO driver is one way to achieve this in a straight forward manner if the application is already using MPI-IO.\n\n- Have the application do 4kB `O_DIRECT` sized I/O to the file and disable locking on the output file. This avoids partial-page IO submissions and, by disabling locking, you avoid contention between clients.\n\n- Have the application write contiguous data.\n\n- Add more disks or use SSD disks for the OSTs. This dramatically improves the IOPS rate. Consider creating larger OSTs rather than many smaller OSTs due to less overhead (journal, connections, etc).\n\n- Use RAID-1+0 OSTs instead of RAID-5/6. There is RAID parity overhead for writing small chunks of data to disk.\n\n## Understanding Why Write Performance is Better Than Read Performance\n\nTypically, the performance of write operations on a Lustre cluster is better than read operations. When doing writes, all clients are sending write RPCs asynchronously. The RPCs are allocated, and written to disk in the order they arrive. In many cases, this allows the back-end storage to aggregate writes efficiently.\n\nIn the case of read operations, the reads from clients may come in a different order and need a lot of seeking to get read from the disk. This noticeably hampers the read throughput.\n\nCurrently, there is no readahead on the OSTs themselves, though the clients do readahead. If there are lots of clients doing reads it would not be possible to do any readahead in any case because of memory consumption (consider that even a single RPC (1 MB) readahead for 1000 clients would consume 1 GB of RAM).", "mimetype": "text/plain", "start_char_idx": 65069, "end_char_idx": 69178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24b29f16-da27-4188-8e59-c27e3184fb61": {"__data__": {"id_": "24b29f16-da27-4188-8e59-c27e3184fb61", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcc81bc0-236c-4ad3-a95f-a0ec7a928645", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e781bb26ad2a7e614addc7147ab5d69298944a8334db1fd1b34d8e4f153b6c50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "653444f8-2f28-4c86-84be-fa5755f9ad79", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7a6bc55c211b8baa3d7d4ada2d3a04dc8f419f8db97055ebc235ff04c80021b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Understanding Why Write Performance is Better Than Read Performance\n\nTypically, the performance of write operations on a Lustre cluster is better than read operations. When doing writes, all clients are sending write RPCs asynchronously. The RPCs are allocated, and written to disk in the order they arrive. In many cases, this allows the back-end storage to aggregate writes efficiently.\n\nIn the case of read operations, the reads from clients may come in a different order and need a lot of seeking to get read from the disk. This noticeably hampers the read throughput.\n\nCurrently, there is no readahead on the OSTs themselves, though the clients do readahead. If there are lots of clients doing reads it would not be possible to do any readahead in any case because of memory consumption (consider that even a single RPC (1 MB) readahead for 1000 clients would consume 1 GB of RAM).\n\nFor file systems that use socklnd (TCP, Ethernet) as interconnect, there is also additional CPU overhead because the client cannot receive data without copying it from the network buffers. In the write case, the client CAN send data without the additional data copy. This means that the client is more likely to become CPU-bound during reads than writes.", "mimetype": "text/plain", "start_char_idx": 68289, "end_char_idx": 69534, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3bfbce9-4dc3-4acd-b10e-f42c3a0c12e2": {"__data__": {"id_": "f3bfbce9-4dc3-4acd-b10e-f42c3a0c12e2", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25fb1c16-d6cc-4b16-80bb-9e1f45c5a2a2", "node_type": "1", "metadata": {}, "hash": "f850919fe07c5edf96bf11dc062ec7762ac0f181d72d34b3ba07f2010ce20132", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Lustre File System Troubleshooting\n\n- [Lustre File System Troubleshooting](#lustre-file-system-troubleshooting)\n  * [Lustre Error Messages](#lustre-error-messages)\n    + [Error Numbers](#error-numbers)\n    + [Viewing Error Messages](#viewing-error-messages)\n  * [Reporting a Lustre File System Bug](#reporting-a-lustre-file-system-bug)\n    + [Searching Jira*for Duplicate Tickets](#searching-jirafor-duplicate-tickets)\n  * [Common Lustre File System Problems](#common-lustre-file-system-problems)\n    + [OST Object is Missing or Damaged](#ost-object-is-missing-or-damaged)\n    + [OSTs Become Read-Only](#osts-become-read-only)\n    + [Identifying a Missing OST](#identifying-a-missing-ost)\n    + [Fixing a Bad LAST_ID on an OST](#fixing-a-bad-last_id-on-an-ost)\n    + [Handling/Debugging \"`Bind: Address already in use`\" Error](#handlingdebugging-bind-address-already-in-use-error)\n    + [Handling/Debugging Error \"- 28\"](#handlingdebugging-error---28)\n    + [Triggering Watchdog for PID NNN](#triggering-watchdog-for-pid-nnn)\n    + [Handling Timeouts on Initial Lustre File System Setup](#handling-timeouts-on-initial-lustre-file-system-setup)\n    + [Handling/Debugging \"LustreError: xxx went back in time\"](#handlingdebugging-lustreerror-xxx-went-back-in-time)\n    + [Lustre Error: \"`Slow Start_Page_Write`\"](#lustre-error-slow-start_page_write)\n    + [Drawbacks in Doing Multi-client O_APPEND Writes](#drawbacks-in-doing-multi-client-o_append-writes)\n    + [Slowdown Occurs During Lustre File System Startup](#slowdown-occurs-during-lustre-file-system-startup)\n    + [Log Message `'Out of Memory`' on OST](#log-message-out-of-memory-on-ost)\n    + [Setting SCSI I/O Sizes](#setting-scsi-io-sizes)\n\nThis chapter provides information about troubleshooting a Lustre file system, submitting a bug to the Jira bug tracking system, and Lustre file system performance tips. It includes the following sections:\n\n- [the section called \u201c Lustre Error Messages\u201d](#lustre-error-messages)\n- [the section called \u201cReporting a Lustre File System Bug\u201d](#reporting-a-lustre-file-system-bug)\n- [the section called \u201cCommon Lustre File System Problems](#common-lustre-file-system-problems)\n\n## Lustre Error Messages\n\nSeveral resources are available to help troubleshoot an issue in a Lustre file system. This section describes error numbers, error messages and logs.\n\n### Error Numbers\n\nError numbers are generated by the Linux operating system and are located in `/usr/include/asm-generic/errno.h`. The Lustre software does not use all of the available Linux error numbers. The exact meaning of an error number depends on where it is used. Here is a summary of the basic errors that Lustre file system users may encounter.\n\n| **Error Number** | **Error Name** | **Description**                                              |\n| ---------------- | -------------- | ------------------------------------------------------------ |\n| -1               | `-EPERM`       | Permission is denied.                                        |\n| -2               | `-ENOENT`      | The requested file or directory does not exist.              |\n| -4               | `-EINTR`       | The operation was interrupted (usually CTRL-C or a killing process). |\n| -5               | `-EIO`         | The operation failed with a read or write error.             |\n| -19              | `-ENODEV`      | No such device is available. The server stopped or failed over. |\n| -22              | `-EINVAL`      | The parameter contains an invalid value.                     |\n| -28              | `-ENOSPC`      | The file system is out-of-space or out of inodes. Use `lfs df` (query the amount of file system space) or `lfs df -i` (query the number of inodes). |\n| -30              | `-EROFS`       | The file system is read-only, likely due to a detected error. |\n| -43              | `-EIDRM`       | The UID/GID does not match any known UID/GID on the MDS. Update etc/hosts and etc/group on the MDS to add the missing user or group.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25fb1c16-d6cc-4b16-80bb-9e1f45c5a2a2": {"__data__": {"id_": "25fb1c16-d6cc-4b16-80bb-9e1f45c5a2a2", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3bfbce9-4dc3-4acd-b10e-f42c3a0c12e2", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "86b084033db9e9b473a657c4d9ae2a359f34615b60668ecff444cb3dff1a1275", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b345a187-792c-4486-9fc0-d0c08c7b3d1a", "node_type": "1", "metadata": {}, "hash": "914e1a0c4d73a2a02d1213b1c554de3484ea7e5b3799867796135b1ed090977e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| -19              | `-ENODEV`      | No such device is available. The server stopped or failed over. |\n| -22              | `-EINVAL`      | The parameter contains an invalid value.                     |\n| -28              | `-ENOSPC`      | The file system is out-of-space or out of inodes. Use `lfs df` (query the amount of file system space) or `lfs df -i` (query the number of inodes). |\n| -30              | `-EROFS`       | The file system is read-only, likely due to a detected error. |\n| -43              | `-EIDRM`       | The UID/GID does not match any known UID/GID on the MDS. Update etc/hosts and etc/group on the MDS to add the missing user or group. |\n| -107             | `-ENOTCONN`    | The client is not connected to this server.                  |\n| -110             | `-ETIMEDOUT`   | The operation took too long and timed out.                   |\n| -122             | `-EDQUOT`      | The operation exceeded the user disk quota and was aborted.  |\n\n### Viewing Error Messages\n\nAs Lustre software code runs on the kernel, single-digit error codes display to the application; these error codes are an indication of the problem. Refer to the kernel console log (dmesg) for all recent kernel messages from that node. On the node, `/var/log/messages` holds a log of all messages for at least the past day.\n\nThe error message initiates with \"LustreError\" in the console log and provides a short description of:\n\n- What the problem is\n- Which process ID had trouble\n- Which server node it was communicating with, and so on.\n\nLustre logs are dumped to `/proc/sys/lnet/debug_path`.\n\nCollect the first group of messages related to a problem, and any messages that precede \"LBUG\" or \"assertion failure\" errors. Messages that mention server nodes (OST or MDS) are specific to that server; you must collect similar messages from the relevant server console logs.\n\nAnother Lustre debug log holds information for a short period of time for action by the Lustre software, which, in turn, depends on the processes on the Lustre node. Use the following command to extract debug logs on each of the nodes, run\n\n```\n$ lctl dk filename\n```\n\n**Note**\n\nLBUG freezes the thread to allow capture of the panic stack. A system reboot is needed to clear the thread.\n\n## Reporting a Lustre File System Bug\n\nIf you cannot resolve a problem by troubleshooting your Lustre file system, other options are:\n\n- Post a question to the [lustre-discuss](http://lists.lustre.org/listinfo.cgi/lustre-discuss-lustre.org) email list or search the archives for information about your issue.\n- Submit a ticket to the [Jira](https://jira.whamcloud.com/secure/Dashboard.jspa)* bug tracking and project management tool used for the Lustre project. If you are a first-time user, you'll need to open an account by clicking on **Sign up** on the Welcome page.\n\nTo submit a Jira ticket, follow these steps:\n\n1. To avoid filing a duplicate ticket, search for existing tickets for your issue. For search tips, see [*the section called \u201cSearching Jira*for Duplicate Tickets\u201d*](#searching-jirafor-duplicate-tickets).\n\n2. To create a ticket, click **+Create Issue** in the upper right corner. Create a separate ticket for each issue you wish to submit.\n\n3. In the form displayed, enter the following information:\n\n   - Project - Select **Lustre** or **Lustre Documentation** or an appropriate project.\n   - Issue type - Select **Bug**.\n   - Summary - Enter a short description of the issue. Use terms that would be useful for someone searching for a similar issue. A LustreError or ASSERT/panic message often makes a good summary.\n   - Affects version(s) - Select your Lustre release.\n   - Environment - Enter your kernel with version number.\n   - Description - Include a detailed description of visible symptoms and, if possible, how the problem is produced. Other useful information may include the behavior you expect to see and what you have tried so far to diagnose the problem.", "mimetype": "text/plain", "start_char_idx": 3319, "end_char_idx": 7271, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b345a187-792c-4486-9fc0-d0c08c7b3d1a": {"__data__": {"id_": "b345a187-792c-4486-9fc0-d0c08c7b3d1a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25fb1c16-d6cc-4b16-80bb-9e1f45c5a2a2", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "06ca2fad31dcd47764ec7fe013881c34245d3691e1f09fbf8202532a321ff4bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d97ea8f-d920-425a-aae5-f1e3acfb4cf0", "node_type": "1", "metadata": {}, "hash": "97331ab55e556efd91e972fffe3d820bbaf4de81dbfd2f444908a79871e69e6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. To create a ticket, click **+Create Issue** in the upper right corner. Create a separate ticket for each issue you wish to submit.\n\n3. In the form displayed, enter the following information:\n\n   - Project - Select **Lustre** or **Lustre Documentation** or an appropriate project.\n   - Issue type - Select **Bug**.\n   - Summary - Enter a short description of the issue. Use terms that would be useful for someone searching for a similar issue. A LustreError or ASSERT/panic message often makes a good summary.\n   - Affects version(s) - Select your Lustre release.\n   - Environment - Enter your kernel with version number.\n   - Description - Include a detailed description of visible symptoms and, if possible, how the problem is produced. Other useful information may include the behavior you expect to see and what you have tried so far to diagnose the problem.\n   - Attachments - Attach log sources such as Lustre debug log dumps (see [*the section called \u201c Diagnostic and Debugging Tools\u201d*](05.03-Debugging%20a%20Lustre%20File%20System.md#diagnostic-and-debugging-tools)), syslogs, or console logs. **Note:** Lustre debug logs must be processed using `lctl df` prior to attaching to a Jira ticket. For more information, see [*the section called \u201cUsing the lctl Tool to View Debug Messages\u201d*](05.03-Debugging%20a%20Lustre%20File%20System.md#using-the-lctl-tool-to-view-debug-messages).\n\nOther fields in the form are used for project tracking and are irrelevant to reporting an issue. You can leave these in their default state.\n\n### Searching Jira*for Duplicate Tickets\n\nBefore submitting a ticket, always search the Jira bug tracker for an existing ticket for your issue. This avoids duplicating effort and may immediately provide you with a solution to your problem.\n\nTo do a search in the Jira bug tracker, select the **Issues** tab and click on **New filter**. Use the filters provided to select criteria for your search. To search for specific text, enter the text in the \"Contains text\" field and click the magnifying glass icon.\n\nWhen searching for text such as an ASSERTION or LustreError message, you can remove NIDs, pointers, and other installation-specific and possibly version-specific text from your search string such as line numbers by following the example below.\n\nOriginal error message:\n\n`\"(filter_io_26.c:` **791**`:filter_commitrw_write()) ASSERTION(oti->oti_transno<=obd->obd_last_committed) failed: oti_transno `**752** `last_committed `**750** `\"`\n\nOptimized search string\n\n`filter_commitrw_write ASSERTION oti_transno obd_last_committed failed:`\n\n## Common Lustre File System Problems\n\n   This section describes how to address common issues encountered with a Lustre file system.\n\n### OST Object is Missing or Damaged\n\nIf the OSS fails to find an object or finds a damaged object, this message appears:\n\n```\nOST object missing or damaged (OST \"ost1\", object 98148, error -2)\n```\n\nIf the reported error is -2 (`-ENOENT`, or \"No such file or directory\"), then the object is no longer present on the OST, even though a file on the MDT is referencing it. This can occur either because the MDT and OST are out of sync, or because an OST object was corrupted and deleted by e2fsck.\n\nIf you have recovered the file system from a disk failure by using e2fsck, then unrecoverable objects may have been deleted or moved to /lost+found in the underlying OST filesystem. Because files on the MDT still reference these objects, attempts to access them produce this error.\n\nIf you have restored the filesystem from a backup of the raw MDT or OST partition, then the restored partition is very likely to be out of sync with the rest of your cluster. No matter which server partition you restored from backup, files on the MDT may reference objects which no longer exist (or did not exist when the backup was taken); accessing those files produces this error.\n\nIf neither of those descriptions is applicable to your situation, then it is possible that you have discovered a programming error that allowed the servers to get out of sync. Please submit a Jira ticket (see [*the section called \u201cReporting a Lustre File System Bug\u201d*](#reporting-a-lustre-file-system-bug)).\n\nIf the reported error is anything else (such as -5, \"`I/O error`\"), it likely indicates a storage device failure.", "mimetype": "text/plain", "start_char_idx": 6407, "end_char_idx": 10703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d97ea8f-d920-425a-aae5-f1e3acfb4cf0": {"__data__": {"id_": "6d97ea8f-d920-425a-aae5-f1e3acfb4cf0", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b345a187-792c-4486-9fc0-d0c08c7b3d1a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9a377a088b7500a578e7509bcd628ca06475b635e12780d7f6c5db9222e00955", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c9b5cf8-8726-4a64-9026-bb3c3516edc8", "node_type": "1", "metadata": {}, "hash": "bc9f58491d8607e888d5a7b6829c3f3503ecb7d5b99d404b3b028d367037240b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because files on the MDT still reference these objects, attempts to access them produce this error.\n\nIf you have restored the filesystem from a backup of the raw MDT or OST partition, then the restored partition is very likely to be out of sync with the rest of your cluster. No matter which server partition you restored from backup, files on the MDT may reference objects which no longer exist (or did not exist when the backup was taken); accessing those files produces this error.\n\nIf neither of those descriptions is applicable to your situation, then it is possible that you have discovered a programming error that allowed the servers to get out of sync. Please submit a Jira ticket (see [*the section called \u201cReporting a Lustre File System Bug\u201d*](#reporting-a-lustre-file-system-bug)).\n\nIf the reported error is anything else (such as -5, \"`I/O error`\"), it likely indicates a storage device failure. The low-level file system returns this error if it is unable to read from the storage device.\n\n**Suggested Action**\n\nIf the reported error is -2, you can consider checking in `lost+found/` on your raw OST device, to see if the missing object is there. However, it is likely that this object is lost forever, and that the file that references the object is now partially or completely lost. Restore this file from backup, or salvage what you can using `dd conv=noerror`and delete it using the `unlink` command.\n\nIf the reported error is anything else, then you should immediately inspect this server for storage problems.\n\n### OSTs Become Read-Only\n\nIf the SCSI devices are inaccessible to the Lustre file system at the block device level, then `ldiskfs` remounts the device read-only to prevent file system corruption. This is a normal behavior. The status in `/proc/fs/lustre/health_check` also shows \"not healthy\" on the affected nodes.\n\nTo determine what caused the \"not healthy\" condition:\n\n- Examine the consoles of all servers for any error indications\n- Examine the syslogs of all servers for any LustreErrors or `LBUG`\n- Check the health of your system hardware and network. (Are the disks working as expected, is the network dropping packets?)\n- Consider what was happening on the cluster at the time. Does this relate to a specific user workload or a system load condition? Is the condition reproducible? Does it happen at a specific time (day, week or month)?\n\nTo recover from this problem, you must restart Lustre services using these file systems. There is no other way to know that the I/O made it to disk, and the state of the cache may be inconsistent with what is on disk.\n\n### Identifying a Missing OST\n\nIf an OST is missing for any reason, you may need to know what files are affected. Although an OST is missing, the files system should be operational. From any mounted client node, generate a list of files that reside on the affected OST. It is advisable to mark the missing OST as 'unavailable' so clients and the MDS do not time out trying to contact it.\n\n1. Generate a list of devices and determine the OST's device number. Run:\n\n   ```\n   $ lctl dl \n   ```\n\n   The lctl dl command output lists the device name and number, along with the device UUID and the number of references on the device.\n\n2. Deactivate the OST (on the OSS at the MDS). Run:\n\n   ```\n   $ lctl --device lustre_device_number deactivate\n   ```\n\n   The OST device number or device name is generated by the lctl dl command.\n\n   The `deactivate` command prevents clients from creating new objects on the specified OST, although you can still access the OST for reading.\n\n   ### Note\n\n   If the OST later becomes available it needs to be reactivated, run:\n\n   ```\n   # lctl --device lustre_device_number activate\n   ```\n\n3. Determine all files that are striped over the missing OST, run:\n\n   ```\n   # lfs find -O {OST_UUID} /mountpoint\n   ```\n\n   This returns a simple list of filenames from the affected file system.\n\n4. If necessary, you can read the valid parts of a striped file, run:\n\n   ```\n   # dd if=filename of=new_filename bs=4k conv=sync,noerror\n   ```\n\n5. You can delete these files with the `unlink` command.", "mimetype": "text/plain", "start_char_idx": 9795, "end_char_idx": 13915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c9b5cf8-8726-4a64-9026-bb3c3516edc8": {"__data__": {"id_": "8c9b5cf8-8726-4a64-9026-bb3c3516edc8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d97ea8f-d920-425a-aae5-f1e3acfb4cf0", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "eddb4cd603b53772c14b3010acc9275b742713c7280c5d5c3986c0cd9f137424", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ddb765e-bef5-46aa-810d-c49fd0426b9d", "node_type": "1", "metadata": {}, "hash": "ee83a4313f2f4ed67195f24bc74f9a4f9fca559537834d35b575f1276df32d97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The `deactivate` command prevents clients from creating new objects on the specified OST, although you can still access the OST for reading.\n\n   ### Note\n\n   If the OST later becomes available it needs to be reactivated, run:\n\n   ```\n   # lctl --device lustre_device_number activate\n   ```\n\n3. Determine all files that are striped over the missing OST, run:\n\n   ```\n   # lfs find -O {OST_UUID} /mountpoint\n   ```\n\n   This returns a simple list of filenames from the affected file system.\n\n4. If necessary, you can read the valid parts of a striped file, run:\n\n   ```\n   # dd if=filename of=new_filename bs=4k conv=sync,noerror\n   ```\n\n5. You can delete these files with the `unlink` command.\n\n   ```\n   # unlink filename {filename ...} \n   ```\n\n   ### Note\n\n   When you run the `unlink` command, it may return an error that the file could not be found, but the file on the MDS has been permanently removed.\n\nIf the file system cannot be mounted, currently there is no way that parses metadata directly from an MDS. If the bad OST does not start, options to mount the file system are to provide a loop device OST in its place or replace it with a newly-formatted OST. In that case, the missing objects are created and are read as zero-filled.\n\n### Fixing a Bad LAST_ID on an OST\n\nEach OST contains a `LAST_ID` file, which holds the last object (pre-)created by the MDS. The MDT contains a `lov_objid` file, with values that represent the last object the MDS has allocated to a file.\n\nDuring normal operation, the MDT keeps pre-created (but unused) objects on the OST, and normally `LAST_ID`should be larger than `lov_objid`. Any small difference in the values is a result of objects being precreated on the OST to improve MDS file creation performance. These precreated objects are not yet allocated to a file, since they are of zero length (empty).\n\nHowever, in the case where `lov_objid` is larger than `LAST_ID`, it indicates the MDS has allocated objects to files that do not exist on the OST. Conversely, if `lov_objid` is significantly less than `LAST_ID` (by at least 20,000 objects) it indicates the OST previously allocated objects at the request of the MDS (which likely contain data) but it doesn't know about them.\n\nIntroduced in Lustre 2.5Since Lustre 2.5 the MDS and OSS will resync the `lov_objid` and `LAST_ID` files automatically if they become out of sync. This may result in some space on the OSTs becoming unavailable until LFSCK is next run, but avoids issues with mounting the filesystem.\n\nIntroduced in Lustre 2.6Since Lustre 2.6 the LFSCK will repair the `LAST_ID` file on the OST automatically based on the objects that exist on the OST, in case it was corrupted.\n\nIn situations where there is on-disk corruption of the OST, for example caused by the disk write cache being lost, or if the OST was restored from an old backup or reformatted, the `LAST_ID` value may become inconsistent and result in a message similar to:\n\n```\n\"myth-OST0002: Too many FIDs to precreate,\nOST replaced or reformatted: LFSCK will clean up\"\n```\n\nA related situation may happen if there is a significant discrepancy between the record of previously-created objects on the OST and the previously-allocated objects on the MDT, for example if the MDT has been corrupted, or restored from backup, which would cause significant data loss if left unchecked. This produces a message like:\n\n```\n\"myth-OST0002: too large difference between\nMDS LAST_ID [0x1000200000000:0x100048:0x0] (1048648) and\nOST LAST_ID [0x1000200000000:0x2232123:0x0] (35856675), trust the OST\"\n```\n\nIn such cases, the MDS will advance the `lov_objid` value to match that of the OST to avoid deleting existing objects, which may contain data. Files on the MDT that reference these objects will not be lost. Any unreferenced OST objects will be attached to the `.lustre/lost+found` directory the next time LFSCK `layout` check is run.", "mimetype": "text/plain", "start_char_idx": 13224, "end_char_idx": 17123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ddb765e-bef5-46aa-810d-c49fd0426b9d": {"__data__": {"id_": "4ddb765e-bef5-46aa-810d-c49fd0426b9d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c9b5cf8-8726-4a64-9026-bb3c3516edc8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "892c3a3f44ed4c5abc2ba77197a2fbc6c08f185b8057b986aef459fcd554c6e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53a165a5-b007-4d8b-b848-d1d192a391aa", "node_type": "1", "metadata": {}, "hash": "b602dba9d7a13f12bb6c84d0d63a099d3464b2ec1667142d4bb87641d4529daa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This produces a message like:\n\n```\n\"myth-OST0002: too large difference between\nMDS LAST_ID [0x1000200000000:0x100048:0x0] (1048648) and\nOST LAST_ID [0x1000200000000:0x2232123:0x0] (35856675), trust the OST\"\n```\n\nIn such cases, the MDS will advance the `lov_objid` value to match that of the OST to avoid deleting existing objects, which may contain data. Files on the MDT that reference these objects will not be lost. Any unreferenced OST objects will be attached to the `.lustre/lost+found` directory the next time LFSCK `layout` check is run.\n\n### Handling/Debugging \"`Bind: Address already in use`\" Error\n\nDuring startup, the Lustre software may report a `bind: Address already in use` error and reject to start the operation. This is caused by a portmap service (often NFS locking) that starts before the Lustre file system and binds to the default port 988. You must have port 988 open from firewall or IP tables for incoming connections on the client, OSS, and MDS nodes. LNet will create three outgoing connections on available, reserved ports to each client-server pair, starting with 1023, 1022 and 1021.\n\nUnfortunately, you cannot set sunprc to avoid port 988. If you receive this error, do the following:\n\n- Start the Lustre file system before starting any service that uses sunrpc.\n\n- Use a port other than 988 for the Lustre file system. This is configured in `/etc/modprobe.d/lustre.conf` as an option to the LNet module. For example:\n\n  ```\n  options lnet accept_port=988\n  ```\n\n- Add modprobe ptlrpc to your system startup scripts before the service that uses sunrpc. This causes the Lustre file system to bind to port 988 and sunrpc to select a different port.\n\n   ### Note\n\nYou can also use the `sysctl` command to mitigate the NFS client from grabbing the Lustre service port. However, this is a partial workaround as other user-space RPC servers still have the ability to grab the port.\n\n### Handling/Debugging Error \"- 28\"\n\nA Linux error -28 (`ENOSPC`) that occurs during a write or sync operation indicates that an existing file residing on an OST could not be rewritten or updated because the OST was full, or nearly full. To verify if this is the case, run on a client:\n\n```\nclient$ lfs df -h\nUUID                       bytes        Used   Available Use% Mounted on\nmyth-MDT0000_UUID          12.9G        1.5G       10.6G  12% /myth[MDT:0]\nmyth-OST0000_UUID           3.6T        3.1T      388.9G  89% /myth[OST:0]\nmyth-OST0001_UUID           3.6T        3.6T       64.0K 100% /myth[OST:1]\nmyth-OST0002_UUID           3.6T        3.1T      394.6G  89% /myth[OST:2]\nmyth-OST0003_UUID           5.4T        5.0T      267.8G  95% /myth[OST:3]\nmyth-OST0004_UUID           5.4T        2.9T        2.2T  57% /myth[OST:4]\n\nfilesystem_summary:        21.6T       17.8T        3.2T  85% /myth\n        \n```\n\nTo address this issue, you can expand the disk space on the OST, or use the `lfs_migrate` command to migrate (move) files to a less full OST. For details on both of these options see [*the section called \u201c Adding a New OST to a Lustre File System\u201d*](03.03-Lustre%20Maintenance.md#adding-a-new-ost-to-a-lustre-file-system).\n\nIntroduced in Lustre 2.6\n\nIn some cases, there may be processes holding files open that are consuming a significant amount of space (e.g. runaway process writing lots of data to an open file that has been deleted).", "mimetype": "text/plain", "start_char_idx": 16578, "end_char_idx": 19940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53a165a5-b007-4d8b-b848-d1d192a391aa": {"__data__": {"id_": "53a165a5-b007-4d8b-b848-d1d192a391aa", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ddb765e-bef5-46aa-810d-c49fd0426b9d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5fdeba781c06c42b733eb60573731d4a294e9271c19e07e590cd1b98628633db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c8b38db-8a7c-492b-879a-e70e1a4336db", "node_type": "1", "metadata": {}, "hash": "4b3c0014903a29ed2cf7758b92390475c874fa75695ef9e55f6f7003ea914753", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For details on both of these options see [*the section called \u201c Adding a New OST to a Lustre File System\u201d*](03.03-Lustre%20Maintenance.md#adding-a-new-ost-to-a-lustre-file-system).\n\nIntroduced in Lustre 2.6\n\nIn some cases, there may be processes holding files open that are consuming a significant amount of space (e.g. runaway process writing lots of data to an open file that has been deleted). It is possible to get a list of all open file handles in the filesystem from the MDS:\n\n```\nmds# lctl get_param mdt.*.exports.*.open_files \nmdt.myth-MDT0000.exports.192.168.20.159@tcp.open_files= \n[0x200003ab4:0x435:0x0] \n[0x20001e863:0x1c1:0x0] \n[0x20001e863:0x1c2:0x0] \n: \n:         \n```\nThese file handles can be converted into pathnames on any client via the `lfs fid2path` command (as root):\n\n```\nclient# lfs fid2path /myth [0x200003ab4:0x435:0x0] [0x20001e863:0x1c1:0x0] [0x20001e863:0x1c2:0x0] \nlfs fid2path: cannot find '[0x200003ab4:0x435:0x0]': No such file or directory /myth/tmp/4M \n/myth/tmp/1G \n: \n:         \n```\nIn some cases, if the file has been deleted from the filesystem, `fid2path` will return an error that the file is not found. You can use the client NID (`192.168.20.159@tcp` in the above example) to determine which node the file is open on, and `lsof` to find and kill the process that is holding the file open:\n\n```\n# lsof /myth \nCOMMAND   PID   USER  FD TYPE    DEVICE      SIZE/OFF               NODE NAME logger  13806 mythtv  0r REG  35,632494 1901048576384 144115440203858997 /myth/logs/job.1283929.log (deleted) \t`\n```\n\nA Linux error -28 (`ENOSPC`) that occurs when a new file is being created may indicate that the MDT has run out of inodes and needs to be made larger. Newly created files are not written to full OSTs, while existing files continue to reside on the OST where they were initially created. To view inode information on the MDT, run on a client:\n\n```\nlfs df -i\nUUID                      Inodes       IUsed       IFree IUse% Mounted on\nmyth-MDT0000_UUID        1910263     1910263           0 100% /myth[MDT:0]\nmyth-OST0000_UUID         947456      360059      587397  89% /myth[OST:0]\nmyth-OST0001_UUID         948864      233748      715116  91% /myth[OST:1]\nmyth-OST0002_UUID         947456      549961      397495  89% /myth[OST:2]\nmyth-OST0003_UUID        1426144      477595      948549  95% /myth[OST:3]\nmyth-OST0004_UUID        1426080      465248     1420832  57% /myth[OST:4]\n\nfilesystem_summary:      1910263     1910263           0 100% /myth\n        \n```\n\nTypically, the Lustre software reports this error to your application. If the application is checking the return code from its function calls, then it decodes it into a textual error message such as `No space left on device`. The numeric error message may also appear in the system log.\n\nFor more information about the `lfs df` command, see [*the section called \u201cChecking File System Free Space\u201d*](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#checking-file-system-free-space).", "mimetype": "text/plain", "start_char_idx": 19544, "end_char_idx": 22557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c8b38db-8a7c-492b-879a-e70e1a4336db": {"__data__": {"id_": "0c8b38db-8a7c-492b-879a-e70e1a4336db", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53a165a5-b007-4d8b-b848-d1d192a391aa", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2d1fb314fa1b231acfbb8a2b063c3ba268a7f8e3d1c59bfa128d870dabf9ab35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "093a0a9d-5f90-45fa-b381-86e79679e187", "node_type": "1", "metadata": {}, "hash": "758b6c132915a64c010f84b0a4734ba24500a4ed09cc37b21b7cc9c98229ccd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the application is checking the return code from its function calls, then it decodes it into a textual error message such as `No space left on device`. The numeric error message may also appear in the system log.\n\nFor more information about the `lfs df` command, see [*the section called \u201cChecking File System Free Space\u201d*](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#checking-file-system-free-space).\n\nYou can also use the `lctl get_param` command to monitor the space and object usage on the OSTs and MDTs from any client:\n\n```\nlctl get_param {osc,mdc}.*.{kbytes,files}{free,avail,total}      \n```\n\n **Note**\n\nYou can find other numeric error codes along with a short name and text description in `/usr/include/asm/errno.h`.\n\n### Triggering Watchdog for PID NNN\n\nIn some cases, a server node triggers a watchdog timer and this causes a process stack to be dumped to the console along with a Lustre kernel debug log being dumped into `/tmp` (by default). The presence of a watchdog timer does NOT mean that the thread OOPSed, but rather that it is taking longer time than expected to complete a given operation. In some cases, this situation is expected.\n\nFor example, if a RAID rebuild is really slowing down I/O on an OST, it might trigger watchdog timers to trip. But another message follows shortly thereafter, indicating that the thread in question has completed processing (after some number of seconds). Generally, this indicates a transient problem. In other cases, it may legitimately signal that a thread is stuck because of a software error (lock inversion, for example).\n\n```\nLustre: 0:0:(watchdog.c:122:lcw_cb()) \n```\n\nThe above message indicates that the watchdog is active for pid 933:\n\nIt was inactive for 100000ms:\n\n```\nLustre: 0:0:(linux-debug.c:132:portals_debug_dumpstack()) \n```\n\nShowing stack for process:\n\n```\n933 ll_ost_25     D F896071A     0   933      1    934   932 (L-TLB)\nf6d87c60 00000046 00000000 f896071a f8def7cc 00002710 00001822 2da48cae\n0008cf1a f6d7c220 f6d7c3d0 f6d86000 f3529648 f6d87cc4 f3529640 f8961d3d\n00000010 f6d87c9c ca65a13c 00001fff 00000001 00000001 00000000 00000001\n```\n\nCall trace:\n\n```\nfilter_do_bio+0x3dd/0xb90 [obdfilter]\ndefault_wake_function+0x0/0x20\nfilter_direct_io+0x2fb/0x990 [obdfilter]\nfilter_preprw_read+0x5c5/0xe00 [obdfilter]\nlustre_swab_niobuf_remote+0x0/0x30 [ptlrpc]\nost_brw_read+0x18df/0x2400 [ost]\nost_handle+0x14c2/0x42d0 [ost]\nptlrpc_server_handle_request+0x870/0x10b0 [ptlrpc]\nptlrpc_main+0x42e/0x7c0 [ptlrpc]\n```\n\n### Handling Timeouts on Initial Lustre File System Setup\n\nIf you come across timeouts or hangs on the initial setup of your Lustre file system, verify that name resolution for servers and clients is working correctly. Some distributions configure `/etc/hosts` so the name of the local machine (as reported by the 'hostname' command) is mapped to local host (127.0.0.1) instead of a proper IP address.\n\nThis might produce this error:\n\n```\nLustreError:(ldlm_handle_cancel()) received cancel for unknown lock cookie\n0xe74021a4b41b954e from nid 0x7f000001 (0:127.0.0.1)\n```\n\n### Handling/Debugging \"LustreError: xxx went back in time\"\n\nEach time the MDS or OSS modifies the state of the MDT or OST disk filesystem for a client, it records a per-target increasing transaction number for the operation and returns it to the client along with the reply to that operation.", "mimetype": "text/plain", "start_char_idx": 22129, "end_char_idx": 25512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "093a0a9d-5f90-45fa-b381-86e79679e187": {"__data__": {"id_": "093a0a9d-5f90-45fa-b381-86e79679e187", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c8b38db-8a7c-492b-879a-e70e1a4336db", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "dceeea030c3c93fa5ada981ad0cc2a78299945f5df952dcee05d5f2fdb7504d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7fd9b77-7787-4c1f-a1ed-8a7afda3ffd8", "node_type": "1", "metadata": {}, "hash": "72a35e12185ed2f6d734a1263d53efac308dfdbdeae78137944b2b9db535b450", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some distributions configure `/etc/hosts` so the name of the local machine (as reported by the 'hostname' command) is mapped to local host (127.0.0.1) instead of a proper IP address.\n\nThis might produce this error:\n\n```\nLustreError:(ldlm_handle_cancel()) received cancel for unknown lock cookie\n0xe74021a4b41b954e from nid 0x7f000001 (0:127.0.0.1)\n```\n\n### Handling/Debugging \"LustreError: xxx went back in time\"\n\nEach time the MDS or OSS modifies the state of the MDT or OST disk filesystem for a client, it records a per-target increasing transaction number for the operation and returns it to the client along with the reply to that operation. Periodically, when the server commits these transactions to disk, the last_committed transaction number is returned to the client to allow it to discard pending operations from memory, as they will no longer be needed for recovery in case of server failure.\n\nIn some cases error messages similar to the following have been observed after a server was restarted or failed over:\n\n```\nLustreError: 3769:0:(import.c:517:ptlrpc_connect_interpret())\ntestfs-ost12_UUID went back in time (transno 831 was previously committed,\nserver now claims 791)!\n```\n\nThis situation arises when:\n\n- You are using a disk device that claims to have data written to disk before it actually does, as in case of a device with a large cache. If that disk device crashes or loses power in a way that causes the loss of the cache, there can be a loss of transactions that you believe are committed. This is a very serious event, and you should run e2fsck against that storage before restarting the Lustre file system.\n- As required by the Lustre software, the shared storage used for failover is completely cache-coherent. This ensures that if one server takes over for another, it sees the most up-to-date and accurate copy of the data. In case of the failover of the server, if the shared storage does not provide cache coherency between all of its ports, then the Lustre software can produce an error.\n\nIf you know the exact reason for the error, then it is safe to proceed with no further action. If you do not know the reason, then this is a serious issue and you should explore it with your disk vendor.\n\nIf the error occurs during failover, examine your disk cache settings. If it occurs after a restart without failover, try to determine how the disk can report that a write succeeded, then lose the Data Device corruption or Disk Errors.\n\n### Lustre Error: \"`Slow Start_Page_Write`\"\n\nThe slow `start_page_write` message appears when the operation takes an extremely long time to allocate a batch of memory pages. Use these pages to receive network traffic first, and then write to disk.\n\n### Drawbacks in Doing Multi-client O_APPEND Writes\n\nIt is possible to do multi-client `O_APPEND` writes to a single file, but there are few drawbacks that may make this a sub-optimal solution. These drawbacks are:\n\n- Each client needs to take an `EOF` lock on all the OSTs, as it is difficult to know which OST holds the end of the file until you check all the OSTs. As all the clients are using the same `O_APPEND`, there is significant locking overhead.\n- The second client cannot get all locks until the end of the writing of the first client, as the taking serializes all writes from the clients.\n- To avoid deadlocks, the taking of these locks occurs in a known, consistent order. As a client cannot know which OST holds the next piece of the file until the client has locks on all OSTS, there is a need of these locks in case of a striped file.\n\n### Slowdown Occurs During Lustre File System Startup\n\nWhen a Lustre file system starts, it needs to read in data from the disk. For the very first mdsrate run after the reboot, the MDS needs to wait on all the OSTs for object pre-creation. This causes a slowdown to occur when the file system starts up.\n\nAfter the file system has been running for some time, it contains more data in cache and hence, the variability caused by reading critical metadata from disk is mostly eliminated. The file system now reads data from the cache.\n\n### Log Message `'Out of Memory`' on OST\n\nWhen planning the hardware for an OSS node, consider the memory usage of several components in the Lustre file system. If insufficient memory is available, an 'out of memory' message can be logged.", "mimetype": "text/plain", "start_char_idx": 24866, "end_char_idx": 29208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7fd9b77-7787-4c1f-a1ed-8a7afda3ffd8": {"__data__": {"id_": "e7fd9b77-7787-4c1f-a1ed-8a7afda3ffd8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc1c46bf782883d25e22126060ceb04b3df8bee5daddd838c61d0654d02a10a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "093a0a9d-5f90-45fa-b381-86e79679e187", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a9a3c7a118f8c65ba9b3e32e561ca9d2dfa4e98f3ed7fa1f9d820dabd6264c25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Slowdown Occurs During Lustre File System Startup\n\nWhen a Lustre file system starts, it needs to read in data from the disk. For the very first mdsrate run after the reboot, the MDS needs to wait on all the OSTs for object pre-creation. This causes a slowdown to occur when the file system starts up.\n\nAfter the file system has been running for some time, it contains more data in cache and hence, the variability caused by reading critical metadata from disk is mostly eliminated. The file system now reads data from the cache.\n\n### Log Message `'Out of Memory`' on OST\n\nWhen planning the hardware for an OSS node, consider the memory usage of several components in the Lustre file system. If insufficient memory is available, an 'out of memory' message can be logged.\n\nDuring normal operation, several conditions indicate insufficient RAM on a server node:\n\n- kernel \"`Out of memory`\" and/or \"`oom-killer`\" messages\n- Lustre \"`kmalloc of 'mmm' (NNNN bytes) failed...`\" messages\n- Lustre or kernel stack traces showing processes stuck in \"`try_to_free_pages`\"\n\nFor information on determining the MDS memory and OSS memory requirements, see [*the section called \u201cDetermining Memory Requirements\u201d*](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-memory-requirements).\n\n### Setting SCSI I/O Sizes\n\nSome SCSI drivers default to a maximum I/O size that is too small for good Lustre file system performance. we have fixed quite a few drivers, but you may still find that some drivers give unsatisfactory performance with the Lustre file system. As the default value is hard-coded, you need to recompile the drivers to change their default. On the other hand, some drivers may have a wrong default set.\n\nIf you suspect bad I/O performance and an analysis of Lustre file system statistics indicates that I/O is not 1 MB, check `/sys/block/*device*/queue/max_sectors_kb`. If the `max_sectors_kb` value is less than 1024, set it to at least 1024 to improve performance. If changing `max_sectors_kb` does not change the I/O size as reported by the Lustre software, you may want to examine the SCSI driver code.", "mimetype": "text/plain", "start_char_idx": 28435, "end_char_idx": 30596, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5fcd38d-e8ef-4c3d-b3e3-489a82ce624e": {"__data__": {"id_": "a5fcd38d-e8ef-4c3d-b3e3-489a82ce624e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52afaf24-82ed-453d-8253-c5839bbceddb", "node_type": "1", "metadata": {}, "hash": "531ec2b721fe92ad4b528ef7ac6fe375b10f677dc744fd2166d1512f2b724eba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Troubleshooting Recovery\n\n- [Troubleshooting Recovery](#troubleshooting-recovery)\n  * [Recovering from Errors or Corruption on a Backing ldiskfs File System](#recovering-from-errors-or-corruption-on-a-backing-ldiskfs-file-system)\n  * [Recovering from Corruption in the Lustre File System](#recovering-from-corruption-in-the-lustre-file-system)\n    + [Working with Orphaned Objects](#working-with-orphaned-objects)\n  * [Recovering from an Unavailable OST](#recovering-from-an-unavailable-ost)\n  * [Checking the file system with LFSCK](#checking-the-file-system-with-lfsck)\n    + [LFSCK switch interface](#lfsck-switch-interface)\n      - [Manually Starting LFSCK](#manually-starting-lfsck)\n        * [Description](#description)\n        * [Usage](#usage)\n        * [Options](#options)\n      - [Manually Stopping LFSCK](#manually-stopping-lfsck)\n        * [Description](#description-1)\n        * [Usage](#usage-1)\n        * [Options](#options-1)\n    + [Check the LFSCK global status](#check-the-lfsck-global-status)\n      - [Description](#description-2)\n      - [Usage](#usage-2)\n      - [Options](#options-2)\n    + [LFSCK status interface](#lfsck-status-interface)\n      - [LFSCK status of OI Scrub via `procfs`](#lfsck-status-of-oi-scrub-via-procfs)\n        * [Description](#description-3)\n        * [Usage](#usage-3)\n        * [Output](#output)\n      - [LFSCK status of namespace via `procfs`](#lfsck-status-of-namespace-via-procfs)\n        * [Description](#description-4)\n        * [Usage](#usage-4)\n        * [Output](#output-1)\n      - [LFSCK status of layout via `procfs`](#lfsck-status-of-layout-via-procfs)\n        * [Description](#description-5)\n        * [Usage](#usage-5)\n        * [Output](#output-2)\n    + [LFSCK adjustment interface](#lfsck-adjustment-interface)\n      - [Rate control](#rate-control)\n        * [Description](#description-6)\n        * [Usage](#usage-6)\n        * [Values](#values)\n      - [Auto scrub](#auto-scrub)\n        * [Description](#description-7)\n        * [Usage](#usage-7)\n        * [Values](#values-1)\n\nThis chapter describes what to do if something goes wrong during recovery. It describes:\n\n- [the section called \u201c Recovering from Errors or Corruption on a Backing ldiskfs File System\u201d](#recovering-from-errors-or-corruption-on-a-backing-ldiskfs-file-system)\n- [the section called \u201c Recovering from Corruption in the Lustre File System\u201d](#recovering-from-corruption-in-the-lustre-file-system)\n- [the section called \u201c Recovering from an Unavailable OST\u201d](#recovering-from-corruption-in-the-lustre-file-system)\n- [the section called \u201c Checking the file system with LFSCK\u201d](#checking-the-file-system-with-lfsck)\n\n## Recovering from Errors or Corruption on a Backing ldiskfs File System\n\nWhen an OSS, MDS, or MGS server crash occurs, it is not necessary to run e2fsck on the file system. `ldiskfs`journaling ensures that the file system remains consistent over a system crash. The backing file systems are never accessed directly from the client, so client crashes are not relevant for server file system consistency.\n\nThe only time it is REQUIRED that `e2fsck` be run on a device is when an event causes problems that ldiskfs journaling is unable to handle, such as a hardware device failure or I/O error. If the ldiskfs kernel code detects corruption on the disk, it mounts the file system as read-only to prevent further corruption, but still allows read access to the device. This appears as error \"-30\" ( `EROFS`) in the syslogs on the server, e.g.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3491, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52afaf24-82ed-453d-8253-c5839bbceddb": {"__data__": {"id_": "52afaf24-82ed-453d-8253-c5839bbceddb", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5fcd38d-e8ef-4c3d-b3e3-489a82ce624e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b8e4cf52644a1bbb9ec664d0758f423d56e5b35a7eb8c80524cec29cd8b41522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4280eac5-bc8c-42ec-954f-1743ecd86131", "node_type": "1", "metadata": {}, "hash": "c2759ac3f79ff1721895ae94d555bd46e28ba7736f306292f9c17b6d00b9b400", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "`ldiskfs`journaling ensures that the file system remains consistent over a system crash. The backing file systems are never accessed directly from the client, so client crashes are not relevant for server file system consistency.\n\nThe only time it is REQUIRED that `e2fsck` be run on a device is when an event causes problems that ldiskfs journaling is unable to handle, such as a hardware device failure or I/O error. If the ldiskfs kernel code detects corruption on the disk, it mounts the file system as read-only to prevent further corruption, but still allows read access to the device. This appears as error \"-30\" ( `EROFS`) in the syslogs on the server, e.g.:\n\n```\nDec 29 14:11:32 mookie kernel: LDISKFS-fs error (device sdz):\n            ldiskfs_lookup: unlinked inode 5384166 in dir #145170469\nDec 29 14:11:32 mookie kernel: Remounting filesystem read-only \n```\n\nIn such a situation, it is normally required that e2fsck only be run on the bad device before placing the device back into service.\n\nIn the vast majority of cases, the Lustre software can cope with any inconsistencies found on the disk and between other devices in the file system.\n\nFor problem analysis, it is strongly recommended that `e2fsck` be run under a logger, like `script`, to record all of the output and changes that are made to the file system in case this information is needed later.\n\nIf time permits, it is also a good idea to first run `e2fsck` in non-fixing mode (-n option) to assess the type and extent of damage to the file system. The drawback is that in this mode, `e2fsck` does not recover the file system journal, so there may appear to be file system corruption when none really exists.\n\nTo address concern about whether corruption is real or only due to the journal not being replayed, you can briefly mount and unmount the `ldiskfs` file system directly on the node with the Lustre file system stopped, using a command similar to:\n\n```\nmount -t ldiskfs /dev/{ostdev} /mnt/ost; umount /mnt/ost\n```\n\nThis causes the journal to be recovered.\n\nThe `e2fsck` utility works well when fixing file system corruption (better than similar file system recovery tools and a primary reason why `ldiskfs` was chosen over other file systems). However, it is often useful to identify the type of damage that has occurred so an `ldiskfs` expert can make intelligent decisions about what needs fixing, in place of`e2fsck`.\n\n```\nroot# {stop lustre services for this device, if running}\nroot# script /tmp/e2fsck.sda\nScript started, file is /tmp/e2fsck.sda\nroot# mount -t ldiskfs /dev/sda /mnt/ost\nroot# umount /mnt/ost\nroot# e2fsck -fn /dev/sda   # don't fix file system, just check for corruption\n:\n[e2fsck output]\n:\nroot# e2fsck -fp /dev/sda   # fix errors with prudent answers (usually yes)\n```\n\n## Recovering from Corruption in the Lustre File System\n\nIn cases where an ldiskfs MDT or OST becomes corrupt, you need to run `e2fsck` to ensure local filesystem consistency, then use `LFSCK` to run a distributed check on the file system to resolve any inconsistencies between the MDTs and OSTs, or among MDTs.\n\n1. Stop the Lustre file system.\n\n2. Run `e2fsck -f` on the individual MDT/OST that had problems to fix any local file system damage.\n\n   We recommend running `e2fsck` under script, to create a log of changes made to the file system in case it is needed later. After `e2fsck` is run, bring up the file system, if necessary, to reduce the outage window.\n\n### Working with Orphaned Objects\n\nThe simplest problem to resolve is that of orphaned objects. When the LFSCK layout check is run, these objects are linked to new files and put into `.lustre/lost+found/MDT*xxxx*` in the Lustre file system (where MDTxxxx is the index of the MDT on which the orphan was found), where they can be examined and saved or deleted as necessary.", "mimetype": "text/plain", "start_char_idx": 2826, "end_char_idx": 6642, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4280eac5-bc8c-42ec-954f-1743ecd86131": {"__data__": {"id_": "4280eac5-bc8c-42ec-954f-1743ecd86131", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52afaf24-82ed-453d-8253-c5839bbceddb", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cd281c61aaa1845ec144b5d405c5d92e36c0ccc9f3684442e63b6e936ad410fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbc2e97c-2895-43e8-b4f6-877f684f93e5", "node_type": "1", "metadata": {}, "hash": "7dd854edb486858109ad249da394a0ff0c69789b5480a5173aebca4cb6966458", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Stop the Lustre file system.\n\n2. Run `e2fsck -f` on the individual MDT/OST that had problems to fix any local file system damage.\n\n   We recommend running `e2fsck` under script, to create a log of changes made to the file system in case it is needed later. After `e2fsck` is run, bring up the file system, if necessary, to reduce the outage window.\n\n### Working with Orphaned Objects\n\nThe simplest problem to resolve is that of orphaned objects. When the LFSCK layout check is run, these objects are linked to new files and put into `.lustre/lost+found/MDT*xxxx*` in the Lustre file system (where MDTxxxx is the index of the MDT on which the orphan was found), where they can be examined and saved or deleted as necessary.\n\nIntroduced in Lustre 2.7With Lustre version 2.7 and later, LFSCK will identify and process orphan objects found on MDTs as well.\n\n## Recovering from an Unavailable OST\n\nOne problem encountered in a Lustre file system environment is when an OST becomes unavailable due to a network partition, OSS node crash, etc. When this happens, the OST's clients pause and wait for the OST to become available again, either on the primary OSS or a failover OSS. When the OST comes back online, the Lustre file system starts a recovery process to enable clients to reconnect to the OST. Lustre servers put a limit on the time they will wait in recovery for clients to reconnect.\n\nDuring recovery, clients reconnect and replay their requests serially, in the same order they were done originally. Until a client receives a confirmation that a given transaction has been written to stable storage, the client holds on to the transaction, in case it needs to be replayed. Periodically, a progress message prints to the log, stating how_many/expected clients have reconnected. If the recovery is aborted, this log shows how many clients managed to reconnect. When all clients have completed recovery, or if the recovery timeout is reached, the recovery period ends and the OST resumes normal request processing.\n\nIf some clients fail to replay their requests during the recovery period, this will not stop the recovery from completing. You may have a situation where the OST recovers, but some clients are not able to participate in recovery (e.g. network problems or client failure), so they are evicted and their requests are not replayed. This would result in any operations on the evicted clients failing, including in-progress writes, which would cause cached writes to be lost. This is a normal outcome; the recovery cannot wait indefinitely, or the file system would be hung any time a client failed. The lost transactions are an unfortunate result of the recovery process.\n\n**Note**\n\nThe failure of client recovery does not indicate or lead to filesystem corruption. This is a normal event that is handled by the MDT and OST, and should not result in any inconsistencies between servers.\n\n**Note**\n\nThe version-based recovery (VBR) feature enables a failed client to be ''skipped'', so remaining clients can replay their requests, resulting in a more successful recovery from a downed OST. For more information about the VBR feature, see *Lustre File System Recovery*(Version-based Recovery).\n\n## Checking the file system with LFSCK\n\nLFSCK is an administrative tool for checking and repair of the attributes specific to a mounted Lustre file system. It is similar in concept to an offline fsck repair tool for a local filesystem, but LFSCK is implemented to run as part of the Lustre file system while the file system is mounted and in use. This allows consistency checking and repair of Lustre-specific metadata without unnecessary downtime, and can be run on the largest Lustre file systems with minimal impact to normal operations.\n\nLFSCK can verify and repair the Object Index (OI) table that is used internally to map Lustre File Identifiers (FIDs) to MDT internal ldiskfs inode numbers, in an internal table called the OI Table. An OI Scrub traverses the OI table and makes corrections where necessary. An OI Scrub is required after restoring from a file-level MDT backup ( [*the section called \u201c Backing Up and Restoring an MDT or OST (ldiskfs Device Level)\u201d*](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-and-restoring-an-mdt-or-ost-ldiskfs-device-level)), or in case the OI Table is otherwise corrupted.", "mimetype": "text/plain", "start_char_idx": 5917, "end_char_idx": 10269, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cbc2e97c-2895-43e8-b4f6-877f684f93e5": {"__data__": {"id_": "cbc2e97c-2895-43e8-b4f6-877f684f93e5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4280eac5-bc8c-42ec-954f-1743ecd86131", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9c67d14127bd2d3795605b141e0e6a84c2aa1df48ebc031573363ce00efbd784", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1291463a-80ab-4f81-8935-7df59e9a2c64", "node_type": "1", "metadata": {}, "hash": "19d55d2ea14e639b9e836aad9463e2c55631a9d4a56778695290f202fe763021", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This allows consistency checking and repair of Lustre-specific metadata without unnecessary downtime, and can be run on the largest Lustre file systems with minimal impact to normal operations.\n\nLFSCK can verify and repair the Object Index (OI) table that is used internally to map Lustre File Identifiers (FIDs) to MDT internal ldiskfs inode numbers, in an internal table called the OI Table. An OI Scrub traverses the OI table and makes corrections where necessary. An OI Scrub is required after restoring from a file-level MDT backup ( [*the section called \u201c Backing Up and Restoring an MDT or OST (ldiskfs Device Level)\u201d*](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-and-restoring-an-mdt-or-ost-ldiskfs-device-level)), or in case the OI Table is otherwise corrupted. Later phases of LFSCK will add further checks to the Lustre distributed file system state. LFSCK namespace scanning can verify and repair the directory FID-in-dirent and LinkEA consistency.\n\nIntroduced in Lustre 2.6In Lustre software release 2.6, LFSCK layout scanning can verify and repair MDT-OST file layout inconsistencies. File layout inconsistencies between MDT-objects and OST-objects that are checked and corrected include dangling reference, unreferenced OST-objects, mismatched references and multiple references.\n\nIntroduced in Lustre 2.7\n\nIn Lustre software release 2.7, LFSCK layout scanning is enhanced to support verify and repair inconsistencies between multiple MDTs.\n\nControl and monitoring of LFSCK is through LFSCK and the  `lctl get_param` command. LFSCK supports three types of interface: switch interface, status interface, and adjustment interface. These interfaces are detailed below.\n\n### LFSCK switch interface\n\n#### Manually Starting LFSCK\n\n##### Description\n\nLFSCK can be started after the MDT is mounted using the `lctl lfsck_start` command.\n\n##### Usage\n\n```\nlctl lfsck_start <-M | --device [MDT,OST]_device> \\\n                    [-A | --all] \\\n                    [-c | --create_ostobj on | off] \\\n                    [-C | --create_mdtobj on | off] \\\n                    [-d | --delay_create_ostobj on | off] \\\n                    [-e | --error {continue | abort}] \\\n                    [-h | --help] \\\n                    [-n | --dryrun on | off] \\\n                    [-o | --orphan] \\\n                    [-r | --reset] \\\n                    [-s | --speed ops_per_sec_limit] \\\n                    [-t | --type check_type[,check_type...]] \\\n                    [-w | --window_size size]\n```\n\n##### Options\n\nThe various `lfsck_start` options are listed and described below. For a complete list of available options, type `lctl lfsck_start -h`.\n\n| **Option**                   | **Description**                                              |\n| ---------------------------- | ------------------------------------------------------------ |\n| `-M | --device`              | The MDT or OST target to start LFSCK on.                     |\n| `-A | --all`                 | Introduced in Lustre 2.6                                                                                        Start LFSCK on all targets on all servers simultaneously. By default, both layout and namespace consistency checking and repair are started. |\n| `-c | --create_ostobj`       | Introduced in Lustre 2.6                                                                                            Create the lost OST-object for dangling LOV EA, `off`(default) or `on`. If not specified, then the default behaviour is to keep the dangling LOV EA there without creating the lost OST-object. |\n| `-C | --create_mdtobj`       | Introduced in Lustre 2.7                                                                                           Create the lost MDT-object for dangling name entry, `off`(default) or `on`. If not specified, then the default behaviour is to keep the dangling name entry there without creating the lost MDT-object. |\n| `-d | --delay_create_ostobj` | Introduced in Lustre 2.9                                                                                                Delay creating the lost OST-object for dangling LOV EA until the orphan OST-objects are handled. `off`(default) or `on`. |\n| `-e | --error`               | Error handle, `continue`(default) or `abort`. Specify whether the LFSCK will stop or not if fails to repair something. If it is not specified, the saved value (when resuming from checkpoint) will be used if present. This option cannot be changed while LFSCK is running.", "mimetype": "text/plain", "start_char_idx": 9475, "end_char_idx": 13997, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1291463a-80ab-4f81-8935-7df59e9a2c64": {"__data__": {"id_": "1291463a-80ab-4f81-8935-7df59e9a2c64", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbc2e97c-2895-43e8-b4f6-877f684f93e5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "531e6d1998fbecae077d1b25393c069d61234708a563f1efb6d990801311549b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f189d9f-5d20-419b-841b-d7f2b85a3235", "node_type": "1", "metadata": {}, "hash": "0438f86917d4254da19ecf84db6aea8da3c12bc13ec16aa849c4cba322720507", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `-C | --create_mdtobj`       | Introduced in Lustre 2.7                                                                                           Create the lost MDT-object for dangling name entry, `off`(default) or `on`. If not specified, then the default behaviour is to keep the dangling name entry there without creating the lost MDT-object. |\n| `-d | --delay_create_ostobj` | Introduced in Lustre 2.9                                                                                                Delay creating the lost OST-object for dangling LOV EA until the orphan OST-objects are handled. `off`(default) or `on`. |\n| `-e | --error`               | Error handle, `continue`(default) or `abort`. Specify whether the LFSCK will stop or not if fails to repair something. If it is not specified, the saved value (when resuming from checkpoint) will be used if present. This option cannot be changed while LFSCK is running. |\n| `-h | --help`                | Operating help information.                                  |\n| `-n | --dryrun`              | Perform a trial without making any changes. `off`(default) or `on`. |\n| `-o | --orphan`              | Introduced in Lustre 2.6                                                                                                     Repair orphan OST-objects for layout LFSCK. |\n| `-r | --reset`               | Reset the start position for the object iteration to the beginning for the specified MDT. By default the iterator will resume scanning from the last checkpoint (saved periodically by LFSCK) provided it is available. |\n| `-s | --speed`               | Set the upper speed limit of LFSCK processing in objects per second. If it is not specified, the saved value (when resuming from checkpoint) or default value of 0 (0 = run as fast as possible) is used. Speed can be adjusted while LFSCK is running with the adjustment interface. |\n| `-t | --type`                | The type of checking/repairing that should be performed. The new LFSCK framework provides a single interface for a variety of system consistency checking/repairing operations including:                                                            Without a specified option, the LFSCK component(s) which ran last time and did not finish or the component(s) corresponding to some known system inconsistency, will be started. Anytime the LFSCK is triggered, the OI scrub will run automatically, so there is no need to specify OI_scrub in that case.       Introduced in Lustre 2.4                                                                               `namespace`: check and repair FID-in-dirent and LinkEA consistency.              Introduced in Lustre 2.7                                                                                      Lustre-2.7 enhances namespace consistency verification under DNE mode.Introduced in Lustre 2.6`layout`: check and repair MDT-OST inconsistency. |\n| `-w | --window_size`         | Introduced in Lustre 2.6                                                                                                      The window size for the async request pipeline. The LFSCK async request pipeline's input/output may have quite different processing speeds, and there may be too many requests in the pipeline as to cause abnormal memory/network pressure. If not specified, then the default window size for the async request pipeline is 1024. |\n\n#### Manually Stopping LFSCK\n\n##### Description\n\nTo stop LFSCK when the MDT is mounted, use the `lctl lfsck_stop` command.\n\n##### Usage\n\n```\nlctl lfsck_stop <-M | --device [MDT,OST]_device> \\\n                    [-A | --all] \\\n                    [-h | --help]\n```\n\n##### Options\n\nThe various `lfsck_stop` options are listed and described below. For a complete list of available options, type `lctl lfsck_stop -h`.\n\n| **Option**      | **Description**                                          |\n| --------------- | -------------------------------------------------------- |\n| `-M | --device` | The MDT or OST target to stop LFSCK on.                  |\n| `-A | --all`    | Stop LFSCK on all targets on all servers simultaneously. |\n| `-h | --help`   | Operating help information.                              |\n\n### Check the LFSCK global status\n\n#### Description\n\nCheck the LFSCK global status via a single `lctl lfsck_query` command on the MDS.\n\n#### Usage\n\n```\nlctl lfsck_query <-M | --device MDT_device> \\\n                    [-h | --help] \\\n                    [-t | --type lfsck_type[,lfsck_type...]] \\\n                    [-w | --wait]\n```\n\n#### Options\n\nThe various `lfsck_query` options are listed and described below.", "mimetype": "text/plain", "start_char_idx": 13067, "end_char_idx": 17704, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f189d9f-5d20-419b-841b-d7f2b85a3235": {"__data__": {"id_": "3f189d9f-5d20-419b-841b-d7f2b85a3235", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1291463a-80ab-4f81-8935-7df59e9a2c64", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "dd4fff4361d638882bc5989fc6ed712c4be1b9d08210d5b0842af2888ad43ceb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6eb837f-45bd-42d5-929a-82e505e2c47c", "node_type": "1", "metadata": {}, "hash": "9f5c2c6ff2899748128fca30281438d497c24e5f926140e9ffc59df075dfc290", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| **Option**      | **Description**                                          |\n| --------------- | -------------------------------------------------------- |\n| `-M | --device` | The MDT or OST target to stop LFSCK on.                  |\n| `-A | --all`    | Stop LFSCK on all targets on all servers simultaneously. |\n| `-h | --help`   | Operating help information.                              |\n\n### Check the LFSCK global status\n\n#### Description\n\nCheck the LFSCK global status via a single `lctl lfsck_query` command on the MDS.\n\n#### Usage\n\n```\nlctl lfsck_query <-M | --device MDT_device> \\\n                    [-h | --help] \\\n                    [-t | --type lfsck_type[,lfsck_type...]] \\\n                    [-w | --wait]\n```\n\n#### Options\n\nThe various `lfsck_query` options are listed and described below. For a complete list of available options, type `lctl lfsck_query -h`.\n\n| **Option**      | **Description**                                              |\n| --------------- | ------------------------------------------------------------ |\n| `-M | --device` | The device to query for LFSCK status.                        |\n| `-h | --help`   | Operating help information.                                  |\n| `-t | --type`   | The LFSCK type(s) that should be queried, including: layout, namespace. |\n| `-w | --wait`   | will wait if the LFSCK is in scanning.                       |\n\n### LFSCK status interface\n\n#### LFSCK status of OI Scrub via `procfs`\n\n##### Description\n\nFor each LFSCK component there is a dedicated procfs interface to trace the corresponding LFSCK component status. For OI Scrub, the interface is the OSD layer procfs interface, named `oi_scrub`. To display OI Scrub status, the standard `lctl get_param` command is used as shown in the usage below.\n\n##### Usage\n\n```\nlctl get_param -n osd-ldiskfs.FSNAME-[MDT_target|OST_target].oi_scrub\n```\n\n##### Output\n\n| **Information**     | **Detail**                                                   |\n| ------------------- | ------------------------------------------------------------ |\n| General Information | - Name: OI_scrub.                                                                                                                                               - OI scrub magic id (an identifier unique to OI scrub).                                                                                              - OI files count.                                                                                                                            --- Status: one of the status - `init`, `scanning`, `completed`, `failed`, `stopped`, `paused`, or`crashed`.                                                                                                                           - Flags: including - `recreated`(OI file(s) is/are removed/recreated), `inconsistent`(restored from file-level backup), `auto`(triggered by non-UI mechanism), and `upgrade`(from Lustre software release 1.8 IGIF format.)                              - Parameters: OI scrub parameters, like `failout`.                                                                 - Time Since Last Completed.                                                                                                            - Time Since Latest Start.                                                                                                              - Time Since Last Checkpoint.                                                                                                               - Latest Start Position: the position for the latest scrub started from.                                        - Last Checkpoint Position.                                                                                                                - First Failure Position: the position for the first object to be repaired.                                  - Current Position. |\n| Statistics          | - `Checked` total number of objects scanned.                                                                              - `Updated` total number of objects repaired.                                                                              - `Failed` total number of objects that failed to be repaired.                                                  - `No Scrub` total number of objects marked `LDISKFS_STATE_LUSTRE_NOSCRUB and skipped`.                                                                                                                                             - `IGIF` total number of objects IGIF scanned.                                                                                        - `Prior Updated` how many objects have been repaired which are triggered by parallel RPC.                                                                                                                                                        - `Success Count` total number of completed OI_scrub runs on the target.                           - `Run Time` how long the scrub has run, tally from the time of scanning from the beginning of the specified MDT target, not include the paused/failure time among checkpoints.                                                                                                                                           - `Average Speed` calculated by dividing `Checked` by `run_time`.                                               - `Real-Time Speed` the speed since last checkpoint if the OI_scrub is running.                   - `Scanned` total number of objects under /lost+found that have been scanned.                   - `Repaired` total number of objects under /lost+found that have been recovered.               - `Failed` total number of objects under /lost+found failed to be scanned or failed to be recovered. |", "mimetype": "text/plain", "start_char_idx": 16893, "end_char_idx": 22720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6eb837f-45bd-42d5-929a-82e505e2c47c": {"__data__": {"id_": "b6eb837f-45bd-42d5-929a-82e505e2c47c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f189d9f-5d20-419b-841b-d7f2b85a3235", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "619e3a872a1474ec9fa855639a34bdd8bc88c7237c1514055ab6926a15d4061e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0426c5b3-ac50-4a0b-bf30-28c4ef7c0c88", "node_type": "1", "metadata": {}, "hash": "a9117f83dccf88078d7cafd11c57fc80a8b140931bd374947e6ced069dd9fa40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `IGIF` total number of objects IGIF scanned.                                                                                        - `Prior Updated` how many objects have been repaired which are triggered by parallel RPC.                                                                                                                                                        - `Success Count` total number of completed OI_scrub runs on the target.                           - `Run Time` how long the scrub has run, tally from the time of scanning from the beginning of the specified MDT target, not include the paused/failure time among checkpoints.                                                                                                                                           - `Average Speed` calculated by dividing `Checked` by `run_time`.                                               - `Real-Time Speed` the speed since last checkpoint if the OI_scrub is running.                   - `Scanned` total number of objects under /lost+found that have been scanned.                   - `Repaired` total number of objects under /lost+found that have been recovered.               - `Failed` total number of objects under /lost+found failed to be scanned or failed to be recovered. |\n\n\n\nIntroduced in Lustre 2.4\n\n#### LFSCK status of namespace via `procfs`\n\n##### Description\n\nThe `namespace` component is responsible for checks described in [the section called \u201c Checking the file system with LFSCK\u201d](#checking-the-file-system-with-lfsck). The `procfs` interface for this component is in the MDD layer, named `lfsck_namespace`. To show the status of this component, `lctl get_param` should be used as described in the usage below.\n\nThe LFSCK namespace status output refers to phase 1 and phase 2. Phase 1 is when the LFSCK main engine, which runs on each MDT, linearly scans its local device, guaranteeing that all local objects are checked. However, there are certain cases in which LFSCK cannot know whether an object is consistent or cannot repair an inconsistency until the phase 1 scanning is completed. During phase 2 of the namespace check, objects with multiple hard-links, objects with remote parents, and other objects which couldn't be verified during phase 1 will be checked.\n\n##### Usage\n\n```\nlctl get_param -n mdd. FSNAME-MDT_target.lfsck_namespace\n```\n\n##### Output\n\n| **Information**     | **Detail**                                                   |\n| ------------------- | ------------------------------------------------------------ |\n| General Information | - Name: `lfsck_namespace`                                                                                                           - LFSCK namespace magic.                                                                                                              - LFSCK namespace version..                                                                                                           - Status: one of the status - `init`, `scanning-phase1`, `scanning-phase2`, `completed`, `failed`,`stopped`, `paused`, `partial`, `co-failed`, `co-stopped` or `co-paused`.                                                                                                                        - Flags: including - `scanned-once`(the first cycle scanning has been completed),`inconsistent`(one or more inconsistent FID-in-dirent or LinkEA entries that have been discovered), `upgrade`(from Lustre software release 1.8 IGIF format.)                                                                                                                - Parameters: including `dryrun`, `all_targets`, `failout`, `broadcast`, `orphan`, `create_ostobj`and `create_mdtobj`.                                                                                             - Time Since Last Completed.                                                                                                                - Time Since Latest Start.                                                                                                                  - Time Since Last Checkpoint.                                                                                                               - Latest Start Position: the position the checking began most recently.                                                                                                                                                - Last Checkpoint Position.                                                                                                             - First Failure Position: the position for the first object to be repaired.                                      - Current Position. |\n| Statistics          | - `Checked Phase1` total number of objects scanned during `scanning-phase1`.                - `Checked Phase2` total number of objects scanned during `scanning-phase2`.                - `Updated Phase1` total number of objects repaired during `scanning-phase1`.                                                                                                                              - `Updated Phase2` total number of objects repaired during `scanning-phase2`.                  - `Failed Phase1` total number of objets that failed to be repaired during `scanning-phase1`.                                                                                                                                               - `Failed Phase2` total number of objets that failed to be repaired during `scanning-phase2`.                                                                                                                                                 - `directories` total number of directories scanned.                                                                                                                                                                          - `multiple_linked_checked` total number of multiple-linked objects that have been scanned.                                                                                                                                                  - `dirent_repaired` total number of FID-in-dirent entries that have been repaired.                                                                                                          - `linkea_repaired` total number of linkEA entries that have been repaired.                                                                                                          - `unknown_inconsistency` total number of undefined inconsistencies found in scanning-phase2.                                                                                                                            - `unmatched_pairs_repaired` total number of unmatched pairs that have been repaired.", "mimetype": "text/plain", "start_char_idx": 21428, "end_char_idx": 28285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0426c5b3-ac50-4a0b-bf30-28c4ef7c0c88": {"__data__": {"id_": "0426c5b3-ac50-4a0b-bf30-28c4ef7c0c88", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6eb837f-45bd-42d5-929a-82e505e2c47c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "87c1f6c5f487671a5d2065306472d92c19cb69cd8350185f7ba9dc378497f956", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f72eb89-bf6c-42e7-9738-de311fde642d", "node_type": "1", "metadata": {}, "hash": "934bf7ddd414385524c57779cd028547e6a7d2a84522999cd592bcae966dad6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `Updated Phase2` total number of objects repaired during `scanning-phase2`.                  - `Failed Phase1` total number of objets that failed to be repaired during `scanning-phase1`.                                                                                                                                               - `Failed Phase2` total number of objets that failed to be repaired during `scanning-phase2`.                                                                                                                                                 - `directories` total number of directories scanned.                                                                                                                                                                          - `multiple_linked_checked` total number of multiple-linked objects that have been scanned.                                                                                                                                                  - `dirent_repaired` total number of FID-in-dirent entries that have been repaired.                                                                                                          - `linkea_repaired` total number of linkEA entries that have been repaired.                                                                                                          - `unknown_inconsistency` total number of undefined inconsistencies found in scanning-phase2.                                                                                                                            - `unmatched_pairs_repaired` total number of unmatched pairs that have been repaired.                                                                                                                                                      - `dangling_repaired` total number of dangling name entries that have been found/repaired.                                                                                                                                          - `multi_referenced_repaired` total number of multiple referenced name entries that have been found/repaired.                                                                                                                                   - `bad_file_type_repaired` total number of name entries with bad file type that have been repaired.                                                                                                                                           - `lost_dirent_repaired` total number of lost name entries that have been re-inserted.                                                                                                                                                   - `striped_dirs_scanned` total number of striped directories (master) that have been scanned.                                                                                                                                               - `striped_dirs_repaired` total number of striped directories (master) that have been repaired.                                                                                                                                                    - `striped_dirs_failed` total number of striped directories (master) that have failed to be verified.                                                                                                                                              - `striped_dirs_disabled` total number of striped directories (master) that have been disabled.                                                                                                                                                - `striped_dirs_skipped` total number of striped directories (master) that have been skipped (for shards verification) because of lost master LMV EA.                                                                                                          - `striped_shards_scanned` total number of striped directory shards (slave) that have been scanned.                                                                                                                                         - `striped_shards_repaired` total number of striped directory shards (slave) that have been repaired.                                                                                                                                          - `striped_shards_failed` total number of striped directory shards (slave) that have failed to be verified.                                                                                                                                      - `striped_shards_skipped` total number of striped directory shards (slave) that have been skipped (for name hash verification) because LFSCK does not know whether the slave LMV EA is valid or not.                                                                                                          - `name_hash_repaired` total number of name entries under striped directory with bad name hash that have been repaired.                                                                                                          - `nlinks_repaired` total number of objects with nlink fixed.                                                                                                          - `mul_linked_repaired` total number of multiple-linked objects that have been repaired.                                                                                                                                                        - `local_lost_found_scanned` total number of objects under /lost+found that have been scanned.                                                                                                                                                                            - `local_lost_found_moved` total number of objects under /lost+found that have been moved to namespace visible directory.                                                                                                       - `local_lost_found_skipped` total number of objects under /lost+found that have been skipped.                                                                                                                                                                                    - `local_lost_found_failed` total number of objects under /lost+found that have failed to be processed.                                                                                                                                                                   - `Success Count` the total number of completed LFSCK runs on the target.                                                                                                          - `Run Time Phase1` the duration of the LFSCK run during `scanning-phase1`. Excluding the time spent paused between checkpoints.                                                                                                          - `Run Time Phase2` the duration of the LFSCK run during `scanning-phase2`. Excluding the time spent paused between checkpoints.                                                                                                          - `Average Speed Phase1` calculated by dividing `checked_phase1` by `run_time_phase1`.                                                                                                                                       - `Average Speed Phase2` calculated by dividing `checked_phase2` by `run_time_phase1`.                                                                                                                                                  - `Real-Time Speed Phase1` the speed since the last checkpoint if the LFSCK is running`scanning-phase1`.                                                                                                                                        - `Real-Time Speed Phase2` the speed since the last checkpoint if the LFSCK is running`scanning-phase2`. |\n\n\n\nIntroduced in Lustre 2.6\n\n#### LFSCK status of layout via `procfs`\n\n##### Description\n\nThe `layout` component is responsible for checking and repairing MDT-OST inconsistency. The `procfs` interface for this component is in the MDD layer, named `lfsck_layout`, and in the OBD layer, named `lfsck_layout`. To show the status of this component `lctl get_param` should be used as described in the usage below.", "mimetype": "text/plain", "start_char_idx": 26586, "end_char_idx": 35126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f72eb89-bf6c-42e7-9738-de311fde642d": {"__data__": {"id_": "3f72eb89-bf6c-42e7-9738-de311fde642d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0426c5b3-ac50-4a0b-bf30-28c4ef7c0c88", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "673881c4eb7186355c04d1f0cfc60652ef8645732950d12b6b754d39a1cdbe60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f6e846f-cad9-40e6-bb21-8ffbd5615435", "node_type": "1", "metadata": {}, "hash": "ecb8e4de26c350972c06ab1da9ac57ea8e957b24a0d0d595a508204bdb0c14cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `Average Speed Phase2` calculated by dividing `checked_phase2` by `run_time_phase1`.                                                                                                                                                  - `Real-Time Speed Phase1` the speed since the last checkpoint if the LFSCK is running`scanning-phase1`.                                                                                                                                        - `Real-Time Speed Phase2` the speed since the last checkpoint if the LFSCK is running`scanning-phase2`. |\n\n\n\nIntroduced in Lustre 2.6\n\n#### LFSCK status of layout via `procfs`\n\n##### Description\n\nThe `layout` component is responsible for checking and repairing MDT-OST inconsistency. The `procfs` interface for this component is in the MDD layer, named `lfsck_layout`, and in the OBD layer, named `lfsck_layout`. To show the status of this component `lctl get_param` should be used as described in the usage below.\n\nThe LFSCK layout status output refers to phase 1 and phase 2. Phase 1 is when the LFSCK main engine, which runs on each MDT/OST, linearly scans its local device, guaranteeing that all local objects are checked. During phase 1 of layout LFSCK, the OST-objects which are not referenced by any MDT-object are recorded in a bitmap. During phase 2 of the layout check, the OST-objects in the bitmap will be re-scanned to check whether they are really orphan objects.\n\n##### Usage\n\n```\nlctl get_param -n mdd.\nFSNAME-\nMDT_target.lfsck_layout\nlctl get_param -n obdfilter.\nFSNAME-\nOST_target.lfsck_layout\n```\n\n##### Output\n\n| **Information**     | **Detail**                                                   |\n| ------------------- | ------------------------------------------------------------ |\n| General Information | - Name: `lfsck_layout`                                                                                                                                                   - LFSCK namespace magic.LFSCK namespace version..                                                                                                                                                   - Status: one of the status - `init`, `scanning-phase1`, `scanning-phase2`, `completed`, `failed`,`stopped`, `paused`, `crashed`, `partial`, `co-failed`, `co-stopped`, or `co-paused`.                                                                                                                                                   - Flags: including - `scanned-once`(the first cycle scanning has been completed),`inconsistent`(one or more MDT-OST inconsistencies have been discovered),`incomplete`(some MDT or OST did not participate in the LFSCK or failed to finish the LFSCK) or `crashed_lastid`(the lastid files on the OST crashed and needs to be rebuilt).                                                                                                                                                   - Parameters: including `dryrun`, `all_targets` and `failout`.                                                                                                                                                   - Time Since Last Completed.                                                                                                                                                   - Time Since Latest Start.                                                                                                                                                   - Time Since Last Checkpoint.                                                                                                                                                   - Latest Start Position: the position the checking began most recently.                                                                                                                                                   - Last Checkpoint Position.                                                                                                                                                   - First Failure Position: the position for the first object to be repaired.                                                                                                                                                   - Current Position. |\n| Statistics          |                                                                                                                                                    - `Success Count:` the total number of completed LFSCK runs on the target.                                                                                                                                                   - `Repaired Dangling:` total number of MDT-objects with dangling reference have been repaired in the scanning-phase1.                                                                                                                                                   - `Repaired Unmatched Pairs` total number of unmatched MDT and OST-object pairs have been repaired in the scanning-phase1`Repaired Multiple Referenced` total number of OST-objects with multiple reference have been repaired in the scanning-phase1.                                                                                                                                                   - `Repaired Orphan` total number of orphan OST-objects have been repaired in the scanning-phase2.                                                                                                                                                   - `Repaired Inconsistent Owner` total number.of OST-objects with incorrect owner information have been repaired in the scanning-phase1.                                                                                                                                                   - `Repaired Others` total number of.other inconsistency repaired in the scanning phases.`Skipped` Number of skipped objects.                                                                                                                                                   - `Failed Phase1` total number of objects that failed to be repaired during `scanning-phase1`.                                                                                                                                                   - `Failed Phase2` total number of objects that failed to be repaired during `scanning-phase2`.                                                                                                                                                   - `Checked Phase1` total number of objects scanned during `scanning-phase1`.                                                                                                                                                   - `Checked Phase2` total number of objects scanned during `scanning-phase2`.                                                                                                                                                   - `Run Time Phase1` the duration of the LFSCK run during `scanning-phase1`. Excluding the time spent paused between checkpoints.                                                                                                                                                   - `Run Time Phase2` the duration of the LFSCK run during `scanning-phase2`. Excluding the time spent paused between checkpoints.", "mimetype": "text/plain", "start_char_idx": 34139, "end_char_idx": 41657, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f6e846f-cad9-40e6-bb21-8ffbd5615435": {"__data__": {"id_": "8f6e846f-cad9-40e6-bb21-8ffbd5615435", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff75fac-b53b-4236-96bd-53e27bb85409", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60f0caddd1eb38e0af0f7d737a07734949869eae5b1747c6b0de96bd78c9bfa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f72eb89-bf6c-42e7-9738-de311fde642d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "598feca1e62fec82adcdcb4431a4bbd16cab35efa02e2f83ec5f9a68d97c9812", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `Repaired Others` total number of.other inconsistency repaired in the scanning phases.`Skipped` Number of skipped objects.                                                                                                                                                   - `Failed Phase1` total number of objects that failed to be repaired during `scanning-phase1`.                                                                                                                                                   - `Failed Phase2` total number of objects that failed to be repaired during `scanning-phase2`.                                                                                                                                                   - `Checked Phase1` total number of objects scanned during `scanning-phase1`.                                                                                                                                                   - `Checked Phase2` total number of objects scanned during `scanning-phase2`.                                                                                                                                                   - `Run Time Phase1` the duration of the LFSCK run during `scanning-phase1`. Excluding the time spent paused between checkpoints.                                                                                                                                                   - `Run Time Phase2` the duration of the LFSCK run during `scanning-phase2`. Excluding the time spent paused between checkpoints.                                                                                                                                                   - `Average Speed Phase1` calculated by dividing `checked_phase1` by `run_time_phase1`.                                                                                                                                                   - `Average Speed Phase2` calculated by dividing `checked_phase2` by `run_time_phase1`.                                                                                                                                                   - `Real-Time Speed Phase1` the speed since the last checkpoint if the LFSCK is running`scanning-phase1`.                                                                                                                                                   - `Real-Time Speed Phase2` the speed since the last checkpoint if the LFSCK is running`scanning-phase2`. |\n\n### LFSCK adjustment interface\n\nIntroduced in Lustre 2.6\n\n#### Rate control\n\n##### Description\n\nThe LFSCK upper speed limit can be changed using `lctl set_param` as shown in the usage below.\n\n##### Usage\n\n```\nlctl set_param mdd.${FSNAME}-${MDT_target}.lfsck_speed_limit=\nN\nlctl set_param obdfilter.${FSNAME}-${OST_target}.lfsck_speed_limit=\nN\n```\n\n##### Values\n\n| 0                | No speed limit (run at maximum speed.)        |\n| ---------------- | --------------------------------------------- |\n| positive integer | Maximum number of objects to scan per second. |\n\n#### Auto scrub\n\n##### Description\n\nThe `auto_scrub` parameter controls whether OI scrub will be triggered when an inconsistency is detected during OI lookup. It can be set as described in the usage and values sections below.\n\nThere is also a `noscrub` mount option (see [*the section called \u201c mount.lustre\u201d*](06.07-System%20Configuration%20Utilities.md#mountlustre)) which can be used to disable automatic OI scrub upon detection of a file-level backup at mount time. If the `noscrub` mount option is specified,`auto_scrub` will also be disabled, so OI scrub will not be triggered when an OI inconsistency is detected. Auto scrub can be renabled after the mount using the command shown in the usage. Manually starting LFSCK after mounting provides finer control over the starting conditions.\n\n##### Usage\n\n```\nlctl set_param osd_ldiskfs.${FSNAME}-${MDT_target}.auto_scrub=N\n```\n\nwhere *N* is an integer as described below.\n\nIntroduced in Lustre 2.5\n\n**Note**\n\nLustre software 2.5 and later supports `-P` option that makes the `set_param` permanent.\n\n##### Values\n\n| 0                | Do not start OI Scrub automatically.                         |\n| ---------------- | ------------------------------------------------------------ |\n| positive integer | Automatically start OI Scrub if inconsistency is detected during OI lookup. |", "mimetype": "text/plain", "start_char_idx": 40055, "end_char_idx": 44530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79fd59a1-0f67-4259-95d0-9593eddb2979": {"__data__": {"id_": "79fd59a1-0f67-4259-95d0-9593eddb2979", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6bd3b87-622a-4b94-bd18-1cf5490321f2", "node_type": "1", "metadata": {}, "hash": "6d52227ab33024b8ee9df0f1acd4539839cde170d9788e99601aeedc5afa3f52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Debugging a Lustre File System\n\n- [Debugging a Lustre File System](#debugging-a-lustre-file-system)\n  * [Diagnostic and Debugging Tools](#diagnostic-and-debugging-tools)\n    + [Lustre Debugging Tools](#lustre-debugging-tools)\n    + [External Debugging Tools](#external-debugging-tools)\n      - [Tools for Administrators and Developers](#tools-for-administrators-and-developers)\n      - [Tools for Developers](#tools-for-developers)\n  * [Lustre Debugging Procedures](#lustre-debugging-procedures)\n    + [Understanding the Lustre Debug Messaging Format](#understanding-the-lustre-debug-messaging-format)\n      - [Lustre Debug Messages](#lustre-debug-messages)\n      - [Format of Lustre Debug Messages](#format-of-lustre-debug-messages)\n      - [Lustre Debug Messages Buffer](#lustre-debug-messages-buffer)\n    + [Using the lctl Tool to View Debug Messages](#using-the-lctl-tool-to-view-debug-messages)\n      - [Sample `lctl` Run](#sample-lctl-run)\n    + [Dumping the Buffer to a File (`debug_daemon`)](#dumping-the-buffer-to-a-file-debug_daemon)\n      - [`lctl debug_daemon` Commands](#lctl-debug_daemon-commands)\n    + [Controlling Information Written to the Kernel Debug Log](#controlling-information-written-to-the-kernel-debug-log)\n    + [Troubleshooting with `strace`](#troubleshooting-with-strace)\n    + [Looking at Disk Content](#looking-at-disk-content)\n    + [Finding the Lustre UUID of an OST](#finding-the-lustre-uuid-of-an-ost)\n    + [Printing Debug Messages to the Console](#printing-debug-messages-to-the-console)\n    + [Tracing Lock Traffic](#tracing-lock-traffic)\n    + [Controlling Console Message Rate Limiting](#controlling-console-message-rate-limiting)\n  * [Lustre Debugging for Developers](#lustre-debugging-for-developers)\n    + [Adding Debugging to the Lustre Source Code](#adding-debugging-to-the-lustre-source-code)\n    + [Accessing the `ptlrpc` Request History](#accessing-the-ptlrpc-request-history)\n    + [Finding Memory Leaks Using `leak_finder.pl`](#finding-memory-leaks-using-leak_finderpl)\n\n\nThis chapter describes tips and information to debug a Lustre file system, and includes the following sections:\n\n- [the section called \u201c Diagnostic and Debugging Tools\u201d](#diagnostic-and-debugging-tools)\n- [the section called \u201cLustre Debugging Procedures\u201d](#lustre-debugging-procedures)\n- [the section called \u201cLustre Debugging for Developers\u201d](#lustre-debugging-for-developers)\n\n## Diagnostic and Debugging Tools\n\nA variety of diagnostic and analysis tools are available to debug issues with the Lustre software. Some of these are provided in Linux distributions, while others have been developed and are made available by the Lustre project.\n\n### Lustre Debugging Tools\n\nThe following in-kernel debug mechanisms are incorporated into the Lustre software:\n\n- **Debug logs** - A circular debug buffer to which Lustre internal debug messages are written (in contrast to error messages, which are printed to the syslog or console). Entries in the Lustre debug log are controlled by a mask set by `lctl set_param debug=*mask*`. The log size defaults to 5 MB per CPU but can be increased as a busy system will quickly overwrite 5 MB. When the buffer fills, the oldest log records are discarded.\n- **lctl get_param debug** - This shows the current debug mask used to delimit the debugging information written out to the kernel debug logs.\n- **lctl debug_kernel file** - Dump the Lustre kernel debug log to the specified file as ASCII text for further debugging and analysis.\n- **lctl set_param debug_mb=size** - This sets the maximum size of the in-kernel Lustre debug buffer, in units of MiB.\n- **Debug daemon** - The debug daemon controls the continuous logging of debug messages to a log file in userspace.\n\nThe following tools are also provided with the Lustre software:\n\n- **lctl** - This tool is used with the debug_kernel option to manually dump the Lustre debugging log or post-process debugging logs that are dumped automatically.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6bd3b87-622a-4b94-bd18-1cf5490321f2": {"__data__": {"id_": "e6bd3b87-622a-4b94-bd18-1cf5490321f2", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79fd59a1-0f67-4259-95d0-9593eddb2979", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b6220eb369046342f4decaa5e99a1a8ac73f116cf69d19b941189fa9c7baba91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "771620ec-5238-4304-aa8d-540848257809", "node_type": "1", "metadata": {}, "hash": "0c2a5c6219afafafc51e7a699e616c625d064dff44481027ab3a9119aa98c7a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The log size defaults to 5 MB per CPU but can be increased as a busy system will quickly overwrite 5 MB. When the buffer fills, the oldest log records are discarded.\n- **lctl get_param debug** - This shows the current debug mask used to delimit the debugging information written out to the kernel debug logs.\n- **lctl debug_kernel file** - Dump the Lustre kernel debug log to the specified file as ASCII text for further debugging and analysis.\n- **lctl set_param debug_mb=size** - This sets the maximum size of the in-kernel Lustre debug buffer, in units of MiB.\n- **Debug daemon** - The debug daemon controls the continuous logging of debug messages to a log file in userspace.\n\nThe following tools are also provided with the Lustre software:\n\n- **lctl** - This tool is used with the debug_kernel option to manually dump the Lustre debugging log or post-process debugging logs that are dumped automatically. For more information about the lctl tool, see [*the section called \u201cUsing the lctl Tool to View Debug Messages\u201d*](#using-the-lctl-tool-to-view-debug-messages)and [*the section called \u201c lctl\u201d*](06.07-System%20Configuration%20Utilities.md#lctl).\n- Lustre subsystem asserts - A panic-style assertion (LBUG) in the kernel causes the Lustre file system to dump the debug log to the file `/tmp/lustre-log.*timestamp*` where it can be retrieved after a reboot. For more information, see [*the section called \u201cViewing Error Messages\u201d*](05.01-Lustre%20File%20System%20Troubleshooting.md#viewing-error-messages).\n- *`lfs `*- This utility provides access to the layout of of a Lustre file, along with other information relevant to users. For more information about lfs, see [*the section called \u201c `lfs` \u201d*](06.03-User%20Utilities.md#lfs).\n\n### External Debugging Tools\n\nThe tools described in this section are provided in the Linux kernel or are available at an external website. For information about using some of these tools for Lustre debugging, see\u00a0[*the section called Lustre Debugging Procedures\u201d*](#lustre-debugging-procedures)\u00a0and\u00a0[*the section called \u201cLustre Debugging for Developers\u201d*](#lustre-debugging-for-developers).\n\n#### Tools for Administrators and Developers\n\nSome general debugging tools provided as a part of the standard Linux distribution are:\n\n- **strace** . This tool allows a system call to be traced.\n- **/var/log/messages** . `syslogd` prints fatal or serious messages at this log.\n- **Crash dumps** . On crash-dump enabled kernels, sysrq c produces a crash dump. The Lustre software enhances this crash dump with a log dump (the last 64 KB of the log) to the console.\n- **debugfs** . Interactive file system debugger.\n\nThe following logging and data collection tools can be used to collect information for debugging Lustre kernel issues:\n\n- **kdump** . A Linux kernel crash utility useful for debugging a system running Red Hat Enterprise Linux. For more information about kdump, see the Red Hat knowledge base article How to troubleshoot kernel crashes, hangs, or reboots with kdump on [Red Hat Enterprise Linux](https://access.redhat.com/solutions/6038). To download `kdump`, install the RPM package via yum install kexec-tools.\n- **netconsole** . Enables kernel-level network logging over UDP. A system requires (SysRq) allows users to collect relevant data through `netconsole`.\n- *wireshark* . A network packet inspection tool that allows debugging of information that was sent between the various Lustre nodes. This tool is built on top of tcpdump and can read packet dumps generated by it. There are plug-ins available to dissassemble the LNet and Lustre protocols. They are included with wireshark since version 2.6.0. See also [Wireshark Website](http://www.wireshark.org/) for more details.\n\n#### Tools for Developers\n\nThe tools described in this section may be useful for debugging a Lustre file system in a development environment.\n\nOf general interest is:\n\n- `*leak_finder.pl* `. This program provided with the Lustre software is useful for finding memory leaks in the code.\n\nA virtual machine is often used to create an isolated development and test environment. Some commonly-used virtual machines are:\n\n- **VirtualBox Open Source Edition** .", "mimetype": "text/plain", "start_char_idx": 3048, "end_char_idx": 7233, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "771620ec-5238-4304-aa8d-540848257809": {"__data__": {"id_": "771620ec-5238-4304-aa8d-540848257809", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6bd3b87-622a-4b94-bd18-1cf5490321f2", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6ed8a05d6d15ac20ead2c4757100d0287bb6b5fa370a714b0fa78ff0cacf71ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd7da413-0f58-4831-8192-83066ed725b5", "node_type": "1", "metadata": {}, "hash": "c954ac15d9d3c614512dd10a4ab95b9cdfc8e947c749cf993f1a51344b3d2079", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- *wireshark* . A network packet inspection tool that allows debugging of information that was sent between the various Lustre nodes. This tool is built on top of tcpdump and can read packet dumps generated by it. There are plug-ins available to dissassemble the LNet and Lustre protocols. They are included with wireshark since version 2.6.0. See also [Wireshark Website](http://www.wireshark.org/) for more details.\n\n#### Tools for Developers\n\nThe tools described in this section may be useful for debugging a Lustre file system in a development environment.\n\nOf general interest is:\n\n- `*leak_finder.pl* `. This program provided with the Lustre software is useful for finding memory leaks in the code.\n\nA virtual machine is often used to create an isolated development and test environment. Some commonly-used virtual machines are:\n\n- **VirtualBox Open Source Edition** . Provides enterprise-class virtualization capability for all major platforms and is available free at [Get Sun VirtualBox](https:// www.virtualbox.org/wiki/Downloads).\n- **VMware Server** . Virtualization platform available as free introductory software at [Download VMware Server](https://my.vmware.com/web/vmware/downloads/).\n- **Xen**. A para-virtualized environment with virtualization capabilities similar to VMware Server and Virtual Box. However, Xen allows the use of modified kernels to provide near-native performance and the ability to emulate shared storage. For more information, go to [xen.org](https://xen.org/).\n\nA variety of debuggers and analysis tools are available including:\n\n- **kgdb** . The Linux Kernel Source Level Debugger kgdb is used in conjunction with the GNU Debugger `gdb` for debugging the Linux kernel. For more information about using `kgdb` with `gdb`, see [Chapter 6. Running Programs Under gdb](https://www.linuxtopia.org/online_books/redhat_linux_debugging_with_gdb/running.html) in the *Red Hat Linux 4 Debugging with GDB* guide.\n- **crash** . Used to analyze saved crash dump data when a system had panicked or locked up or appears unresponsive. For more information about using crash to analyze a crash dump, see:\n  - Overview on how to use crash by the author: White Paper: [Red Hat Crash Utility](https://crashutility.github.io/crash_whitepaper.html)\n\n## Lustre Debugging Procedures\n\nThe procedures below may be useful to administrators or developers debugging a Lustre files system.\n\n### Understanding the Lustre Debug Messaging Format\n\nLustre debug messages are categorized by originating subsystem, message type, and location in the source code. For a list of subsystems and message types, see [*the section called \u201cLustre Debug Messages\u201d*](#lustre-debug-messages).\n\n**Note**\n\nFor a current list of subsystems and debug message types, see`libcfs/include/libcfs/libcfs_debug.h` in the Lustre software tree\n\nThe elements of a Lustre debug message are described in [*the section called \u201cFormat of Lustre Debug Messages\u201d*](#format-of-lustre-debug-messages) Format of Lustre Debug Messages.\n\n#### Lustre Debug Messages\n\nEach Lustre debug message has the tag of the subsystem it originated in, the message type, and the location in the source code.", "mimetype": "text/plain", "start_char_idx": 6359, "end_char_idx": 9522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd7da413-0f58-4831-8192-83066ed725b5": {"__data__": {"id_": "fd7da413-0f58-4831-8192-83066ed725b5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "771620ec-5238-4304-aa8d-540848257809", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a3cdc30ab01e53d5c1cb43bd96f73b11567d426ac27288d4b5fe7798d42df582", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c469705-e1c5-418e-b1e0-e40a215c9857", "node_type": "1", "metadata": {}, "hash": "2cbb406eeb222cd3013554b37ee31510c975fb91c7926bb9cfaa47fbfa14eea0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Understanding the Lustre Debug Messaging Format\n\nLustre debug messages are categorized by originating subsystem, message type, and location in the source code. For a list of subsystems and message types, see [*the section called \u201cLustre Debug Messages\u201d*](#lustre-debug-messages).\n\n**Note**\n\nFor a current list of subsystems and debug message types, see`libcfs/include/libcfs/libcfs_debug.h` in the Lustre software tree\n\nThe elements of a Lustre debug message are described in [*the section called \u201cFormat of Lustre Debug Messages\u201d*](#format-of-lustre-debug-messages) Format of Lustre Debug Messages.\n\n#### Lustre Debug Messages\n\nEach Lustre debug message has the tag of the subsystem it originated in, the message type, and the location in the source code. The subsystems and debug types used are as follows:\n\n- Standard Subsystems:\n\n  mdc, mds, osc, ost, obdclass, obdfilter, llite, ptlrpc, portals, lnd, ldlm, lov\n\n- Debug Types:\n\n- | **Types**    | **Description**                                              |\n  | ------------ | ------------------------------------------------------------ |\n  | **trace**    | Function entry/exit markers                                  |\n  | **dlmtrace** | Distributed locking-related information                      |\n  | **inode**    |                                                              |\n  | **super**    |                                                              |\n  | **malloc**   | Memory allocation or free information                        |\n  | **cache**    | Cache-related information                                    |\n  | **info**     | Non-critical general information                             |\n  | **dentry**   | kernel namespace cache handling                              |\n  | **mmap**     | Memory-mapped IO interface                                   |\n  | **page**     | Page cache and bulk data transfers                           |\n  | **info**     | Miscellaneous informational messages                         |\n  | **net**      | LNet network related debugging                               |\n  | **console**  | Significant system events, printed to console                |\n  | **warning**  | Significant but non-fatal exceptions, printed to console     |\n  | **error**    | Critical error messages, printed to console                  |\n  | **neterror** | Significant LNet error messages                              |\n  | **emerg**    | Fatal system errors, printed to console                      |\n  | **config**   | Configuration and setup, enabled by default                  |\n  | **ha**       | Failover and recovery-related information, enabled by default |\n  | **hsm**      | Hierarchical space management/tiering                        |\n  | **ioctl**    | IOCTL-related information, enabled by default                |\n  | **layout**   | File layout handling (PFL, FLR, DoM)                         |\n  | **lfsck**    | Filesystem consistency checking, enabled by default          |\n  | **other**    | Miscellaneious other debug messages                          |\n  | **quota**    | Space accounting and management                              |\n  | **reada**    | Client readahead management                                  |\n  | **rpctrace** | Remote request/reply tracing and debugging                   |\n  | **sec**      | Security, Kerberos, Shared Secret Key handling               |\n  | **snapshot** | Filesystem snapshot management                               |\n  | **vfstrace** | Kernel VFS interface operations                              |\n\n#### Format of Lustre Debug Messages\n\nThe Lustre software uses the `CDEBUG()` and `CERROR()` macros to print the debug or error messages. To print the message, the `CDEBUG()` macro uses the function `libcfs_debug_msg()` (`libcfs/libcfs/tracefile.c`). The message format is described below, along with an example.\n\n| **Description**                   | **Parameter**                                   |\n| --------------------------------- | ----------------------------------------------- |\n| **subsystem**                     | 800000                                          |\n| **debug mask**                    | 000010                                          |\n| **smp_processor_id**              | 0                                               |\n| **seconds.microseconds**          | 1081880847.677302                               |\n| **stack size**                    | 1204                                            |\n| **pid**                           | 2973                                            |\n| **host pid (UML only) or zero**   | 31070                                           |\n| **(file:line #:function_name())** | (obd_mount.c:2089:lustre_fill_super())          |\n| **debug message**                 | kmalloced '*obj': 24 at a375571c (tot 17447717) |\n\n#### Lustre Debug Messages Buffer\n\nLustre debug messages are maintained in a buffer, with the maximum buffer size specified (in MBs) by the `debug_mb` parameter (`lctl get_param debug_mb`).", "mimetype": "text/plain", "start_char_idx": 8762, "end_char_idx": 13797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c469705-e1c5-418e-b1e0-e40a215c9857": {"__data__": {"id_": "4c469705-e1c5-418e-b1e0-e40a215c9857", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd7da413-0f58-4831-8192-83066ed725b5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "76bb2613d213a202ef261d6cd2380e319d5410477e3995457cb98d4f5dabf50f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55c2ee46-422a-47fb-ba08-a90f897c30e6", "node_type": "1", "metadata": {}, "hash": "075235c4465155957b75334dbc0b65aad1781ee18716377ee1052802195e9a04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The buffer is circular, so debug messages are kept until the allocated buffer limit is reached, and then the first messages are overwritten.\n\n### Using the lctl Tool to View Debug Messages\n\nThe `lctl` tool allows debug messages to be filtered based on subsystems and message types to extract information useful for troubleshooting from a kernel debug log. For a command reference, see [*the section called \u201c lctl\u201d*](06.07-System%20Configuration%20Utilities.md#lctl).\n\nYou can use `lctl` to:\n\n- Obtain a list of all the types and subsystems:\n\n  ```\n  lctl > debug_list subsystems|types\n  ```\n\n- Filter the debug log:\n\n  ```\n  lctl > filter subsystem_name|debug_type\n  ```\n\n**Note**\n\nWhen `lctl` filters, it removes unwanted lines from the displayed output. This does not affect the contents of the debug log in the kernel's memory. As a result, you can print the log many times with different filtering levels without worrying about losing data.\n\n- Show debug messages belonging to certain subsystem or type:\n\n  ```\n  lctl > show subsystem_name|debug_type\n  ```\n\n  `debug_kernel` pulls the data from the kernel logs, filters it appropriately, and displays or saves it as per the specified options\n\n  ```\n  lctl > debug_kernel [output filename]\n  ```\n\n  If the debugging is being done on User Mode Linux (UML), it might be useful to save the logs on the host machine so that they can be used at a later time.\n\n- Filter a log on disk, if you already have a debug log saved to disk (likely from a crash):\n\n  ```\n  lctl > debug_file input_file [output_file] \n  ```\n\n  During the debug session, you can add markers or breaks to the log for any reason:\n\n  ```\n  lctl > mark [marker text] \n  ```\n\n  The marker text defaults to the current date and time in the debug log (similar to the example shown below):\n\n  ```\n  DEBUG MARKER: Tue Mar 5 16:06:44 EST 2002 \n  ```\n\n- Completely flush the kernel debug buffer:\n\n  ```\n  lctl > clear\n  ```\n\n**Note**\n\nDebug messages displayed with `lctl` are also subject to the kernel debug masks; the filters are additive.\n\n#### Sample `lctl` Run\n\nBelow is a sample run using the `lctl` command.\n\n```\nbash-2.04# ./lctl \nlctl > debug_kernel /tmp/lustre_logs/log_all \nDebug log: 324 lines, 324 kept, 0 dropped. \nlctl > filter trace \nDisabling output of type \"trace\" \nlctl > debug_kernel /tmp/lustre_logs/log_notrace \nDebug log: 324 lines, 282 kept, 42 dropped. \nlctl > show trace \nEnabling output of type \"trace\" \nlctl > filter portals \nDisabling output from subsystem \"portals\" \nlctl > debug_kernel /tmp/lustre_logs/log_noportals \nDebug log: 324 lines, 258 kept, 66 dropped. \n```\n\n### Dumping the Buffer to a File (`debug_daemon`)\n\nThe `lctl debug_daemon` command is used to continuously dump the `debug_kernel` buffer to a user-specified file. This functionality uses a kernel thread to continuously dump the messages from the kernel debug log, so that much larger debug logs can be saved over a longer time than would fit in the kernel ringbuffer.\n\nThe `debug_daemon` is highly dependent on file system write speed. File system write operations may not be fast enough to flush out all of the `debug_buffer` if the Lustre file system is under heavy system load and continues to log debug messages to the `debug_buffer`. The `debug_daemon` will write the message `DEBUG MARKER: Trace buffer full` into the `debug_buffer` to indicate the `debug_buffer` contents are overlapping before the `debug_daemon`flushes data to a file.\n\nUsers can use the `lctl debug_daemon` command to start or stop the Lustre daemon from dumping the `debug_buffer` to a file.\n\n#### `lctl debug_daemon` Commands\n\nTo initiate the `debug_daemon` to start dumping the `debug_buffer` into a file, run as the root user:\n\n```\nlctl debug_daemon start filename [megabytes]\n```\n\nThe debug log will be written to the specified filename from the kernel. The file will be limited to the optionally specified number of megabytes.", "mimetype": "text/plain", "start_char_idx": 13798, "end_char_idx": 17713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55c2ee46-422a-47fb-ba08-a90f897c30e6": {"__data__": {"id_": "55c2ee46-422a-47fb-ba08-a90f897c30e6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c469705-e1c5-418e-b1e0-e40a215c9857", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1f4c6dd8ef1b83fb9a6b49ebf10a819f02fbdbde447d914141949cfb33970971", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03c25273-8887-4cda-976f-c061b6d442be", "node_type": "1", "metadata": {}, "hash": "a6df26135f3a073f7ce4a4af78fc7c7edef6d4cce1b7b1ccb20fc503beb231c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The `debug_daemon` will write the message `DEBUG MARKER: Trace buffer full` into the `debug_buffer` to indicate the `debug_buffer` contents are overlapping before the `debug_daemon`flushes data to a file.\n\nUsers can use the `lctl debug_daemon` command to start or stop the Lustre daemon from dumping the `debug_buffer` to a file.\n\n#### `lctl debug_daemon` Commands\n\nTo initiate the `debug_daemon` to start dumping the `debug_buffer` into a file, run as the root user:\n\n```\nlctl debug_daemon start filename [megabytes]\n```\n\nThe debug log will be written to the specified filename from the kernel. The file will be limited to the optionally specified number of megabytes.\n\nThe daemon wraps around and dumps data to the beginning of the file when the output file size is over the limit of the user-specified file size. To decode the dumped file to ASCII and sort the log entries by time, run:\n\n```\nlctl debug_file filename > newfile\n```\n\nThe output is internally sorted by the `lctl` command.\n\nTo stop the `debug_daemon` operation and flush the file output, run:\n\n```\nlctl debug_daemon stop\n```\n\nOtherwise, `debug_daemon` is shut down as part of the Lustre file system shutdown process. Users can restart `debug_daemon` by using start command after each stop command issued.\n\nThis is an example using `debug_daemon` with the interactive mode of `lctl` to dump debug logs to a 40 MB file.\n\n```\nlctl\n```\n\n```\nlctl > debug_daemon start /var/log/lustre.40.bin 40 \n```\n\n```\nrun filesystem operations to debug\n```\n\n```\nlctl > debug_daemon stop \n```\n\n```\nlctl > debug_file /var/log/lustre.bin /var/log/lustre.log\n```\n\nTo start another daemon with an unlimited file size, run:\n\n```\nlctl > debug_daemon start /var/log/lustre.bin \n```\n\nThe text message `*** End of debug_daemon trace log ***` appears at the end of each output file.\n\n### Controlling Information Written to the Kernel Debug Log\n\nThe `lctl set_param subsystem_debug=*subsystem_mask*` and `lctl set_param debug=*debug_mask*` are used to determine which information is written to the debug log. The subsystem_debug mask determines the information written to the log based on the functional area of the code (such as lnet, osc, or ldlm). The debug mask controls information based on the message type (such as info, error, trace, or malloc). For a complete list of possible debug masks use the `lctl debug_list types` command.\n\nTo turn off Lustre debugging completely:\n\n```\nlctl set_param debug=0 \n```\n\nTo turn on full Lustre debugging:\n\n```\nlctl set_param debug=-1 \n```\n\nTo list all possible debug masks:\n\n```\nlctl debug_list types\n```\n\nTo log only messages related to network communications:\n\n```\nlctl set_param debug=net \n```\n\nTo turn on logging of messages related to network communications and existing debug flags:\n\n```\nlctl set_param debug=+net \n```\n\nTo turn off network logging with changing existing flags:\n\n```\nlctl set_param debug=-net \n```\n\nThe various options available to print to kernel debug logs are listed in `libcfs/include/libcfs/libcfs.h`\n\n### Troubleshooting with `strace`\n\nThe `strace` utility provided with the Linux distribution enables system calls to be traced by intercepting all the system calls made by a process and recording the system call name, arguments, and return values.\n\nTo invoke `strace` on a program, enter:\n\n```\n$ strace program [arguments] \n```\n\nSometimes, a system call may fork child processes. In this situation, use the `-f` option of `strace` to trace the child processes:\n\n```\n$ strace -f program [arguments] \n```\n\nTo redirect the `strace` output to a file, enter:\n\n```\n$ strace -o filename program [arguments] \n```\n\nUse the `-ff` option, along with `-o`, to save the trace output in `filename.pid`, where `pid` is the process ID of the process being traced. Use the `-ttt` option to timestamp all lines in the strace output, so they can be correlated to operations in the lustre kernel debug log.\n\n### Looking at Disk Content\n\nIn a Lustre file system, the inodes on the metadata server contain extended attributes (EAs) that store information about file striping.", "mimetype": "text/plain", "start_char_idx": 17044, "end_char_idx": 21106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03c25273-8887-4cda-976f-c061b6d442be": {"__data__": {"id_": "03c25273-8887-4cda-976f-c061b6d442be", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55c2ee46-422a-47fb-ba08-a90f897c30e6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c9a2d96749f230ea277cafd416154fc2aa18de8c06bafa40c55b85aa2f2a33d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68180694-3e1d-4002-9d96-522b76ecd18f", "node_type": "1", "metadata": {}, "hash": "87696b289d1239c40026e820d739d46e5aa9dc9dd98ed6ecef85510aa5fc0b22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this situation, use the `-f` option of `strace` to trace the child processes:\n\n```\n$ strace -f program [arguments] \n```\n\nTo redirect the `strace` output to a file, enter:\n\n```\n$ strace -o filename program [arguments] \n```\n\nUse the `-ff` option, along with `-o`, to save the trace output in `filename.pid`, where `pid` is the process ID of the process being traced. Use the `-ttt` option to timestamp all lines in the strace output, so they can be correlated to operations in the lustre kernel debug log.\n\n### Looking at Disk Content\n\nIn a Lustre file system, the inodes on the metadata server contain extended attributes (EAs) that store information about file striping. EAs contain a list of all object IDs and their locations (that is, the OST that stores them). The `lfs`tool can be used to obtain this information for a given file using the `getstripe` subcommand. Use a corresponding `lfs setstripe` command to specify striping attributes for a new file or directory.\n\nThe `lfs getstripe` command takes a Lustre filename as input and lists all the objects that form a part of this file. To obtain this information for the file `/mnt/testfs/frog` in a Lustre file system, run:\n\n```\n$ lfs getstripe /mnt/testfs/frog\nlmm_stripe_count:   2\nlmm_stripe_size:    1048576\nlmm_pattern:        1\nlmm_layout_gen:     0\nlmm_stripe_offset:  2\n        obdidx           objid          objid           group\n             2          818855        0xc7ea7               0\n             0          873123        0xd52a3               0\n        \n```\n\nThe `debugfs` tool is provided in the `e2fsprogs` package. It can be used for interactive debugging of an `ldiskfs` file system. The `debugfs` tool can either be used to check status or modify information in the file system. In a Lustre file system, all objects that belong to a file are stored in an underlying `ldiskfs` file system on the OSTs. The file system uses the object IDs as the file names. Once the object IDs are known, use the `debugfs` tool to obtain the attributes of all objects from different OSTs.", "mimetype": "text/plain", "start_char_idx": 20433, "end_char_idx": 22487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68180694-3e1d-4002-9d96-522b76ecd18f": {"__data__": {"id_": "68180694-3e1d-4002-9d96-522b76ecd18f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03c25273-8887-4cda-976f-c061b6d442be", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "179e240a88c633910b7257f2990eedd4023136f412cacbe6fdd52af75425c19a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ab42aa1-762a-43b6-ae4f-9241ac5b56db", "node_type": "1", "metadata": {}, "hash": "667bdf460da965eb27e639abb5870b93c996907e3449e6978183ead7c7a16ac5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It can be used for interactive debugging of an `ldiskfs` file system. The `debugfs` tool can either be used to check status or modify information in the file system. In a Lustre file system, all objects that belong to a file are stored in an underlying `ldiskfs` file system on the OSTs. The file system uses the object IDs as the file names. Once the object IDs are known, use the `debugfs` tool to obtain the attributes of all objects from different OSTs.\n\nA sample run for the `/mnt/testfs/frog` file used in the above example is shown here:\n\n```\n$ debugfs -c -R \"stat O/0/d$((818855 % 32))/818855\" /dev/vgmyth/lvmythost2\n\ndebugfs 1.41.90.wc3 (28-May-2011)\n/dev/vgmyth/lvmythost2: catastrophic mode - not reading inode or group bitmaps\nInode: 227649   Type: regular    Mode:  0666   Flags: 0x80000\nGeneration: 1375019198    Version: 0x0000002f:0000728f\nUser:  1000   Group:  1000   Size: 2800\nFile ACL: 0    Directory ACL: 0\nLinks: 1   Blockcount: 8\nFragment:  Address: 0    Number: 0    Size: 0\n ctime: 0x4e177fe5:00000000 -- Fri Jul  8 16:08:37 2011\n atime: 0x4d2e2397:00000000 -- Wed Jan 12 14:56:39 2011\n mtime: 0x4e177fe5:00000000 -- Fri Jul  8 16:08:37 2011\ncrtime: 0x4c3b5820:a364117c -- Mon Jul 12 12:00:00 2010\nSize of extra inode fields: 28\nExtended attributes stored in inode body: \n  fid = \"08 80 24 00 00 00 00 00 28 8a e7 fc 00 00 00 00 a7 7e 0c 00 00 00 00 00\n 00 00 00 00 00 00 00 00 \" (32)\n  fid: objid=818855 seq=0 parent=[0x248008:0xfce78a28:0x0] stripe=0\nEXTENTS:\n(0):63331288\n```\n\n### Finding the Lustre UUID of an OST\n\nTo determine the Lustre UUID of an OST disk (for example, if you mix up the cables on your OST devices or the SCSI bus numbering suddenly changes and the SCSI devices get new names), it is possible to extract this from the last_rcvd file using debugfs:\n\n```\ndebugfs -c -R \"dump last_rcvd /tmp/last_rcvd\" /dev/sdc\nstrings /tmp/last_rcvd | head -1\nmyth-OST0004_UUID\n      \n```\n\nIt is also possible (and easier) to extract this from the file system label using the `dumpe2fs` command:\n\n```\ndumpe2fs -h /dev/sdc | grep volume\ndumpe2fs 1.41.90.wc3 (28-May-2011)\nFilesystem volume name:   myth-OST0004\n      \n```\n\nThe debugfs and dumpe2fs commands are well documented in the `debugfs(8)` and `dumpe2fs(8)` manual pages.\n\n### Printing Debug Messages to the Console\n\nTo dump debug messages to the console (`/var/log/messages`), set the corresponding debug mask in the `printk`flag:\n\n```\nlctl set_param printk=-1 \n```\n\nThis slows down the system dramatically. It is also possible to selectively enable or disable this capability for particular flags using:`lctl set_param printk=+vfstrace` and `lctl set_param printk=-vfstrace `.\n\nIt is possible to disable warning, error, and console messages, though it is strongly recommended to have something like `lctl debug_daemon` running to capture this data to a local file system for failure detection purposes.\n\n### Tracing Lock Traffic\n\nThe Lustre software provides a specific debug type category for tracing lock traffic.", "mimetype": "text/plain", "start_char_idx": 22030, "end_char_idx": 25033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ab42aa1-762a-43b6-ae4f-9241ac5b56db": {"__data__": {"id_": "8ab42aa1-762a-43b6-ae4f-9241ac5b56db", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68180694-3e1d-4002-9d96-522b76ecd18f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6c048c55a5fc0983e059903009c97dfa541bae509d3b8ffe743842c5cabf9b14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94780ac9-a5d4-4faa-ac54-be9ac8408ca2", "node_type": "1", "metadata": {}, "hash": "db37f1b883448186afa67b1b4d19f4483e8b47d661d263d9ad590fe519540640", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Printing Debug Messages to the Console\n\nTo dump debug messages to the console (`/var/log/messages`), set the corresponding debug mask in the `printk`flag:\n\n```\nlctl set_param printk=-1 \n```\n\nThis slows down the system dramatically. It is also possible to selectively enable or disable this capability for particular flags using:`lctl set_param printk=+vfstrace` and `lctl set_param printk=-vfstrace `.\n\nIt is possible to disable warning, error, and console messages, though it is strongly recommended to have something like `lctl debug_daemon` running to capture this data to a local file system for failure detection purposes.\n\n### Tracing Lock Traffic\n\nThe Lustre software provides a specific debug type category for tracing lock traffic. Use:\n\n```\nlctl> filter all_types \nlctl> show dlmtrace \nlctl> debug_kernel [filename]  \n```\n\n### Controlling Console Message Rate Limiting\n\nSome console messages which are printed by Lustre are rate limited. When such messages are printed, they may be followed by a message saying \"Skipped N previous similar message(s),\" where N is the number of messages skipped. This rate limiting can be completely disabled by a libcfs module parameter called `libcfs_console_ratelimit`. To disable console message rate limiting, add this line to `/etc/modprobe.d/lustre.conf` and then reload Lustre modules.\n\n```\noptions libcfs libcfs_console_ratelimit=0\n```\n\nIt is also possible to set the minimum and maximum delays between rate-limited console messages using the module parameters `libcfs_console_max_delay` and `libcfs_console_min_delay`. Set these in `/etc/modprobe.d/lustre.conf` and then reload Lustre modules. Additional information on libcfs module parameters is available via `modinfo`:\n\n```\nmodinfo libcfs\n```\n\n## Lustre Debugging for Developers\n\nThe procedures in this section may be useful to developers debugging Lustre source code.\n\n### Adding Debugging to the Lustre Source Code\n\nThe debugging infrastructure provides a number of macros that can be used in Lustre source code to aid in debugging or reporting serious errors.\n\nTo use these macros, you will need to set the `DEBUG_SUBSYSTEM` variable at the top of the file as shown below:\n\n```\n#define DEBUG_SUBSYSTEM S_PORTALS\n```\n\nA list of available macros with descriptions is provided in the table below.\n\n| **Macro**                                | **Description**                                              |\n| ---------------------------------------- | ------------------------------------------------------------ |\n| **LBUG()**                               | A panic-style assertion in the kernel which causes the Lustre file system to dump its circular log to the `/tmp/lustre-log` file. This file can be retrieved after a reboot. `LBUG()`freezes the thread to allow capture of the panic stack. A system reboot is needed to clear the thread. |\n| **LASSERT()**                            | Validates a given expression as true, otherwise calls LBUG(). The failed expression is printed on the console, although the values that make up the expression are not printed. |\n| **LASSERTF()**                           | Similar to `LASSERT()` but allows a free-format message to be printed, like `printf/printk`. |\n| **CDEBUG()**                             | The basic, most commonly used debug macro that takes just one more argument than standard `printf()` - the debug type. This message adds to the debug log with the debug mask set accordingly. Later, when a user retrieves the log for troubleshooting, they can filter based on this type.                                                                                                                                                   `CDEBUG(D_INFO, \"debug message: rc=%d\\n\", number);` |\n| **CDEBUG_LIMIT()**                       | Behaves similarly to `CDEBUG()`, but rate limits this message when printing to the console (for `D_WARN`, `D_ERROR`, and `D_CONSOLE` message types. This is useful for messages that use a variable debug mask:                                                                                                                                                   `CDEBUG(mask, \"maybe bad: rc=%d\\n\", rc);` |\n| **CERROR()**                             | Internally using `CDEBUG_LIMIT(D_ERROR, ...)`, which unconditionally prints the message in the debug log and to the console. This is appropriate for serious errors or fatal conditions. Messages printed to the console are prefixed with `LustreError:`, and are rate-limited, to avoid flooding the console with duplicates.                                                                                                                                                   `CERROR(\"Something bad happened: rc=%d\\n\", rc);` |\n| **CWARN()**                              | Behaves similarly to `CERROR()`, but prefixes the messages with `Lustre:`. This is appropriate for important, but not fatal conditions.", "mimetype": "text/plain", "start_char_idx": 24289, "end_char_idx": 29222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94780ac9-a5d4-4faa-ac54-be9ac8408ca2": {"__data__": {"id_": "94780ac9-a5d4-4faa-ac54-be9ac8408ca2", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ab42aa1-762a-43b6-ae4f-9241ac5b56db", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "139bbf6826cfdb08aa41f705c167fd18b06c18f3920280fd8eabf0dbdc95bad6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de6b6565-4653-4051-b0fd-509673094b31", "node_type": "1", "metadata": {}, "hash": "f8d29bde4a838f54b12a26f5917990b82bbacbe4e5b41d2c7e25a114df5b5e0e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is useful for messages that use a variable debug mask:                                                                                                                                                   `CDEBUG(mask, \"maybe bad: rc=%d\\n\", rc);` |\n| **CERROR()**                             | Internally using `CDEBUG_LIMIT(D_ERROR, ...)`, which unconditionally prints the message in the debug log and to the console. This is appropriate for serious errors or fatal conditions. Messages printed to the console are prefixed with `LustreError:`, and are rate-limited, to avoid flooding the console with duplicates.                                                                                                                                                   `CERROR(\"Something bad happened: rc=%d\\n\", rc);` |\n| **CWARN()**                              | Behaves similarly to `CERROR()`, but prefixes the messages with `Lustre:`. This is appropriate for important, but not fatal conditions. Messages printed to the console are rate-limited. |\n| **CNETERR()**                            | Behaves similarly to `CERROR()`, but prints error messages for LNet if `D_NETERR` is set in the `debug` mask. This is appropriate for serious networking errors. Messages printed to the console are rate-limited. |\n| **DEBUG_REQ()**                          | Prints information about the given `ptlrpc_request` structure.                                                                                                                                                   `DEBUG_REQ(D_RPCTRACE, req, \"\"Handled RPC: rc=%d\\n\", rc);` |\n| **ENTRY**                                | Add messages to the entry of a function to aid in call tracing (takes no arguments). When using these macros, cover all exit conditions with a single `EXIT`, `GOTO()`, or `RETURN()` macro to avoid confusion when the debug log reports that a function was entered, but never exited. |\n| **EXIT**                                 | Mark the exit of a function, to match `ENTRY` (takes no arguments). |\n| **GOTO()**                               | Mark when code jumps via `goto` to the end of a function, to match `ENTRY`, and prints out the goto label and function return code in signed and unsigned decimal, and hexadecimal format. |\n| **RETURN()**                             | Mark the exit of a function, to match `ENTRY`, and prints out the function return code in signed and unsigned decimal, and hexadecimal format. |\n| **LDLM_DEBUG()** **LDLM_DEBUG_NOLOCK()** | Used when tracing LDLM locking operations. These macros build a thin trace that shows the locking requests on a node, and can also be linked across the client and server node using the printed lock handles. |\n| **OBD_FAIL_CHECK()**                     | Allows insertion of failure points into the Lustre source code. This is useful to generate regression tests that can hit a very specific sequence of events. This works in conjunction with \"`lctl set_param fail_loc=*fail_loc*`\" to set a specific failure point for which a given `OBD_FAIL_CHECK()` will test. |\n| **OBD_FAIL_TIMEOUT()**                   | Similar to `OBD_FAIL_CHECK()`. Useful to simulate hung, blocked or busy processes or network devices. If the given `fail_loc` is hit, `OBD_FAIL_TIMEOUT()` waits for the specified number of seconds. |\n| **OBD_RACE()**                           | Similar to `OBD_FAIL_CHECK()`. Useful to have multiple processes execute the same code concurrently to provoke locking races. The first process to hit `OBD_RACE()` sleeps until a second process hits `OBD_RACE()`, then both processes continue. |\n| **OBD_FAIL_ONCE**                        | A flag set on a `fail_loc` breakpoint to cause the `OBD_FAIL_CHECK()` condition to be hit only one time. Otherwise, a `fail_loc` is permanent until it is cleared with \"`lctl set_param fail_loc=0`\". |\n| **OBD_FAIL_RAND**                        | A flag set on a `fail_loc` breakpoint to cause `OBD_FAIL_CHECK()` to fail randomly; on average every (1 / fail_val) times. |\n| **OBD_FAIL_SKIP**                        | A flag set on a `fail_loc` breakpoint to cause `OBD_FAIL_CHECK()` to succeed `fail_val`times, and then fail permanently or once with `OBD_FAIL_ONCE`. |\n| **OBD_FAIL_SOME**                        | A flag set on `fail_loc` breakpoint to cause `OBD_FAIL_CHECK` to fail `fail_val` times, and then succeed. |", "mimetype": "text/plain", "start_char_idx": 28230, "end_char_idx": 32622, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de6b6565-4653-4051-b0fd-509673094b31": {"__data__": {"id_": "de6b6565-4653-4051-b0fd-509673094b31", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c98af996-876a-4161-b507-b7f760731c52", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "29fcf36825c72aff032bc17e8477eb71e41c4189c8c62795dadd973793cc90c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94780ac9-a5d4-4faa-ac54-be9ac8408ca2", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ae8c19f849d2b59659387515da53ef657d5e20bcfe0a012baa79c20263ce7609", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Otherwise, a `fail_loc` is permanent until it is cleared with \"`lctl set_param fail_loc=0`\". |\n| **OBD_FAIL_RAND**                        | A flag set on a `fail_loc` breakpoint to cause `OBD_FAIL_CHECK()` to fail randomly; on average every (1 / fail_val) times. |\n| **OBD_FAIL_SKIP**                        | A flag set on a `fail_loc` breakpoint to cause `OBD_FAIL_CHECK()` to succeed `fail_val`times, and then fail permanently or once with `OBD_FAIL_ONCE`. |\n| **OBD_FAIL_SOME**                        | A flag set on `fail_loc` breakpoint to cause `OBD_FAIL_CHECK` to fail `fail_val` times, and then succeed. |\n\n\n\n### Accessing the `ptlrpc` Request History\n\nEach service maintains a request history, which can be useful for first occurrence troubleshooting.\n\n`ptlrpc` is an RPC protocol layered on LNet that deals with stateful servers and has semantics and built-in support for recovery.\n\nThe ptlrpc request history works as follows:\n\n1. `request_in_callback()` adds the new request to the service's request history.\n2. When a request buffer becomes idle, it is added to the service's request buffer history list.\n3. Buffers are culled from the service request buffer history if it has grown above `req_buffer_history_max` and its reqs are removed from the service request history.\n\nRequest history is accessed and controlled using the following parameters for each service:\n\n- `req_buffer_history_len`\n\n  Number of request buffers currently in the history\n\n- `req_buffer_history_max`\n\n  Maximum number of request buffers to keep\n\n- `req_history`\n\n  The request history\n\nRequests in the history include \"live\" requests that are currently being handled. Each line in `req_history` looks like:\n\n```\nsequence:target_NID:client_NID:cliet_xid:request_length:rpc_phase service_specific_data \n```\n\n| **Parameter**    | **Description**                                              |\n| ---------------- | ------------------------------------------------------------ |\n| **seq**          | Request sequence number                                      |\n| `*target NID*`   | Destination `NID` of the incoming request                    |\n| `*client ID*`    | Client `PID` and `NID`                                       |\n| `*xid*`          | `rq_xid`                                                     |\n| **length**       | Size of the request message                                  |\n| **phase**        | New (waiting to be handled or could not be unpacked)Interpret (unpacked or being handled)Complete (handled) |\n| **svc specific** | Service-specific request printout. Currently, the only service that does this is the OST (which prints the opcode if the message has been unpacked successfully |\n\n### Finding Memory Leaks Using `leak_finder.pl`\n\nMemory leaks can occur in code when memory has been allocated and then not freed once it is no longer required. The `leak_finder.pl` program provides a way to find memory leaks.\n\nBefore running this program, you must turn on debugging to collect all `malloc` and free entries. Run:\n\n```\nlctl set_param debug=+malloc \n```\n\nThen complete the following steps:\n\n1. Dump the log into a user-specified log file using lctl (see [*the section called \u201cUsing the lctl Tool to View Debug Messages\u201d*](#using-the-lctl-tool-to-view-debug-messages)).\n\n2. Run the leak finder on the newly-created log dump:\n\n   ```\n   perl leak_finder.pl ascii-logname\n   ```\n\nThe output is:\n\n```\nmalloced 8bytes at a3116744 (called pathcopy) \n(lprocfs_status.c:lprocfs_add_vars:80) \nfreed 8bytes at a3116744 (called pathcopy) \n(lprocfs_status.c:lprocfs_add_vars:80) \n```\n\nThe tool displays the following output to show the leaks found:\n\n```\nLeak:32bytes allocated at a23a8fc(service.c:ptlrpc_init_svc:144,debug file line 241)\n```", "mimetype": "text/plain", "start_char_idx": 32008, "end_char_idx": 35753, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55347ab6-1108-482d-ab43-548ed534e865": {"__data__": {"id_": "55347ab6-1108-482d-ab43-548ed534e865", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8575c6da-600e-4aee-9e5e-c3d974cc49b8", "node_type": "1", "metadata": {}, "hash": "1c1827423bbc3ca20c9c1dd2b786b7d5c2667e4978ed325349f2f303d7862b8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Lustre File System Recovery\n\n- [Lustre File System Recovery](#lustre-file-system-recovery)\n  * [Recovery Overview](#recovery-overview)\n    + [Client Failure](#client-failure)\n    + [Client Eviction](#client-eviction)\n    + [MDS Failure (Failover)](#mds-failure-failover)\n    + [OST Failure (Failover)](#ost-failure-failover)\n    + [Network Partition](#network-partition)\n    + [Failed Recovery](#failed-recovery)\n  * [Metadata Replay](#metadata-replay)\n    + [XID Numbers](#xid-numbers)\n    + [Transaction Numbers](#transaction-numbers)\n    + [Replay and Resend](#replay-and-resend)\n    + [Client Replay List](#client-replay-list)\n    + [Server Recovery](#server-recovery)\n    + [Request Replay](#request-replay)\n    + [Gaps in the Replay Sequence](#gaps-in-the-replay-sequence)\n    + [Lock Recovery](#lock-recovery)\n    + [Request Resend](#request-resend)\n  * [Reply Reconstruction](#reply-reconstruction)\n    + [Required State](#required-state)\n    + [Reconstruction of Open Replies](#reconstruction-of-open-replies)\n      - [Finding the File Handle](#finding-the-file-handle)\n      - [Finding the Resource/fid](#finding-the-resourcefid)\n      - [Finding the Lock Handle](#finding-the-lock-handle)\n    + [Multiple Reply Data per Client](#multiple-reply-data-per-client)L 2.8\n  * [Version-based Recovery](#version-based-recovery)\n    + [VBR Messages](#vbr-messages)\n    + [Tips for Using VBR](#tips-for-using-vbr)\n  * [Commit on Share](#commit-on-share)\n    + [Working with Commit on Share](#working-with-commit-on-share)\n    + [Tuning Commit On Share](#tuning-commit-on-share)\n  * [Imperative Recovery](#imperative-recovery)\n    + [MGS role](#mgs-role)\n    + [Tuning Imperative Recovery](#tuning-imperative-recovery)\n      - [ir_factor](#ir_factor)\n      - [Disabling Imperative Recovery](#disabling-imperative-recovery)\n      - [Checking Imperative Recovery State - MGS](#checking-imperative-recovery-state---mgs)\n      - [Checking Imperative Recovery State - client](#checking-imperative-recovery-state---client)\n      - [Target Instance Number](#target-instance-number)\n    + [Configuration Suggestions for Imperative Recovery](#configuration-suggestions-for-imperative-recovery)\n  * [Suppressing Pings](#suppressing-pings)\n    + [\"suppress_pings\" Kernel Module Parameter](#suppress_pings-kernel-module-parameter)\n    + [Client Death Notification](#client-death-notification)\n\n\nThis chapter describes how recovery is implemented in a Lustre file system and includes the following sections:\n\n- [the section called \u201c Recovery Overview\u201d](#recovery-overview)\n- [the section called \u201cMetadata Replay\u201d](#recovery-overview)\n- [the section called \u201cReply Reconstruction\u201d](#recovery-overview)\n- [the section called \u201cVersion-based Recovery\u201d](#version-based-recovery)\n- [the section called \u201cCommit on Share\u201d](#commit-on-share)\n- [the section called \u201cImperative Recovery\u201d](#imperative-recovery)\n\n## Recovery Overview\n\nThe recovery feature provided in the Lustre software is responsible for dealing with node or network failure and returning the cluster to a consistent, performant state. Because the Lustre software allows servers to perform asynchronous update operations to the on-disk file system (i.e., the server can reply without waiting for the update to synchronously commit to disk), the clients may have state in memory that is newer than what the server can recover from disk after a crash.\n\nA handful of different types of failures can cause recovery to occur:\n\n- Client (compute node) failure\n- MDS failure (and failover)\n- OST failure (and failover)\n- Transient network partition\n\nFor Lustre, all Lustre file system failure and recovery operations are based on the concept of connection failure; all imports or exports associated with a given connection are considered to fail if any of them fail.  [*the section called \u201cImperative Recovery\u201d*](#imperative-recovery) feature allows the MGS to actively inform clients when a target restarts after a failure, failover, or other interruption to speed up recovery.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8575c6da-600e-4aee-9e5e-c3d974cc49b8": {"__data__": {"id_": "8575c6da-600e-4aee-9e5e-c3d974cc49b8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55347ab6-1108-482d-ab43-548ed534e865", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cef39b7df8b3a39255275364b8badd075f61e0df963ee5fa6ec00615faba4872", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c26a9ce-334c-4348-8f53-a4d55a0b7d12", "node_type": "1", "metadata": {}, "hash": "4572dc4474ddba66aac2cda287ea9acd4b9939f8352d3db709957b2d7393cd45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because the Lustre software allows servers to perform asynchronous update operations to the on-disk file system (i.e., the server can reply without waiting for the update to synchronously commit to disk), the clients may have state in memory that is newer than what the server can recover from disk after a crash.\n\nA handful of different types of failures can cause recovery to occur:\n\n- Client (compute node) failure\n- MDS failure (and failover)\n- OST failure (and failover)\n- Transient network partition\n\nFor Lustre, all Lustre file system failure and recovery operations are based on the concept of connection failure; all imports or exports associated with a given connection are considered to fail if any of them fail.  [*the section called \u201cImperative Recovery\u201d*](#imperative-recovery) feature allows the MGS to actively inform clients when a target restarts after a failure, failover, or other interruption to speed up recovery.\n\nFor information on Lustre file system recovery, see [*the section called \u201cMetadata Replay\u201d*](#metadata-replay). For information on recovering from a corrupt file system, see [*the section called \u201cCommit on Share\u201d*](#commit-on-share). For information on resolving orphaned objects, a common issue after recovery, see [*the section called \u201c Working with Orphaned Objects\u201d*](05.02-Troubleshooting%20Recovery.md#working-with-orphaned-objects). For information on imperative recovery see [*the section called \u201cImperative Recovery\u201d*](#imperative-recovery).\n\n### Client Failure\n\nRecovery from client failure in a Lustre file system is based on lock revocation and other resources, so surviving clients can continue their work uninterrupted. If a client fails to timely respond to a blocking lock callback from the Distributed Lock Manager (DLM) or fails to communicate with the server in a long period of time (i.e., no pings), the client is forcibly removed from the cluster (evicted). This enables other clients to acquire locks blocked by the dead client's locks, and also frees resources (file handles, export data) associated with that client. Note that this scenario can be caused by a network partition, as well as an actual client node system failure. [*the section called \u201cNetwork Partition\u201d*](#network-partition) describes this case in more detail.\n\n### Client Eviction\n\nIf a client is not behaving properly from the server's point of view, it will be evicted. This ensures that the whole file system can continue to function in the presence of failed or misbehaving clients. An evicted client must invalidate all locks, which in turn, results in all cached inodes becoming invalidated and all cached data being flushed.\n\nReasons why a client might be evicted:\n\n- Failure to respond to a server request in a timely manner\n  - Blocking lock callback (i.e., client holds lock that another client/server wants)\n  - Lock completion callback (i.e., client is granted lock previously held by another client)\n  - Lock glimpse callback (i.e., client is asked for size of object by another client)\n  - Server shutdown notification (with simplified interoperability)\n- Failure to ping the server in a timely manner, unless the server is receiving no RPC traffic at all (which may indicate a network partition).\n\n### MDS Failure (Failover)\n\nHighly-available (HA) Lustre file system operation requires that the metadata server have a peer configured for failover, including the use of a shared storage device for the MDT backing file system. The actual mechanism for detecting peer failure, power off (STONITH) of the failed peer (to prevent it from continuing to modify the shared disk), and takeover of the Lustre MDS service on the backup node depends on external HA software such as Heartbeat. It is also possible to have MDS recovery with a single MDS node. In this case, recovery will take as long as is needed for the single MDS to be restarted.\n\nWhen [*the section called \u201cImperative Recovery\u201d*](#imperative-recovery) is enabled, clients are notified of an MDS restart (either the backup or a restored primary). Clients always may detect an MDS failure either by timeouts of in-flight requests or idle-time ping messages. In either case the clients then connect to the new backup MDS and use the Metadata Replay protocol. Metadata Replay is responsible for ensuring that the backup MDS re-acquires state resulting from transactions whose effects were made visible to clients, but which were not committed to the disk.\n\nThe reconnection to a new (or restarted) MDS is managed by the file system configuration loaded by the client when the file system is first mounted.", "mimetype": "text/plain", "start_char_idx": 3081, "end_char_idx": 7684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c26a9ce-334c-4348-8f53-a4d55a0b7d12": {"__data__": {"id_": "8c26a9ce-334c-4348-8f53-a4d55a0b7d12", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8575c6da-600e-4aee-9e5e-c3d974cc49b8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fa77a5312271cf94123619eb4d74d03809a8b2de284ba9dfcb1e427a965d609b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8069e7d9-0b7d-418f-9b52-4da09682b7d2", "node_type": "1", "metadata": {}, "hash": "207ff792fb7327c17ef449421b6dbe819b7118479325b95aa5a860be5e532d29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is also possible to have MDS recovery with a single MDS node. In this case, recovery will take as long as is needed for the single MDS to be restarted.\n\nWhen [*the section called \u201cImperative Recovery\u201d*](#imperative-recovery) is enabled, clients are notified of an MDS restart (either the backup or a restored primary). Clients always may detect an MDS failure either by timeouts of in-flight requests or idle-time ping messages. In either case the clients then connect to the new backup MDS and use the Metadata Replay protocol. Metadata Replay is responsible for ensuring that the backup MDS re-acquires state resulting from transactions whose effects were made visible to clients, but which were not committed to the disk.\n\nThe reconnection to a new (or restarted) MDS is managed by the file system configuration loaded by the client when the file system is first mounted. If a failover MDS has been configured (using the `--failnode=` option to `mkfs.lustre` or `tunefs.lustre`), the client tries to reconnect to both the primary and backup MDS until one of them responds that the failed MDT is again available. At that point, the client begins recovery. For more information, see [*the section called \u201cMetadata Replay\u201d*](#metadata-replay).\n\nTransaction numbers are used to ensure that operations are replayed in the order they were originally performed, so that they are guaranteed to succeed and present the same file system state as before the failure. In addition, clients inform the new server of their existing lock state (including locks that have not yet been granted). All metadata and lock replay must complete before new, non-recovery operations are permitted. In addition, only clients that were connected at the time of MDS failure are permitted to reconnect during the recovery window, to avoid the introduction of state changes that might conflict with what is being replayed by previously-connected clients.\n\nIf multiple MDTs are in use, active-active failover is possible (e.g. two MDS nodes, each actively serving one or more different MDTs for the same filesystem). See [*the section called \u201c MDT Failover Configuration (Active/Active)\u201d*](02-Introducing%20the%20Lustre%20File%20System.md#mdt-failover-configuration-activepassive) for more information.\n\n### OST Failure (Failover)\n\nWhen an OST fails or has communication problems with the client, the default action is that the corresponding OSC enters recovery, and I/O requests going to that OST are blocked waiting for OST recovery or failover. It is possible to administratively mark the OSC as *inactive* on the client, in which case file operations that involve the failed OST will return an IO error (`-EIO`). Otherwise, the application waits until the OST has recovered or the client process is interrupted (e.g. ,with *CTRL-C*).\n\nThe MDS (via the LOV) detects that an OST is unavailable and skips it when assigning objects to new files. When the OST is restarted or re-establishes communication with the MDS, the MDS and OST automatically perform orphan recovery to destroy any objects that belong to files that were deleted while the OST was unavailable. For more information, see [*Troubleshooting Recovery*](05.02-Troubleshooting%20Recovery.md) (Working with Orphaned Objects).\n\nWhile the OSC to OST operation recovery protocol is the same as that between the MDC and MDT using the Metadata Replay protocol, typically the OST commits bulk write operations to disk synchronously and each reply indicates that the request is already committed and the data does not need to be saved for recovery. In some cases, the OST replies to the client before the operation is committed to disk (e.g. truncate, destroy, setattr, and I/O operations in newer releases of the Lustre software), and normal replay and resend handling is done, including resending of the bulk writes. In this case, the client keeps a copy of the data available in memory until the server indicates that the write has committed to disk.\n\nTo force an OST recovery, unmount the OST and then mount it again. If the OST was connected to clients before it failed, then a recovery process starts after the remount, enabling clients to reconnect to the OST and replay transactions in their queue. When the OST is in recovery mode, all new client connections are refused until the recovery finishes. The recovery is complete when either all previously-connected clients reconnect and their transactions are replayed or a client connection attempt times out. If a connection attempt times out, then all clients waiting to reconnect (and their transactions) are lost.", "mimetype": "text/plain", "start_char_idx": 6807, "end_char_idx": 11412, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8069e7d9-0b7d-418f-9b52-4da09682b7d2": {"__data__": {"id_": "8069e7d9-0b7d-418f-9b52-4da09682b7d2", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c26a9ce-334c-4348-8f53-a4d55a0b7d12", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d8986953d57e241c7bd77104f2109f8fce9fce0df35df0740ffa82ae8ab249e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "158dd72e-689f-4485-b4cd-6cc55f14dffa", "node_type": "1", "metadata": {}, "hash": "7c25c8919f1b6d6df424d8310bf3f30d5907fb3bbbbc1de2176bde19783693da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "truncate, destroy, setattr, and I/O operations in newer releases of the Lustre software), and normal replay and resend handling is done, including resending of the bulk writes. In this case, the client keeps a copy of the data available in memory until the server indicates that the write has committed to disk.\n\nTo force an OST recovery, unmount the OST and then mount it again. If the OST was connected to clients before it failed, then a recovery process starts after the remount, enabling clients to reconnect to the OST and replay transactions in their queue. When the OST is in recovery mode, all new client connections are refused until the recovery finishes. The recovery is complete when either all previously-connected clients reconnect and their transactions are replayed or a client connection attempt times out. If a connection attempt times out, then all clients waiting to reconnect (and their transactions) are lost.\n\n**Note**\n\nIf you know an OST will not recover a previously-connected client (if, for example, the client has crashed), you can manually abort the recovery using this command:\n\n```\noss# lctl --device lustre_device_number abort_recovery\n```\n\nTo determine an OST's device number and device name, run the `lctl dl` command. Sample `lctl dl`command output is shown below:\n\n```\n7 UP obdfilter ddn_data-OST0009 ddn_data-OST0009_UUID 1159 \n```\n\nIn this example, 7 is the OST device number. The device name is `ddn_data-OST0009`. In most instances, the device name can be used in place of the device number.\n\n### Network Partition\n\nNetwork failures may be transient. To avoid invoking recovery, the client tries, initially, to re-send any timed out request to the server. If the resend also fails, the client tries to re-establish a connection to the server. Clients can detect harmless partition upon reconnect if the server has not had any reason to evict the client.\n\nIf a request was processed by the server, but the reply was dropped (i.e., did not arrive back at the client), the server must reconstruct the reply when the client resends the request, rather than performing the same request twice.\n\n### Failed Recovery\n\nIn the case of failed recovery, a client is evicted by the server and must reconnect after having flushed its saved state related to that server, as described in [*the section called \u201cClient Eviction\u201d*](#client-eviction), above. Failed recovery might occur for a number of reasons, including:\n\n- Failure of recovery\n  - Recovery fails if the operations of one client directly depend on the operations of another client that failed to participate in recovery. Otherwise, Version Based Recovery (VBR) allows recovery to proceed for all of the connected clients, and only missing clients are evicted.\n  - Manual abort of recovery\n- Manual eviction by the administrator\n\n## Metadata Replay\n\nHighly available Lustre file system operation requires that the MDS have a peer configured for failover, including the use of a shared storage device for the MDS backing file system. When a client detects an MDS failure, it connects to the new MDS and uses the metadata replay protocol to replay its requests.\n\nMetadata replay ensures that the failover MDS re-accumulates state resulting from transactions whose effects were made visible to clients, but which were not committed to the disk.\n\n### XID Numbers\n\nEach request sent by the client contains an XID number, which is a client-unique, monotonically increasing 64-bit integer. The initial value of the XID is chosen so that it is highly unlikely that the same client node reconnecting to the same server after a reboot would have the same XID sequence. The XID is used by the client to order all of the requests that it sends, until such a time that the request is assigned a transaction number. The XID is also used in Reply Reconstruction to uniquely identify per-client requests at the server.\n\n### Transaction Numbers\n\nEach client request processed by the server that involves any state change (metadata update, file open, write, etc., depending on server type) is assigned a transaction number by the server that is a target-unique, monotonically increasing, server-wide 64-bit integer. The transaction number for each file system-modifying request is sent back to the client along with the reply to that client request. The transaction numbers allow the client and server to unambiguously order every modification to the file system in case recovery is needed.\n\nEach reply sent to a client (regardless of request type) also contains the last committed transaction number that indicates the highest transaction number committed to the file system.", "mimetype": "text/plain", "start_char_idx": 10480, "end_char_idx": 15126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "158dd72e-689f-4485-b4cd-6cc55f14dffa": {"__data__": {"id_": "158dd72e-689f-4485-b4cd-6cc55f14dffa", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8069e7d9-0b7d-418f-9b52-4da09682b7d2", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "af317dc43dd2f8d69c1fc5cedb0e2aba22455db6ad68ba09e0d5834043bde5de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cddf58d-b096-4684-8060-9c47d1338342", "node_type": "1", "metadata": {}, "hash": "7bf92a402435f9acf4509ff6df4b5bb565925e3ceed5d7a5f260c0af675bc191", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The XID is used by the client to order all of the requests that it sends, until such a time that the request is assigned a transaction number. The XID is also used in Reply Reconstruction to uniquely identify per-client requests at the server.\n\n### Transaction Numbers\n\nEach client request processed by the server that involves any state change (metadata update, file open, write, etc., depending on server type) is assigned a transaction number by the server that is a target-unique, monotonically increasing, server-wide 64-bit integer. The transaction number for each file system-modifying request is sent back to the client along with the reply to that client request. The transaction numbers allow the client and server to unambiguously order every modification to the file system in case recovery is needed.\n\nEach reply sent to a client (regardless of request type) also contains the last committed transaction number that indicates the highest transaction number committed to the file system. The `ldiskfs` and `ZFS` backing file systems that the Lustre software uses enforces the requirement that any earlier disk operation will always be committed to disk before a later disk operation, so the last committed transaction number also reports that any requests with a lower transaction number have been committed to disk.\n\n### Replay and Resend\n\nLustre file system recovery can be separated into two distinct types of operations: *replay* and *resend*.\n\n*Replay* operations are those for which the client received a reply from the server that the operation had been successfully completed. These operations need to be redone in exactly the same manner after a server restart as had been reported before the server failed. Replay can only happen if the server failed; otherwise it will not have lost any state in memory.\n\n*Resend* operations are those for which the client never received a reply, so their final state is unknown to the client. The client sends unanswered requests to the server again in XID order, and again awaits a reply for each one. In some cases, resent requests have been handled and committed to disk by the server (possibly also having dependent operations committed), in which case, the server performs reply reconstruction for the lost reply. In other cases, the server did not receive the lost request at all and processing proceeds as with any normal request. These are what happen in the case of a network interruption. It is also possible that the server received the request, but was unable to reply or commit it to disk before failure.\n\n### Client Replay List\n\nAll file system-modifying requests have the potential to be required for server state recovery (replay) in case of a server failure. Replies that have an assigned transaction number that is higher than the last committed transaction number received in any reply from each server are preserved for later replay in a per-server replay list. As each reply is received from the server, it is checked to see if it has a higher last committed transaction number than the previous highest last committed number. Most requests that now have a lower transaction number can safely be removed from the replay list. One exception to this rule is for open requests, which need to be saved for replay until the file is closed so that the MDS can properly reference count open-unlinked files.\n\n### Server Recovery\n\nA server enters recovery if it was not shut down cleanly. If, upon startup, if any client entries are in the `last_rcvd`file for any previously connected clients, the server enters recovery mode and waits for these previously-connected clients to reconnect and begin replaying or resending their requests. This allows the server to recreate state that was exposed to clients (a request that completed successfully) but was not committed to disk before failure.\n\nIn the absence of any client connection attempts, the server waits indefinitely for the clients to reconnect. This is intended to handle the case where the server has a network problem and clients are unable to reconnect and/or if the server needs to be restarted repeatedly to resolve some problem with hardware or software. Once the server detects client connection attempts - either new clients or previously-connected clients - a recovery timer starts and forces recovery to finish in a finite time regardless of whether the previously-connected clients are available or not.\n\nIf no client entries are present in the `last_rcvd` file, or if the administrator manually aborts recovery, the server does not wait for client reconnection and proceeds to allow all clients to connect.\n\nAs clients connect, the server gathers information from each one to determine how long the recovery needs to take. Each client reports its connection UUID, and the server does a lookup for this UUID in the `last_rcvd` file to determine if this client was previously connected. If not, the client is refused connection and it will retry until recovery is completed.", "mimetype": "text/plain", "start_char_idx": 14127, "end_char_idx": 19138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0cddf58d-b096-4684-8060-9c47d1338342": {"__data__": {"id_": "0cddf58d-b096-4684-8060-9c47d1338342", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "158dd72e-689f-4485-b4cd-6cc55f14dffa", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e8a0729e754804e17a73e4e8a33d6b00be15341021b14510566cd94f60ca300f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e21dba46-ecd5-4089-8416-247339c9b6b2", "node_type": "1", "metadata": {}, "hash": "98e0f7bde06a6945bf0fa3b303f8392379236f1f032e754587a2058c0a8fabef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is intended to handle the case where the server has a network problem and clients are unable to reconnect and/or if the server needs to be restarted repeatedly to resolve some problem with hardware or software. Once the server detects client connection attempts - either new clients or previously-connected clients - a recovery timer starts and forces recovery to finish in a finite time regardless of whether the previously-connected clients are available or not.\n\nIf no client entries are present in the `last_rcvd` file, or if the administrator manually aborts recovery, the server does not wait for client reconnection and proceeds to allow all clients to connect.\n\nAs clients connect, the server gathers information from each one to determine how long the recovery needs to take. Each client reports its connection UUID, and the server does a lookup for this UUID in the `last_rcvd` file to determine if this client was previously connected. If not, the client is refused connection and it will retry until recovery is completed. Each client reports its last seen transaction, so the server knows when all transactions have been replayed. The client also reports the amount of time that it was previously waiting for request completion so that the server can estimate how long some clients might need to detect the server failure and reconnect.\n\nIf the client times out during replay, it attempts to reconnect. If the client is unable to reconnect, `REPLAY` fails and it returns to `DISCON` state. It is possible that clients will timeout frequently during `REPLAY`, so reconnection should not delay an already slow process more than necessary. We can mitigate this by increasing the timeout during replay.\n\n### Request Replay\n\nIf a client was previously connected, it gets a response from the server telling it that the server is in recovery and what the last committed transaction number on disk is. The client can then iterate through its replay list and use this last committed transaction number to prune any previously-committed requests. It replays any newer requests to the server in transaction number order, one at a time, waiting for a reply from the server before replaying the next request.\n\nOpen requests that are on the replay list may have a transaction number lower than the server's last committed transaction number. The server processes those open requests immediately. The server then processes replayed requests from all of the clients in transaction number order, starting at the last committed transaction number to ensure that the state is updated on disk in exactly the same manner as it was before the crash. As each replayed request is processed, the last committed transaction is incremented. If the server receives a replay request from a client that is higher than the current last committed transaction, that request is put aside until other clients provide the intervening transactions. In this manner, the server replays requests in the same sequence as they were previously executed on the server until either all clients are out of requests to replay or there is a gap in a sequence.\n\n### Gaps in the Replay Sequence\n\nIn some cases, a gap may occur in the reply sequence. This might be caused by lost replies, where the request was processed and committed to disk but the reply was not received by the client. It can also be caused by clients missing from recovery due to partial network failure or client death.\n\nIn the case where all clients have reconnected, but there is a gap in the replay sequence the only possibility is that some requests were processed by the server but the reply was lost. Since the client must still have these requests in its resend list, they are processed after recovery is finished.\n\nIn the case where all clients have not reconnected, it is likely that the failed clients had requests that will no longer be replayed. The VBR feature is used to determine if a request following a transaction gap is safe to be replayed. Each item in the file system (MDS inode or OST object) stores on disk the number of the last transaction in which it was modified. Each reply from the server contains the previous version number of the objects that it affects. During VBR replay, the server matches the previous version numbers in the resend request against the current version number. If the versions match, the request is the next one that affects the object and can be safely replayed. For more information, see [*the section called \u201cVersion-based Recovery\u201d*](#version-based-recovery).\n\n### Lock Recovery\n\nIf all requests were replayed successfully and all clients reconnected, clients then do lock replay locks -- that is, every client sends information about every lock it holds from this server and its state (whenever it was granted or not, what mode, what properties and so on), and then recovery completes successfully. Currently, the Lustre software does not do lock verification and just trusts clients to present an accurate lock state.", "mimetype": "text/plain", "start_char_idx": 18099, "end_char_idx": 23112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e21dba46-ecd5-4089-8416-247339c9b6b2": {"__data__": {"id_": "e21dba46-ecd5-4089-8416-247339c9b6b2", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cddf58d-b096-4684-8060-9c47d1338342", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ea9e3a85bc86ae5b21c632c49291f95ad465848bdeadf86c0e5a0e6ca5c5a449", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acdcc031-14ca-4291-b81d-2312b00d3d13", "node_type": "1", "metadata": {}, "hash": "6d77dc24e8495a5e626b036caf036f345004421901f3cddd7b7196e0c3f89167", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each reply from the server contains the previous version number of the objects that it affects. During VBR replay, the server matches the previous version numbers in the resend request against the current version number. If the versions match, the request is the next one that affects the object and can be safely replayed. For more information, see [*the section called \u201cVersion-based Recovery\u201d*](#version-based-recovery).\n\n### Lock Recovery\n\nIf all requests were replayed successfully and all clients reconnected, clients then do lock replay locks -- that is, every client sends information about every lock it holds from this server and its state (whenever it was granted or not, what mode, what properties and so on), and then recovery completes successfully. Currently, the Lustre software does not do lock verification and just trusts clients to present an accurate lock state. This does not impart any security concerns since Lustre software release 1.x clients are trusted for other information (e.g. user ID) during normal operation also.\n\nAfter all of the saved requests and locks have been replayed, the client sends an `MDS_GETSTATUS` request with last-replay flag set. The reply to that request is held back until all clients have completed replay (sent the same flagged getstatus request), so that clients don't send non-recovery requests before recovery is complete.\n\n### Request Resend\n\nOnce all of the previously-shared state has been recovered on the server (the target file system is up-to-date with client cache and the server has recreated locks representing the locks held by the client), the client can resend any requests that did not receive an earlier reply. This processing is done like normal request processing, and, in some cases, the server may do reply reconstruction.\n\n## Reply Reconstruction\n\nWhen a reply is dropped, the MDS needs to be able to reconstruct the reply when the original request is re-sent. This must be done without repeating any non-idempotent operations, while preserving the integrity of the locking system. In the event of MDS failover, the information used to reconstruct the reply must be serialized on the disk in transactions that are joined or nested with those operating on the disk.\n\n### Required State\n\nFor the majority of requests, it is sufficient for the server to store three pieces of data in the `last_rcvd` file:\n\n- XID of the request\n- Resulting transno (if any)\n- Result code (`req->rq_status`)\n\nFor open requests, the \"disposition\" of the open must also be stored.\n\n### Reconstruction of Open Replies\n\nAn open reply consists of up to three pieces of information (in addition to the contents of the \"request log\"):\n\n- File handle\n- Lock handle\n- `mds_body` with information about the file created (for `O_CREAT`)\n\nThe disposition, status and request data (re-sent intact by the client) are sufficient to determine which type of lock handle was granted, whether an open file handle was created, and which resource should be described in the `mds_body`.\n\n#### Finding the File Handle\n\nThe file handle can be found in the XID of the request and the list of per-export open file handles. The file handle contains the resource/FID.\n\n#### Finding the Resource/fid\n\nThe file handle contains the resource/fid.\n\n#### Finding the Lock Handle\n\nThe lock handle can be found by walking the list of granted locks for the resource looking for one with the appropriate remote file handle (present in the re-sent request). Verify that the lock has the right mode (determined by performing the disposition/request/status analysis above) and is granted to the proper client.\n\nIntroduced in Lustre 2.8\n\n### Multiple Reply Data per Client\n\nSince Lustre 2.8, the MDS is able to save several reply data per client. The reply data are stored in the\u00a0`reply_data`internal file of the MDT. Additionally to the XID of the request, the transaction number, the result code and the open \"disposition\", the reply data contains a generation number that identifies the client thanks to the content of the\u00a0`last_rcvd`\u00a0file.\n\n## Version-based Recovery\n\nThe Version-based Recovery (VBR) feature improves Lustre file system reliability in cases where client requests (RPCs) fail to replay during recover[1].\n\nIn pre-VBR releases of the Lustre software, if the MGS or an OST went down and then recovered, a recovery process was triggered in which clients attempted to replay their requests. Clients were only allowed to replay RPCs in serial order. If a particular client could not replay its requests, then those requests were lost as well as the requests of clients later in the sequence.", "mimetype": "text/plain", "start_char_idx": 22229, "end_char_idx": 26851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "acdcc031-14ca-4291-b81d-2312b00d3d13": {"__data__": {"id_": "acdcc031-14ca-4291-b81d-2312b00d3d13", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e21dba46-ecd5-4089-8416-247339c9b6b2", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "17c8b1dfd7e75aff431d8aa6e3e91cd01bc70c2400e1408027a5c5f813e04fd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd4654a1-8ea5-4074-8cac-3d77074b1767", "node_type": "1", "metadata": {}, "hash": "7ac2acdd8d8af5add79d31d1fb1d48e7b277e3e3d287e0ec79cabef8a9353cc0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The reply data are stored in the\u00a0`reply_data`internal file of the MDT. Additionally to the XID of the request, the transaction number, the result code and the open \"disposition\", the reply data contains a generation number that identifies the client thanks to the content of the\u00a0`last_rcvd`\u00a0file.\n\n## Version-based Recovery\n\nThe Version-based Recovery (VBR) feature improves Lustre file system reliability in cases where client requests (RPCs) fail to replay during recover[1].\n\nIn pre-VBR releases of the Lustre software, if the MGS or an OST went down and then recovered, a recovery process was triggered in which clients attempted to replay their requests. Clients were only allowed to replay RPCs in serial order. If a particular client could not replay its requests, then those requests were lost as well as the requests of clients later in the sequence. The ''downstream'' clients never got to replay their requests because of the wait on the earlier client's RPCs. Eventually, the recovery period would time out (so the component could accept new requests), leaving some number of clients evicted and their requests and data lost.\n\nWith VBR, the recovery mechanism does not result in the loss of clients or their data, because changes in inode versions are tracked, and more clients are able to reintegrate into the cluster. With VBR, inode tracking looks like this:\n\n- Each inod[2]e stores a version, that is, the number of the last transaction (transno) in which the inode was changed.\n- When an inode is about to be changed, a pre-operation version of the inode is saved in the client's data.\n- The client keeps the pre-operation inode version and the post-operation version (transaction number) for replay, and sends them in the event of a server failure.\n- If the pre-operation version matches, then the request is replayed. The post-operation version is assigned on all inodes modified in the request.\n\n**Note**\n\nAn RPC can contain up to four pre-operation versions, because several inodes can be involved in an operation. In the case of a ''rename'' operation, four different inodes can be modified.\n\nDuring normal operation, the server:\n\n- Updates the versions of all inodes involved in a given operation\n- Returns the old and new inode versions to the client with the reply\n\nWhen the recovery mechanism is underway, VBR follows these steps:\n\n1. VBR only allows clients to replay transactions if the affected inodes have the same version as during the original execution of the transactions, even if there is gap in transactions due to a missed client.\n2. The server attempts to execute every transaction that the client offers, even if it encounters a re-integration failure.\n3. When the replay is complete, the client and server check if a replay failed on any transaction because of inode version mismatch. If the versions match, the client gets a successful re-integration message. If the versions do not match, then the client is evicted.\n\nVBR recovery is fully transparent to users. It may lead to slightly longer recovery times if the cluster loses several clients during server recovery.\n\n----------------------------------------------------------------------------------------------------------------------------------------\n\n[1] There are two scenarios under which client RPCs are not replayed: (1) Non-functioning or isolated clients do not reconnect, and they cannot replay their RPCs, causing a gap in the replay sequence. These clients get errors and are evicted. (2) Functioning clients connect, but they cannot replay some or all of their RPCs that occurred after the gap caused by the non-functioning/isolated clients. These clients get errors (caused by the failed clients). With VBR, these requests have a better chance to replay because the \"gaps\" are only related to specific files that the missing client(s) changed.\n\n[2] Usually, there are two inodes, a parent and a child.\n\n----------------------------------------------------------------------------------------------------------------------------------------\n\n### VBR Messages\n\nThe VBR feature is built into the Lustre file system recovery functionality. It cannot be disabled. These are some VBR messages that may be displayed:\n\n```\nDEBUG_REQ(D_WARNING, req, \"Version mismatch during replay\\n\");\n```\n\nThis message indicates why the client was evicted. No action is needed.\n\n```\nCWARN(\"%s: version recovery fails, reconnecting\\n\");\n```\n\nThis message indicates why the recovery failed. No action is needed.\n\n### Tips for Using VBR\n\nVBR will be successful for clients which do not share data with other client. Therefore, the strategy for reliable use of VBR is to store a client's data in its own directory, where possible. VBR can recover these clients, even if other clients are lost.", "mimetype": "text/plain", "start_char_idx": 25992, "end_char_idx": 30768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd4654a1-8ea5-4074-8cac-3d77074b1767": {"__data__": {"id_": "dd4654a1-8ea5-4074-8cac-3d77074b1767", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acdcc031-14ca-4291-b81d-2312b00d3d13", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "edd99d4a2ada700246ce6df6a9b9a27ac82bdc18c0d28cd17b858b048cd71633", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f043b67b-01a5-4b06-b43c-d1ee02c25170", "node_type": "1", "metadata": {}, "hash": "364d0859305985ac1f660c77c3930153f2eaa6599fbd2f4b4ca22d337226bb77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2] Usually, there are two inodes, a parent and a child.\n\n----------------------------------------------------------------------------------------------------------------------------------------\n\n### VBR Messages\n\nThe VBR feature is built into the Lustre file system recovery functionality. It cannot be disabled. These are some VBR messages that may be displayed:\n\n```\nDEBUG_REQ(D_WARNING, req, \"Version mismatch during replay\\n\");\n```\n\nThis message indicates why the client was evicted. No action is needed.\n\n```\nCWARN(\"%s: version recovery fails, reconnecting\\n\");\n```\n\nThis message indicates why the recovery failed. No action is needed.\n\n### Tips for Using VBR\n\nVBR will be successful for clients which do not share data with other client. Therefore, the strategy for reliable use of VBR is to store a client's data in its own directory, where possible. VBR can recover these clients, even if other clients are lost.\n\n## Commit on Share\n\nThe commit-on-share (COS) feature makes Lustre file system recovery more reliable by preventing missing clients from causing cascading evictions of other clients. With COS enabled, if some Lustre clients miss the recovery window after a reboot or a server failure, the remaining clients are not evicted.\n\n**Note**\n\nThe commit-on-share feature is enabled, by default.\n\n### Working with Commit on Share\n\nTo illustrate how COS works, let's first look at the old recovery scenario. After a service restart, the MDS would boot and enter recovery mode. Clients began reconnecting and replaying their uncommitted transactions. Clients could replay transactions independently as long as their transactions did not depend on each other (one client's transactions did not depend on a different client's transactions). The MDS is able to determine whether one transaction is dependent on another transaction via the [*the section called \u201cVersion-based Recovery\u201d*](#version-based-recovery) feature.\n\nIf there was a dependency between client transactions (for example, creating and deleting the same file), and one or more clients did not reconnect in time, then some clients may have been evicted because their transactions depended on transactions from the missing clients. Evictions of those clients caused more clients to be evicted and so on, resulting in \"cascading\" client evictions.\n\nCOS addresses the problem of cascading evictions by eliminating dependent transactions between clients. It ensures that one transaction is committed to disk if another client performs a transaction dependent on the first one. With no dependent, uncommitted transactions to apply, the clients replay their requests independently without the risk of being evicted.\n\n### Tuning Commit On Share\n\nCommit on Share can be enabled or disabled using the `mdt.commit_on_sharing` tunable (0/1). This tunable can be set when the MDS is created (`mkfs.lustre`) or when the Lustre file system is active, using the `lctl set/get_param` or `lctl conf_param` commands.\n\nTo set a default value for COS (disable/enable) when the file system is created, use:\n\n```\n--param mdt.commit_on_sharing=0/1\n```\n\nTo disable or enable COS when the file system is running, use:\n\n```\nlctl set_param mdt.*.commit_on_sharing=0/1\n```\n\n**Note**\n\nEnabling COS may cause the MDS to do a large number of synchronous disk operations, hurting performance. Placing the `ldiskfs` journal on a low-latency external device may improve file system performance.\n\n## Imperative Recovery\n\nLarge-scale Lustre filesystems will experience server hardware failures over their lifetime, and it is important that servers can recover in a timely manner after such failures. High Availability software can move storage targets over to a backup server automatically. Clients can detect the server failure by RPC timeouts, which must be scaled with system size to prevent false diagnosis of server death in cases of heavy load. The purpose of imperative recovery is to reduce the recovery window by actively informing clients of server failure. The resulting reduction in the recovery window will minimize target downtime and therefore increase overall system availability.\n\nImperative Recovery does not remove previous recovery mechanisms, and client timeout-based recovery actions can occur in a cluster when IR is enabled as each client can still independently disconnect and reconnect from a target. In case of a mix of IR and non-IR clients connecting to an OST or MDT, the server cannot reduce its recovery timeout window, because it cannot be sure that all clients have been notified of the server restart in a timely manner. Even in such mixed environments the time to complete recovery may be reduced, since IR-enabled clients will still be notified to reconnect to the server promptly and allow recovery to complete as soon as the last non-IR client detects the server failure.", "mimetype": "text/plain", "start_char_idx": 29847, "end_char_idx": 34696, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f043b67b-01a5-4b06-b43c-d1ee02c25170": {"__data__": {"id_": "f043b67b-01a5-4b06-b43c-d1ee02c25170", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd4654a1-8ea5-4074-8cac-3d77074b1767", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "075b8d13fe59c0abee5b2c141e095bebef1cc07ae2f378ae8e203ffa2734b45a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62d79756-d8c9-422b-acd9-0ba998c6d162", "node_type": "1", "metadata": {}, "hash": "63abfa989645a52ebe8e48231a3834348b24fbaa0d01205dd576bd642c7e25d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The purpose of imperative recovery is to reduce the recovery window by actively informing clients of server failure. The resulting reduction in the recovery window will minimize target downtime and therefore increase overall system availability.\n\nImperative Recovery does not remove previous recovery mechanisms, and client timeout-based recovery actions can occur in a cluster when IR is enabled as each client can still independently disconnect and reconnect from a target. In case of a mix of IR and non-IR clients connecting to an OST or MDT, the server cannot reduce its recovery timeout window, because it cannot be sure that all clients have been notified of the server restart in a timely manner. Even in such mixed environments the time to complete recovery may be reduced, since IR-enabled clients will still be notified to reconnect to the server promptly and allow recovery to complete as soon as the last non-IR client detects the server failure.\n\n### MGS role\n\nThe MGS now holds additional information about Lustre targets, in the form of a Target Status Table. Whenever a target registers with the MGS, there is a corresponding entry in this table identifying the target. This entry includes NID information, and state/version information for the target. When a client mounts the file system, it caches a locked copy of this table, in the form of a Lustre configuration log. When a target restart occurs, the MGS revokes the client lock, forcing all clients to reload the table. Any new targets will have an updated version number, the client detects this and reconnects to the restarted target. Since successful IR notification of server restart depends on all clients being registered with the MGS, and there is no other node to notify clients in case of MGS restart, the MGS will disable IR for a period when it first starts. This interval is configurable, as shown in [*the section called \u201cTuning Imperative Recovery\u201d*](#tuning-imperative-recovery).\n\nBecause of the increased importance of the MGS in recovery, it is strongly recommended that the MGS node be separate from the MDS. If the MGS is co-located on the MDS node, then in case of MDS/MGS failure there will be no IR notification for the MDS restart, and clients will always use timeout-based recovery for the MDS. IR notification would still be used in the case of OSS failure and recovery.\n\nUnfortunately, it\u2019s impossible for the MGS to know how many clients have been successfully notified or whether a specific client has received the restarting target information. The only thing the MGS can do is tell the target that, for example, all clients are imperative recovery-capable, so it is not necessary to wait as long for all clients to reconnect. For this reason, we still require a timeout policy on the target side, but this timeout value can be much shorter than normal recovery.\n\n### Tuning Imperative Recovery\n\nImperative recovery has a default parameter set which means it can work without any extra configuration. However, the default parameter set only fits a generic configuration. The following sections discuss the configuration items for imperative recovery.\n\n#### ir_factor\n\nIr_factor is used to control targets\u2019 recovery window. If imperative recovery is enabled, the recovery timeout window on the restarting target is calculated by: *new timeout = recovery_time \\* ir_factor / 10* Ir_factor must be a value in range of [1, 10]. The default value of ir_factor is 5. The following example will set imperative recovery timeout to 80% of normal recovery timeout on the target testfs-OST0000:\n\n```\nlctl conf_param obdfilter.testfs-OST0000.ir_factor=8\n```\n\n**Note**\n\nIf this value is too small for the system, clients may be unnecessarily evicted\n\nYou can read the current value of the parameter in the standard manner with *lctl get_param*:\n\n```\n# lctl get_param obdfilter.testfs-OST0000.ir_factor\n# obdfilter.testfs-OST0000.ir_factor=8 \n```\n\n#### Disabling Imperative Recovery\n\nImperative recovery can be disabled manually by a mount option. For example, imperative recovery can be disabled on an OST by:\n\n```\n# mount -t lustre -onoir /dev/sda /mnt/ost1\n```\n\nImperative recovery can also be disabled on the client side with the same mount option:\n\n```\n# mount -t lustre -onoir mymgsnid@tcp:/testfs /mnt/testfs\n```\n\n**Note**\n\nWhen a single client is deactivated in this manner, the MGS will deactivate imperative recovery for the whole cluster. IR-enabled clients will still get notification of target restart, but targets will not be allowed to shorten the recovery window.", "mimetype": "text/plain", "start_char_idx": 33737, "end_char_idx": 38305, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62d79756-d8c9-422b-acd9-0ba998c6d162": {"__data__": {"id_": "62d79756-d8c9-422b-acd9-0ba998c6d162", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f043b67b-01a5-4b06-b43c-d1ee02c25170", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b41441627b4dfedf15ef7a337ae81c190a9c42cffa1cc0d452116d674449221e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb72eaaf-ade5-4c07-8b12-df384333a32b", "node_type": "1", "metadata": {}, "hash": "1388394eb6a4dd0a6f824b69d11104117aa3abe911581c4b23f4992cffbc7035", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, imperative recovery can be disabled on an OST by:\n\n```\n# mount -t lustre -onoir /dev/sda /mnt/ost1\n```\n\nImperative recovery can also be disabled on the client side with the same mount option:\n\n```\n# mount -t lustre -onoir mymgsnid@tcp:/testfs /mnt/testfs\n```\n\n**Note**\n\nWhen a single client is deactivated in this manner, the MGS will deactivate imperative recovery for the whole cluster. IR-enabled clients will still get notification of target restart, but targets will not be allowed to shorten the recovery window.\n\nYou can also disable imperative recovery globally on the MGS by writing `state=disabled\u2019 to the controlling procfs entry\n\n```\n# lctl set_param mgs.MGS.live.testfs=\"state=disabled\"\n```\n\nThe above command will disable imperative recovery for file system named *testfs*.\n\n#### Checking Imperative Recovery State - MGS\n\nYou can get the imperative recovery state from the MGS. Let\u2019s take an example and explain states of imperative recovery:\n\n```\n[mgs]$ lctl get_param mgs.MGS.live.testfs\n...\nimperative_recovery_state:\n    state: full\n    nonir_clients: 0\n    nidtbl_version: 242\n    notify_duration_total: 0.470000\n    notify_duation_max: 0.041000\n    notify_count: 38\n```\n\n| **Item**                  | **Meaning**                                                  |\n| ------------------------- | ------------------------------------------------------------ |\n| **state**                 | **full:** IR is working, all clients are connected and can be notified.**partial:** some clients are not IR capable.**disabled:** IR is disabled, no client notification.**startup:** the MGS was just restarted, so not all clients may reconnect to the MGS. |\n| **nonir_clients**         | Number of non-IR capable clients in the system.              |\n| **nidtbl_version**        | Version number of the target status table. Client version must match MGS. |\n| **notify_duration_total** | [Seconds.microseconds] Total time spent by MGS notifying clients |\n| **notify_duration_max**   | [Seconds.microseconds] Maximum notification time for the MGS to notify a single IR client. |\n| **notify_count**          | Number of MGS restarts - to obtain average notification time, divide `notify_duration_total` by `notify_count` |\n\n#### Checking Imperative Recovery State - client\n\nA `client\u2019 in IR means a Lustre client or a MDT. You can get the IR state on any node which running client or MDT, those nodes will always have an MGC running. An example from a client:\n\n```\n[client]$ lctl get_param mgc.*.ir_state\nmgc.MGC192.168.127.6@tcp.ir_state=\nimperative_recovery: ON\nclient_state:\n    - { client: testfs-client, nidtbl_version: 242 }\n\t\n```\n\nAn example from a MDT:\n\n```\nmgc.MGC192.168.127.6@tcp.ir_state=\nimperative_recovery: ON\nclient_state:\n    - { client: testfs-MDT0000, nidtbl_version: 242 }\n\t\n```\n\n| **Item**                         | **Meaning**                                                  |\n| -------------------------------- | ------------------------------------------------------------ |\n| **imperative_recovery**          | `imperative_recovery`can be ON or OFF. If it\u2019s OFF state, then IR is disabled by administrator at mount time. Normally this should be ON state. |\n| **client_state: client:**        | The name of the client                                       |\n| **client_state: nidtbl_version** | Version number of the target status table. Client version must match MGS. |\n\n#### Target Instance Number\n\nThe Target Instance number is used to determine if a client is connecting to the latest instance of a target. We use the lowest 32 bit of mount count as target instance number. For an OST you can get the target instance number of testfs-OST0001 in this way (the command is run from an OSS login prompt):\n\n```\n$ lctl get_param obdfilter.testfs-OST0001*.instance\nobdfilter.testfs-OST0001.instance=5\n```\n\nFrom a client, query the relevant OSC:\n\n```\n$ lctl get_param osc.testfs-OST0001-osc-*.import |grep instance\n    instance: 5\n```\n\n### Configuration Suggestions for Imperative Recovery\n\nWe used to build the MGS and MDT0000 on the same target to save a server node.", "mimetype": "text/plain", "start_char_idx": 37774, "end_char_idx": 41880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb72eaaf-ade5-4c07-8b12-df384333a32b": {"__data__": {"id_": "fb72eaaf-ade5-4c07-8b12-df384333a32b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7b48e7a8aeb36e121c5d8a675b92626afdf10e58799b77b4ec47dea63a66a13b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62d79756-d8c9-422b-acd9-0ba998c6d162", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3abd383b9361bf550b9ce7fca7995a1f8f0da92074c555067196fbf4e0c4cbe7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Client version must match MGS. |\n\n#### Target Instance Number\n\nThe Target Instance number is used to determine if a client is connecting to the latest instance of a target. We use the lowest 32 bit of mount count as target instance number. For an OST you can get the target instance number of testfs-OST0001 in this way (the command is run from an OSS login prompt):\n\n```\n$ lctl get_param obdfilter.testfs-OST0001*.instance\nobdfilter.testfs-OST0001.instance=5\n```\n\nFrom a client, query the relevant OSC:\n\n```\n$ lctl get_param osc.testfs-OST0001-osc-*.import |grep instance\n    instance: 5\n```\n\n### Configuration Suggestions for Imperative Recovery\n\nWe used to build the MGS and MDT0000 on the same target to save a server node. However, to make IR work efficiently, we strongly recommend running the MGS node on a separate node for any significant Lustre file system installation. There are three main advantages of doing this:\n\n1. Be able to notify clients when MDT0000 recovered.\n2. Improved load balance. The load on the MDS may be very high which may make the MGS unable to notify the clients in time.\n3. Robustness. The MGS code is simpler and much smaller compared to the MDS code. This means the chance of an MGS downtime due to a software bug is very low.\n\n## Suppressing Pings\n\nOn clusters with large numbers of clients and OSTs, OBD_PING messages may impose significant performance overheads. There is an option to suppress pings, allowing ping overheads to be considerably reduced. Before turning on this option, administrators should consider the following requirements and understand the trade-offs involved:\n\n- When suppressing pings, a server cannot detect client deaths, since clients do not send pings that are only to keep their connections alive. Therefore, a mechanism external to the Lustre file system shall be set up to notify Lustre targets of client deaths in a timely manner, so that stale connections do not exist for too long and lock callbacks to dead clients do not always have to wait for timeouts.\n- Without pings, a client has to rely on Imperative Recovery to notify it of target failures, in order to join recoveries in time. This dictates that the client shall eargerly keep its MGS connection alive. Thus, a highly available standalone MGS is recommended and, on the other hand, MGS pings are always sent regardless of how the option is set.\n- If a client has uncommitted requests to a target and it is not sending any new requests on the connection, it will still ping that target even when pings should be suppressed. This is because the client needs to query the target's last committed transaction numbers in order to free up local uncommitted requests (and possibly other resources associated). However, these pings shall stop as soon as all the uncommitted requests have been freed or new requests need to be sent, rendering the pings unnecessary.\n\n### \"suppress_pings\" Kernel Module Parameter\n\nThe new option that controls whether pings are suppressed is implemented as the ptlrpc kernel module parameter \"suppress_pings\". Setting it to \"1\" on a server turns on ping suppressing for all targets on that server, while leaving it with the default value \"0\" gives previous pinging behavior. The parameter is ignored on clients and the MGS. While the parameter is recommended to be set persistently via the modprobe.conf(5) mechanism, it also accept online changes through sysfs. Note that an online change only affects connections established later; existing connections' pinging behaviors stay the same.\n\n### Client Death Notification\n\nThe required external client death notification shall write UUIDs of dead clients into targets' \"evict_client\" procfs entries like\n\n```\n/proc/fs/lustre/obdfilter/testfs-OST0000/evict_client\n/proc/fs/lustre/obdfilter/testfs-OST0001/evict_client\n/proc/fs/lustre/mdt/testfs-MDT0000/evict_client\n      \n```\n\nClients' UUIDs can be obtained from their \"uuid\" procfs entries like\n\n```\n/proc/fs/lustre/llite/testfs-ffff8800612bf800/uuid\n```", "mimetype": "text/plain", "start_char_idx": 41153, "end_char_idx": 45163, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a81acd2-ae13-4df8-b38b-46b2a7069503": {"__data__": {"id_": "7a81acd2-ae13-4df8-b38b-46b2a7069503", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f90c1baa-80d1-4e8d-88ff-db4fa0baee38", "node_type": "1", "metadata": {}, "hash": "470b9086f673a44e19b5d9f6da7fc5b60a115e3c7b4c2a1f87965f1af3a0e20c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Lustre Parameters\n\n- [Lustre Parameters](#lustre-parameters)\n  * [Introduction to Lustre Parameters](#introduction-to-lustre-parameters)\n    + [Identifying Lustre File Systems and Servers](#identifying-lustre-file-systems-and-servers)\n  * [Tuning Multi-Block Allocation (mballoc)](#tuning-multi-block-allocation-mballoc)\n  * [Monitoring Lustre File System I/O](#monitoring-lustre-file-system-io)\n    + [Monitoring the Client RPC Stream](#monitoring-the-client-rpc-stream)\n    + [Monitoring Client Activity](#monitoring-client-activity)\n    + [Monitoring Client Read-Write Offset Statistics](#monitoring-client-read-write-offset-statistics)\n    + [Monitoring Client Read-Write Extent Statistics](#monitoring-client-read-write-extent-statistics)\n      - [Client-Based I/O Extent Size Survey](#client-based-io-extent-size-survey)\n      - [Per-Process Client I/O Statistics](#per-process-client-io-statistics)\n    + [Monitoring the OST Block I/O Stream](#monitoring-the-ost-block-io-stream)\n  * [Tuning Lustre File System I/O](#tuning-lustre-file-system-io)\n    + [Tuning the Client I/O RPC Stream](#tuning-the-client-io-rpc-stream)\n    + [Tuning File Readahead and Directory Statahead](#tuning-file-readahead-and-directory-statahead)\n      - [Tuning File Readahead](#tuning-file-readahead)\n      - [Tuning Directory Statahead and AGL](#tuning-directory-statahead-and-agl)\n    + [Tuning OSS Read Cache](#tuning-oss-read-cache)\n      - [Using OSS Read Cache](#using-oss-read-cache)\n    + [Enabling OSS Asynchronous Journal Commit](#enabling-oss-asynchronous-journal-commit)\n    + [Tuning the Client Metadata RPC Stream](#tuning-the-client-metadata-rpc-stream)\n      - [Configuring the Client Metadata RPC Stream](#configuring-the-client-metadata-rpc-stream)\n      - [Monitoring the Client Metadata RPC Stream](#monitoring-the-client-metadata-rpc-stream)\n  * [Configuring Timeouts in a Lustre File System](#configuring-timeouts-in-a-lustre-file-system)\n    + [Configuring Adaptive Timeouts](#configuring-adaptive-timeouts)\n      - [Interpreting Adaptive Timeout Information](#interpreting-adaptive-timeout-information)\n    + [Setting Static Timeouts](#setting-static-timeouts)\n  * [Monitoring LNet](#monitoring-lnet)\n  * [Allocating Free Space on OSTs](#allocating-free-space-on-osts)\n  * [Configuring Locking](#configuring-locking)\n  * [Setting MDS and OSS Thread Counts](#setting-mds-and-oss-thread-counts)\n  * [Enabling and Interpreting Debugging Logs](#enabling-and-interpreting-debugging-logs)\n    + [Interpreting OST Statistics](#interpreting-ost-statistics)\n    + [Interpreting MDT Statistics](#interpreting-mdt-statistics)\n\n\nThe `/proc` and `/sys` file systems acts as an interface to internal data structures in the kernel. This chapter describes parameters and tunables that are useful for optimizing and monitoring aspects of a Lustre file system. It includes these sections:\n\n- [the section called \u201cEnabling and Interpreting Debugging Logs\u201d](#enabling-and-interpreting-debugging-logs)\n\n## Introduction to Lustre Parameters\n\nLustre parameters and statistics files provide an interface to internal data structures in the kernel that enables monitoring and tuning of many aspects of Lustre file system and application performance. These data structures include settings and metrics for components such as memory, networking, file systems, and kernel housekeeping routines, which are available throughout the hierarchical file layout.\n\nTypically, metrics are accessed via `lctl get_param` files and settings are changed by via `lctl set_param`. While it is possible to access parameters in `/proc` and `/sys` directly, the location of these parameters may change between releases, so it is recommended to always use `lctl` to access the parameters from userspace scripts. Some data is server-only, some data is client-only, and some data is exported from the client to the server and is thus duplicated in both locations.\n\n**Note**\n\nIn the examples in this chapter, `#` indicates a command is entered as root. Lustre servers are named according to the convention `*fsname*-*MDT|OSTnumber*`. The standard UNIX wildcard designation (*) is used.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4146, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f90c1baa-80d1-4e8d-88ff-db4fa0baee38": {"__data__": {"id_": "f90c1baa-80d1-4e8d-88ff-db4fa0baee38", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a81acd2-ae13-4df8-b38b-46b2a7069503", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5d936180d25083897404a8a708522c6d26b5060be4468632da5d497bcdf18603", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b198a0b8-1dee-44bf-8fd2-1e7a6cb721e0", "node_type": "1", "metadata": {}, "hash": "ba31ac7c3fe3626032a288e3cc331b94578505711f39c529d85d5a1879214499", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These data structures include settings and metrics for components such as memory, networking, file systems, and kernel housekeeping routines, which are available throughout the hierarchical file layout.\n\nTypically, metrics are accessed via `lctl get_param` files and settings are changed by via `lctl set_param`. While it is possible to access parameters in `/proc` and `/sys` directly, the location of these parameters may change between releases, so it is recommended to always use `lctl` to access the parameters from userspace scripts. Some data is server-only, some data is client-only, and some data is exported from the client to the server and is thus duplicated in both locations.\n\n**Note**\n\nIn the examples in this chapter, `#` indicates a command is entered as root. Lustre servers are named according to the convention `*fsname*-*MDT|OSTnumber*`. The standard UNIX wildcard designation (*) is used.\n\nSome examples are shown below:\n\n- To obtain data from a Lustre client:\n\n  ```\n  # lctl list_param osc.*\n  osc.testfs-OST0000-osc-ffff881071d5cc00\n  osc.testfs-OST0001-osc-ffff881071d5cc00\n  osc.testfs-OST0002-osc-ffff881071d5cc00\n  osc.testfs-OST0003-osc-ffff881071d5cc00\n  osc.testfs-OST0004-osc-ffff881071d5cc00\n  osc.testfs-OST0005-osc-ffff881071d5cc00\n  osc.testfs-OST0006-osc-ffff881071d5cc00\n  osc.testfs-OST0007-osc-ffff881071d5cc00\n  osc.testfs-OST0008-osc-ffff881071d5cc00\n  ```\n\n  In this example, information about OST connections available on a client is displayed (indicated by \"osc\").\n\n- To see multiple levels of parameters, use multiple wildcards:\n\n  ```\n  # lctl list_param osc.*.*\n  osc.testfs-OST0000-osc-ffff881071d5cc00.active\n  osc.testfs-OST0000-osc-ffff881071d5cc00.blocksize\n  osc.testfs-OST0000-osc-ffff881071d5cc00.checksum_type\n  osc.testfs-OST0000-osc-ffff881071d5cc00.checksums\n  osc.testfs-OST0000-osc-ffff881071d5cc00.connect_flags\n  osc.testfs-OST0000-osc-ffff881071d5cc00.contention_seconds\n  osc.testfs-OST0000-osc-ffff881071d5cc00.cur_dirty_bytes\n  ...\n  osc.testfs-OST0000-osc-ffff881071d5cc00.rpc_stats\n  ```\n\n- To view a specific file, use `lctl get_param`:\n\n  ```\n  # lctl get_param osc.lustre-OST0000*.rpc_stats\n  ```\n\nFor more information about using `lctl`, see [*the section called \u201cSetting Parameters with `lctl`\u201d*](03.02-Lustre%20Operations.md#setting-parameters-with-lctl).\n\nData can also be viewed using the `cat` command with the full path to the file. The form of the `cat` command is similar to that of the `lctl get_param` command with some differences. Unfortunately, as the Linux kernel has changed over the years, the location of statistics and parameter files has also changed, which means that the Lustre parameter files may be located in either the `/proc` directory, in the `/sys` directory, and/or in the`/sys/kernel/debug` directory, depending on the kernel version and the Lustre version being used. The `lctl`command insulates scripts from these changes and is preferred over direct file access, unless as part of a high-performance monitoring system. In the `cat` command:\n\n- Replace the dots in the path with slashes.\n\n- Prepend the path with the appropriate directory component:\n\n  ```\n  /{proc,sys}/{fs,sys}/{lustre,lnet}\n  ```\n\nFor example, an `lctl get_param` command may look like this:\n\n```\n# lctl get_param osc.", "mimetype": "text/plain", "start_char_idx": 3236, "end_char_idx": 6531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b198a0b8-1dee-44bf-8fd2-1e7a6cb721e0": {"__data__": {"id_": "b198a0b8-1dee-44bf-8fd2-1e7a6cb721e0", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f90c1baa-80d1-4e8d-88ff-db4fa0baee38", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ffdb26c499f2f75b7b65015be530a6c81e4ef7dc05d6b4226dd269e5e57c1dbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "711b0b2a-c247-48ab-b9ae-0b2f7ef3376e", "node_type": "1", "metadata": {}, "hash": "f30ad701452fb3dd227fb0f9cae800d2176eb57a591756e5f35d71694463e60b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unfortunately, as the Linux kernel has changed over the years, the location of statistics and parameter files has also changed, which means that the Lustre parameter files may be located in either the `/proc` directory, in the `/sys` directory, and/or in the`/sys/kernel/debug` directory, depending on the kernel version and the Lustre version being used. The `lctl`command insulates scripts from these changes and is preferred over direct file access, unless as part of a high-performance monitoring system. In the `cat` command:\n\n- Replace the dots in the path with slashes.\n\n- Prepend the path with the appropriate directory component:\n\n  ```\n  /{proc,sys}/{fs,sys}/{lustre,lnet}\n  ```\n\nFor example, an `lctl get_param` command may look like this:\n\n```\n# lctl get_param osc.*.uuid\nosc.testfs-OST0000-osc-ffff881071d5cc00.uuid=594db456-0685-bd16-f59b-e72ee90e9819\nosc.testfs-OST0001-osc-ffff881071d5cc00.uuid=594db456-0685-bd16-f59b-e72ee90e9819\n...\n```\n\nThe equivalent `cat` command may look like this:\n\n```\n# cat /proc/fs/lustre/osc/*/uuid\n594db456-0685-bd16-f59b-e72ee90e9819\n594db456-0685-bd16-f59b-e72ee90e9819\n...\n```\n\nor like this:\n\n```\n# cat /sys/fs/lustre/osc/*/uuid\n594db456-0685-bd16-f59b-e72ee90e9819\n594db456-0685-bd16-f59b-e72ee90e9819\n...\n```\n\nThe `llstat` utility can be used to monitor some Lustre file system I/O activity over a specified time period. For more details, see [*the section called \u201c llstat\u201d*](06.07-System%20Configuration%20Utilities.md#llstat).\n\nSome data is imported from attached clients and is available in a directory called `exports` located in the corresponding per-service directory on a Lustre server. For example:\n\n```\noss:/root# lctl list_param obdfilter.testfs-OST0000.exports.*\n# hash ldlm_stats stats uuid\n```\n\n### Identifying Lustre File Systems and Servers\n\nSeveral parameter files on the MGS list existing Lustre file systems and file system servers. The examples below are for a Lustre file system called `testfs` with one MDT and three OSTs.\n\n- To view all known Lustre file systems, enter:\n\n  ```\n  mgs# lctl get_param mgs.*.filesystems\n  testfs\n  ```\n\n- To view the names of the servers in a file system in which least one server is running, enter:\n\n  ```\n  lctl get_param mgs.*.live.<filesystem name>\n  ```\n\n  For example:\n\n  ```\n  mgs# lctl get_param mgs.", "mimetype": "text/plain", "start_char_idx": 5754, "end_char_idx": 8066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "711b0b2a-c247-48ab-b9ae-0b2f7ef3376e": {"__data__": {"id_": "711b0b2a-c247-48ab-b9ae-0b2f7ef3376e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b198a0b8-1dee-44bf-8fd2-1e7a6cb721e0", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b70d2f94936517c54e98d1f4cd868d29909c0919c291b7ae33e8dbf684ddc722", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f41449d3-0812-4d39-8a70-b2dc1f0baf70", "node_type": "1", "metadata": {}, "hash": "07a2ba65e35278262c8a5f3bed1d63f623f5318266a140e87691560752d92e3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example:\n\n```\noss:/root# lctl list_param obdfilter.testfs-OST0000.exports.*\n# hash ldlm_stats stats uuid\n```\n\n### Identifying Lustre File Systems and Servers\n\nSeveral parameter files on the MGS list existing Lustre file systems and file system servers. The examples below are for a Lustre file system called `testfs` with one MDT and three OSTs.\n\n- To view all known Lustre file systems, enter:\n\n  ```\n  mgs# lctl get_param mgs.*.filesystems\n  testfs\n  ```\n\n- To view the names of the servers in a file system in which least one server is running, enter:\n\n  ```\n  lctl get_param mgs.*.live.<filesystem name>\n  ```\n\n  For example:\n\n  ```\n  mgs# lctl get_param mgs.*.live.testfs\n  fsname: testfs\n  flags: 0x20     gen: 45\n  testfs-MDT0000\n  testfs-OST0000\n  testfs-OST0001\n  testfs-OST0002 \n  \n  Secure RPC Config Rules: \n  \n  imperative_recovery_state:\n      state: startup\n      nonir_clients: 0\n      nidtbl_version: 6\n      notify_duration_total: 0.001000\n      notify_duation_max:  0.001000\n      notify_count: 4\n  ```\n\n- To list all configured devices on the local node, enter:\n\n  ```\n  # lctl device_list\n  0 UP mgs MGS MGS 11\n  1 UP mgc MGC192.168.10.34@tcp 1f45bb57-d9be-2ddb-c0b0-5431a49226705\n  2 UP mdt MDS MDS_uuid 3\n  3 UP lov testfs-mdtlov testfs-mdtlov_UUID 4\n  4 UP mds testfs-MDT0000 testfs-MDT0000_UUID 7\n  5 UP osc testfs-OST0000-osc testfs-mdtlov_UUID 5\n  6 UP osc testfs-OST0001-osc testfs-mdtlov_UUID 5\n  7 UP lov testfs-clilov-ce63ca00 08ac6584-6c4a-3536-2c6d-b36cf9cbdaa04\n  8 UP mdc testfs-MDT0000-mdc-ce63ca00 08ac6584-6c4a-3536-2c6d-b36cf9cbdaa05\n  9 UP osc testfs-OST0000-osc-ce63ca00 08ac6584-6c4a-3536-2c6d-b36cf9cbdaa05\n  10 UP osc testfs-OST0001-osc-ce63ca00 08ac6584-6c4a-3536-2c6d-b36cf9cbdaa05\n  ```\n\n  The information provided on each line includes:\n\n  \\- Device number\n\n  \\- Device status (UP, INactive, or STopping)\n\n  \\- Device name\n\n  \\- Device UUID\n\n  \\- Reference count (how many users this device has)\n\n- To display the name of any server, view the device label:\n\n  ```\n  mds# e2label /dev/sda\n  testfs-MDT0000 \n  ```\n\n## Tuning Multi-Block Allocation (mballoc)\n\nCapabilities supported by `mballoc` include:\n\n- Pre-allocation for single files to help to reduce fragmentation.\n- Pre-allocation for a group of files to enable packing of small files into large, contiguous chunks.\n- Stream allocation to help decrease the seek rate.\n\nThe following `mballoc` tunables are available:\n\n| **Field**           | **Description**                                              |\n| ------------------- | ------------------------------------------------------------ |\n| `mb_max_to_scan`    | Maximum number of free chunks that `mballoc` finds before a final decision to avoid a livelock situation. |\n| `mb_min_to_scan`    | Minimum number of free chunks that `mballoc` searches before picking the best chunk for allocation. This is useful for small requests to reduce fragmentation of big free chunks. |\n| `mb_order2_req`     | For requests equal to 2^N, where N >= `mb_order2_req`, a fast search is done using a base 2 buddy allocation service.", "mimetype": "text/plain", "start_char_idx": 7399, "end_char_idx": 10476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f41449d3-0812-4d39-8a70-b2dc1f0baf70": {"__data__": {"id_": "f41449d3-0812-4d39-8a70-b2dc1f0baf70", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "711b0b2a-c247-48ab-b9ae-0b2f7ef3376e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d059391a022ce7dd4efcd24fa2e9f71e2bedd4885282c59a3444f28f6e535bd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf25aa15-c988-428b-b362-e61342ad4e63", "node_type": "1", "metadata": {}, "hash": "bca69a0c4d89b2bb377f7e1e96e44218c74c5d91e4a9485907fb124044eb7fe1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Pre-allocation for a group of files to enable packing of small files into large, contiguous chunks.\n- Stream allocation to help decrease the seek rate.\n\nThe following `mballoc` tunables are available:\n\n| **Field**           | **Description**                                              |\n| ------------------- | ------------------------------------------------------------ |\n| `mb_max_to_scan`    | Maximum number of free chunks that `mballoc` finds before a final decision to avoid a livelock situation. |\n| `mb_min_to_scan`    | Minimum number of free chunks that `mballoc` searches before picking the best chunk for allocation. This is useful for small requests to reduce fragmentation of big free chunks. |\n| `mb_order2_req`     | For requests equal to 2^N, where N >= `mb_order2_req`, a fast search is done using a base 2 buddy allocation service. |\n| `mb_small_req`      | `mb_small_req` - Defines (in MB) the upper bound of \"small requests\".`mb_large_req` - Defines (in MB) the lower bound of \"large requests\".Requests are handled differently based on size:< `mb_small_req` - Requests are packed together to form large, aggregated requests.> `mb_small_req` and < `mb_large_req` - Requests are primarily allocated linearly.> `mb_large_req` - Requests are allocated since hard disk seek time is less of a concern in this case.In general, small requests are combined to create larger requests, which are then placed close to one another to minimize the number of seeks required to access the data. |\n| `mb_large_req`      |                                                              |\n| `prealloc_table`    | A table of values used to preallocate space when a new request is received. By default, the table looks like this:`prealloc_table 4 8 16 32 64 128 256 512 1024 2048 `When a new request is received, space is preallocated at the next higher increment specified in the table. For example, for requests of less than 4 file system blocks, 4 blocks of space are preallocated; for requests between 4 and 8, 8 blocks are preallocated; and so forthAlthough customized values can be entered in the table, the performance of general usage file systems will not typically be improved by modifying the table (in fact, in ext4 systems, the table values are fixed). However, for some specialized workloads, tuning the `prealloc_table` values may result in smarter preallocation decisions. |\n| `mb_group_prealloc` | The amount of space (in kilobytes) preallocated for groups of small requests. |\n\nBuddy group cache information found in `/sys/fs/ldiskfs/*disk_device*/mb_groups` may be useful for assessing on-disk fragmentation. For example:\n\n```\ncat /proc/fs/ldiskfs/loop0/mb_groups \n#group: free free frags first pa [ 2^0 2^1 2^2 2^3 2^4 2^5 2^6 2^7 2^8 2^9 \n     2^10 2^11 2^12 2^13] \n#0    : 2936 2936 1     42    0  [ 0   0   0   1   1   1   1   2   0   1 \n     2    0    0    0   ]\n```\n\nIn this example, the columns show:\n\n- \\#group number\n- Available blocks in the group\n- Blocks free on a disk\n- Number of free fragments\n- First free block in the group\n- Number of preallocated chunks (not blocks)\n- A series of available chunks of different sizes\n\n## Monitoring Lustre File System I/O\n\nA number of system utilities are provided to enable collection of data related to I/O activity in a Lustre file system. In general, the data collected describes:\n\n- Data transfer rates and throughput of inputs and outputs external to the Lustre file system, such as network requests or disk I/O operations performed\n- Data about the throughput or transfer rates of internal Lustre file system data, such as locks or allocations.\n\n**Note**\n\nIt is highly recommended that you complete baseline testing for your Lustre file system to determine normal I/O activity for your hardware, network, and system workloads. Baseline data will allow you to easily determine when performance becomes degraded in your system. Two particularly useful baseline statistics are:\n\n- `brw_stats` \u2013 Histogram data characterizing I/O requests to the OSTs.", "mimetype": "text/plain", "start_char_idx": 9621, "end_char_idx": 13649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf25aa15-c988-428b-b362-e61342ad4e63": {"__data__": {"id_": "bf25aa15-c988-428b-b362-e61342ad4e63", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f41449d3-0812-4d39-8a70-b2dc1f0baf70", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "66df7f7877f6725b57301e5019de7aa253e57a98ce6e35d5c921685d5faf2477", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a58cacc-7968-44c4-afcb-78c33ab1f5b6", "node_type": "1", "metadata": {}, "hash": "3b555521d7cb2558bb13633457c6e69c16857ab2896999db4c4b62b87f0eac94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In general, the data collected describes:\n\n- Data transfer rates and throughput of inputs and outputs external to the Lustre file system, such as network requests or disk I/O operations performed\n- Data about the throughput or transfer rates of internal Lustre file system data, such as locks or allocations.\n\n**Note**\n\nIt is highly recommended that you complete baseline testing for your Lustre file system to determine normal I/O activity for your hardware, network, and system workloads. Baseline data will allow you to easily determine when performance becomes degraded in your system. Two particularly useful baseline statistics are:\n\n- `brw_stats` \u2013 Histogram data characterizing I/O requests to the OSTs. For more details, see [*the section called \u201cMonitoring the OST Block I/O Stream\u201d*](#monitoring-the-ost-block-io-stream).\n- `rpc_stats` \u2013 Histogram data showing information about RPCs made by clients. For more details, see [*the section called \u201cMonitoring the Client RPC Stream\u201d*](#monitoring-the-client-rpc-stream).\n\n### Monitoring the Client RPC Stream\n\nThe `rpc_stats` file contains histogram data showing information about remote procedure calls (RPCs) that have been made since this file was last cleared. The histogram data can be cleared by writing any value into the `rpc_stats` file.", "mimetype": "text/plain", "start_char_idx": 12938, "end_char_idx": 14241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a58cacc-7968-44c4-afcb-78c33ab1f5b6": {"__data__": {"id_": "0a58cacc-7968-44c4-afcb-78c33ab1f5b6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf25aa15-c988-428b-b362-e61342ad4e63", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04d764ff8e1cfa44c97c1d1c118ff1af3e0ddc90e7e408a30258132e552ad784", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63168870-1025-44bc-b22f-87ac8190c3b8", "node_type": "1", "metadata": {}, "hash": "a16e75d53b7360587798d488620dd2ed079704f60ab3731b2f484761d15fb167", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Baseline data will allow you to easily determine when performance becomes degraded in your system. Two particularly useful baseline statistics are:\n\n- `brw_stats` \u2013 Histogram data characterizing I/O requests to the OSTs. For more details, see [*the section called \u201cMonitoring the OST Block I/O Stream\u201d*](#monitoring-the-ost-block-io-stream).\n- `rpc_stats` \u2013 Histogram data showing information about RPCs made by clients. For more details, see [*the section called \u201cMonitoring the Client RPC Stream\u201d*](#monitoring-the-client-rpc-stream).\n\n### Monitoring the Client RPC Stream\n\nThe `rpc_stats` file contains histogram data showing information about remote procedure calls (RPCs) that have been made since this file was last cleared. The histogram data can be cleared by writing any value into the `rpc_stats` file.\n\n**Example:**\n\n```\n# lctl get_param osc.testfs-OST0000-osc-ffff810058d2f800.rpc_stats\nsnapshot_time:            1372786692.389858 (secs.usecs)\nread RPCs in flight:      0\nwrite RPCs in flight:     1\ndio read RPCs in flight:  0\ndio write RPCs in flight: 0\npending write pages:      256\npending read pages:       0\n\n                     read                   write\npages per rpc   rpcs   % cum % |       rpcs   % cum %\n1:                 0   0   0   |          0   0   0\n2:                 0   0   0   |          1   0   0\n4:                 0   0   0   |          0   0   0\n8:                 0   0   0   |          0   0   0\n16:                0   0   0   |          0   0   0\n32:                0   0   0   |          2   0   0\n64:                0   0   0   |          2   0   0\n128:               0   0   0   |          5   0   0\n256:             850 100 100   |      18346  99 100\n\n                     read                   write\nrpcs in flight  rpcs   % cum % |       rpcs   % cum %\n0:               691  81  81   |       1740   9   9\n1:                48   5  86   |        938   5  14\n2:                29   3  90   |       1059   5  20\n3:                17   2  92   |       1052   5  26\n4:                13   1  93   |        920   5  31\n5:                12   1  95   |        425   2  33\n6:                10   1  96   |        389   2  35\n7:                30   3 100   |      11373  61  97\n8:                 0   0 100   |        460   2 100\n\n                     read                   write\noffset          rpcs   % cum % |       rpcs   % cum %\n0:               850 100 100   |      18347  99  99\n1:                 0   0 100   |          0   0  99\n2:                 0   0 100   |          0   0  99\n4:                 0   0 100   |          0   0  99\n8:                 0   0 100   |          0   0  99\n16:                0   0 100   |          1   0  99\n32:                0   0 100   |          1   0  99\n64:                0   0 100   |          3   0  99\n128:               0   0 100   |          4   0 100\n```\n\nThe header information includes:\n\n- `snapshot_time` - UNIX epoch instant the file was read.", "mimetype": "text/plain", "start_char_idx": 13429, "end_char_idx": 16370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63168870-1025-44bc-b22f-87ac8190c3b8": {"__data__": {"id_": "63168870-1025-44bc-b22f-87ac8190c3b8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a58cacc-7968-44c4-afcb-78c33ab1f5b6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "08d38b4989a7c7eb795f0e3728aadf396de79f7bd58cf35ef4233b30b1b59651", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8cecca9-eafc-4bf0-b3ed-e7f26954e26f", "node_type": "1", "metadata": {}, "hash": "991362b55b619a1b6a7536e34dd43478f34073e48ec55b63dec467083f440fdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `read RPCs in flight` - Number of read RPCs issued by the OSC, but not complete at the time of the snapshot. This value should always be less than or equal to `max_rpcs_in_flight`.\n- `write RPCs in flight` - Number of write RPCs issued by the OSC, but not complete at the time of the snapshot. This value should always be less than or equal to `max_rpcs_in_flight`.\n- `dio read RPCs in flight` - Direct I/O (as opposed to block I/O) read RPCs issued but not completed at the time of the snapshot.\n- `dio write RPCs in flight` - Direct I/O (as opposed to block I/O) write RPCs issued but not completed at the time of the snapshot.\n- `pending write pages` - Number of pending write pages that have been queued for I/O in the OSC.\n- `pending read pages` - Number of pending read pages that have been queued for I/O in the OSC.\n\nThe tabular data is described in the table below. Each row in the table shows the number of reads or writes (ios) occurring for the statistic, the relative percentage (%) of total reads or writes, and the cumulative percentage (cum %) to that point in the table for the statistic.\n\n| **Field**      | **Description**                                              |\n| -------------- | ------------------------------------------------------------ |\n| pages per RPC  | Shows cumulative RPC reads and writes organized according to the number of pages in the RPC. A single page RPC increments the 0: row. |\n| RPCs in flight | Shows the number of RPCs that are pending when an RPC is sent. When the first RPC is sent, the 0: row is incremented. If the first RPC is sent while another RPC is pending, the 1: row is incremented and so on |\n| offset         | The page index of the first page read from or written to the object by the RPC. |\n\n**Analysis:**\n\nThis table provides a way to visualize the concurrency of the RPC stream. Ideally, you will see a large clump around the `max_rpcs_in_flight value`, which shows that the network is being kept busy.\n\nFor information about optimizing the client I/O RPC stream, see [*the section called \u201cTuning the Client I/O RPC Stream\u201d*](#tuning-the-client-io-rpc-stream).\n\n### Monitoring Client Activity\n\nThe `stats` file maintains statistics accumulate during typical operation of a client across the VFS interface of the Lustre file system. Only non-zero parameters are displayed in the file.\n\nClient statistics are enabled by default.\n\n**Note**\n\nStatistics for all mounted file systems can be discovered by entering:\n\n```\nlctl get_param llite.*.stats\n```\n\n**Example:**\n\n```\nclient# lctl get_param llite.*.stats\nsnapshot_time          1308343279.169704 secs.usecs\ndirty_pages_hits       14819716 samples [regs]\ndirty_pages_misses     81473472 samples [regs]\nread_bytes             36502963 samples [bytes] 1 26843582 55488794\nwrite_bytes            22985001 samples [bytes] 0 125912 3379002\nbrw_read               2279 samples [pages] 1 1 2270\nioctl                  186749 samples [regs]\nopen                   3304805 samples [regs]\nclose                  3331323 samples [regs]\nseek                   48222475 samples [regs]\nfsync                  963 samples [regs]\ntruncate               9073 samples [regs]\nsetxattr               19059 samples [regs]\ngetxattr               61169 samples [regs]\n```\n\nThe statistics can be cleared by echoing an empty string into the `stats` file or by using the command:\n\n```\nlctl set_param llite.*.stats=0\n```\n\nThe statistics displayed are described in the table below.\n\n| **Entry**           | **Description**                                              |\n| ------------------- | ------------------------------------------------------------ |\n| `snapshot_time`     | UNIX epoch instant the stats file was read.                  |\n| `dirty_page_hits`   | The number of write operations that have been satisfied by the dirty page cache. See [*the section called \u201cTuning the Client I/O RPC Stream\u201d*](#tuning-the-client-io-rpc-stream) for more information about dirty cache behavior in a Lustre file system. |\n| `dirty_page_misses` | The number of write operations that were not satisfied by the dirty page cache. |\n| `read_bytes`        | The number of read operations that have occurred.", "mimetype": "text/plain", "start_char_idx": 16371, "end_char_idx": 20559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8cecca9-eafc-4bf0-b3ed-e7f26954e26f": {"__data__": {"id_": "c8cecca9-eafc-4bf0-b3ed-e7f26954e26f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63168870-1025-44bc-b22f-87ac8190c3b8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7ee666a9e7cb5ed7a01bedee5dfdcabc060887a6d0fbdafdc0c4bd4b1ce605ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1812762-419a-45d7-876c-35ea1f2cd008", "node_type": "1", "metadata": {}, "hash": "5e903f31f9b319c480b4fb699115f26c8efd2d890cd76d86c1fd941beac6cdc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*.stats=0\n```\n\nThe statistics displayed are described in the table below.\n\n| **Entry**           | **Description**                                              |\n| ------------------- | ------------------------------------------------------------ |\n| `snapshot_time`     | UNIX epoch instant the stats file was read.                  |\n| `dirty_page_hits`   | The number of write operations that have been satisfied by the dirty page cache. See [*the section called \u201cTuning the Client I/O RPC Stream\u201d*](#tuning-the-client-io-rpc-stream) for more information about dirty cache behavior in a Lustre file system. |\n| `dirty_page_misses` | The number of write operations that were not satisfied by the dirty page cache. |\n| `read_bytes`        | The number of read operations that have occurred. Three additional parameters are displayed:minThe minimum number of bytes read in a single request since the counter was reset.maxThe maximum number of bytes read in a single request since the counter was reset.sumThe accumulated sum of bytes of all read requests since the counter was reset. |\n| `write_bytes`       | The number of write operations that have occurred. Three additional parameters are displayed:minThe minimum number of bytes written in a single request since the counter was reset.maxThe maximum number of bytes written in a single request since the counter was reset.sumThe accumulated sum of bytes of all write requests since the counter was reset. |\n| `brw_read`          | The number of pages that have been read. Three additional parameters are displayed:minThe minimum number of bytes read in a single block read/write (`brw`) read request since the counter was reset.maxThe maximum number of bytes read in a single `brw` read requests since the counter was reset.sumThe accumulated sum of bytes of all `brw` read requests since the counter was reset. |\n| `ioctl`             | The number of combined file and directory `ioctl` operations. |\n| `open`              | The number of open operations that have succeeded.           |\n| `close`             | The number of close operations that have succeeded.          |\n| `seek`              | The number of times `seek` has been called.                  |\n| `fsync`             | The number of times `fsync` has been called.                 |\n| `truncate`          | The total number of calls to both locked and lockless `truncate`. |\n| `setxattr`          | The number of times extended attributes have been set.       |\n| `getxattr`          | The number of times value(s) of extended attributes have been fetched. |\n\n**Analysis:**\n\nInformation is provided about the amount and type of I/O activity is taking place on the client.\n\n### Monitoring Client Read-Write Offset Statistics\n\nWhen the `offset_stats` parameter is set, statistics are maintained for occurrences of a series of read or write calls from a process that did not access the next sequential location. The `OFFSET` field is reset to 0 (zero) whenever a different file is read or written.\n\n### Note\n\nBy default, statistics are not collected in the `offset_stats`, `extents_stats`, and `extents_stats_per_process` files to reduce monitoring overhead when this information is not needed. The collection of statistics in all three of these files is activated by writing anything, except for 0 (zero) and \"disable\", into any one of the files.\n\n**Example:**\n\n```\n# lctl get_param llite.testfs-f57dee0.offset_stats\nsnapshot_time: 1155748884.591028 (secs.usecs)\n             RANGE   RANGE    SMALLEST   LARGEST\nR/W   PID    START   END      EXTENT     EXTENT    OFFSET\nR     8385   0       128      128        128       0\nR     8385   0       224      224        224       -128\nW     8385   0       250      50         100       0\nW     8385   100     1110     10         500       -150\nW     8384   0       5233     5233       5233      0\nR     8385   500     600      100        100       -610\n```\n\nIn this example, `snapshot_time` is the UNIX epoch instant the file was read. The tabular data is described in the table below.\n\nThe `offset_stats` file can be cleared by entering:\n\n```\nlctl set_param llite.", "mimetype": "text/plain", "start_char_idx": 19768, "end_char_idx": 23897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1812762-419a-45d7-876c-35ea1f2cd008": {"__data__": {"id_": "d1812762-419a-45d7-876c-35ea1f2cd008", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8cecca9-eafc-4bf0-b3ed-e7f26954e26f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b88727dc4a52e8d0d1c1963af6156071845444f200edfbbfdaf0c812f9a62558", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7288a503-3adf-425f-b755-ede9169970dc", "node_type": "1", "metadata": {}, "hash": "3bbb469c157aa98571fef7ac979e6bd8f6efee0877093ce027f35cc46db6d044", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The tabular data is described in the table below.\n\nThe `offset_stats` file can be cleared by entering:\n\n```\nlctl set_param llite.*.offset_stats=0\n```\n\n| **Field**             | **Description**                                              |\n| --------------------- | ------------------------------------------------------------ |\n| R/W                   | Indicates if the non-sequential call was a read or write     |\n| PID                   | Process ID of the process that made the read/write call.     |\n| RANGE START/RANGE END | Range in which the read/write calls were sequential.         |\n| SMALLEST EXTENT       | Smallest single read/write in the corresponding range (in bytes). |\n| LARGEST EXTENT        | Largest single read/write in the corresponding range (in bytes). |\n| OFFSET                | Difference between the previous range end and the current range start. |\n\n**Analysis:**\n\nThis data provides an indication of how contiguous or fragmented the data is. For example, the fourth entry in the example above shows the writes for this RPC were sequential in the range 100 to 1110 with the minimum write 10 bytes and the maximum write 500 bytes. The range started with an offset of -150 from the `RANGE END` of the previous entry in the example.\n\n### Monitoring Client Read-Write Extent Statistics\n\nFor in-depth troubleshooting, client read-write extent statistics can be accessed to obtain more detail about read/write I/O extents for the file system or for a particular process.\n\n**Note**\n\nBy default, statistics are not collected in the `offset_stats`, `extents_stats`, and `extents_stats_per_process` files to reduce monitoring overhead when this information is not needed. The collection of statistics in all three of these files is activated by writing anything, except for 0 (zero) and \"disable\", into any one of the files.\n\n#### Client-Based I/O Extent Size Survey\n\nThe `extents_stats` histogram in the `llite` directory shows the statistics for the sizes of the read/write I/O extents. This file does not maintain the per process statistics.\n\n**Example:**\n\n```\n# lctl get_param llite.testfs-*.extents_stats\nsnapshot_time:                     1213828728.348516 (secs.usecs)\n                       read           |            write\nextents          calls  %      cum%   |     calls  %     cum%\n\n0K - 4K :        0      0      0      |     2      2     2\n4K - 8K :        0      0      0      |     0      0     2\n8K - 16K :       0      0      0      |     0      0     2\n16K - 32K :      0      0      0      |     20     23    26\n32K - 64K :      0      0      0      |     0      0     26\n64K - 128K :     0      0      0      |     51     60    86\n128K - 256K :    0      0      0      |     0      0     86\n256K - 512K :    0      0      0      |     0      0     86\n512K - 1024K :   0      0      0      |     0      0     86\n1M - 2M :        0      0      0      |     11     13    100\n```\n\nIn this example, `snapshot_time` is the UNIX epoch instant the file was read. The table shows cumulative extents organized according to size with statistics provided separately for reads and writes. Each row in the table shows the number of RPCs for reads and writes respectively (`calls`), the relative percentage of total calls (`%`), and the cumulative percentage to that point in the table of calls (`cum %`).\n\nThe file can be cleared by issuing the following command:\n\n```\n# lctl set_param llite.testfs-*.extents_stats=1\n```\n\n#### Per-Process Client I/O Statistics\n\nThe `extents_stats_per_process` file maintains the I/O extent size statistics on a per-process basis.", "mimetype": "text/plain", "start_char_idx": 23768, "end_char_idx": 27362, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7288a503-3adf-425f-b755-ede9169970dc": {"__data__": {"id_": "7288a503-3adf-425f-b755-ede9169970dc", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1812762-419a-45d7-876c-35ea1f2cd008", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a30ef524c503bf88a1029873481900bf0fdf7eca675d7863558ed8d386a2347e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "418eaabd-57dd-45c3-8d13-02e5a47d941d", "node_type": "1", "metadata": {}, "hash": "3e9bc2a124e71274c42f10f4b5272a88d470c1b2fa70242e86c6c6ed9bdb2b5c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The table shows cumulative extents organized according to size with statistics provided separately for reads and writes. Each row in the table shows the number of RPCs for reads and writes respectively (`calls`), the relative percentage of total calls (`%`), and the cumulative percentage to that point in the table of calls (`cum %`).\n\nThe file can be cleared by issuing the following command:\n\n```\n# lctl set_param llite.testfs-*.extents_stats=1\n```\n\n#### Per-Process Client I/O Statistics\n\nThe `extents_stats_per_process` file maintains the I/O extent size statistics on a per-process basis.\n\n**Example:**\n\n```\n# lctl get_param llite.testfs-*.extents_stats_per_process\nsnapshot_time:                     1213828762.204440 (secs.usecs)\n                          read            |             write\nextents            calls   %      cum%    |      calls   %       cum%\n \nPID: 11488\n   0K - 4K :       0       0       0      |      0       0       0\n   4K - 8K :       0       0       0      |      0       0       0\n   8K - 16K :      0       0       0      |      0       0       0\n   16K - 32K :     0       0       0      |      0       0       0\n   32K - 64K :     0       0       0      |      0       0       0\n   64K - 128K :    0       0       0      |      0       0       0\n   128K - 256K :   0       0       0      |      0       0       0\n   256K - 512K :   0       0       0      |      0       0       0\n   512K - 1024K :  0       0       0      |      0       0       0\n   1M - 2M :       0       0       0      |      10      100     100\n \nPID: 11491\n   0K - 4K :       0       0       0      |      0       0       0\n   4K - 8K :       0       0       0      |      0       0       0\n   8K - 16K :      0       0       0      |      0       0       0\n   16K - 32K :     0       0       0      |      20      100     100\n   \nPID: 11424\n   0K - 4K :       0       0       0      |      0       0       0\n   4K - 8K :       0       0       0      |      0       0       0\n   8K - 16K :      0       0       0      |      0       0       0\n   16K - 32K :     0       0       0      |      0       0       0\n   32K - 64K :     0       0       0      |      0       0       0\n   64K - 128K :    0       0       0      |      16      100     100\n \nPID: 11426\n   0K - 4K :       0       0       0      |      1       100     100\n \nPID: 11429\n   0K - 4K :       0       0       0      |      1       100     100\n \n```\n\nThis table shows cumulative extents organized according to size for each process ID (PID) with statistics provided separately for reads and writes. Each row in the table shows the number of RPCs for reads and writes respectively (`calls`), the relative percentage of total calls (`%`), and the cumulative percentage to that point in the table of calls (`cum %`).", "mimetype": "text/plain", "start_char_idx": 26768, "end_char_idx": 29558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "418eaabd-57dd-45c3-8d13-02e5a47d941d": {"__data__": {"id_": "418eaabd-57dd-45c3-8d13-02e5a47d941d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7288a503-3adf-425f-b755-ede9169970dc", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2bdbc3a41e669fcbcfd32793ecd5bc08be09aa7382d5989c990c670fad8f7643", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c61082d9-b1cb-4af1-96cc-845e7027aae8", "node_type": "1", "metadata": {}, "hash": "f5e438b03e946fc50c92897a7f9fc75feed3ad67c9ce487c2bc3af4c818404b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each row in the table shows the number of RPCs for reads and writes respectively (`calls`), the relative percentage of total calls (`%`), and the cumulative percentage to that point in the table of calls (`cum %`).\n\n### Monitoring the OST Block I/O Stream\n\nThe `brw_stats` parameter file below the `osd-ldiskfs` or `osd-zfs` directory contains histogram data showing statistics for number of I/O requests sent to the disk, their size, and whether they are contiguous on the disk or not.\n\n**Example:**\n\nEnter on the OSS or MDS:\n\n```\noss# lctl get_param osd-*.", "mimetype": "text/plain", "start_char_idx": 29344, "end_char_idx": 29902, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c61082d9-b1cb-4af1-96cc-845e7027aae8": {"__data__": {"id_": "c61082d9-b1cb-4af1-96cc-845e7027aae8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "418eaabd-57dd-45c3-8d13-02e5a47d941d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "41df7da6f36f90c1283c32a40691bd2c9dec95a7ef6e44b8493108de260ebea4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c212ba32-d78c-4e9e-b894-68a0fd6363a9", "node_type": "1", "metadata": {}, "hash": "4d91275c28b730c5f9e58818d6d579a43136f8b44d3e5fae0ede3f0a28f650a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each row in the table shows the number of RPCs for reads and writes respectively (`calls`), the relative percentage of total calls (`%`), and the cumulative percentage to that point in the table of calls (`cum %`).\n\n### Monitoring the OST Block I/O Stream\n\nThe `brw_stats` parameter file below the `osd-ldiskfs` or `osd-zfs` directory contains histogram data showing statistics for number of I/O requests sent to the disk, their size, and whether they are contiguous on the disk or not.\n\n**Example:**\n\nEnter on the OSS or MDS:\n\n```\noss# lctl get_param osd-*.*.brw_stats\nsnapshot_time: 1372775039.769045 (secs.usecs)\n read | write\npages per bulk r/w rpcs % cum % | rpcs % cum %\n1: 108 100 100 | 39 0 0\n2: 0 0 100 | 6 0 0\n4: 0 0 100 | 1 0 0\n8: 0 0 100 | 0 0 0\n16: 0 0 100 | 4 0 0\n32: 0 0 100 | 17 0 0\n64: 0 0 100 | 12 0 0\n128: 0 0 100 | 24 0 0\n256: 0 0 100 | 23142 99 100\n read | write\ndiscontiguous pages rpcs % cum % | rpcs % cum %\n0: 108 100 100 | 23245 100 100\n read | write\ndiscontiguous blocks rpcs % cum % | rpcs % cum %\n0: 108 100 100 | 23243 99 99\n1: 0 0 100 | 2 0 100\n read | write\ndisk fragmented I/Os ios % cum % | ios % cum %\n0: 94 87 87 | 0 0 0\n1: 14 12 100 | 23243 99 99\n2: 0 0 100 | 2 0 100\n read | write\ndisk I/Os in flight ios % cum % | ios % cum %\n1: 14 100 100 | 20896 89 89\n2: 0 0 100 | 1071 4 94\n3: 0 0 100 | 573 2 96\n4: 0 0 100 | 300 1 98\n5: 0 0 100 | 166 0 98\n6: 0 0 100 | 108 0 99\n7: 0 0 100 | 81 0 99\n8: 0 0 100 | 47 0 99\n9: 0 0 100 | 5 0 100\n read | write\nI/O time (1/1000s) ios % cum % | ios % cum %\n1: 94 87 87 | 0 0 0\n2: 0 0 87 | 7 0 0\n4: 14 12 100 | 27 0 0\n8: 0 0 100 | 14 0 0\n16: 0 0 100 | 31 0 0\n32: 0 0 100 | 38 0 0\n64: 0 0 100 | 18979 81 82\n128: 0 0 100 | 943 4 86\n256: 0 0 100 | 1233 5 91\n512: 0 0 100 | 1825 7 99\n1K: 0 0 100 | 99 0 99\n2K: 0 0 100 | 0 0 99\n4K: 0 0 100 | 0 0 99\n8K: 0 0 100 | 49 0 100\n read | write\ndisk I/O size ios % cum % | ios % cum %\n4K: 14 100 100 | 41 0 0\n8K: 0 0 100 | 6 0 0\n16K: 0 0 100 | 1 0 0\n32K: 0 0 100 | 0 0 0\n64K: 0 0 100 | 4 0 0\n128K: 0 0 100 | 17 0 0\n256K: 0 0 100 | 12 0 0\n512K: 0 0 100 | 24 0 0\n1M: 0 0 100 | 23142 99 100\n```\n\nThe tabular data is described in the table below.", "mimetype": "text/plain", "start_char_idx": 29344, "end_char_idx": 31490, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c212ba32-d78c-4e9e-b894-68a0fd6363a9": {"__data__": {"id_": "c212ba32-d78c-4e9e-b894-68a0fd6363a9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c61082d9-b1cb-4af1-96cc-845e7027aae8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "81709412fc504baf356c3b1bd8487def5087602fe06b4e9e9b4506d926e5fe40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e36592a5-5f66-4fb7-9b35-78e111ec7192", "node_type": "1", "metadata": {}, "hash": "6f430c981587d69bb4791d9f3352fc915c3bd658e2eccfcbc10803bdb304ea83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each row in the table shows the number of reads and writes occurring for the statistic (`ios`), the relative percentage of total reads or writes (`%`), and the cumulative percentage to that point in the table for the statistic (`cum %`).\n\n| **Field**              | **Description**                                              |\n| ---------------------- | ------------------------------------------------------------ |\n| `pages per bulk r/w`   | Number of pages per RPC request, which should match aggregate client `rpc_stats` (see [*the section called \u201cMonitoring the Client RPC Stream\u201d*](#monitoring-the-client-rpc-stream)). |\n| `discontiguous pages`  | Number of discontinuities in the logical file offset of each page in a single RPC. |\n| `discontiguous blocks` | Number of discontinuities in the physical block allocation in the file system for a single RPC. |\n| `disk fragmented I/Os` | Number of I/Os that were not written entirely sequentially.  |\n| `disk I/Os in flight`  | Number of disk I/Os currently pending.                       |\n| `I/O time (1/1000s)`   | Amount of time for each I/O operation to complete.           |\n| `disk I/O size`        | Size of each I/O operation.                                  |\n\n**Analysis:**\n\nThis data provides an indication of extent size and distribution in the file system.\n\n## Tuning Lustre File System I/O\n\nEach OSC has its own tree of tunables. For example:\n\n```\n$ lctl lctl list_param osc.*.*\nosc.myth-OST0000-osc-ffff8804296c2800.active\nosc.myth-OST0000-osc-ffff8804296c2800.blocksize\nosc.myth-OST0000-osc-ffff8804296c2800.checksum_dump\nosc.myth-OST0000-osc-ffff8804296c2800.checksum_type\nosc.myth-OST0000-osc-ffff8804296c2800.checksums\nosc.myth-OST0000-osc-ffff8804296c2800.connect_flags\n:\n:\nosc.myth-OST0000-osc-ffff8804296c2800.state\nosc.myth-OST0000-osc-ffff8804296c2800.stats\nosc.myth-OST0000-osc-ffff8804296c2800.timeouts\nosc.myth-OST0000-osc-ffff8804296c2800.unstable_stats\nosc.myth-OST0000-osc-ffff8804296c2800.uuid\nosc.myth-OST0001-osc-ffff8804296c2800.active\nosc.myth-OST0001-osc-ffff8804296c2800.blocksize\nosc.myth-OST0001-osc-ffff8804296c2800.checksum_dump\nosc.myth-OST0001-osc-ffff8804296c2800.checksum_type\n:\n:\n```\n\nThe following sections describe some of the parameters that can be tuned in a Lustre file system.\n\n### Tuning the Client I/O RPC Stream\n\nIdeally, an optimal amount of data is packed into each I/O RPC and a consistent number of issued RPCs are in progress at any time. To help optimize the client I/O RPC stream, several tuning variables are provided to adjust behavior according to network conditions and cluster size. For information about monitoring the client I/O RPC stream, see [*the section called \u201cMonitoring the Client RPC Stream\u201d*](#monitoring-the-client-rpc-stream).\n\nRPC stream tunables include:\n\n- `osc.*osc_instance*.checksums` - Controls whether the client will calculate data integrity checksums for the bulk data transferred to the OST. Data integrity checksums are enabled by default. The algorithm used can be set using the `checksum_type` parameter.\n\n- `osc.*osc_instance*.checksum_type` - Controls the data integrity checksum algorithm used by the client. The available algorithms are determined by the set of algorihtms. The checksum algorithm used by default is determined by first selecting the fastest algorithms available on the OST, and then selecting the fastest of those algorithms on the client, which depends on available optimizations in the CPU hardware and kernel. The default algorithm can be overridden by writing the algorithm name into the `checksum_type` parameter. Available checksum types can be seen on the client by reading the `checksum_type` parameter. Currently supported checksum types are: `adler`, `crc32`, `crc32c`\n\n  Introduced in Lustre 2.12In Lustre release 2.12 additional checksum types were added to allow end-to-end checksum integration with T10-PI capable hardware.", "mimetype": "text/plain", "start_char_idx": 31491, "end_char_idx": 35401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e36592a5-5f66-4fb7-9b35-78e111ec7192": {"__data__": {"id_": "e36592a5-5f66-4fb7-9b35-78e111ec7192", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c212ba32-d78c-4e9e-b894-68a0fd6363a9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cc534485d52619abbab66d3be2016a10bf464a415848cabe08a593cee3481b15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92f7a61e-d750-4a9e-a119-75ad62738585", "node_type": "1", "metadata": {}, "hash": "14a679918a8e5b9c2b929c426406baf6cd819b24cb0d17da731d51b871a0d8e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The algorithm used can be set using the `checksum_type` parameter.\n\n- `osc.*osc_instance*.checksum_type` - Controls the data integrity checksum algorithm used by the client. The available algorithms are determined by the set of algorihtms. The checksum algorithm used by default is determined by first selecting the fastest algorithms available on the OST, and then selecting the fastest of those algorithms on the client, which depends on available optimizations in the CPU hardware and kernel. The default algorithm can be overridden by writing the algorithm name into the `checksum_type` parameter. Available checksum types can be seen on the client by reading the `checksum_type` parameter. Currently supported checksum types are: `adler`, `crc32`, `crc32c`\n\n  Introduced in Lustre 2.12In Lustre release 2.12 additional checksum types were added to allow end-to-end checksum integration with T10-PI capable hardware. The client will compute the appropriate checksum type, based on the checksum type used by the storage, for the RPC checksum, which will be verified by the server and passed on to the storage. The T10-PI checksum types are: `t10ip512`, `t10ip4K`, `t10crc512`, `t10crc4K`\n\n- `osc.*osc_instance*.max_dirty_mb` - Controls how many MiB of dirty data can be written into the client pagecache for writes by *each* OSC. When this limit is reached, additional writes block until previously-cached data is written to the server. This may be changed by the `lctl set_param` command. Only values larger than 0 and smaller than the lesser of 2048 MiB or 1/4 of client RAM are valid. Performance can suffers if the client cannot aggregate enough data per OSC to form a full RPC (as set by the `max_pages_per_rpc`) parameter, unless the application is doing very large writes itself.\n\n  To maximize performance, the value for `max_dirty_mb` is recommended to be at least 4 * `max_pages_per_rpc` * `max_rpcs_in_flight`.\n\n- `osc.*osc_instance*.cur_dirty_bytes` - A read-only value that returns the current number of bytes written and cached by this OSC.\n\n- `osc.*osc_instance*.max_pages_per_rpc` - The maximum number of pages that will be sent in a single RPC request to the OST. The minimum value is one page and the maximum value is 16 MiB (4096 on systems with `PAGE_SIZE` of 4 KiB), with the default value of 4 MiB in one RPC. The upper limit may also be constrained by `ofd.*.brw_size` setting on the OSS, and applies to all clients connected to that OST. It is also possible to specify a units suffix (e.g. `max_pages_per_rpc=4M`), so the RPC size can be set independently of the client `PAGE_SIZE`.\n\n- `osc.*osc_instance*.max_rpcs_in_flight` - The maximum number of concurrent RPCs in flight from an OSC to its OST. If the OSC tries to initiate an RPC but finds that it already has the same number of RPCs outstanding, it will wait to issue further RPCs until some complete. The minimum setting is 1 and maximum setting is 256. The default value is 8 RPCs.\n\n  To improve small file I/O performance, increase the `max_rpcs_in_flight` value.\n\n- `llite.*fsname_instance*.max_cache_mb` - Maximum amount of read+write data cached by the client. The default value is 1/2 of the client RAM.\n\n**Note**\n\nThe value for `*osc_instance*` and `*fsname_instance*` are unique to each mount point to allow associating osc, mdc, lov, lmv, and llite parameters with the same mount point. However, it is common for scripts to use a wildcard `*` or a filesystem-specific wildcard `*fsname-\\**` to specify the parameter settings uniformly on all clients. For example:\n\n```\nclient$ lctl get_param osc.testfs-OST0000*.rpc_stats\nosc.testfs-OST0000-osc-ffff88107412f400.rpc_stats=\nsnapshot_time:         1375743284.337839 (secs.usecs)\nread RPCs in flight:  0\nwrite RPCs in flight: 0\n```\n\n### Tuning File Readahead and Directory Statahead\n\nFile readahead and directory statahead enable reading of data into memory before a process requests the data.", "mimetype": "text/plain", "start_char_idx": 34481, "end_char_idx": 38415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92f7a61e-d750-4a9e-a119-75ad62738585": {"__data__": {"id_": "92f7a61e-d750-4a9e-a119-75ad62738585", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e36592a5-5f66-4fb7-9b35-78e111ec7192", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4798b2745940c303de1273c06a63044afc37853a3c3a6e63d1df33af853fa746", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce9db1af-3c90-40d9-bfe0-be3aaca2470d", "node_type": "1", "metadata": {}, "hash": "1d02ef0a0aa776ed34dcd6199ee04be5261bafdbd5253722d628349b5fc422ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, it is common for scripts to use a wildcard `*` or a filesystem-specific wildcard `*fsname-\\**` to specify the parameter settings uniformly on all clients. For example:\n\n```\nclient$ lctl get_param osc.testfs-OST0000*.rpc_stats\nosc.testfs-OST0000-osc-ffff88107412f400.rpc_stats=\nsnapshot_time:         1375743284.337839 (secs.usecs)\nread RPCs in flight:  0\nwrite RPCs in flight: 0\n```\n\n### Tuning File Readahead and Directory Statahead\n\nFile readahead and directory statahead enable reading of data into memory before a process requests the data. File readahead prefetches file content data into memory for `read()` related calls, while directory statahead fetches file metadata into memory for `readdir()` and `stat()` related calls. When readahead and statahead work well, a process that accesses data finds that the information it needs is available immediately in memory on the client when requested without the delay of network I/O.\n\n#### Tuning File Readahead\n\nFile readahead is triggered when two or more sequential reads by an application fail to be satisfied by data in the Linux buffer cache. The size of the initial readahead is determined by the RPC size and the file stripe size, but will typically be at least 1 MiB. Additional readaheads grow linearly and increment until the per-file or per-system readahead cache limit on the client is reached.\n\nReadahead tunables include:\n\n- `llite.*fsname_instance*.max_read_ahead_mb` - Controls the maximum amount of data readahead on a file. Files are read ahead in RPC-sized chunks (4 MiB, or the size of the `read()` call, if larger) after the second sequential read on a file descriptor. Random reads are done at the size of the `read()` call only (no readahead). Reads to non-contiguous regions of the file reset the readahead algorithm, and readahead is not triggered until sequential reads take place again.\n\n  This is the global limit for all files and cannot be larger than 1/2 of the client RAM. To disable readahead, set`max_read_ahead_mb=0`.\n\n- `llite.*fsname_instance*.max_read_ahead_per_file_mb` - Controls the maximum number of megabytes (MiB) of data that should be prefetched by the client when sequential reads are detected on a file. This is the per-file readahead limit and cannot be larger than `max_read_ahead_mb`.\n\n- `llite.*fsname_instance*.max_read_ahead_whole_mb` - Controls the maximum size of a file in MiB that is read in its entirety upon access, regardless of the size of the `read()` call. This avoids multiple small read RPCs on relatively small files, when it is not possible to efficiently detect a sequential read pattern before the whole file has been read.\n\n  The default value is the greater of 2 MiB or the size of one RPC, as given by `max_pages_per_rpc`.\n\n#### Tuning Directory Statahead and AGL\n\nMany system commands, such as `ls \u2013l`, `du`, and `find`, traverse a directory sequentially. To make these commands run efficiently, the directory statahead can be enabled to improve the performance of directory traversal.\n\nThe statahead tunables are:\n\n- `statahead_max` - Controls the maximum number of file attributes that will be prefetched by the statahead thread. By default, statahead is enabled and `statahead_max` is 32 files.\n\n  To disable statahead, set `statahead_max` to zero via the following command on the client:\n\n  ```\n  lctl set_param llite.*.statahead_max=0\n  ```\n\n  To change the maximum statahead window size on a client:\n\n  ```\n  lctl set_param llite.*.statahead_max=n\n  ```\n\n  The maximum `statahead_max` is 8192 files.\n\n  The directory statahead thread will also prefetch the file size/block attributes from the OSTs, so that all file attributes are available on the client when requested by an application. This is controlled by the asynchronous glimpse lock (AGL) setting. The AGL behaviour can be disabled by setting:\n\n  ```\n  lctl set_param llite.", "mimetype": "text/plain", "start_char_idx": 37862, "end_char_idx": 41736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce9db1af-3c90-40d9-bfe0-be3aaca2470d": {"__data__": {"id_": "ce9db1af-3c90-40d9-bfe0-be3aaca2470d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92f7a61e-d750-4a9e-a119-75ad62738585", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "05b0d9dfae68e634e225652536f5fe0e1828261d5e45c8c447c3271e332b20ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1912527-e3e0-4212-8be5-fbe640d49244", "node_type": "1", "metadata": {}, "hash": "1c8c6e65665bb3d4704aa79097e07e498977e406b7f870e5493fa05bc81f7669", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By default, statahead is enabled and `statahead_max` is 32 files.\n\n  To disable statahead, set `statahead_max` to zero via the following command on the client:\n\n  ```\n  lctl set_param llite.*.statahead_max=0\n  ```\n\n  To change the maximum statahead window size on a client:\n\n  ```\n  lctl set_param llite.*.statahead_max=n\n  ```\n\n  The maximum `statahead_max` is 8192 files.\n\n  The directory statahead thread will also prefetch the file size/block attributes from the OSTs, so that all file attributes are available on the client when requested by an application. This is controlled by the asynchronous glimpse lock (AGL) setting. The AGL behaviour can be disabled by setting:\n\n  ```\n  lctl set_param llite.*.statahead_agl=0\n  ```\n\n- `statahead_stats` - A read-only interface that provides current statahead and AGL statistics, such as how many times statahead/AGL has been triggered since the last mount, how many statahead/AGL failures have occurred due to an incorrect prediction or other causes.\n\n  ### Note\n\n  AGL behaviour is affected by statahead since the inodes processed by AGL are built by the statahead thread. If statahead is disabled, then AGL is also disabled.\n\n### Tuning Server Read Cache\n\nThe server read cache feature provides read-only caching of file data on an OSS or MDS (for Data-onMDT). This functionality uses the Linux page cache to store the data and uses as much physical memory as is allocated.\n\nThe server read cache can improves Lustre file system performance in these situations:\n\n- Many clients are accessing the same data set (as in HPC applications or when diskless clients boot from the Lustre file system).\n- One client is writing data while another client is reading it (i.e., clients are exchanging data via the filesystem).\n- A client has very limited caching of its own.\n\nThe server read cache offers these benefits:\n\n- Allows servers to cache read data more frequently.\n- Improves repeated reads to match network speeds instead of storage speeds.\n- Provides the building blocks for server write cache (small-write aggregation).\n\n#### Using OSS Read Cache\n\nThe server read cache is implemented on the OSS and MDS, and does not require any special support on the client side. Since the server read cache uses the memory available in the Linux page cache, the appropriate amount of memory for the cache should be determined based on I/O patterns. If the data is mostly reads, then more cache is beneficial on the server than would be needed for mostly writes.\n\nThe server read cache is managed using the following tunables. Many tunables are available for both `osd-ldiskfs` and `osd-zfs`, but in some cases the implementation of `osd-zfs` prevents their use.\n\n- `read_cache_enable` \\- High-level control of whether data read from storage during a read request is kept in memory and available for later read requests for the same data, without having to re-read it from storage. By default, read cache is enabled (`read_cache_enable=1`) for HDD OSDs and automatically disabled for flash OSDs (`nonrotational=1`). The read cache cannot be disabled for `osd-zfs`, and as a result this parameter is unavailable for that backend.\n\n  When the server receives a read request from a client, it reads data from storage into its memory and sends the data to the client. If read cache is enabled for the target, and the RPC and object size also meet the other criterion below, this data may stay in memory after the client request has completed. If later read requests for the same data are received, if the data is still in cache the server skips reading it from storage. The cache is managed by the Linux kernel globally across all targets on that server so that the infrequently used cache pages are dropped from memory when the free memory is running low.\n\n  If read cache is disabled (`read_cache_enable=0`), or the read or object is large enough that it will not benefit from caching, the server discards the data after the read request from the client is completed. For subsequent read requests the server again reads the data from storage.\n\n  To disable read cache on all targets of a server, run:\n\n  ```\n   oss1# lctl set_param osd-*.*.read_cache_enable=0\n  ```\n\n  To re-enable read cache on one target, run:\n\n  ```\n   oss1# lctl set_param osd-*.", "mimetype": "text/plain", "start_char_idx": 41030, "end_char_idx": 45314, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1912527-e3e0-4212-8be5-fbe640d49244": {"__data__": {"id_": "e1912527-e3e0-4212-8be5-fbe640d49244", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce9db1af-3c90-40d9-bfe0-be3aaca2470d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7221072c929148eab9a776034d412f8a61e5f1da527d2ab2e0943af06b664e09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16042130-d987-4e09-ab98-c62f1032782a", "node_type": "1", "metadata": {}, "hash": "4f095b756abdd8c5588ea08747f92e4f397257e6b99a7a0120c9082852c9e509", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If later read requests for the same data are received, if the data is still in cache the server skips reading it from storage. The cache is managed by the Linux kernel globally across all targets on that server so that the infrequently used cache pages are dropped from memory when the free memory is running low.\n\n  If read cache is disabled (`read_cache_enable=0`), or the read or object is large enough that it will not benefit from caching, the server discards the data after the read request from the client is completed. For subsequent read requests the server again reads the data from storage.\n\n  To disable read cache on all targets of a server, run:\n\n  ```\n   oss1# lctl set_param osd-*.*.read_cache_enable=0\n  ```\n\n  To re-enable read cache on one target, run:\n\n  ```\n   oss1# lctl set_param osd-*.{target_name}.read_cache_enable=1\n  ```\n\n  To check if read cache is enabled on targets on a server, run:\n\n  ```\n   oss1# lctl get_param osd-*.*.read_cache_enable\n  ```\n\n- `writethrough_cache_enable` - High-level control of whether data sent to the server as a write request is kept in the read cache and available for later reads, or if it is discarded when the write completes. By default, writethrough cache is enabled (`writethrough_cache_enable=1`) for HDD OSDs and automatically disabled for flash OSDs (`nonrotational=1`). The write cache cannot be disabled for `osd-zfs`, and as a result this parameter is unavailable for that backend.\n\n  When the server receives write requests from a client, it fetches data from the client into its memory and writes the data to storage. If the writethrough cache is enabled for the target, and the RPC and object size meet the other criterion below, this data may stay in memory after the write request has completed. If later read or partial-block write requests for this same data are received, if the data is still in cache the server skips reading it from storage.\n\n  If the writethrough cache is disabled (`writethrough_cache_enabled=0`), or the write or object is large enough that it will not benefit from caching, the server discards the data after the write request from the client is completed. For subsequent read requests, or partial-page write requests, the server must re-read the data from storage.\n\n  Enabling writethrough cache is advisable if clients are doing small or unaligned writes that would cause partial-page updates, or if the files written by one node are immediately being read by other nodes. Some examples where enabling writethrough cache might be useful include producer-consumer I/O models or shared-file writes that are not aligned on 4096-byte boundaries.\n\n  Disabling the writethrough cache is advisable when files are mostly written to the file system but are not re-read within a short time period, or files are only written and re-read by the same node, regardless of whether the I/O is aligned or not.\n\n  To disable writethrough cache on all targets on a server, run:\n\n  ```\n   oss1# lctl set_param osd-*.*.writethrough_cache_enable=0\n  ```\n\n  To re-enable the writethrough cache on one OST, run:\n\n  ```\n   oss1# lctl set_param osd-*.{OST_name}.writethrough_cache_enable=1\n  ```\n\n  To check if the writethrough cache is enabled, run:\n\n  ```\n   oss1# lctl get_param osd-*.*.writethrough_cache_enable\n  ```\n\n- `readcache_max_filesize` - Controls the maximum size of an object that both the read cache and writethrough cache will try to keep in memory. Objects larger than `readcache_max_filesize` will not be kept in cache for either reads or writes regardless of the read_cache_enable or `writethrough_cache_enable` settings.\n\n  Setting this tunable can be useful for workloads where relatively small objects are repeatedly accessed by many clients, such as job startup objects, executables, log objects, etc., but large objects are read or written only once. By not putting the larger objects into the cache, it is much more likely that more of the smaller objects will remain in cache for a longer time.\n\n  When setting readcache_max_filesize, the input value can be specified in bytes, or can have a suffix to indicate other binary units such as K (kibibytes), M (mebibytes), G (gibibytes), T (tebibytes), or P (pebibytes).", "mimetype": "text/plain", "start_char_idx": 44505, "end_char_idx": 48725, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16042130-d987-4e09-ab98-c62f1032782a": {"__data__": {"id_": "16042130-d987-4e09-ab98-c62f1032782a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1912527-e3e0-4212-8be5-fbe640d49244", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3c557032dbb03a62e3c5d3ed5e7ee4486539ecdb5a779f753e3d329daf7e91e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "985ee1c8-15ae-4481-b610-1452034f970b", "node_type": "1", "metadata": {}, "hash": "83d2bb62f70414eb5e13eb5d13520b7103acbbcfc68954a5705828361ef20767", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Objects larger than `readcache_max_filesize` will not be kept in cache for either reads or writes regardless of the read_cache_enable or `writethrough_cache_enable` settings.\n\n  Setting this tunable can be useful for workloads where relatively small objects are repeatedly accessed by many clients, such as job startup objects, executables, log objects, etc., but large objects are read or written only once. By not putting the larger objects into the cache, it is much more likely that more of the smaller objects will remain in cache for a longer time.\n\n  When setting readcache_max_filesize, the input value can be specified in bytes, or can have a suffix to indicate other binary units such as K (kibibytes), M (mebibytes), G (gibibytes), T (tebibytes), or P (pebibytes).\n\n  To limit the maximum cached object size to 64 MiB on all OSTs of a server, run:\n\n  ```\n   oss1# lctl set_param osd-*.*.readcache_max_filesize=64M\n  ```\n\n  To disable the maximum cached object size on all targets, run:\n\n  ```\n   oss1# lctl set_param osd-*.*.readcache_max_filesize=-1\n  ```\n\n  To check the current maximum cached object size on all targets of a server, run:\n\n  ```\n   oss1# lctl get_param osd-*.*.readcache_max_filesize\n  ```\n\n* `readcache_max_io_mb` - Controls the maximum size of a single read IO that will be cached in memory. Reads larger than `readcache_max_io_mb` will be read directly from storage and bypass the page cache completely. This avoids significant CPU overhead at high IO rates. The read cache cannot be disabled for osd-zfs, and as a result this parameter is unavailable for that backend.\n\n  When setting `readcache_max_io_mb`, the input value can be specified in mebibytes, or can have a suffix to indicate other binary units such as K (kibibytes), M (mebibytes), G (gibibytes), T (tebibytes), or P (pebibytes).\n\n* `writethrough_max_io_mb` - Controls the maximum size of a single writes IO that will be cached in memory. Writes larger than `writethrough_max_io_mb` will be written directly to storage and bypass the page cache entirely. This avoids significant CPU overhead at high IO rates. The write cache cannot be disabled for `osd-zfs`, and as a result this parameter is unavailable for that backend.\n\n  When setting `writethrough_max_io_mb`, the input value can be specified in mebibytes, or can have a suffix to indicate other binary units such as K (kibibytes), M (mebibytes), G (gibibytes), T (tebibytes), or P (pebibytes).\n\n### Enabling OSS Asynchronous Journal Commit\n\nThe OSS asynchronous journal commit feature asynchronously writes data to disk without forcing a journal flush. This reduces the number of seeks and significantly improves performance on some hardware.\n\n**Note**\n\nAsynchronous journal commit cannot work with direct I/O-originated writes (`O_DIRECT` flag set). In this case, a journal flush is forced.\n\nWhen the asynchronous journal commit feature is enabled, client nodes keep data in the page cache (a page reference). Lustre clients monitor the last committed transaction number (`transno`) in messages sent from the OSS to the clients. When a client sees that the last committed `transno` reported by the OSS is at least equal to the bulk write `transno`, it releases the reference on the corresponding pages. To avoid page references being held for too long on clients after a bulk write, a 7 second ping request is scheduled (the default OSS file system commit time interval is 5 seconds) after the bulk write reply is received, so the OSS has an opportunity to report the last committed `transno`.\n\nIf the OSS crashes before the journal commit occurs, then intermediate data is lost. However, OSS recovery functionality incorporated into the asynchronous journal commit feature causes clients to replay their write requests and compensate for the missing disk updates by restoring the state of the file system.\n\nBy default, `sync_journal` is enabled (`sync_journal=1`), so that journal entries are committed synchronously. To enable asynchronous journal commit, set the `sync_journal` parameter to `0` by entering:\n\n```\n$ lctl set_param obdfilter.", "mimetype": "text/plain", "start_char_idx": 47950, "end_char_idx": 52048, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "985ee1c8-15ae-4481-b610-1452034f970b": {"__data__": {"id_": "985ee1c8-15ae-4481-b610-1452034f970b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16042130-d987-4e09-ab98-c62f1032782a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "9f03574a74e7819c12db704629c41a5cdee19fa089b65f8fe8aab8f17c2ec60c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e31e850a-a069-44c2-a904-0d8f38b57c45", "node_type": "1", "metadata": {}, "hash": "61c82f375661fa50080552d301acb274f508bb791bce8d5bf6f768329e2dee1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To avoid page references being held for too long on clients after a bulk write, a 7 second ping request is scheduled (the default OSS file system commit time interval is 5 seconds) after the bulk write reply is received, so the OSS has an opportunity to report the last committed `transno`.\n\nIf the OSS crashes before the journal commit occurs, then intermediate data is lost. However, OSS recovery functionality incorporated into the asynchronous journal commit feature causes clients to replay their write requests and compensate for the missing disk updates by restoring the state of the file system.\n\nBy default, `sync_journal` is enabled (`sync_journal=1`), so that journal entries are committed synchronously. To enable asynchronous journal commit, set the `sync_journal` parameter to `0` by entering:\n\n```\n$ lctl set_param obdfilter.*.sync_journal=0 \nobdfilter.lol-OST0001.sync_journal=0\n```\n\nAn associated `sync-on-lock-cancel` feature (enabled by default) addresses a data consistency issue that can result if an OSS crashes after multiple clients have written data into intersecting regions of an object, and then one of the clients also crashes. A condition is created in which the POSIX requirement for continuous writes is violated along with a potential for corrupted data. With `sync-on-lock-cancel` enabled, if a cancelled lock has any volatile writes attached to it, the OSS synchronously writes the journal to disk on lock cancellation. Disabling the `sync-on-lock-cancel` feature may enhance performance for concurrent write workloads, but it is recommended that you not disable this feature.\n\nThe `sync_on_lock_cancel` parameter can be set to the following values:\n\n- `always` - Always force a journal flush on lock cancellation (default when `async_journal` is enabled).\n- `blocking` - Force a journal flush only when the local cancellation is due to a blocking callback.\n- `never` - Do not force any journal flush (default when `async_journal` is disabled).\n\nFor example, to set `sync_on_lock_cancel` to not to force a journal flush, use a command similar to:\n\n```\n$ lctl get_param obdfilter.*.sync_on_lock_cancel\nobdfilter.lol-OST0001.sync_on_lock_cancel=never\n```\n\nIntroduced in Lustre 2.8\n\n### Tuning the Client Metadata RPC Stream\n\nThe client metadata RPC stream represents the metadata RPCs issued in parallel by a client to a MDT target. The metadata RPCs can be split in two categories: the requests that do not modify the file system (like getattr operation), and the requests that do modify the file system (like create, unlink, setattr operations). To help optimize the client metadata RPC stream, several tuning variables are provided to adjust behavior according to network conditions and cluster size.\n\nNote that increasing the number of metadata RPCs issued in parallel might improve the performance metadata intensive parallel applications, but as a consequence it will consume more memory on the client and on the MDS.\n\n#### Configuring the Client Metadata RPC Stream\n\nThe MDC `max_rpcs_in_flight` parameter defines the maximum number of metadata RPCs, both modifying and non-modifying RPCs, that can be sent in parallel by a client to a MDT target. This includes every file system metadata operations, such as file or directory stat, creation, unlink. The default setting is 8, minimum setting is 1 and maximum setting is 256.\n\nTo set the `max_rpcs_in_flight` parameter, run the following command on the Lustre client:\n\n```\nclient$ lctl set_param mdc.*.max_rpcs_in_flight=16\n```\n\nThe MDC `max_mod_rpcs_in_flight` parameter defines the maximum number of file system modifying RPCs that can be sent in parallel by a client to a MDT target. For example, the Lustre client sends modify RPCs when it performs file or directory creation, unlink, access permission modification or ownership modification. The default setting is 7, minimum setting is 1 and maximum setting is 256.\n\nTo set the `max_mod_rpcs_in_flight` parameter, run the following command on the Lustre client:\n\n```\nclient$ lctl set_param mdc.*.max_mod_rpcs_in_flight=12\n```\n\nThe `max_mod_rpcs_in_flight` value must be strictly less than the `max_rpcs_in_flight` value. It must also be less or equal to the MDT `max_mod_rpcs_per_client` value. If one of theses conditions is not enforced, the setting fails and an explicit message is written in the Lustre log.", "mimetype": "text/plain", "start_char_idx": 51208, "end_char_idx": 55565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e31e850a-a069-44c2-a904-0d8f38b57c45": {"__data__": {"id_": "e31e850a-a069-44c2-a904-0d8f38b57c45", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "985ee1c8-15ae-4481-b610-1452034f970b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3b0d0f6b563f100f5835ca3cf404cabf004628890319168209f92742f358441f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99e9aaa6-a99c-4893-8e80-494b3ab92314", "node_type": "1", "metadata": {}, "hash": "012a55c764c4eba83a14e1a7c40cbf0cd4f74eab5001cbe291c087462d2230ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, the Lustre client sends modify RPCs when it performs file or directory creation, unlink, access permission modification or ownership modification. The default setting is 7, minimum setting is 1 and maximum setting is 256.\n\nTo set the `max_mod_rpcs_in_flight` parameter, run the following command on the Lustre client:\n\n```\nclient$ lctl set_param mdc.*.max_mod_rpcs_in_flight=12\n```\n\nThe `max_mod_rpcs_in_flight` value must be strictly less than the `max_rpcs_in_flight` value. It must also be less or equal to the MDT `max_mod_rpcs_per_client` value. If one of theses conditions is not enforced, the setting fails and an explicit message is written in the Lustre log.\n\nThe MDT `max_mod_rpcs_per_client` parameter is a tunable of the kernel module `mdt` that defines the maximum number of file system modifying RPCs in flight allowed per client. The parameter can be updated at runtime, but the change is effective to new client connections only. The default setting is 8.\n\nTo set the `max_mod_rpcs_per_client` parameter, run the following command on the MDS:\n\n```\nmds$ echo 12 > /sys/module/mdt/parameters/max_mod_rpcs_per_client\n```\n\n#### Monitoring the Client Metadata RPC Stream\n\nThe `rpc_stats` file contains histogram data showing information about modify metadata RPCs. It can be helpful to identify the level of parallelism achieved by an application doing modify metadata operations.\n\n**Example:**\n\n```\nclient$ lctl get_param mdc.*.rpc_stats\nsnapshot_time:         1441876896.567070 (secs.usecs)\nmodify_RPCs_in_flight:  0\n\n                        modify\nrpcs in flight        rpcs   % cum %\n0:                       0   0   0\n1:                      56   0   0\n2:                      40   0   0\n3:                      70   0   0\n4                       41   0   0\n5:                      51   0   1\n6:                      88   0   1\n7:                     366   1   2\n8:                    1321   5   8\n9:                    3624  15  23\n10:                   6482  27  50\n11:                   7321  30  81\n12:                   4540  18 100\n```\n\nThe file information includes:\n\n- `snapshot_time` - UNIX epoch instant the file was read.\n- `modify_RPCs_in_flight` - Number of modify RPCs issued by the MDC, but not completed at the time of the snapshot. This value should always be less than or equal to `max_mod_rpcs_in_flight`.\n- `rpcs in flight` - Number of modify RPCs that are pending when a RPC is sent, the relative percentage (`%`) of total modify RPCs, and the cumulative percentage (`cum %`) to that point.\n\nIf a large proportion of modify metadata RPCs are issued with a number of pending metadata RPCs close to the`max_mod_rpcs_in_flight` value, it means the `max_mod_rpcs_in_flight` value could be increased to improve the modify metadata performance.\n\n## Configuring Timeouts in a Lustre File System\n\nIn a Lustre file system, RPC timeouts are set using an adaptive timeouts mechanism, which is enabled by default. Servers track RPC completion times and then report back to clients estimates for completion times for future RPCs. Clients use these estimates to set RPC timeout values. If the processing of server requests slows down for any reason, the server estimates for RPC completion increase, and clients then revise RPC timeout values to allow more time for RPC completion.\n\nIf the RPCs queued on the server approach the RPC timeout specified by the client, to avoid RPC timeouts and disconnect/reconnect cycles, the server sends an \"early reply\" to the client, telling the client to allow more time. Conversely, as server processing speeds up, RPC timeout values decrease, resulting in faster detection if the server becomes non-responsive and quicker connection to the failover partner of the server.\n\n### Configuring Adaptive Timeouts\n\nThe adaptive timeout parameters in the table below can be set persistently system-wide using `lctl conf_param` on the MGS.", "mimetype": "text/plain", "start_char_idx": 54885, "end_char_idx": 58790, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99e9aaa6-a99c-4893-8e80-494b3ab92314": {"__data__": {"id_": "99e9aaa6-a99c-4893-8e80-494b3ab92314", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e31e850a-a069-44c2-a904-0d8f38b57c45", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0432aa9a855125ed057133db7e2bc490490dd1fd316be2c977bedf6c732acf46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ecf32d5-1f81-4826-a762-dd2328a8f8c0", "node_type": "1", "metadata": {}, "hash": "ce03d4bf0a44ce6bc88173b9f3d8850f1a6a1135d0e308a9160c7589d04ebff0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Servers track RPC completion times and then report back to clients estimates for completion times for future RPCs. Clients use these estimates to set RPC timeout values. If the processing of server requests slows down for any reason, the server estimates for RPC completion increase, and clients then revise RPC timeout values to allow more time for RPC completion.\n\nIf the RPCs queued on the server approach the RPC timeout specified by the client, to avoid RPC timeouts and disconnect/reconnect cycles, the server sends an \"early reply\" to the client, telling the client to allow more time. Conversely, as server processing speeds up, RPC timeout values decrease, resulting in faster detection if the server becomes non-responsive and quicker connection to the failover partner of the server.\n\n### Configuring Adaptive Timeouts\n\nThe adaptive timeout parameters in the table below can be set persistently system-wide using `lctl conf_param` on the MGS. For example, the following command sets the `at_max` value for all servers and clients associated with the file system `testfs`:\n\n```\nlctl conf_param testfs.sys.at_max=1500\n```\n\n**Note**\n\nClients that access multiple Lustre file systems must use the same parameter values for all file systems.\n\n| **Parameter**      | **Description**                                              |\n| ------------------ | ------------------------------------------------------------ |\n| `at_min`           | Minimum adaptive timeout (in seconds). The default value is 0. The `at_min` parameter is the minimum processing time that a server will report. Ideally, `at_min` should be set to its default value. Clients base their timeouts on this value, but they do not use this value directly.If, for unknown reasons (usually due to temporary network outages), the adaptive timeout value is too short and clients time out their RPCs, you can increase the `at_min` value to compensate for this. |\n| `at_max`           | Maximum adaptive timeout (in seconds). The `at_max` parameter is an upper-limit on the service time estimate. If `at_max` is reached, an RPC request times out.Setting `at_max` to 0 causes adaptive timeouts to be disabled and a fixed timeout method to be used instead (see [*the section called \u201cSetting Static Timeouts\u201d*](#setting-static-timeouts)                                                                                                   **Note**                                                                                                                                                                                                                                             If slow hardware causes the service estimate to increase beyond the default value of `at_max`, increase `at_max` to the maximum time you are willing to wait for an RPC completion. |\n| `at_history`       | Time period (in seconds) within which adaptive timeouts remember the slowest event that occurred. The default is 600. |\n| `at_early_margin`  | Amount of time before the Lustre server sends an early reply (in seconds). Default is 5. |\n| `at_extra`         | Incremental amount of time that a server requests with each early reply (in seconds). The server does not know how much time the RPC will take, so it asks for a fixed value. The default is 30, which provides a balance between sending too many early replies for the same RPC and overestimating the actual completion time.When a server finds a queued request about to time out and needs to send an early reply out, the server adds the `at_extra` value. If the time expires, the Lustre server drops the request, and the client enters recovery status and reconnects to restore the connection to normal status.If you see multiple early replies for the same RPC asking for 30-second increases, change the `at_extra` value to a larger number to cut down on early replies sent and, therefore, network load. |\n| `ldlm_enqueue_min` | Minimum lock enqueue time (in seconds). The default is 100. The time it takes to enqueue a lock, `ldlm_enqueue`, is the maximum of the measured enqueue estimate (influenced by `at_min` and `at_max` parameters), multiplied by a weighting factor and the value of `ldlm_enqueue_min`.Lustre Distributed Lock Manager (LDLM) lock enqueues have a dedicated minimum value for `ldlm_enqueue_min`. Lock enqueue timeouts increase as the measured enqueue times increase (similar to adaptive timeouts). |\n\n#### Interpreting Adaptive Timeout Information\n\nAdaptive timeout information can be obtained via `lctl get_param {osc,mdc}.*.timeouts` files on each client and `lctl get_param {ost,mds}.*.*.timeouts` on each server. To read information from a `timeouts` file, enter a command similar to:\n\n```\n# lctl get_param -n ost.", "mimetype": "text/plain", "start_char_idx": 57837, "end_char_idx": 62574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ecf32d5-1f81-4826-a762-dd2328a8f8c0": {"__data__": {"id_": "0ecf32d5-1f81-4826-a762-dd2328a8f8c0", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99e9aaa6-a99c-4893-8e80-494b3ab92314", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "483c50cecf3eee812e274e62c1a3fcb6a705f01fffe5f698109ef6ea5a659bfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afbef011-e553-4f2e-8b82-95c02380243b", "node_type": "1", "metadata": {}, "hash": "494caabd6729e43faddcce4575f5d32f2d9057d2a1f67816718d62d6f8b96ada", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The default is 100. The time it takes to enqueue a lock, `ldlm_enqueue`, is the maximum of the measured enqueue estimate (influenced by `at_min` and `at_max` parameters), multiplied by a weighting factor and the value of `ldlm_enqueue_min`.Lustre Distributed Lock Manager (LDLM) lock enqueues have a dedicated minimum value for `ldlm_enqueue_min`. Lock enqueue timeouts increase as the measured enqueue times increase (similar to adaptive timeouts). |\n\n#### Interpreting Adaptive Timeout Information\n\nAdaptive timeout information can be obtained via `lctl get_param {osc,mdc}.*.timeouts` files on each client and `lctl get_param {ost,mds}.*.*.timeouts` on each server. To read information from a `timeouts` file, enter a command similar to:\n\n```\n# lctl get_param -n ost.*.ost_io.timeouts\nservice : cur 33  worst 34 (at 1193427052, 1600s ago) 1 1 33 2\n```\n\nIn this example, the `ost_io` service on this node is currently reporting an estimated RPC service time of 33 seconds. The worst RPC service time was 34 seconds, which occurred 26 minutes ago.\n\nThe output also provides a history of service times. Four \"bins\" of adaptive timeout history are shown, with the maximum RPC time in each bin reported. In both the 0-150s bin and the 150-300s bin, the maximum RPC time was 1. The 300-450s bin shows the worst (maximum) RPC time at 33 seconds, and the 450-600s bin shows a maximum of RPC time of 2 seconds. The estimated service time is the maximum value in the four bins (33 seconds in this example).\n\nService times (as reported by the servers) are also tracked in the client OBDs, as shown in this example:\n\n```\n# lctl get_param osc.*.timeouts\nlast reply : 1193428639, 0d0h00m00s ago\nnetwork    : cur  1 worst  2 (at 1193427053, 0d0h26m26s ago)  1  1  1  1\nportal 6   : cur 33 worst 34 (at 1193427052, 0d0h26m27s ago) 33 33 33  2\nportal 28  : cur  1 worst  1 (at 1193426141, 0d0h41m38s ago)  1  1  1  1\nportal 7   : cur  1 worst  1 (at 1193426141, 0d0h41m38s ago)  1  0  1  1\nportal 17  : cur  1 worst  1 (at 1193426177, 0d0h41m02s ago)  1  0  0  1\n```\n\nIn this example, portal 6, the `ost_io` service portal, shows the history of service estimates reported by the portal.\n\nServer statistic files also show the range of estimates including min, max, sum, and sum-squared. For example:\n\n```\n# lctl get_param mdt.*.mdt.stats\n...\nreq_timeout               6 samples [sec] 1 10 15 105\n...\n```\n\n### Setting Static Timeouts\n\nThe Lustre software provides two sets of static (fixed) timeouts, LND timeouts and Lustre timeouts, which are used when adaptive timeouts are not enabled.\n\n- **LND timeouts** - LND timeouts ensure that point-to-point communications across a network complete in a finite time in the presence of failures, such as packages lost or broken connections. LND timeout parameters are set for each individual LND.\n\n  LND timeouts are logged with the `S_LND` flag set. They are not printed as console messages, so check the Lustre log for `D_NETERROR` messages or enable printing of `D_NETERROR` messages to the console using:\n\n  ```\n  lctl set_param printk=+neterror\n  ```\n\n  Congested routers can be a source of spurious LND timeouts. To avoid this situation, increase the number of LNet router buffers to reduce back-pressure and/or increase LND timeouts on all nodes on all connected networks. Also consider increasing the total number of LNet router nodes in the system so that the aggregate router bandwidth matches the aggregate server bandwidth.", "mimetype": "text/plain", "start_char_idx": 61804, "end_char_idx": 65266, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afbef011-e553-4f2e-8b82-95c02380243b": {"__data__": {"id_": "afbef011-e553-4f2e-8b82-95c02380243b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ecf32d5-1f81-4826-a762-dd2328a8f8c0", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "eb113850be1aee53b2814436fb6536dca867c2b0548111f9223529ada4a64d7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1428a947-0b91-4c1b-9042-05dc93c55c0f", "node_type": "1", "metadata": {}, "hash": "9e5790fd07de87603c0efecafab48676a5913f19702158161c65e4cd122b3b63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **LND timeouts** - LND timeouts ensure that point-to-point communications across a network complete in a finite time in the presence of failures, such as packages lost or broken connections. LND timeout parameters are set for each individual LND.\n\n  LND timeouts are logged with the `S_LND` flag set. They are not printed as console messages, so check the Lustre log for `D_NETERROR` messages or enable printing of `D_NETERROR` messages to the console using:\n\n  ```\n  lctl set_param printk=+neterror\n  ```\n\n  Congested routers can be a source of spurious LND timeouts. To avoid this situation, increase the number of LNet router buffers to reduce back-pressure and/or increase LND timeouts on all nodes on all connected networks. Also consider increasing the total number of LNet router nodes in the system so that the aggregate router bandwidth matches the aggregate server bandwidth.\n\n- **Lustre timeouts** - Lustre timeouts ensure that Lustre RPCs complete in a finite time in the presence of failures when adaptive timeouts are not enabled. Adaptive timeouts are enabled by default. To disable adaptive timeouts at run time, set `at_max` to 0 by running on the MGS:\n\n  ```\n  # lctl conf_param fsname.sys.at_max=0\n  ```\n\n  ### Note\n\n  Changing the status of adaptive timeouts at runtime may cause a transient client timeout, recovery, and reconnection.\n\n  Lustre timeouts are always printed as console messages.\n\n  If Lustre timeouts are not accompanied by LND timeouts, increase the Lustre timeout on both servers and clients. Lustre timeouts are set using a command such as the following:\n\n  ```\n  # lctl set_param timeout=30\n  ```\n\n  Lustre timeout parameters are described in the table below.\n\n| Parameter          | Description                                                  |\n| ------------------ | ------------------------------------------------------------ |\n| `timeout`          | The time that a client waits for a server to complete an RPC (default 100s). Servers wait half this time for a normal client RPC to complete and a quarter of this time for a single bulk request (read or write of up to 4 MB) to complete. The client pings recoverable targets (MDS and OSTs) at one quarter of the timeout, and the server waits one and a half times the timeout before evicting a client for being \"stale.\"Lustre client sends periodic 'ping' messages to servers with which it has had no communication for the specified period of time. Any network activity between a client and a server in the file system also serves as a ping. |\n| `ldlm_timeout`     | The time that a server waits for a client to reply to an initial AST (lock cancellation request). The default is 20s for an OST and 6s for an MDS. If the client replies to the AST, the server will give it a normal timeout (half the client timeout) to flush any dirty data and release the lock. |\n| `fail_loc`         | An internal debugging failure hook. The default value of `0` means that no failure will be triggered or injected. |\n| `dump_on_timeout`  | Triggers a dump of the Lustre debug log when a timeout occurs. The default value of `0` (zero) means a dump of the Lustre debug log will not be triggered. |\n| `dump_on_eviction` | Triggers a dump of the Lustre debug log when an eviction occurs. The default value of `0`(zero) means a dump of the Lustre debug log will not be triggered. |\n\n## Monitoring LNet\n\nLNet information is located via `lctl get_param` in these parameters:\n\n- `peers` - Shows all NIDs known to this node and provides information on the queue state.", "mimetype": "text/plain", "start_char_idx": 64379, "end_char_idx": 67919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1428a947-0b91-4c1b-9042-05dc93c55c0f": {"__data__": {"id_": "1428a947-0b91-4c1b-9042-05dc93c55c0f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afbef011-e553-4f2e-8b82-95c02380243b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "fb69f63bc82e10c904e318b6e6cc98a2422fb5d14c5ce9a3371c3686d7d17353", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f98d5f2-3512-45bc-8316-047df59c3bfe", "node_type": "1", "metadata": {}, "hash": "42898bc64aeda52a831129714d1dfaba4c0656e71fcc9fb3b97566d8d54e549e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `fail_loc`         | An internal debugging failure hook. The default value of `0` means that no failure will be triggered or injected. |\n| `dump_on_timeout`  | Triggers a dump of the Lustre debug log when a timeout occurs. The default value of `0` (zero) means a dump of the Lustre debug log will not be triggered. |\n| `dump_on_eviction` | Triggers a dump of the Lustre debug log when an eviction occurs. The default value of `0`(zero) means a dump of the Lustre debug log will not be triggered. |\n\n## Monitoring LNet\n\nLNet information is located via `lctl get_param` in these parameters:\n\n- `peers` - Shows all NIDs known to this node and provides information on the queue state.\n\n  Example:\n\n  ```\n  # lctl get_param peers\n  nid                refs   state  max  rtr  min   tx    min   queue\n  0@lo               1      ~rtr   0    0    0     0     0     0\n  192.168.10.35@tcp  1      ~rtr   8    8    8     8     6     0\n  192.168.10.36@tcp  1      ~rtr   8    8    8     8     6     0\n  192.168.10.37@tcp  1      ~rtr   8    8    8     8     6     0\n  ```\n\n  The fields are explained in the table below:\n\n  | **Field** | **Description**                                              |\n  | --------- | ------------------------------------------------------------ |\n  | `refs`    | A reference count.                                           |\n  | `state`   | If the node is a router, indicates the state of the router. Possible values are:`NA` - Indicates the node is not a router.`up/down`- Indicates if the node (router) is up or down. |\n  | `max`     | Maximum number of concurrent sends from this peer.           |\n  | `rtr`     | Number of available routing buffer credits.                  |\n  | `min`     | Minimum number of routing buffer credits seen.               |\n  | `tx`      | Number of available send credits.                            |\n  | `min`     | Minimum number of send credits seen.                         |\n  | `queue`   | Total bytes in active/queued sends.                          |\n\n  Credits are initialized to allow a certain number of operations (in the example above the table, eight as shown in the max column. LNet keeps track of the minimum number of credits ever seen over time showing the peak congestion that has occurred during the time monitored. Fewer available credits indicates a more congested resource.\n\n  The number of credits currently available is shown in the tx column. The maximum number of send credits is shown in the max column and never changes. The number of currently active transmits can be derived by (max - tx), as long as tx is greater than or equal to 0. Once tx is less than 0, it indicates the number of transmits on that peer which have been queued for lack of credits.\n\n  The number of router buffer credits available for consumption by a peer is shown in rtr column. The number of routing credits can be configured separately at the LND level or at the LNet level by using the `peer_buffer_credits` module parameter for the appropriate module. If the routing credits is not set explicitly, it'll default to the maximum transmit credits defined by peer_credits module parameter. Whenever a gateway routes a message from a peer, it decrements the number of available routing credits for that peer. If that value goes to zero, then messages will be queued. Negative values show the number of queued message waiting to be routed. The number of messages which are currently being routed from a peer can be derived by `(max_rtr_credits - rtr)`.\n\n  LNet also limits concurrent sends and number of router buffers allocated to a single peer so that no peer can occupy all these resources.\n\n- `nis` - Shows current queue health on the node.\n\n  Example:\n\n  ```\n  # lctl get_param nis\n  nid                    refs   peer    max   tx    min\n  0@lo                   3      0       0     0     0\n  192.168.10.34@tcp      4      8       256   256   252\n  ```\n\n  The fields are explained in the table below.", "mimetype": "text/plain", "start_char_idx": 67235, "end_char_idx": 71205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f98d5f2-3512-45bc-8316-047df59c3bfe": {"__data__": {"id_": "8f98d5f2-3512-45bc-8316-047df59c3bfe", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1428a947-0b91-4c1b-9042-05dc93c55c0f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1d28e46135e4a896ec20235cc2acf7c95cbad4ce2b2cc888e87dc37e4c6487a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3d99e67-2928-4caf-8e15-b2e796767ce5", "node_type": "1", "metadata": {}, "hash": "528261c12181b44b2956dd6479b426f8464a0eadebc7d22b110f7f15acf45b61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If that value goes to zero, then messages will be queued. Negative values show the number of queued message waiting to be routed. The number of messages which are currently being routed from a peer can be derived by `(max_rtr_credits - rtr)`.\n\n  LNet also limits concurrent sends and number of router buffers allocated to a single peer so that no peer can occupy all these resources.\n\n- `nis` - Shows current queue health on the node.\n\n  Example:\n\n  ```\n  # lctl get_param nis\n  nid                    refs   peer    max   tx    min\n  0@lo                   3      0       0     0     0\n  192.168.10.34@tcp      4      8       256   256   252\n  ```\n\n  The fields are explained in the table below.\n\n  | **Field** | **Description**                                              |\n  | --------- | ------------------------------------------------------------ |\n  | `nid`     | Network interface.                                           |\n  | `refs`    | Internal reference counter.                                  |\n  | `peer`    | Number of peer-to-peer send credits on this NID. Credits are used to size buffer pools. |\n  | `max`     | Total number of send credits on this NID.                    |\n  | `tx`      | Current number of send credits available on this NID.        |\n  | `min`     | Lowest number of send credits available on this NID.         |\n  | `queue`   | Total bytes in active/queued sends.                          |\n\n  **Analysis:**\n\n  Subtracting `max` from `tx` (`max` - `tx`) yields the number of sends currently active. A large or increasing number of active sends may indicate a problem.\n\n## Allocating Free Space on OSTs\n\n  Free space is allocated using either a round-robin or a weighted algorithm. The allocation method is determined by the maximum amount of free-space imbalance between the OSTs. When free space is relatively balanced across OSTs, the faster round-robin allocator is used, which maximizes network balancing. The weighted allocator is used when any two OSTs are out of balance by more than a specified threshold.\n\n  Free space distribution can be tuned using these two tunable parameters:\n\n  - `lod.*.qos_threshold_rr` - The threshold at which the allocation method switches from round-robin to weighted is set in this file. The default is to switch to the weighted algorithm when any two OSTs are out of balance by more than 17 percent.\n  - `lod.*.qos_prio_free` - The weighting priority used by the weighted allocator can be adjusted in this file. Increasing the value of `qos_prio_free` puts more weighting on the amount of free space available on each OST and less on how stripes are distributed across OSTs. The default value is 91 percent weighting for free space rebalancing and 9 percent for OST balancing. When the free space priority is set to 100, weighting is based entirely on free space and location is no longer used by the striping algorithm.\n  - Introduced in Lustre 2.9`osp.*.reserved_mb_low` - The low watermark used to stop object allocation if available space is less than this. The default is 0.1% of total OST size.\n  - Introduced in Lustre 2.9`osp.*.reserved_mb_high` - The high watermark used to start object allocation if available space is more than this. The default is 0.2% of total OST size.\n\n  For more information about monitoring and managing free space, see [*the section called \u201cManaging Free Space\u201d*](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space).\n\n## Configuring Locking\n\n  The `lru_size` parameter is used to control the number of client-side locks in the LRU cached locks queue. LRU size is normally dynamic, based on load to optimize the number of locks cached on nodes that have different workloads (e.g., login/build nodes vs. compute nodes vs. backup nodes).\n\n  The total number of locks available is a function of the server RAM. The default limit is 50 locks/1 MB of RAM. If memory pressure is too high, the LRU size is shrunk. The number of locks on the server is limited to*num_osts_per_oss \\* num_clients * lru_size* as follows:\n\n  - To enable automatic LRU sizing, set the `lru_size` parameter to 0.", "mimetype": "text/plain", "start_char_idx": 70509, "end_char_idx": 74646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3d99e67-2928-4caf-8e15-b2e796767ce5": {"__data__": {"id_": "a3d99e67-2928-4caf-8e15-b2e796767ce5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f98d5f2-3512-45bc-8316-047df59c3bfe", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "93912e96f93f592eec3c27c4ed4690dad21e9030c3b6106777f977631a4a9210", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8421f599-df9f-4940-b7e8-102d2fe80f2a", "node_type": "1", "metadata": {}, "hash": "6d88eac7547552fe153377e746007ab98f5612d116914a4f900a5502b87bf4eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Configuring Locking\n\n  The `lru_size` parameter is used to control the number of client-side locks in the LRU cached locks queue. LRU size is normally dynamic, based on load to optimize the number of locks cached on nodes that have different workloads (e.g., login/build nodes vs. compute nodes vs. backup nodes).\n\n  The total number of locks available is a function of the server RAM. The default limit is 50 locks/1 MB of RAM. If memory pressure is too high, the LRU size is shrunk. The number of locks on the server is limited to*num_osts_per_oss \\* num_clients * lru_size* as follows:\n\n  - To enable automatic LRU sizing, set the `lru_size` parameter to 0. In this case, the `lru_size` parameter shows the current number of locks being used on the client. Dynamic LRU resizing is enabled by default.\n  - To specify a maximum number of locks, set the `lru_size` parameter to a value other than zero. A good default value for compute nodes is around `100 * *num_cpus*`. It is recommended that you only set `lru_size` to be signifivantly larger on a few login nodes where multiple users access the file system interactively.\n\n  To clear the LRU on a single client, and, as a result, flush client cache without changing the `lru_size` value, run:\n\n  ```\n  # lctl set_param ldlm.namespaces.osc_name|mdc_name.lru_size=clear\n  ```\n\n  If the LRU size is set lower than the number of existing locks, *unused* locks are canceled immediately. Use `clear` to cancel all locks without changing the value.\n\n  **Note**\n\n  The `lru_size` parameter can only be set temporarily using `lctl set_param`, it cannot be set permanently.\n\n  To disable dynamic LRU resizing on the clients, run for example:\n\n  ```\n  # lctl set_param ldlm.namespaces.*osc*.lru_size=5000\n  ```\n\n  To determine the number of locks being granted with dynamic LRU resizing, run:\n\n  ```\n  $ lctl get_param ldlm.namespaces.*.pool.limit\n  ```\n\n  The `lru_max_age` parameter is used to control the age of client-side locks in the LRU cached locks queue. This limits how long unused locks are cached on the client, and avoids idle clients from holding locks for an excessive time, which reduces memory usage on both the client and server, as well as reducing work during server recovery.\n\n  The `lru_max_age` is set and printed in milliseconds, and by default is 3900000 ms (65 minutes).\n\n  Introduced in Lustre 2.11Since Lustre 2.11, in addition to setting the maximum lock age in milliseconds, it can also be set using a suffix of `s`or `ms` to indicate seconds or milliseconds, respectively. For example to set the client's maximum lock age to 15 minutes (900s) run:\n\n  ```\n  # lctl set_param ldlm.namespaces.*MDT*.lru_max_age=900s\n  # lctl get_param ldlm.namespaces.*MDT*.lru_max_age\n  ldlm.namespaces.myth-MDT0000-mdc-ffff8804296c2800.lru_max_age=900000\n  ```\n\n## Setting MDS and OSS Thread Counts\n\n  MDS and OSS thread counts tunable can be used to set the minimum and maximum thread counts or get the current number of running threads for the services listed in the table below.\n\n| **Service**                  | **Description**                             |\n| ---------------------------- | ------------------------------------------- |\n| `mds.MDS.mdt`                | Main metadata operations service            |\n| `mds.MDS.mdt_readpage`       | Metadata `readdir` service                  |\n| `mds.MDS.mdt_setattr`        | Metadata `setattr/close` operations service |\n| `ost.OSS.ost`                | Main data operations service                |\n| `ost.OSS.ost_io`             | Bulk data I/O services                      |\n| `ost.OSS.ost_create`         | OST object pre-creation service             |\n| `ldlm.services.ldlm_canceld` | DLM lock cancel service                     |\n| `ldlm.services.ldlm_cbd`     | DLM lock grant service                      |\n\n  For each service, tunable parameters as shown below are available.", "mimetype": "text/plain", "start_char_idx": 73983, "end_char_idx": 77884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8421f599-df9f-4940-b7e8-102d2fe80f2a": {"__data__": {"id_": "8421f599-df9f-4940-b7e8-102d2fe80f2a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3d99e67-2928-4caf-8e15-b2e796767ce5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "992b2718fc3c2a15718af5c9fa9bcee6b294f40204dda6cc3df8b7e6478e9fba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8dbfe61-bcfd-43c4-a0dc-892717c00341", "node_type": "1", "metadata": {}, "hash": "7c3d4f2a512a1c301bc258af6cc8379453c2589005419c52b1b8f30365c8d618", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| **Service**                  | **Description**                             |\n| ---------------------------- | ------------------------------------------- |\n| `mds.MDS.mdt`                | Main metadata operations service            |\n| `mds.MDS.mdt_readpage`       | Metadata `readdir` service                  |\n| `mds.MDS.mdt_setattr`        | Metadata `setattr/close` operations service |\n| `ost.OSS.ost`                | Main data operations service                |\n| `ost.OSS.ost_io`             | Bulk data I/O services                      |\n| `ost.OSS.ost_create`         | OST object pre-creation service             |\n| `ldlm.services.ldlm_canceld` | DLM lock cancel service                     |\n| `ldlm.services.ldlm_cbd`     | DLM lock grant service                      |\n\n  For each service, tunable parameters as shown below are available.\n\n  - To temporarily set these tunables, run:\n\n    ```\n    # lctl set_param service.threads_min|max|started=num \n    ```\n\n  - To permanently set this tunable, run:\n\n    ```\n    # lctl conf_param obdname|fsname.obdtype.threads_min|max|started \n    ```\n\n    Introduced in Lustre 2.5For version 2.5 or later, run:`# lctl set_param -P *service*.threads_*min|max|started*`\n\n  The following examples show how to set thread counts and get the number of running threads for the service `ost_io` using the tunable `*service*.threads_*min|max|started*`.\n\n  - To get the number of running threads, run:\n\n    ```\n    # lctl get_param ost.OSS.ost_io.threads_started\n    ost.OSS.ost_io.threads_started=128\n    ```\n\n  - To set the number of threads to the maximum value (512), run:\n\n    ```\n    # lctl get_param ost.OSS.ost_io.threads_max\n    ost.OSS.ost_io.threads_max=512\n    ```\n\n  - To set the maximum thread count to 256 instead of 512 (to avoid overloading the storage or for an array with requests), run:\n\n    ```\n    # lctl set_param ost.OSS.ost_io.threads_max=256\n    ost.OSS.ost_io.threads_max=256\n    ```\n\n  - To set the maximum thread count to 256 instead of 512 permanently, run:\n\n    ```\n    # lctl conf_param testfs.ost.ost_io.threads_max=256\n    ```\n\n    Introduced in Lustre 2.5For version 2.5 or later, run:`# lctl set_param -P ost.OSS.ost_io.threads_max=256 ost.OSS.ost_io.threads_max=256 `\n\n  - To check if the `threads_max` setting is active, run:\n\n    ```\n    # lctl get_param ost.OSS.ost_io.threads_max\n    ost.OSS.ost_io.threads_max=256\n    ```\n\n  **Note**\n\n  If the number of service threads is changed while the file system is running, the change may not take effect until the file system is stopped and rest. If the number of service threads in use exceeds the new `threads_max` value setting, service threads that are already running will not be stopped.\n\n  See also [*Tuning a Lustre File System*](04.03-Tuning%20a%20Lustre%20File%20System.md)\n\n## Enabling and Interpreting Debugging Logs\n\nBy default, a detailed log of all operations is generated to aid in debugging. Flags that control debugging are found via `lctl get_param debug`.\n\nThe overhead of debugging can affect the performance of Lustre file system. Therefore, to minimize the impact on performance, the debug level can be lowered, which affects the amount of debugging information kept in the internal log buffer but does not alter the amount of information to goes into syslog. You can raise the debug level when you need to collect logs to debug problems.\n\nThe debugging mask can be set using \"symbolic names\". The symbolic format is shown in the examples below.", "mimetype": "text/plain", "start_char_idx": 77025, "end_char_idx": 80526, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8dbfe61-bcfd-43c4-a0dc-892717c00341": {"__data__": {"id_": "f8dbfe61-bcfd-43c4-a0dc-892717c00341", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8421f599-df9f-4940-b7e8-102d2fe80f2a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ea8b0ef69d644fa123682ba4b6f3e0d6f8f3fae32a8bdd797f0bb012af2a1942", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef810dae-afea-41ef-9c96-acb5c588b910", "node_type": "1", "metadata": {}, "hash": "32d31c67224da92481cfae69e3285c617f83667605dcd174c18738272acc35fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See also [*Tuning a Lustre File System*](04.03-Tuning%20a%20Lustre%20File%20System.md)\n\n## Enabling and Interpreting Debugging Logs\n\nBy default, a detailed log of all operations is generated to aid in debugging. Flags that control debugging are found via `lctl get_param debug`.\n\nThe overhead of debugging can affect the performance of Lustre file system. Therefore, to minimize the impact on performance, the debug level can be lowered, which affects the amount of debugging information kept in the internal log buffer but does not alter the amount of information to goes into syslog. You can raise the debug level when you need to collect logs to debug problems.\n\nThe debugging mask can be set using \"symbolic names\". The symbolic format is shown in the examples below.\n\n- To verify the debug level used, examine the parameter that controls debugging by running:\n\n  ```\n  # lctl get_param debug \n  debug=\n  ioctl neterror warning error emerg ha config console\n  ```\n\n- To turn off debugging except for network error debugging, run the following command on all nodes concerned:\n\n  ```\n  # sysctl -w lnet.debug=\"neterror\" \n  debug=neterror\n  ```\n\n- To turn off debugging completely (except for the minimum error reporting to the console), run the following command on all nodes concerned:\n\n  ```\n  # lctl set_param debug=0 \n  debug=0\n  ```\n\n- To set an appropriate debug level for a production environment, run:\n\n  ```\n  # lctl set_param debug=\"warning dlmtrace error emerg ha rpctrace vfstrace\" \n  debug=warning dlmtrace error emerg ha rpctrace vfstrace\n  ```\n\n  The flags shown in this example collect enough high-level information to aid debugging, but they do not cause any serious performance impact.\n\n- To add new flags to flags that have already been set, precede each one with a \"`+`\":\n\n  ```\n  # lctl set_param debug=\"+neterror +ha\" \n  debug=+neterror +ha\n  # lctl get_param debug \n  debug=neterror warning error emerg ha console\n  ```\n\n- To remove individual flags, precede them with a \"`-`\":\n\n  ```\n  # lctl set_param debug=\"-ha\" \n  debug=-ha\n  # lctl get_param debug \n  debug=neterror warning error emerg console\n  ```\n\nDebugging parameters include:\n\n- `subsystem_debug` - Controls the debug logs for subsystems.\n- `debug_path` - Indicates the location where the debug log is dumped when triggered automatically or manually. The default path is `/tmp/lustre-log`.\n\nThese parameters can also be set using:\n\n```\nsysctl -w lnet.debug={value}\n```\n\nAdditional useful parameters:\n\n- `panic_on_lbug` - Causes ''panic'' to be called when the Lustre software detects an internal problem (an `LBUG`log entry); panic crashes the node. This is particularly useful when a kernel crash dump utility is configured. The crash dump is triggered when the internal inconsistency is detected by the Lustre software.\n\n- `upcall` - Allows you to specify the path to the binary which will be invoked when an `LBUG` log entry is encountered. This binary is called with four parameters:\n\n  \\- The string ''`LBUG`''.\n\n  \\- The file where the `LBUG` occurred.\n\n  \\- The function name.\n\n  \\- The line number in the file\n\n### Interpreting OST Statistics\n\n**Note**\n\nSee also [*the section called \u201c llobdstat\u201d*](06.07-System%20Configuration%20Utilities.md#llobdstat)(`llobdstat`) and [*the section called \u201c `CollectL` \u201d*](03.01-Monitoring%20a%20Lustre%20File%20System.md#collectl) (`collectl`).\n\nOST `stats` files can be used to provide statistics showing activity for each OST. For example:\n\n```\n# lctl get_param osc.testfs-OST0000-osc.stats \nsnapshot_time                      1189732762.835363\nost_create                 1\nost_get_info               1\nost_connect                1\nost_set_info               1\nobd_ping                   212\n```\n\nUse the `llstat` utility to monitor statistics over time.\n\nTo clear the statistics, use the `-c` option to `llstat`. To specify how frequently the statistics should be reported (in seconds), use the `-i` option.", "mimetype": "text/plain", "start_char_idx": 79755, "end_char_idx": 83696, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef810dae-afea-41ef-9c96-acb5c588b910": {"__data__": {"id_": "ef810dae-afea-41ef-9c96-acb5c588b910", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8dbfe61-bcfd-43c4-a0dc-892717c00341", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "39da3a97efd3bd84405d81589db12de7f3a20429d96d2436ec2ee5e0ea1bdcab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "341f7f39-897d-431f-a822-05346c2f06a9", "node_type": "1", "metadata": {}, "hash": "da02f509e17f7910986e7646b8c3728a80a55e778d184d2a942f6ecc03c4b15f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "OST `stats` files can be used to provide statistics showing activity for each OST. For example:\n\n```\n# lctl get_param osc.testfs-OST0000-osc.stats \nsnapshot_time                      1189732762.835363\nost_create                 1\nost_get_info               1\nost_connect                1\nost_set_info               1\nobd_ping                   212\n```\n\nUse the `llstat` utility to monitor statistics over time.\n\nTo clear the statistics, use the `-c` option to `llstat`. To specify how frequently the statistics should be reported (in seconds), use the `-i` option. In the example below, the `-c` option clears the statistics and `-i10` option reports statistics every 10 seconds:\n\n```\n$ llstat -c -i10 ost_io\n \n/usr/bin/llstat: STATS on 06/06/07 \n        /proc/fs/lustre/ost/OSS/ost_io/ stats on 192.168.16.35@tcp\nsnapshot_time                              1181074093.276072\n \n/proc/fs/lustre/ost/OSS/ost_io/stats @ 1181074103.284895\nName        Cur.  Cur. #\n            Count Rate Events Unit  last   min    avg       max    stddev\nreq_waittime 8    0    8    [usec]  2078   34     259.75    868    317.49\nreq_qdepth   8    0    8    [reqs]  1      0      0.12      1      0.35\nreq_active   8    0    8    [reqs]  11     1      1.38      2      0.52\nreqbuf_avail 8    0    8    [bufs]  511    63     63.88     64     0.35\nost_write    8    0    8    [bytes] 169767 72914  212209.62 387579 91874.29\n \n/proc/fs/lustre/ost/OSS/ost_io/stats @ 1181074113.290180\nName        Cur.  Cur. #\n            Count Rate Events Unit  last    min   avg       max    stddev\nreq_waittime 31   3    39   [usec]  30011   34    822.79    12245  2047.71\nreq_qdepth   31   3    39   [reqs]  0       0     0.03      1      0.16\nreq_active   31   3    39   [reqs]  58      1     1.77      3      0.74\nreqbuf_avail 31   3    39   [bufs]  1977    63    63.79     64     0.41\nost_write    30   3    38   [bytes] 1028467 15019 315325.16 910694 197776.51\n \n/proc/fs/lustre/ost/OSS/ost_io/stats @ 1181074123.325560\nName        Cur.  Cur. #\n            Count Rate Events Unit  last    min    avg       max    stddev\nreq_waittime 21   2    60   [usec]  14970   34     784.32    12245  1878.66\nreq_qdepth   21   2    60   [reqs]  0       0      0.02      1      0.13\nreq_active   21   2    60   [reqs]  33      1      1.70      3      0.70\nreqbuf_avail 21   2    60   [bufs]  1341    63     63.82     64     0.39\nost_write    21   2    59   [bytes] 7648424 15019  332725.08 910694 180397.87\n```\n\nThe columns in this example are described in the table below.\n\n| **Parameter** | **Description**                                              |\n| ------------- | ------------------------------------------------------------ |\n| `Name`        | Name of the service event. See the tables below for descriptions of service events that are tracked. |\n| `Cur.", "mimetype": "text/plain", "start_char_idx": 83132, "end_char_idx": 85947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "341f7f39-897d-431f-a822-05346c2f06a9": {"__data__": {"id_": "341f7f39-897d-431f-a822-05346c2f06a9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13aa227-5c7b-43f8-8e13-41665aef4267", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "77324390ed16df1d6651b92027f0934010beb122dcb89c8c76f88d1932023534", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef810dae-afea-41ef-9c96-acb5c588b910", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8570004d84b79d406e5d902411a5bcb34b7aad8b9637d0946d4af50992f356fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| **Parameter** | **Description**                                              |\n| ------------- | ------------------------------------------------------------ |\n| `Name`        | Name of the service event. See the tables below for descriptions of service events that are tracked. |\n| `Cur. Count`  | Number of events of each type sent in the last interval.     |\n| `Cur. Rate`   | Number of events per second in the last interval.            |\n| `# Events`    | Total number of such events since the events have been cleared. |\n| `Unit`        | Unit of measurement for that statistic (microseconds, requests, buffers). |\n| `last`        | Average rate of these events (in units/event) for the last interval during which they arrived. For instance, in the above mentioned case of `ost_destroy` it took an average of 736 microseconds per destroy for the 400 object destroys in the previous 10 seconds. |\n| `min`         | Minimum rate (in units/events) since the service started.    |\n| `avg`         | Average rate.                                                |\n| `max`         | Maximum rate.                                                |\n| `stddev`      | Standard deviation (not measured in some cases)              |\n\nEvents common to all services are shown in the table below.\n\n| **Parameter**  | **Description**                                              |\n| -------------- | ------------------------------------------------------------ |\n| `req_waittime` | Amount of time a request waited in the queue before being handled by an available server thread. |\n| `req_qdepth`   | Number of requests waiting to be handled in the queue for this service. |\n| `req_active`   | Number of requests currently being handled.                  |\n| `reqbuf_avail` | Number of unsolicited lnet request buffers for this service. |\n\nSome service-specific events of interest are described in the table below.\n\n| **Parameter**  | **Description**                                              |\n| -------------- | ------------------------------------------------------------ |\n| `ldlm_enqueue` | Time it takes to enqueue a lock (this includes file open on the MDS) |\n| `mds_reint`    | Time it takes to process an MDS modification record (includes `create`, `mkdir`, `unlink`, `rename`and `setattr`) |\n\n### Interpreting MDT Statistics\n\n**Note**\n\nSee also [*the section called \u201c llobdstat\u201d*](06.07-System%20Configuration%20Utilities.md#llobdstat)(`llobdstat`) and [*the section called \u201c `CollectL` \u201d*](03.01-Monitoring%20a%20Lustre%20File%20System.md#collectl) (`collectl`).\n\nMDT `stats` files can be used to track MDT statistics for the MDS. The example below shows sample output from an MDT `stats` file.\n\n```\n# lctl get_param mds.*-MDT0000.stats\nsnapshot_time                   1244832003.676892 secs.usecs \nopen                            2 samples [reqs]\nclose                           1 samples [reqs]\ngetxattr                        3 samples [reqs]\nprocess_config                  1 samples [reqs]\nconnect                         2 samples [reqs]\ndisconnect                      2 samples [reqs]\nstatfs                          3 samples [reqs]\nsetattr                         1 samples [reqs]\ngetattr                         3 samples [reqs]\nllog_init                       6 samples [reqs] \nnotify                          16 samples [reqs]\n```", "mimetype": "text/plain", "start_char_idx": 85657, "end_char_idx": 89004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf8e5da9-8aff-43dd-a423-661803a1bf5c": {"__data__": {"id_": "cf8e5da9-8aff-43dd-a423-661803a1bf5c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef22e630-71f4-4fb3-9927-54e882dc31d0", "node_type": "1", "metadata": {}, "hash": "fcd70eaf2189cdb43482e7073b210312870727c4a1192c518c6491689b0e726d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## User Utilities\n\n- [User Utilities](#user-utilities)\n- [`lfs`](#lfs)\n  * [Synopsis](#synopsis)\n  * [Description](#description)\n  * [Options](#options)\n  * [Examples](#examples)\n  * [See Also](#see-also)\n- [`lfs_migrate`](#lfs_migrate)\n  * [Synopsis](#synopsis-1)\n  * [Description](#description-1)\n  * [Options](#options-1)\n  * [Examples](#examples-1)\n  * [See Also](#see-also-1)\n- [`filefrag`](#filefrag)\n  * [Synopsis](#synopsis-2)\n  * [Description](#description-2)\n  * [Options](#options-2)\n  * [Examples](#examples-2)\n- [`mount`](#mount)\n- [Handling Timeouts](#handling-timeouts)\n\n\nThis chapter describes user utilities.\n\n## `lfs`\n\nThe\u00a0`lfs`\u00a0utility can be used for user configuration routines and monitoring.\n\n### Synopsis\n\n```\nlfs\nlfs changelog [--follow] mdt_name [startrec [endrec]]\nlfs changelog_clear mdt_name id endrec\nlfs check mds|osts|servers\nlfs data_version [-nrw] filename\nlfs df [-i] [-h] [--pool]-p fsname[.pool] [path] [--lazy]\nlfs find [[!] --atime|-A [-+]N] [[!] --mtime|-M [-+]N]\n         [[!] --ctime|-C [-+]N] [--maxdepth|-D N] [--name|-n pattern]\n         [--print|-p] [--print0|-P] [[!] --obd|-O ost_name[,ost_name...]]\n         [[!] --size|-S [+-]N[kMGTPE]] --type |-t {bcdflpsD}]\n         [[!] --gid|-g|--group|-G gname|gid]\n         [[!] --uid|-u|--user|-U uname|uid]\n         dirname|filename\nlfs getname [-h]|[path...]\nlfs getstripe [--obd|-O ost_name] [--quiet|-q] [--verbose|-v]\t\t  \n              [--stripe-count|-c] [--stripe-index|-i]\n              [--stripe-size|-s] [--pool|-p] [--directory|-d]\n              [--mdt-index|-M] [--recursive|-r] [--raw|-R]\n              [--layout|-L]\n              dirname|filename ...\nlfs setstripe [--size|-s stripe_size]  [--stripe-count|-c stripe_count]\n              [--overstripe-count|-C stripe_count]\n              [--stripe-index|-i start_ost_index]\n              [--ost-list|-o ost_indicies]\n              [--pool|-p pool]\n              dirname|filename\nlfs setstripe -d dir\nlfs osts [path]\nlfs pool_list filesystem[.pool]| pathname\nlfs quota [-q] [-v] [-h] [-o obd_uuid|-I ost_idx|-i mdt_idx]\n          [-u username|uid|-g group|gid|-p projid] /mount_point\nlfs quota -t -u|-g|-p /mount_point\nlfs quotacheck [-ug] /mount_point\nlfs quotachown [-i] /mount_point\nlfs quotainv [-ug] [-f] /mount_point\nlfs quotaon [-ugf] /mount_point\nlfs quotaoff [-ug] /mount_point\nlfs setquota {-u|--user|-g|--group|-p|--project} uname|uid|gname|gid|projid\n             [--block-softlimit block_softlimit]\n             [--block-hardlimit block_hardlimit]\n             [--inode-softlimit inode_softlimit]\n             [--inode-hardlimit inode_hardlimit]\n             /mount_point\nlfs setquota -u|--user|-g|--group|-p|--project uname|uid|gname|gid|projid\n             [-b block_softlimit] [-B block_hardlimit]\n             [-i inode-softlimit] [-I inode_hardlimit]\n             /mount_point\nlfs setquota -t -u|-g|-p [--block-grace block_grace]\n             [--inode-grace inode_grace]\n             /mount_point\nlfs setquota -t -u|-g|-p [-b block_grace] [-i inode_grace]\n             /mount_point\nlfs help\n```\n\n**Note**\n\nIn the above example, the *`/mount_point `* parameter refers to the mount point of the Lustre file system.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3184, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef22e630-71f4-4fb3-9927-54e882dc31d0": {"__data__": {"id_": "ef22e630-71f4-4fb3-9927-54e882dc31d0", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf8e5da9-8aff-43dd-a423-661803a1bf5c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8157a7d0576d7c2db8053431176e1ac4b376270982b5c0622e441356191cf077", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bff3eb95-0e97-49d4-b008-e836adaa76b2", "node_type": "1", "metadata": {}, "hash": "650e511f30997683c266385d07fff437d9a82e6ed616e2ab18e8342198e9ae09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Note**\n\nThe old lfs quota output was very detailed and contained cluster-wide quota statistics (including cluster-wide limits for a user/group and cluster-wide usage for a user/group), as well as statistics for each MDS/OST. Now, `lfs quota` has been updated to provide only cluster-wide statistics, by default. To obtain the full report of cluster-wide limits, usage and statistics, use the `-v` option with `lfs quota`.\n\nThe `quotacheck, quotaon` and `quotaoff` sub-commands were deprecated in the Lustre 2.4 release, and removed completely in the Lustre 2.8 release. See Section \u201c Enabling Disk Quotas\u201d for details on configuring and checking quotas.\n\n### Description\n\nThe `lfs` utility is used to create a new file with a specific striping pattern, determine the default striping pattern, gather the extended attributes (object numbers and location) for a specific file, find files with specific attributes, list OST information or set quota limits. It can be invoked interactively without any arguments or in a non-interactive mode with one of the supported arguments.\n\n### Options\n\nThe various `lfs` options are listed and described below. For a complete list of available options, type help at the `lfs`prompt.\n\n| **Option**                                                   | **Description**                                              |                                                              |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| `changelog`                                                  | Shows the metadata changes on an MDT. Start and end points are optional. The `--follow` option blocks on new changes; this option is only valid when run directly on the MDT node. |                                                              |\n| `changelog_clear`                                            | Indicates that changelog records previous to `*endrec* `are no longer of interest to a particular consumer `*id* `, potentially allowing the MDT to free up disk space. An `*endrec* `of 0 indicates the current last record. Changelog consumers must be registered on the MDT node using `lctl`. |                                                              |\n| `check`                                                      | Displays the status of MDS or OSTs (as specified in the command) or all servers (MDS and OSTs). |                                                              |\n| `data_version [-nrw] *filename*`                             | Displays the current version of file data. If `-n` is specified, the data version is read without taking a lock. As a consequence, the data version could be outdated if there are dirty caches on filesystem clients, but this option will not force data flushes and has less of an impact on the filesystem. If `-r` is specified, the data version is read after dirty pages on clients are flushed. If `-w` is specified, the data version is read after all caching pages on clients are flushed.Even with `-r` or `-w`, race conditions are possible and the data version should be checked before and after an operation to be confident the data did not change during it.The data version is the sum of the last committed transaction numbers of all data objects of a file. It is used by HSM policy engines for verifying that file data has not been changed during an archive operation or before a release operation, and by OST migration, primarily for verifying that file data has not been changed during a data copy, when done in non-blocking mode. |                                                              |\n| `df [-i] [-h] [--pool|-p*fsname*[. *pool*] [ *path*] [--lazy]` | Use `-i` to report file system disk space usage or inode usage of each MDT or OST or, if a pool is specified with the `-p` option, a subset of OSTs.By default, the usage of all mounted Lustre file systems is reported. If the`path` option is included, only the usage for the specified file system is reported. If the `-h` option is included, the output is printed in human-readable format, using SI base-2 suffixes for **M**ega-, **G**iga-, **T**era-, **P**eta-, or**E**xabytes.If the `--lazy` option is specified, any OST that is currently disconnected from the client will be skipped. Using the `--lazy` option prevents the `df` output from being blocked when an OST is offline. Only the space on the OSTs that can currently be accessed are returned. The `llite.*.lazystatfs` tunable can be enabled to make this the default behaviour for all `statfs()`operations.", "mimetype": "text/plain", "start_char_idx": 3186, "end_char_idx": 7842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bff3eb95-0e97-49d4-b008-e836adaa76b2": {"__data__": {"id_": "bff3eb95-0e97-49d4-b008-e836adaa76b2", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef22e630-71f4-4fb3-9927-54e882dc31d0", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3ed6e4b8c48ccaa3e680ab5135b7d7a779b2b47f13b69d7d9febff6f930682fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2f4a512-beb1-4d9a-8509-6d0f9a4c4d48", "node_type": "1", "metadata": {}, "hash": "e6b9f26d99f6f5d1556efe764c24041ad315cdb79471757b1c918d23ca2b7844", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the`path` option is included, only the usage for the specified file system is reported. If the `-h` option is included, the output is printed in human-readable format, using SI base-2 suffixes for **M**ega-, **G**iga-, **T**era-, **P**eta-, or**E**xabytes.If the `--lazy` option is specified, any OST that is currently disconnected from the client will be skipped. Using the `--lazy` option prevents the `df` output from being blocked when an OST is offline. Only the space on the OSTs that can currently be accessed are returned. The `llite.*.lazystatfs` tunable can be enabled to make this the default behaviour for all `statfs()`operations. |                                                              |\n| `find`                                                       | Searches the directory tree rooted at the given directory/filename for files that match the given parameters.Using `!` before an option negates its meaning (files NOT matching the parameter). Using `+` before a numeric value means files with the parameter OR MORE. Using `-` before a numeric value means files with the parameter OR LESS. |                                                              |\n|                                                              | `--atime`                                                    | File was last accessed N*24 hours ago. (There is no guarantee that `atime` is kept coherent across the cluster.) OSTs by default only hold a transient `atime` that is updated when clients do read requests. Permanent `atime` is written to the MDT when the file is closed. However, on-disk atime is only updated if it is more than 60 seconds old (`mdd.*.atime_diff`). <br />In Lustre 2.14, it is possible to set the OSTs to persistently store atime with each object, in order to get more accurate persistent atime updates for files that are open for a long time via the similarly-named `obdfilter.*.atime_diff `parameter.<br />The client considers the latest atime from all OSTs and MDTs. If a setattr is set by user, then it is updated on both the MDT and OST, allowing the atime to go backward. |\n|                                                              | `--ctime`                                                    | File status was last changed N*24 hours ago.                 |\n|                                                              | `--mtime`                                                    | File data was last modified N*24 hours ago.                  |\n|                                                              | `--obd`                                                      | File has an object on a specific OST(s).                     |\n|                                                              | `--size`                                                     | File has a size in bytes, or kilo-, Mega-, Giga-, Tera-, Peta- or Exabytes if a suffix is given. |\n|                                                              | `--type`                                                     | File has the type - block, character, directory, pipe, file, symlink, socket or door (used in Solaris operating system). |\n|                                                              | `--uid`                                                      | File has a specific numeric user ID.                         |\n|                                                              | `--user`                                                     | File owned by a specific user (numeric user ID allowed).     |\n|                                                              | `--gid`                                                      | File has a specific group ID.                                |\n|                                                              | `--group`                                                    | File belongs to a specific group (numeric group ID allowed). |\n|                                                              | - `-maxdepth`                                                | Limits find to descend at most N levels of the directory tree. |\n|                                                              | `--print`/ `--print0`                                        | Prints the full filename, followed by a new line or NULL character correspondingly. |\n| `osts [path]`                                                | Lists all OSTs for the file system. If a path located on a mounted Lustre file system is specified, then only OSTs belonging to this file system are displayed. |                                                              |\n| `getname [path...]`                                          | List each Lustre file system instance associated with each Lustre mount point. If no path is specified, all Lustre mount points are interrogated. If a list of paths is provided, the instance of each path is provided. If the path is not a Lustre instance 'No such device' is returned. |                                                              |\n| `getstripe`                                                  | Lists striping information for a given filename or directory. By default, the stripe count, stripe size and offset are returned.If you only want specific striping information, then the options of `--stripe-count`, `--stripe-size`, `--stripe-index`, `--layout`, or `--pool` plus various combinations of these options can be used to retrieve specific information.If the `--raw` option is specified, the stripe information is printed without substituting the file system default values for unspecified fields.", "mimetype": "text/plain", "start_char_idx": 7196, "end_char_idx": 12798, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2f4a512-beb1-4d9a-8509-6d0f9a4c4d48": {"__data__": {"id_": "a2f4a512-beb1-4d9a-8509-6d0f9a4c4d48", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bff3eb95-0e97-49d4-b008-e836adaa76b2", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4195c55d2cb78950f07b5ef5d4861747490fb7736cea87560933276f517457b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3291a1b-e1f9-438f-be38-11bc31513976", "node_type": "1", "metadata": {}, "hash": "c8ab55776f4a52e41b21c3cc4c363cd82a6370982f305a0c9f27e7a1de6be62c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|                                                              |\n| `getname [path...]`                                          | List each Lustre file system instance associated with each Lustre mount point. If no path is specified, all Lustre mount points are interrogated. If a list of paths is provided, the instance of each path is provided. If the path is not a Lustre instance 'No such device' is returned. |                                                              |\n| `getstripe`                                                  | Lists striping information for a given filename or directory. By default, the stripe count, stripe size and offset are returned.If you only want specific striping information, then the options of `--stripe-count`, `--stripe-size`, `--stripe-index`, `--layout`, or `--pool` plus various combinations of these options can be used to retrieve specific information.If the `--raw` option is specified, the stripe information is printed without substituting the file system default values for unspecified fields. If the striping EA is not set, 0, 0, and -1 will be printed for the stripe count, size, and offset respectively.Introduced in Lustre 2.4The `--mdt-index` prints the index of the MDT for a given directory. See [*the section called \u201cRemoving an MDT from the File System\u201d*](03.03-Lustre%20Maintenance.md#removing-and-restoring-mdts-and-osts)). |                                                              |\n|                                                              | `--obd *ost_name*`                                           | Lists files that have an object on a specific OST.           |\n|                                                              | `--quiet`                                                    | Lists details about the file's object ID information.        |\n|                                                              | `--stripe-count`                                             | Prints additional striping information.                      |\n|                                                              | `--count`                                                    | Lists the stripe count (how many OSTs to use).               |\n|                                                              | `--index`                                                    | Lists the index for each OST in the file system.             |\n|                                                              | `--offset`                                                   | Lists the OST index on which file striping starts.           |\n|                                                              | `--pool`                                                     | Lists the pools to which a file belongs.                     |\n|                                                              | `--size`                                                     | Lists the stripe size (how much data to write to one OST before moving to the next OST). |\n|                                                              | `--directory`                                                | Lists entries about a specified directory instead of its contents (in the same manner as `ls -d`). |\n|                                                              | `--recursive`                                                | Recurses into all sub-directories.                           |\n| `setstripe`                                                  |                                                              | Create new files with a specific file layout (stripe pattern) configuration[a]. |\n|                                                              | `--stripe-count stripe_cnt`                                  | Number of OSTs over which to stripe a file. A `stripe_cnt` of 0 uses the file system-wide default stripe count (default is 1). A `stripe_cnt` of -1 stripes over all available OSTs. |\n|                                                              | `--overstripe-count stripe_cnt`                              | The same as --stripe-count, but allows overstriping, which will place more than one stripe per OST if `stripe_cnt` is greater than the number of OSTs. Overstriping is useful for matching the number of stripes to the number of processes, or with very fast OSTs, where one stripe per OST is not enough to get full performance. |\n|                                                              | `--size stripe_size`[b]                                      | Number of bytes to store on an OST before moving to the next OST. A stripe_size of 0 uses the file system's default stripe size, (default is 1 MB). Can be specified with **k**(KB), **m**(MB), or **g**(GB), respectively. |\n|                                                              | `--stripe-index start_ost_index`                             | The OST index (base 10, starting at 0) on which to start striping for this file. A start_ost_index value of -1 allows the MDS to choose the starting index. This is the default value, and it means that the MDS selects the starting OST as it wants. We strongly recommend selecting this default, as it allows space and load balancing to be done by the MDS as needed. The `start_ost_index`value has no relevance on whether the MDS will use round-robin or QoS weighted allocation for the remaining stripes in the file. |\n|                                                              | `--ost-index ost_indices`                                    | This option is used to specify the exact stripe layout on the the file system. `ost_indices` is a list of OSTs referenced by their indices and index ranges separated by commas, e.g. `1,2-4,7`.", "mimetype": "text/plain", "start_char_idx": 11748, "end_char_idx": 17471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3291a1b-e1f9-438f-be38-11bc31513976": {"__data__": {"id_": "b3291a1b-e1f9-438f-be38-11bc31513976", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2f4a512-beb1-4d9a-8509-6d0f9a4c4d48", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e60ef3507d5a7364e60232601d670d5100f1eab2cd4eab64971a89795bf9f295", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bd52d44-4e46-401e-bf97-41825fcb2955", "node_type": "1", "metadata": {}, "hash": "4e1f1e3a35a8a9c2b6ab44bea40ee4c5de3d945a0db40cfecf5b411e080293be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A start_ost_index value of -1 allows the MDS to choose the starting index. This is the default value, and it means that the MDS selects the starting OST as it wants. We strongly recommend selecting this default, as it allows space and load balancing to be done by the MDS as needed. The `start_ost_index`value has no relevance on whether the MDS will use round-robin or QoS weighted allocation for the remaining stripes in the file. |\n|                                                              | `--ost-index ost_indices`                                    | This option is used to specify the exact stripe layout on the the file system. `ost_indices` is a list of OSTs referenced by their indices and index ranges separated by commas, e.g. `1,2-4,7`. |\n|                                                              | `--pool pool`                                                | Name of the pre-defined pool of OSTs (see [*the section called \u201c lctl\u201d*](06.07-System%20Configuration%20Utilities.md#lctl)) that will be used for striping. The `stripe_cnt`, `stripe_size` and `start_ost` values are used as well. The start-ost value must be part of the pool or an error is returned. |\n| `setstripe -d`                                               | Deletes default striping on the specified directory.         |                                                              |\n| `pool_list {filesystem}[.poolname]|{pathname}`               | Lists pools in the file system or pathname, or OSTs in the file system's pool. |                                                              |\n| `quota [-q] [-v] [-o*obd_uuid*|-i *mdt_idx*|-I*ost_idx*] [-u|-g|-p*uname|uid|gname|gid|projid]**/mount_point*` | Displays disk usage and limits, either for the full file system or for objects on a specific OBD. A user or group name or an usr, group and project ID can be specified. If all user, group project ID are omitted, quotas for the current UID/GID are shown. The `-q` option disables printing of additional descriptions (including column titles). It fills in blank spaces in the `grace` column with zeros (when there is no grace period set), to ensure that the number of columns is consistent. The `-v` option provides more verbose (per-OBD statistics) output. |                                                              |\n| `quota -t *-u|-g|-p**/mount_point*`                          | Displays block and inode grace times for user ( `-u`) or group ( `-g`) or project ( `-p`) quotas. |                                                              |\n| `setquota {-u|-g|-p uname|uid|gname|gid|projid}  [--block-softlimit block_softlimit ] [--block-hardlimit block_hardlimit] [--inode-softlimit inode_softlimit] [--inode-hardlimit inode_hardlimit] /mount_point` | Sets file system quotas for users, groups or one project. Limits can be specified with `--{block|inode}-{softlimit|hardlimit}` or their short equivalents `-b`, `-B`, `-i`, `-I`. Users can set 1, 2, 3 or 4 limits. [c]Also, limits can be specified with special suffixes, -b, -k, -m, -g, -t, and -p to indicate units of 1, 2^10, 2^20, 2^30, 2^40 and 2^50, respectively. By default, the block limits unit is 1 kilobyte (1,024), and block limits are always kilobyte-grained (even if specified in bytes). See [*the section called \u201cExamples\u201d*](#examples). |                                                              |\n| `setquota -t -u|-g|-p [--block-grace *block_grace*] [--inode-grace *inode_grace*]*/mount_point*` | Sets the file system quota grace times for users or groups. Grace time is specified in ' `XXwXXdXXhXXmXXs`' format or as an integer seconds value. See [*the section called \u201cExamples\u201d*](#examples). |                                                              |\n| `help`                                                       | Provides brief help on various `lfs` arguments.              |                                                              |\n| `exit/quit`                                                  | Quits the interactive `lfs` session.                         |                                                              |\n| [a]The file cannot exist prior to using `setstripe`. A directory must exist prior to using `setstripe`.[b]The default stripe-size is 0. The default start-ost is -1.", "mimetype": "text/plain", "start_char_idx": 16716, "end_char_idx": 21004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8bd52d44-4e46-401e-bf97-41825fcb2955": {"__data__": {"id_": "8bd52d44-4e46-401e-bf97-41825fcb2955", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3291a1b-e1f9-438f-be38-11bc31513976", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d69686fa1154fe77984f6330ab9c558aff86257d191f42a8c4075145978ceca7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40d42af7-e390-4057-a7d4-d9e25e6d22b3", "node_type": "1", "metadata": {}, "hash": "0f0bfee7b0a2c9b5e94c09c0996fce79dd90d8e344ffb95c37d6536fc2126d2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|                                                              |\n| `setquota -t -u|-g|-p [--block-grace *block_grace*] [--inode-grace *inode_grace*]*/mount_point*` | Sets the file system quota grace times for users or groups. Grace time is specified in ' `XXwXXdXXhXXmXXs`' format or as an integer seconds value. See [*the section called \u201cExamples\u201d*](#examples). |                                                              |\n| `help`                                                       | Provides brief help on various `lfs` arguments.              |                                                              |\n| `exit/quit`                                                  | Quits the interactive `lfs` session.                         |                                                              |\n| [a]The file cannot exist prior to using `setstripe`. A directory must exist prior to using `setstripe`.[b]The default stripe-size is 0. The default start-ost is -1. Do NOT confuse them! If you set start-ost to 0, all new file creations occur on OST 0 (seldom a good idea).                                                                                [c]The old `setquota` interface is supported, but it may be removed in a future Lustre software release. |                                                              |                                                              |\n\n### Examples\n\nCreates a file striped on two OSTs with 128 KB on each stripe.\n\n```\n$ lfs setstripe -s 128k -c 2 /mnt/lustre/file1\n```\n\nDeletes a default stripe pattern on a given directory. New files use the default striping pattern.\n\n```\n$ lfs setstripe -d /mnt/lustre/dir\n```\n\nLists the detailed object allocation of a given file.\n\n```\n$ lfs getstripe -v /mnt/lustre/file1\n```\n\nList all the mounted Lustre file systems and corresponding Lustre instances.\n\n```\n$ lfs getname\n```\n\nEfficiently lists all files in a given directory and its subdirectories.\n\n```\n$ lfs find /mnt/lustre\n```\n\nRecursively lists all regular files in a given directory more than 30 days old.\n\n```\n$ lfs find /mnt/lustre -mtime +30 -type f -print\n```\n\nRecursively lists all files in a given directory that have objects on OST2-UUID. The lfs check servers command checks the status of all servers (MDT and OSTs).\n\n```\n$ lfs find --obd OST2-UUID /mnt/lustre/\n```\n\nLists all OSTs in the file system.\n\n```\n$ lfs osts\n```\n\nLists space usage per OST and MDT in human-readable format.\n\n```\n$ lfs df -h\n```\n\nLists inode usage per OST and MDT.\n\n```\n$ lfs df -i\n```\n\nList space or inode usage for a specific OST pool.\n\n```\n$ lfs df --pool \nfilesystem[.\npool] | \npathname\n```\n\nList quotas of user 'bob'.\n\n```\n$ lfs quota -u bob /mnt/lustre\n```\n\nList quotas of project ID '1'.\n\n```\n$ lfs quota -p 1 /mnt/lustre\n```\n\nShow grace times for user quotas on `/mnt/lustre`.\n\n```\n$ lfs quota -t -u /mnt/lustre\n```\n\nSets quotas of user 'bob', with a 1 GB block quota hardlimit and a 2 GB block quota softlimit.\n\n```\n$ lfs setquota -u bob --block-softlimit 2000000 --block-hardlimit 1000000\n/mnt/lustre\n```\n\nSets grace times for user quotas: 1000 seconds for block quotas, 1 week and 4 days for inode quotas.\n\n```\n$ lfs setquota -t -u --block-grace 1000 --inode-grace 1w4d /mnt/lustre\n```\n\nChecks the status of all servers (MDT, OST)\n\n```\n$ lfs check servers\n```\n\nCreates a file striped on two OSTs from the pool `my_pool`\n\n```\n$ lfs setstripe --pool my_pool -c 2 /mnt/lustre/file\n```\n\nLists the pools defined for the mounted Lustre file system `/mnt/lustre`\n\n```\n$ lfs pool_list /mnt/lustre/\n```\n\nLists the OSTs which are members of the pool `my_pool` in file system `my_fs`\n\n```\n$ lfs pool_list my_fs.my_pool\n```\n\nFinds all directories/files associated with `poolA`.\n\n```\n$ lfs find /mnt/lustre --pool poolA\n```\n\nFinds all directories/files not associated with a pool.", "mimetype": "text/plain", "start_char_idx": 20028, "end_char_idx": 23847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40d42af7-e390-4057-a7d4-d9e25e6d22b3": {"__data__": {"id_": "40d42af7-e390-4057-a7d4-d9e25e6d22b3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bd52d44-4e46-401e-bf97-41825fcb2955", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f9d223cdd7324014b15b29385b660e91dcd079b643035712f56971149c577e3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a9c618f-fd1e-4ead-98e4-58cfbb669a04", "node_type": "1", "metadata": {}, "hash": "2fc22b4ee9a30601b82659ad124d070c3e97fe9c303e7c4e31c1d6832a5c7350", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\n$ lfs find /mnt/lustre --pool poolA\n```\n\nFinds all directories/files not associated with a pool.\n\n```\n$ lfs find /mnt//lustre --pool \"\"\n```\n\nFinds all directories/files associated with pool.\n\n```\n$ lfs find /mnt/lustre ! --pool \"\"\n```\n\nAssociates a directory with the pool `my_pool`, so all new files and directories are created in the pool.\n\n```\n$ lfs setstripe --pool my_pool /mnt/lustre/dir\n```\n\n### See Also\n\n[*the section called \u201c lctl\u201d*](06.07-System%20Configuration%20Utilities.md#lctl)\n\n## `lfs_migrate`\n\nThe\u00a0`lfs_migrate`\u00a0utility is a simple to migrate file\u00a0*data*\u00a0between OSTs.\n\n### Synopsis\n\n```\nlfs_migrate [lfs_setstripe_options]\n\t[-h] [-n] [-q] [-R] [-s] [-y] [-0] [file|directory ...]\n```\n\n### Description\n\nThe `lfs_migrate` utility is a tool to assist migration of file data between Lustre OSTs. The utility copies each specified file to a temporary file using supplied `lfs setstripe` options, if any, optionally verifies the file contents have not changed, and then swaps the layout (OST objects) from the temporary file and the original file (for Lustre 2.5 and later), or renames the temporary file to the original filename. This allows the user/administrator to balance space usage between OSTs, or move files off OSTs that are starting to show hardware problems (though are still functional) or will be removed.\n\n**Warning**\n\nFor versions of Lustre before 2.5, `lfs_migrate` was not integrated with the MDS at all. That made it UNSAFE for use on files that were being modified by other applications, since the file was migrated through a copy and rename of the file. With Lustre 2.5 and later, the new file layout is swapped with the existing file layout, which ensures that the user-visible inode number is kept, and open file handles and locks on the file are kept.\n\nFiles to be migrated can be specified as command-line arguments. If a directory is specified on the command-line then all files within the directory are migrated. If no files are specified on the command-line, then a list of files is read from the standard input, making `lfs_migrate` suitable for use with `lfs find` to locate files on specific OSTs and/or matching other file attributes, and other tools that generate a list of files on standard output.\n\nUnless otherwise specified through command-line options, the file allocation policies on the MDS dictate where the new files are placed, taking into account whether specific OSTs have been disabled on the MDS via `lctl` (preventing new files from being allocated there), whether some OSTs are overly full (reducing the number of files placed on those OSTs), or if there is a specific default file striping for the parent directory (potentially changing the stripe count, stripe size, OST pool, or OST index of a new file).\n\n**Note**\n\nThe `lfs_migrate` utility can also be used in some cases to reduce file fragmentation. File fragmentation will typically reduce Lustre file system performance. File fragmentation may be observed on an aged file system and will commonly occur if the file was written by many threads. Provided there is sufficient free space (or if it was written when the file system was nearly full) that is less fragmented than the file being copied, re-writing a file with `lfs_migrate` will result in a migrated file with reduced fragmentation. The tool `filefrag` can be used to report file fragmentation. See [*the section called \u201c `filefrag` \u201d*](#filefrag)\n\n**Note**\n\nAs long as a file has extent lengths of tens of megabytes ( *read_bandwidth \\* seek_time*) or more, the read performance for the file will not be significantly impacted by fragmentation, since the read pipeline can be filled by large reads from disk even with an occasional disk seek.\n\n### Options\n\nOptions supporting `lfs_migrate` are described below.\n\n| **Option**        | **Description**                                              |\n| ----------------- | ------------------------------------------------------------ |\n| `-c*stripecount*` | Restripe file using the specified stripe count. This option may not be specified at the same time as the `-R` option. |\n| `-h`              | Display help information.                                    |\n| `-l`              | Migrate files with hard links (skips, by default).", "mimetype": "text/plain", "start_char_idx": 23747, "end_char_idx": 28013, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a9c618f-fd1e-4ead-98e4-58cfbb669a04": {"__data__": {"id_": "7a9c618f-fd1e-4ead-98e4-58cfbb669a04", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40d42af7-e390-4057-a7d4-d9e25e6d22b3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2b28a6d679e41b6c0869918274ca76bc008bd49e865b01dd3ded11f28b01a738", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "074dd052-0636-443f-9ac8-53eaa76ddfdd", "node_type": "1", "metadata": {}, "hash": "57394d45b3f42c3f50690959d501064f0c6e4d2d3ac8632410f9a8d882e903c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The tool `filefrag` can be used to report file fragmentation. See [*the section called \u201c `filefrag` \u201d*](#filefrag)\n\n**Note**\n\nAs long as a file has extent lengths of tens of megabytes ( *read_bandwidth \\* seek_time*) or more, the read performance for the file will not be significantly impacted by fragmentation, since the read pipeline can be filled by large reads from disk even with an occasional disk seek.\n\n### Options\n\nOptions supporting `lfs_migrate` are described below.\n\n| **Option**        | **Description**                                              |\n| ----------------- | ------------------------------------------------------------ |\n| `-c*stripecount*` | Restripe file using the specified stripe count. This option may not be specified at the same time as the `-R` option. |\n| `-h`              | Display help information.                                    |\n| `-l`              | Migrate files with hard links (skips, by default). Files with multiple hard links are split into multiple separate files by `lfs_migrate`, so they are skipped, by default, to avoid breaking the hard links. |\n| `-n`              | Only print the names of files to be migrated.                |\n| `-q`              | Run quietly (does not print filenames or status).            |\n| `-R`              | Restripe file using default directory striping instead of keeping striping. This option may not be specified at the same time as the `-c` option. |\n| `-s`              | Skip file data comparison after migrate. Default is to compare migrated file against original to verify correctness. |\n| `-y`              | Answer ' `y`' to usage warning without prompting (for scripts, use with caution). |\n| `-0`              | Expect NUL-terminated filenames on standard input, as generated by `lfs find -print0` or `find -print0`. This allows filenames with embedded newlines to be handled correctly. |\n\n### Examples\n\nRebalance all files in `/mnt/lustre/dir`:\n\n```\n$ lfs_migrate /mnt/lustre/dir\n```\n\nMigrate files in /test filesystem on OST0004 larger than 4 GB in size and older than a day old:\n\n```\n$ lfs find /test -obd test-OST0004 -size +4G -mtime +1 | lfs_migrate -y\n```\n\n### See Also\n\n*the section called \u201c `lfs` \u201d*\n\n## `filefrag`\n\nThe\u00a0`e2fsprogs`\u00a0package contains the\u00a0`filefrag`\u00a0tool which reports the extent of file fragmentation.\n\n### Synopsis\n\n```\nfilefrag [ -belsv ] [ files...  ]\n```\n\n### Description\n\nThe `filefrag` utility reports the extent of fragmentation in a given file. The `filefrag` utility obtains the extent information from Lustre files using the `FIEMAP ioctl`, which is efficient and fast, even for very large files.\n\nIn default mode [5], `filefrag` prints the number of physically discontiguous extents in the file. In extent or verbose mode, each extent is printed with details such as the blocks allocated on each OST. For a Lustre file system, the extents are printed in device offset order (i.e. all of the extents for one OST first, then the next OST, etc.), not file logical offset order. If the file logical offset order was used, the Lustre striping would make the output very verbose and difficult to see if there was file fragmentation or not.\n\n**Note**\n\nNote that as long as a file has extent lengths of tens of megabytes or more (i.e. *read_bandwidth \\* seek_time > extent_length*), the read performance for the file will not be significantly impacted by fragmentation, since file readahead can fully utilize the disk disk bandwidth even with occasional seeks.\n\nIn default mode [6], `filefrag` returns the number of physically discontiguous extents in the file. In extent or verbose mode, each extent is printed with details. For a Lustre file system, the extents are printed in device offset order, not logical offset order.\n\n-------------\n\n[5]The default mode is faster than the verbose/extent mode since it only counts the number of extents.\n\n[6]The default mode is faster than the verbose/extent mode \n\n-------\n\n### Options\n\nThe options and descriptions for the `filefrag` utility are listed below.\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `-b`       | Uses the 1024-byte blocksize for the output. By default, this blocksize is used by the Lustre file system, since OSTs may use different block sizes.", "mimetype": "text/plain", "start_char_idx": 27064, "end_char_idx": 31418, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "074dd052-0636-443f-9ac8-53eaa76ddfdd": {"__data__": {"id_": "074dd052-0636-443f-9ac8-53eaa76ddfdd", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a9c618f-fd1e-4ead-98e4-58cfbb669a04", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "24334b036984508736a83f5dafa33a60cc88835f30f42376f42596634df7aba5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10e6fb7d-a3ca-4e23-91e2-288b52556e32", "node_type": "1", "metadata": {}, "hash": "a552cac14a7b2cf31ce62739e38995624fde6bc811b2870e6c92d62be8b1870c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In default mode [6], `filefrag` returns the number of physically discontiguous extents in the file. In extent or verbose mode, each extent is printed with details. For a Lustre file system, the extents are printed in device offset order, not logical offset order.\n\n-------------\n\n[5]The default mode is faster than the verbose/extent mode since it only counts the number of extents.\n\n[6]The default mode is faster than the verbose/extent mode \n\n-------\n\n### Options\n\nThe options and descriptions for the `filefrag` utility are listed below.\n\n| **Option** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `-b`       | Uses the 1024-byte blocksize for the output. By default, this blocksize is used by the Lustre file system, since OSTs may use different block sizes. |\n| `-e`       | Uses the extent mode when printing the output. This is the default for Lustre files in verbose mode. |\n| `-l`       | Displays extents in LUN offset order. This is the only available mode for Lustre. |\n| `-s`       | Synchronizes any unwritten file data to disk before requesting the mapping. |\n| `-v`       | Prints the file's layout in verbose mode when checking file fragmentation, including the logical to physical mapping for each extent in the file and the OST index. |\n\n### Examples\n\nLists default output.\n\n```\n$ filefrag /mnt/lustre/foo\n/mnt/lustre/foo: 13 extents found\n```\n\nLists verbose output in extent format.\n\n```\n$ filefrag -v /mnt/lustre/foo\nFilesystem type is: bd00bd0\nFile size of /mnt/lustre/foo is 1468297786 (1433888 blocks of 1024 bytes)\n ext:     device_logical:        physical_offset: length:  dev: flags:\n   0:        0..  122879: 2804679680..2804802559: 122880: 0002: network\n   1:   122880..  245759: 2804817920..2804940799: 122880: 0002: network\n   2:   245760..  278527: 2804948992..2804981759:  32768: 0002: network\n   3:   278528..  360447: 2804982784..2805064703:  81920: 0002: network\n   4:   360448..  483327: 2805080064..2805202943: 122880: 0002: network\n   5:   483328..  606207: 2805211136..2805334015: 122880: 0002: network\n   6:   606208..  729087: 2805342208..2805465087: 122880: 0002: network\n   7:   729088..  851967: 2805473280..2805596159: 122880: 0002: network\n   8:   851968..  974847: 2805604352..2805727231: 122880: 0002: network\n   9:   974848.. 1097727: 2805735424..2805858303: 122880: 0002: network\n  10:  1097728.. 1220607: 2805866496..2805989375: 122880: 0002: network\n  11:  1220608.. 1343487: 2805997568..2806120447: 122880: 0002: network\n  12:  1343488.. 1433599: 2806128640..2806218751:  90112: 0002: network\n/mnt/lustre/foo: 13 extents found\n```\n\n## `mount`\n\nThe standard `mount(8)` Linux command is used to mount a Lustre file system. When mounting a Lustre file system, mount(8) executes the `/sbin/mount.lustre` command to complete the mount.", "mimetype": "text/plain", "start_char_idx": 30557, "end_char_idx": 33440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10e6fb7d-a3ca-4e23-91e2-288b52556e32": {"__data__": {"id_": "10e6fb7d-a3ca-4e23-91e2-288b52556e32", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ecbe8f7b5101da12b2b49c50ddc59b42075bc0de5cf1daedf5b0924f19a04802", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "074dd052-0636-443f-9ac8-53eaa76ddfdd", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "dc7f87db191bddb260f0cf1d5c8b2cdcb00defdcdd57a7cbb9e6a066e06590fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When mounting a Lustre file system, mount(8) executes the `/sbin/mount.lustre` command to complete the mount. The mount command supports these options specific to a Lustre file system:\n\n| **Server options**     | **Description**                                              |\n| ---------------------- | ------------------------------------------------------------ |\n| `abort_recov`          | Aborts recovery when starting a target                       |\n| `nosvc`                | Starts only MGS/MGC servers                                  |\n| `nomgs`                | Start a MDT with a co-located MGS without starting the MGS   |\n| `exclude`              | Starts with a dead OST                                       |\n| `md_stripe_cache_size` | Sets the stripe cache size for server side disk with a striped raid configuration |\n\n| **Client options**              | **Description**                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| `flock/noflock/localflock`      | Enables/disables global flock or local flock support         |\n| `user_xattr/nouser_xattr`       | Enables/disables user-extended attributes                    |\n| `user_fid2path/nouser_fid2path` | Enables/disables FID to path translation by regular users    |\n| `retry=`                        | Number of times a client will retry to mount the file system |\n\n## Handling Timeouts\n\nTimeouts are the most common cause of hung applications. After a timeout involving an MDS or failover OST, applications attempting to access the disconnected resource wait until the connection gets established.\n\nWhen a client performs any remote operation, it gives the server a reasonable amount of time to respond. If a server does not reply either due to a down network, hung server, or any other reason, a timeout occurs which requires a recovery.\n\nIf a timeout occurs, a message (similar to this one), appears on the console of the client, and in`/var/log/messages`:\n\n```\nLustreError: 26597:(client.c:810:ptlrpc_expire_one_request()) @@@ timeout\n\nreq@a2d45200 x5886/t0 o38->mds_svc_UUID@NID_mds_UUID:12 lens 168/64 ref 1 fl\n\nRPC:/0/0 rc 0\n```", "mimetype": "text/plain", "start_char_idx": 33331, "end_char_idx": 35533, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9a34abb-43de-4671-b277-a483dacf3813": {"__data__": {"id_": "f9a34abb-43de-4671-b277-a483dacf3813", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/41-Programming Interfaces.md", "file_name": "41-Programming Interfaces.md", "file_type": "text/markdown", "file_size": 4426, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a4f6215-823c-41aa-9fdb-a39e2944433f", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/41-Programming Interfaces.md", "file_name": "41-Programming Interfaces.md", "file_type": "text/markdown", "file_size": 4426, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "16618e445361b1e0a1852febdb6b836b7a8f5c188f0d60657a182183787b9f39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50681729-1b26-434e-a7a1-b3374e83150b", "node_type": "1", "metadata": {}, "hash": "8f7afec4668a81957b5550cdffa6a7b7b32909181a2c639172a3c3be2774bab8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Programming Interfaces\n\n- [Programming Interfaces](#programming-interfaces)\n- [User/Group Upcall](#usergroup-upcall)\n  * [Synopsis](#synopsis)\n  * [Description](#description)\n    + [Primary and Secondary Groups](#primary-and-secondary-groups)\n  * [Data Structures](#data-structures)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "50681729-1b26-434e-a7a1-b3374e83150b": {"__data__": {"id_": "50681729-1b26-434e-a7a1-b3374e83150b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/41-Programming Interfaces.md", "file_name": "41-Programming Interfaces.md", "file_type": "text/markdown", "file_size": 4426, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a4f6215-823c-41aa-9fdb-a39e2944433f", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/41-Programming Interfaces.md", "file_name": "41-Programming Interfaces.md", "file_type": "text/markdown", "file_size": 4426, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "16618e445361b1e0a1852febdb6b836b7a8f5c188f0d60657a182183787b9f39", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9a34abb-43de-4671-b277-a483dacf3813", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/41-Programming Interfaces.md", "file_name": "41-Programming Interfaces.md", "file_type": "text/markdown", "file_size": 4426, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "83a10b485a49594aa73cf185619e5fcf3a004a935942dc19ead68fc6c2b2845f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Programming Interfaces\n\n- [Programming Interfaces](#programming-interfaces)\n- [User/Group Upcall](#usergroup-upcall)\n  * [Synopsis](#synopsis)\n  * [Description](#description)\n    + [Primary and Secondary Groups](#primary-and-secondary-groups)\n  * [Data Structures](#data-structures)\n\n\nThis chapter describes public programming interfaces to that can be used to control various aspects of a Lustre file system from userspace. This chapter includes the following sections:\n\n- [the section called \u201cUser/Group Upcall\u201d](#usergroup-upcall)\n- [the section called \u201cData Structures\u201d](#data-structures)\n\n**Note**\n\nLustre programming interface man pages are found in the `lustre/doc` folder.\n\n## User/Group Upcall\n\nThis section describes the supplementary user/group upcall, which allows the MDS to retrieve and verify the supplementary groups to which a particular user is assigned. This avoids the need to pass all the supplementary groups from the client to the MDS with every RPC.\n\n**Note**\n\nFor information about universal UID/GID requirements in a Lustre file system environment, see [*the section called \u201cEnvironmental Requirements\u201d*](02.05-Installing%20the%20Lustre%20Software.md#environmental-requirements).\n\n### Synopsis\n\nThe MDS uses the utility as specified by `lctl get_param mdt.${FSNAME}-MDT{xxxx}.identity_upcall` to look up the supplied UID in order to retrieve the user's supplementary group membership. The result is temporarily cached in the kernel (for five minutes, by default) to avoid the overhead of calling into userspace repeatedly.\n\n### Description\n\nThe `identity_upcall` parameter contains the path to an executable that is run to map a numeric UID to a group membership list. This upcall executable opens the `mdt.${FSNAME}-MDT{xxxx}.identity_info` parameter file and writes the related `identity_downcall_data` data structure (see *[the section called \u201cData Structures\u201d](#data-structures)*). The upcall is configured with `lctl set_param mdt.${FSNAME}-MDT{xxxx}.identity_upcall`.\n\nThe default identity upcall program installed is `lustre/utils/l_getidentity.c` in the Lustre source distribution.\n\n#### Primary and Secondary Groups\n\nThe mechanism for the primary/secondary group is as follows:\n\n- The MDS issues an upcall (set per MDS) to map the numeric UID to the supplementary group(s).\n- If there is no upcall or if there is an upcall and it fails, one supplementary group at most will be added as supplied by the client.\n- The default upcall `/usr/sbin/l_getidentity` can interact with the user/group database on the MDS to map the UID to the GID and supplementary GID. The user/group database depends on how authentication is configured on the MDS, such as local `/etc/passwd`, Network Information Service (NIS), Lightweight Directory Access Protocol (LDAP), or SMB Domain services, as configured. If the upcall interface is set to NONE, then upcall is disabled, and the MDS uses only the UID, GID, and one supplementary GID supplied by the client.\n- The MDS will wait a limited time for the group upcall program to complete, to avoid MDS threads and clients hanging due to errors accessing a remote service node. The upcall must finish within 30s before the MDS will continue without the supplementary data. The upcall timeout in seconds can be set on the MDS using: `lctl set_param mdt.*.identity_acquire_expire=*seconds*`\n- The default group upcall is set permanently by `mkfs.lustre`. To set a custom upcall for a particular filesystem, use `tunefs.lustre --param` or `lctl set_param -P mdt.FSNAME-MDTxxxx.identity_upcall=path`\n- The group downcall data is cached by the kernel to avoid repeated upcalls for the same user slowing down the MDS. This cache is expired from the kernel after 1200s (20 minutes) by default. The cache age in seconds can be set on the MDS using: `lctl set_param mdt.*.identity_expire=seconds`\n- To force eviction of cached identity data (e.g. after adding or removing a user from a supplementary group), the cache entry for a specific numeric UID can be flushed on the MDS using:`lctl set_param mdt.*.identity_flush=UID` To flush the cached records for all users from cache, use `-1` for the UID: `lctl set_param mdt.*.identity_flush=-1`\n\n### Data Structures\n\n```\nstruct perm_downcall_data {\n     __u64 pdd_nid;\n     __u32 pdd_perm;\n     __u32 pdd_padding;\n};\n\nstruct identity_downcall_data{\n     __u32        idd_magic;\n     :         \n     :\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e570bbb-dda6-43c6-a96e-7482602f3c63": {"__data__": {"id_": "9e570bbb-dda6-43c6-a96e-7482602f3c63", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abc006a-7fe9-4567-a652-25157dbb2a76", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ed96385-8787-408a-89de-630559f85dde", "node_type": "1", "metadata": {}, "hash": "611d3e4a2a0bd8e2991a73a7827754d5c4959f0a13f5f0acb5c0f861eacde9fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Setting Lustre Properties in a C Program (`llapi`)\n\n- [Setting Lustre Properties in a C Program (`llapi`)](#setting-lustre-properties-in-a-c-program-llapi)\n- [`llapi_file_create`](#llapi_file_create)\n  * [Synopsis](#synopsis)\n  * [Description](#description)\n  * [Examples](#examples)\n- [llapi_file_get_stripe](#llapi_file_get_stripe)\n  * [Synopsis](#synopsis-1)\n  * [Description](#description-1)\n  * [Return Values](#return-values)\n  * [Errors](#errors)\n  * [Examples](#examples-1)\n- [`llapi_file_open`](#llapi_file_open)\n  * [Synopsis](#synopsis-2)\n  * [Description](#description-2)\n  * [Return Values](#return-values-1)\n  * [Errors](#errors-1)\n  * [Example](#example)\n- [`llapi_quotactl`](#llapi_quotactl)\n  * [Synopsis](#synopsis-3)\n  * [Description](#description-3)\n  * [Return Values](#return-values-2)\n  * [Errors](#errors-2)\n- [`llapi_path2fid`](#llapi_path2fid)\n  * [Synopsis](#synopsis-4)\n  * [Description](#description-4)\n  * [Return Values](#return-values-3)\n- [`llapi_ladvise`](#llapi_ladvise)\n  * [Synopsis](#synopsis-5)\n  * [Description](#description-5)\n  * [Return Values](#return-values-4)\n  * [Errors](#errors-3)\n- [Example Using the `llapi` Library](#example-using-the-llapi-library)\n  * [See Also](#see-also)\n\n\nThis chapter describes the `llapi` library of commands used for setting Lustre file properties within a C program running in a cluster environment, such as a data processing or MPI application. The commands described in this chapter are:\n\n- [the section called \u201c `llapi_file_create` \u201d](#llapi_file_create)\n- [the section called \u201c llapi_file_get_stripe\u201d](#llapi_file_get_stripe)\n- [the section called \u201c `llapi_file_open` \u201d](#llapi_file_open)\n- [the section called \u201c `llapi_quotactl` \u201d](#llapi_quotactl)\n- [the section called \u201c `llapi_path2fid` \u201d](#llapi_path2fid)\n\n**Note**\n\nLustre programming interface man pages are found in the `lustre/doc` folder.\n\n## `llapi_file_create`\n\nUse\u00a0`llapi_file_create`\u00a0to set Lustre properties for a new file.\n\n### Synopsis\n\n```\n#include <lustre/lustreapi.h>\n\nint llapi_file_create(char *name, long stripe_size, int stripe_offset, int stripe_count, int stripe_pattern);\n```\n\n### Description\n\nThe `llapi_file_create()` function sets a file descriptor's Lustre file system striping information. The file descriptor is then accessed with `open()`.\n\n| **Option**            | **Description**                                              |\n| --------------------- | ------------------------------------------------------------ |\n| `llapi_file_create()` | If the file already exists, this parameter returns to '`EEXIST`'. If the stripe parameters are invalid, this parameter returns to '`EINVAL`'. |\n| `stripe_size`         | This value must be an even multiple of system page size, as shown by `getpagesize()`. The default Lustre stripe size is 4MB. |\n| `stripe_offset`       | Indicates the starting OST for this file.                    |\n| `stripe_count`        | Indicates the number of OSTs that this file will be striped across. |\n| `stripe_pattern`      | Indicates the RAID pattern.                                  |\n\n**Note**\n\nCurrently, only RAID 0 is supported. To use the system defaults, set these values: `stripe_size` = 0, `stripe_offset` = -1, `stripe_count` = 0, `stripe_pattern` = 0\n\n### Examples\n\nSystem default size is 4 MB.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ed96385-8787-408a-89de-630559f85dde": {"__data__": {"id_": "6ed96385-8787-408a-89de-630559f85dde", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abc006a-7fe9-4567-a652-25157dbb2a76", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e570bbb-dda6-43c6-a96e-7482602f3c63", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2fb61a40f6e3a4939b21c75e1a1c6745594c90df322f50789d497bb5674d65a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "805e0cb0-a020-4fab-88d1-3575f69b6684", "node_type": "1", "metadata": {}, "hash": "88352bfaa0e8ba2d416f0b33da8a5a3d73f79d3b749b183b0a1daff3b96e1cef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the stripe parameters are invalid, this parameter returns to '`EINVAL`'. |\n| `stripe_size`         | This value must be an even multiple of system page size, as shown by `getpagesize()`. The default Lustre stripe size is 4MB. |\n| `stripe_offset`       | Indicates the starting OST for this file.                    |\n| `stripe_count`        | Indicates the number of OSTs that this file will be striped across. |\n| `stripe_pattern`      | Indicates the RAID pattern.                                  |\n\n**Note**\n\nCurrently, only RAID 0 is supported. To use the system defaults, set these values: `stripe_size` = 0, `stripe_offset` = -1, `stripe_count` = 0, `stripe_pattern` = 0\n\n### Examples\n\nSystem default size is 4 MB.\n\n```\nchar *tfile = TESTFILE;\nint stripe_size = 65536\n```\n\nTo start at default, run:\n\n```\nint stripe_offset = -1\n```\n\nTo start at the default, run:\n\n```\nint stripe_count = 1\n```\n\nTo set a single stripe for this example, run:\n\n```\nint stripe_pattern = 0\n```\n\nCurrently, only RAID 0 is supported.\n\n```\nint stripe_pattern = 0; \nint rc, fd; \nrc = llapi_file_create(tfile, stripe_size,stripe_offset, stripe_count,stripe_pattern);\n```\n\nResult code is inverted, you may return with '`EINVAL`' or an ioctl error.\n\n```\nif (rc) {\nfprintf(stderr,\"llapi_file_create failed: %d (%s) 0, rc, strerror(-rc));return -1; }\n```\n\n`llapi_file_create` closes the file descriptor. You must re-open the descriptor. To do this, run:\n\n```\nfd = open(tfile, O_CREAT | O_RDWR | O_LOV_DELAY_CREATE, 0644); if (fd < 0) \\ { \nfprintf(stderr, \"Can't open %s file: %s0, tfile,\nstr-\nerror(errno));\nreturn -1;\n}\n```\n\n## llapi_file_get_stripe\n\nUse `llapi_file_get_stripe` to get striping information for a file or directory on a Lustre file system.\n\n### Synopsis\n\n```\n#include <lustre/lustreapi.h>\n \nint llapi_file_get_stripe(const char *path, void *lum);\n```\n### Description\n\nThe `llapi_file_get_stripe()` function returns striping information for a file or directory *path* in *lum* (which should point to a large enough memory region) in one of the following formats:\n\n```\nstruct lov_user_md_v1 {\n__u32 lmm_magic;\n__u32 lmm_pattern;\n__u64 lmm_object_id;\n__u64 lmm_object_seq;\n__u32 lmm_stripe_size;\n__u16 lmm_stripe_count;\n__u16 lmm_stripe_offset;\nstruct lov_user_ost_data_v1 lmm_objects[0];\n} __attribute__((packed));\nstruct lov_user_md_v3 {\n__u32 lmm_magic;\n__u32 lmm_pattern;\n__u64 lmm_object_id;\n__u64 lmm_object_seq;\n__u32 lmm_stripe_size;\n__u16 lmm_stripe_count;\n__u16 lmm_stripe_offset;\nchar lmm_pool_name[LOV_MAXPOOLNAME];\nstruct lov_user_ost_data_v1 lmm_objects[0];\n} __attribute__((packed));\n```\n\n### Return Values\n\n`llapi_file_get_stripe()` returns:\n\n`0` On success\n\n`!= 0` On failure, `errno` is set appropriately\n\n### Errors\n\n| **Errors**     | **Description**                                     |\n| -------------- | --------------------------------------------------- |\n| `ENOMEM`       | Failed to allocate memory                           |\n| `ENAMETOOLONG` | Path was too long                                   |\n| `ENOENT`       | Path does not point to a file or directory          |\n| `ENOTTY`       | Path does not point to a Lustre file system         |\n| `EFAULT`       | Memory region pointed by lum is not properly mapped |\n\n### Examples\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <errno.h>\n#include <lustre/lustreapi.h>\n\nstatic inline int maxint(int a, int b)\n{\n\treturn a > b ?", "mimetype": "text/plain", "start_char_idx": 2580, "end_char_idx": 5983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "805e0cb0-a020-4fab-88d1-3575f69b6684": {"__data__": {"id_": "805e0cb0-a020-4fab-88d1-3575f69b6684", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abc006a-7fe9-4567-a652-25157dbb2a76", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ed96385-8787-408a-89de-630559f85dde", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ffafd385f9c52fa4fb47c75f05edc5b4c53d2e9427c53c2e4e06e79852f09974", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed6544c1-11ec-45a5-bf0d-9d4b7ebfcbb3", "node_type": "1", "metadata": {}, "hash": "1179f8844eb5aef10ec02706045405fd09115a6a5a03c34f4beac6ff2ff66ffc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a : b;\n}\nstatic void *alloc_lum()\n{\n\tint v1, v3, join;\n\tv1 = sizeof(struct lov_user_md_v1) +\n\t\tLOV_MAX_STRIPE_COUNT * sizeof(struct lov_user_ost_data_v1);\n\tv3 = sizeof(struct lov_user_md_v3) +\n\t\tLOV_MAX_STRIPE_COUNT * sizeof(struct lov_user_ost_data_v1);\n\treturn malloc(maxint(v1, v3));\n}\nint main(int argc, char** argv)\n{\n\tstruct lov_user_md *lum_file = NULL;\n\tint rc;\n\tint lum_size;\n\tif (argc != 2) {\n\t\tfprintf(stderr, \"Usage: %s <filename>\\n\", argv[0]);\n\t\treturn 1;\n\t}\n\tlum_file = alloc_lum();\n\tif (lum_file == NULL) {\n\t\trc = ENOMEM;\n\t\tgoto cleanup;\n\t}\n\trc = llapi_file_get_stripe(argv[1], lum_file);\n\tif (rc) {\n\t\trc = errno;\n\t\tgoto cleanup;\n\t}\n\t/* stripe_size stripe_count */\n\tprintf(\"%d %d\\n\",\n\t\t\tlum_file->lmm_stripe_size,\n\t\t\tlum_file->lmm_stripe_count);\ncleanup:\n\tif (lum_file != NULL)\n\t\tfree(lum_file);\n\treturn rc;\n}\n```\n\n## `llapi_file_open`\n\nThe\u00a0`llapi_file_open`\u00a0command opens (or creates) a file or device on a Lustre file system.\n\n### Synopsis\n\n```\n#include <lustre/lustreapi.h>\nint llapi_file_open(const char *name, int flags, int mode, \n   unsigned long long stripe_size, int stripe_offset, \n   int stripe_count, int stripe_pattern);\nint llapi_file_create(const char *name, unsigned long long stripe_size, \n   int stripe_offset, int stripe_count, \n   int stripe_pattern);\n```\n\n### Description\n\nThe `llapi_file_create()` call is equivalent to the `llapi_file_open` call with *flags* equal to `O_CREAT|O_WRONLY`and *mode* equal to `0644`, followed by file close.\n\n`llapi_file_open()` opens a file with a given name on a Lustre file system.\n\n| **Option**       | **Description**                                              |\n| ---------------- | ------------------------------------------------------------ |\n| `flags`          | Can be a combination of `O_RDONLY`, `O_WRONLY`, `O_RDWR`, `O_CREAT`, `O_EXCL`, `O_NOCTTY`, `O_TRUNC`, `O_APPEND`, `O_NONBLOCK`, `O_SYNC`, `FASYNC`, `O_DIRECT`, `O_LARGEFILE`, `O_DIRECTORY`, `O_NOFOLLOW`, `O_NOATIME`. |\n| `mode`           | Specifies the permission bits to be used for a new file when `O_CREAT` is used. |\n| `stripe_size`    | Specifies stripe size (in bytes). Should be multiple of 64 KB, not exceeding 4 GB. |\n| `stripe_offset`  | Specifies an OST index from which the file should start. The default value is -1. |\n| `stripe_count`   | Specifies the number of OSTs to stripe the file across. The default value is -1. |\n| `stripe_pattern` | Specifies the striping pattern. In this release of the Lustre software, only `LOV_PATTERN_RAID0`is available. The default value is 0. |\n\n### Return Values\n\n`llapi_file_open()` and `llapi_file_create()` return:\n\n`>=0` On success, for `llapi_file_open` the return value is a file descriptor\n\n`<0` On failure, the absolute value is an error code\n\n### Errors\n\n| **Errors** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `EINVAL`   | `stripe_size` or `stripe_offset` or `stripe_count` or `stripe_pattern` is invalid. |\n| `EEXIST`   | Striping information has already been set and cannot be altered; `name` already exists. |\n| `EALREADY` | Striping information has already been set and cannot be altered |\n| `ENOTTY`   | `name` may not point to a Lustre file system.", "mimetype": "text/plain", "start_char_idx": 5984, "end_char_idx": 9243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed6544c1-11ec-45a5-bf0d-9d4b7ebfcbb3": {"__data__": {"id_": "ed6544c1-11ec-45a5-bf0d-9d4b7ebfcbb3", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abc006a-7fe9-4567-a652-25157dbb2a76", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "805e0cb0-a020-4fab-88d1-3575f69b6684", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d18add60cb742c75b0b7fb4bc1df2856aabbe10cfdf5f0cc9ba96e0dae7bdf9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2890be5-78c3-4a3b-a90d-816749ed643c", "node_type": "1", "metadata": {}, "hash": "53c4f57e62578c28c91f9b2b835f316a8a77779809da74f79994db3fd39a31d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The default value is 0. |\n\n### Return Values\n\n`llapi_file_open()` and `llapi_file_create()` return:\n\n`>=0` On success, for `llapi_file_open` the return value is a file descriptor\n\n`<0` On failure, the absolute value is an error code\n\n### Errors\n\n| **Errors** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `EINVAL`   | `stripe_size` or `stripe_offset` or `stripe_count` or `stripe_pattern` is invalid. |\n| `EEXIST`   | Striping information has already been set and cannot be altered; `name` already exists. |\n| `EALREADY` | Striping information has already been set and cannot be altered |\n| `ENOTTY`   | `name` may not point to a Lustre file system.                |\n\n### Example\n\n```\n#include <stdio.h>\n#include <lustre/lustreapi.h>\n\nint main(int argc, char *argv[])\n{\n\tint rc;\n\tif (argc != 2)\n\t\treturn -1;\n\trc = llapi_file_create(argv[1], 1048576, 0, 2, LOV_PATTERN_RAID0);\n\tif (rc < 0) {\n\t\tfprintf(stderr, \"file creation has failed, %s\\n\",         strerror(-rc));\n\t\treturn -1;\n\t}\n\tprintf(\"%s with stripe size 1048576, striped across 2 OSTs,\"\n\t\t\t\" has been created!\\n\", argv[1]);\n\treturn 0;\n}\n```\n\n## `llapi_quotactl`\n\nUse\u00a0`llapi_quotact`l to manipulate disk quotas on a Lustre file system.\n\n### Synopsis\n\n```\n#include <lustre/lustreapi.h>\nint llapi_quotactl(char\" \" *mnt,\" \" struct if_quotactl\" \" *qctl)\n \nstruct if_quotactl {\n        __u32                   qc_cmd;\n        __u32                   qc_type;\n        __u32                   qc_id;\n        __u32                   qc_stat;\n        struct obd_dqinfo       qc_dqinfo;\n        struct obd_dqblk        qc_dqblk;\n        char                    obd_type[16];\n        struct obd_uuid         obd_uuid;\n};\nstruct obd_dqblk {\n        __u64 dqb_bhardlimit;\n        __u64 dqb_bsoftlimit;\n        __u64 dqb_curspace;\n        __u64 dqb_ihardlimit;\n        __u64 dqb_isoftlimit;\n        __u64 dqb_curinodes;\n        __u64 dqb_btime;\n        __u64 dqb_itime;\n        __u32 dqb_valid;\n        __u32 padding;\n};\nstruct obd_dqinfo {\n        __u64 dqi_bgrace;\n        __u64 dqi_igrace;\n        __u32 dqi_flags;\n        __u32 dqi_valid;\n};\nstruct obd_uuid {\n        char uuid[40];\n};\n```\n\n### Description\n\nThe `llapi_quotactl()` command manipulates disk quotas on a Lustre file system mount. qc_cmd indicates a command to be applied to UID `qc_id` or GID `qc_id`.\n\n| **Option**          | **Description**                                              |\n| ------------------- | ------------------------------------------------------------ |\n| `LUSTRE_Q_GETQUOTA` | Gets disk quota limits and current usage for user or group *qc_id*. *qc_type* is `USRQUOTA` or `GRPQUOTA`. *uuid* may be filled with `OBD UUID` string to query quota information from a specific node. *dqb_valid* may be set nonzero to query information only from MDS. If *uuid* is an empty string and *dqb_valid* is zero then cluster-wide limits and usage are returned. On return, *obd_dqblk* contains the requested information (block limits unit is kilobyte). Quotas must be turned on before using this command. |\n| `LUSTRE_Q_SETQUOTA` | Sets disk quota limits for user or group *qc_id*. *qc_type* is `USRQUOTA` or `GRPQUOTA`. *dqb_valid*must be set to `QIF_ILIMITS`, `QIF_BLIMITS` or `QIF_LIMITS` (both inode limits and block limits) dependent on updating limits. *obd_dqblk* must be filled with limits values (as set in *dqb_valid*, block limits unit is kilobyte). Quotas must be turned on before using this command.", "mimetype": "text/plain", "start_char_idx": 8495, "end_char_idx": 12024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2890be5-78c3-4a3b-a90d-816749ed643c": {"__data__": {"id_": "f2890be5-78c3-4a3b-a90d-816749ed643c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abc006a-7fe9-4567-a652-25157dbb2a76", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed6544c1-11ec-45a5-bf0d-9d4b7ebfcbb3", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "dcecfcf9576c050cbafb28cfec10c9676b2b0c83a2b645307cd6edbd8b0e9f23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97ce037c-aa05-4ddf-b60f-406f02a30279", "node_type": "1", "metadata": {}, "hash": "05ae5eeff5d5f4bcf2ad550f54a2ff7122e2243008aed288e45d212e8cd35b8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If *uuid* is an empty string and *dqb_valid* is zero then cluster-wide limits and usage are returned. On return, *obd_dqblk* contains the requested information (block limits unit is kilobyte). Quotas must be turned on before using this command. |\n| `LUSTRE_Q_SETQUOTA` | Sets disk quota limits for user or group *qc_id*. *qc_type* is `USRQUOTA` or `GRPQUOTA`. *dqb_valid*must be set to `QIF_ILIMITS`, `QIF_BLIMITS` or `QIF_LIMITS` (both inode limits and block limits) dependent on updating limits. *obd_dqblk* must be filled with limits values (as set in *dqb_valid*, block limits unit is kilobyte). Quotas must be turned on before using this command. |\n| `LUSTRE_Q_GETINFO`  | Gets information about quotas. *qc_type* is either `USRQUOTA` or `GRPQUOTA`. On return,*dqi_igrace* is inode grace time (in seconds), *dqi_bgrace* is block grace time (in seconds),*dqi_flags* is not used by the current release of the Lustre software. |\n| `LUSTRE_Q_SETINFO`  | Sets quota information (like grace times). *qc_type* is either `USRQUOTA` or `GRPQUOTA`.*dqi_igrace* is inode grace time (in seconds), *dqi_bgrace* is block grace time (in seconds),*dqi_flags* is not used by the current release of the Lustre software and must be zeroed. |\n\n### Return Values\n\n`llapi_quotactl()` returns:\n\n`0` On success\n\n`-1 `On failure and sets error number (`errno`) to indicate the error\n\n### Errors\n\n`llapi_quotactl` errors are described below.\n\n| **Errors** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `EFAULT`   | *qctl* is invalid.                                           |\n| `ENOSYS`   | Kernel or Lustre modules have not been compiled with the `QUOTA` option. |\n| `ENOMEM`   | Insufficient memory to complete operation.                   |\n| `ENOTTY`   | *qc_cmd* is invalid.                                         |\n| `ENOENT`   | *uuid* does not correspond to OBD or *mnt* does not exist.   |\n| `EPERM`    | The call is privileged and the caller is not the super user. |\n| `ESRCH`    | No disk quota is found for the indicated user. Quotas have not been turned on for this file system. |\n\n## `llapi_path2fid`\n\nUse\u00a0`llapi_path2fid`\u00a0to get the FID from the pathname.\n\n### Synopsis\n\n```\n#include <lustre/lustreapi.h>\n \nint llapi_path2fid(const char *path, unsigned long long *seq, unsigned long *oid, unsigned long *ver)\n```\n\n### Description\n\nThe `llapi_path2fid` function returns the FID (sequence : object ID : version) for the pathname.\n\n### Return Values\n\n`llapi_path2fid` returns:\n\n`0` On success\n\nnon-zero value On failure\n\n\n\nIntroduced in Lustre 2.9\n\n## `llapi_ladvise`\n\nUse\u00a0`llapi_ladvise`\u00a0to give IO advice/hints on a Lustre file to the server.\n\n### Synopsis\n\n```\n#include <lustre/lustreapi.h>\nint llapi_ladvise(int fd, unsigned long long flags,\n                  int num_advise, struct llapi_lu_ladvise *ladvise);\n                                \nstruct llapi_lu_ladvise {\n  __u16 lla_advice;       /* advice type */\n  __u16 lla_value1;       /* values for different advice types */\n  __u32 lla_value2;\n  __u64 lla_start;        /* first byte of extent for advice */\n  __u64 lla_end;          /* last byte of extent for advice */\n  __u32 lla_value3;\n  __u32 lla_value4;\n};\n```\n\n### Description\n\nThe `llapi_ladvise` function passes an array of *num_advise* I/O hints (up to a maximum of *LAH_COUNT_MAX*items) in ladvise for the file descriptor *fd* from an application to one or more Lustre servers.", "mimetype": "text/plain", "start_char_idx": 11373, "end_char_idx": 14865, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97ce037c-aa05-4ddf-b60f-406f02a30279": {"__data__": {"id_": "97ce037c-aa05-4ddf-b60f-406f02a30279", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abc006a-7fe9-4567-a652-25157dbb2a76", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2890be5-78c3-4a3b-a90d-816749ed643c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1c0fb047da72923fd068174e73c7d57ea7ba686afea7019d2cb1b585e5362baf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "083b98a1-e73e-45b1-a255-ba723e128e20", "node_type": "1", "metadata": {}, "hash": "3d80a9d51c50297612249b3d945f02ffb5ebb12312cf95ed86c1b7caa576b0d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Optionally, *flags* can modify how the advice will be processed via bitwise-or'd values:\n\n- `LF_ASYNC`: Clients return to userspace immediately after submitting ladvise RPCs, leaving server threads to handle the advices asynchronously.\n- `LF_UNSET`: Unset/clear a previous advice (Currently only supports LU_ADVISE_LOCKNOEXPAND).\n\nEach of the *ladvise* elements is an *llapi_lu_ladvise* structure, which contains the following fields:\n\n| **Field**                                        | **Description**                                              |\n| ------------------------------------------------ | ------------------------------------------------------------ |\n| `lla_ladvice`                                    | Specifies the advice for the given file range, currently one of:`LU_LADVISE_WILLREAD`: Prefetch data into server cache using optimum I/O size for the server.`LU_LADVISE_DONTNEED`: Clean cached data for the specified file range(s) on the server. |\n| `lla_start`                                      | The offset in bytes for the start of this advice.            |\n| `lla_end`                                        | The offset in bytes (non-inclusive) for the end of this advice. |\n| `lla_value1``lla_value2``lla_value3``lla_value4` | Additional arguments for future advice types and should be set to zero if not explicitly required for a given advice type. Advice-specific names for these fields follow. |\n| `lla_lockahead_mode`                             | When using LU_ADVISE_LOCKAHEAD, the 'lla_value1' field is used to communicate the requested lock mode, and can be referred to as lla_lockahead_mode. |\n| `lla_peradvice_flags`                            | When using advices which support them, the 'lla_value2' field is used to communicate per-advice flags and can be referred to as 'lla_peradvice_flags'. Both LF_ASYNC and LF_UNSET are supported as peradvice flags. |\n| `lla_lockahead_result`                           | When using LU_ADVISE_LOCKAHEAD, the 'lla_value3' field is used to communicate the result of the request, and can be referred to as lla_lockahead_result. |\n\n`llapi_ladvise()` forwards the advice to Lustre servers without guaranteeing how and when servers will react to the advice. Actions may or may not be triggered when the advices are received, depending on the type of the advice as well as the real-time decision of the affected server-side components.\n\nA typical usage of `llapi_ladvise()` is to enable applications and users (via `lfs ladvise`) with external knowledge about application I/O patterns to intervene in server-side I/O handling. For example, if a group of different clients are doing small random reads of a file, prefetching pages into OSS cache with big linear reads before the random IO is an overall net benefit. Fetching that data into each client cache with *fadvise()* may not be beneficial, due to much more data being sent to the clients.\n\nLU_LADVISE_LOCKAHEAD merits a special comment. While it is possible and encouraged to use it directly in your application to avoid lock contention (primarily for writing to a single file from multiple clients), it will also be available in the MPI-I/O / MPICH library from ANL for use with the i/o aggregation mode of that library. This is intended (eventually) to be the primary way this feature is used.\n\nAt the time of writing, this support is proposed as a patch but is not yet merged in to the public ANL code base. Users are encouraged to check their MPICH documentation and/or check with their library provider about support.\n\nWhile conceptually similar to the *posix_fadvise* and Linux *fadvise* system calls, the main difference of`llapi_ladvise()` is that *fadvise() / posix_fadvise()* are client side mechanisms that do not pass advice to the filesystem, while `llapi_ladvise()` sends advice or hints to one or more Lustre servers on which the file is stored. In some cases it may be desirable to use both interfaces.\n\n### Return Values\n\n`llapi_ladvise` returns:\n\n`0` On success\n\n`-1` if an error occurred (in which case, errno is set appropriately).\n\n### Errors\n\n| **Error**  | **Description**                                            |\n| ---------- | ---------------------------------------------------------- |\n| `ENOMEM`   | Insufficient memory to complete operation.                 |\n| `EINVAL`   | One or more invalid arguments are given.                   |\n| `EFAULT`   | Memory region pointed by `ladvise` is not properly mapped. |\n| `ENOTSUPP` | Advice type is not supported.", "mimetype": "text/plain", "start_char_idx": 14866, "end_char_idx": 19385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "083b98a1-e73e-45b1-a255-ba723e128e20": {"__data__": {"id_": "083b98a1-e73e-45b1-a255-ba723e128e20", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abc006a-7fe9-4567-a652-25157dbb2a76", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97ce037c-aa05-4ddf-b60f-406f02a30279", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c6924067c80fa767980bc17c2e8dbc3258200a2249d4ac2bba995a415639bc6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b5fb6b8-1035-4281-b23d-892df0040323", "node_type": "1", "metadata": {}, "hash": "82eedeea9974730c9124f907aec4a47827600222278b71ede2213db436d5680c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In some cases it may be desirable to use both interfaces.\n\n### Return Values\n\n`llapi_ladvise` returns:\n\n`0` On success\n\n`-1` if an error occurred (in which case, errno is set appropriately).\n\n### Errors\n\n| **Error**  | **Description**                                            |\n| ---------- | ---------------------------------------------------------- |\n| `ENOMEM`   | Insufficient memory to complete operation.                 |\n| `EINVAL`   | One or more invalid arguments are given.                   |\n| `EFAULT`   | Memory region pointed by `ladvise` is not properly mapped. |\n| `ENOTSUPP` | Advice type is not supported.                              |\n\n## Example Using the `llapi` Library\n\nUse `llapi_file_create` to set Lustre software properties for a new file. For a synopsis and description of `llapi_file_create` and examples of how to use it, see [*Configuration Files and Module Parameters*](06.06-Configuration%20Files%20and%20Module%20Parameters.md).\n\nYou can set striping from inside programs like `ioctl`. To compile the sample program, you need to install the Lustre client source RPM.\n\n**A simple C program to demonstrate striping API - libtest.c**\n\n```\n/* -*- mode: c; c-basic-offset: 8; indent-tabs-mode: nil; -*-\n * vim:expandtab:shiftwidth=8:tabstop=8:\n *\n * lustredemo - a simple example of lustreapi functions\n */\n#include <stdio.h>\n#include <fcntl.h>\n#include <dirent.h>\n#include <errno.h>\n#include <stdlib.h>\n#include <lustre/lustreapi.h>\n#define MAX_OSTS 1024\n#define LOV_EA_SIZE(lum, num) (sizeof(*lum) + num * sizeof(*lum->lmm_objects))\n#define LOV_EA_MAX(lum) LOV_EA_SIZE(lum, MAX_OSTS)\n\n/*\n * This program provides crude examples of using the lustreapi API functions\n */\n/* Change these definitions to suit */\n\n#define TESTDIR \"/tmp\"           /* Results directory */\n#define TESTFILE \"lustre_dummy\"  /* Name for the file we create/destroy */\n#define FILESIZE 262144                    /* Size of the file in words */\n#define DUMWORD \"DEADBEEF\"       /* Dummy word used to fill files */\n#define MY_STRIPE_WIDTH 2                  /* Set this to the number of OST required */\n#define MY_LUSTRE_DIR \"/mnt/lustre/ftest\"\n\nint close_file(int fd)\n{\n        if (close(fd) < 0) {\n                fprintf(stderr, \"File close failed: %d (%s)\\n\", errno, strerror(errno));\n                return -1;\n        }\n        return 0;\n}\n\nint write_file(int fd)\n{\n        char *stng =  DUMWORD;\n        int cnt = 0;\n\n        for( cnt = 0; cnt < FILESIZE; cnt++) {\n                write(fd, stng, sizeof(stng));\n        }\n        return 0;\n}\n/* Open a file, set a specific stripe count, size and starting OST\n *    Adjust the parameters to suit */\nint open_stripe_file()\n{\n        char *tfile = TESTFILE;\n        int stripe_size = 65536;    /* System default is 4M */\n        int stripe_offset = -1;     /* Start at default */\n        int stripe_count = MY_STRIPE_WIDTH;  /*Single stripe for this demo*/\n        int stripe_pattern = 0;     /* only RAID 0 at this time */\n        int rc, fd;\n\n        rc = llapi_file_create(tfile,\n                        stripe_size,stripe_offset,stripe_count,stripe_pattern);\n        /* result code is inverted, we may return -EINVAL or an ioctl error.", "mimetype": "text/plain", "start_char_idx": 18757, "end_char_idx": 21957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b5fb6b8-1035-4281-b23d-892df0040323": {"__data__": {"id_": "6b5fb6b8-1035-4281-b23d-892df0040323", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abc006a-7fe9-4567-a652-25157dbb2a76", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "083b98a1-e73e-45b1-a255-ba723e128e20", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d5587b1ce6d729f6caedc3d4cd58081182b51d73797e151827817bed83d6f411", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c62bf41-01d7-4136-b751-2040b5a5bd96", "node_type": "1", "metadata": {}, "hash": "184f23e53b4f7590794375ac9107eccb0198d4f8312a9608120e1a6e1395e00d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "* We borrow an error message from sanity.c\n         */\n        if (rc) {\n                fprintf(stderr,\"llapi_file_create failed: %d (%s) \\n\", rc, strerror(-rc));\n                return -1;\n        }\n        /* llapi_file_create closes the file descriptor, we must re-open */\n        fd = open(tfile, O_CREAT | O_RDWR | O_LOV_DELAY_CREATE, 0644);\n        if (fd < 0) {\n                fprintf(stderr, \"Can't open %s file: %d (%s)\\n\", tfile, errno, strerror(errno));\n                return -1;\n        }\n        return fd;\n}\n\n/* output a list of uuids for this file */\nint get_my_uuids(int fd)\n{\n        struct obd_uuid uuids[1024], *uuidp;        /* Output var */\n        int obdcount = 1024;\n        int rc,i;\n\n        rc = llapi_lov_get_uuids(fd, uuids, &obdcount);\n        if (rc != 0) {\n                fprintf(stderr, \"get uuids failed: %d (%s)\\n\",errno, strerror(errno));\n        }\n        printf(\"This file system has %d obds\\n\", obdcount);\n        for (i = 0, uuidp = uuids; i < obdcount; i++, uuidp++) {\n                printf(\"UUID %d is %s\\n\",i, uuidp->uuid);\n        }\n        return 0;\n}\n\n/* Print out some LOV attributes. List our objects */\nint get_file_info(char *path)\n{\n\n        struct lov_user_md *lump;\n        int rc;\n        int i;\n\n        lump = malloc(LOV_EA_MAX(lump));\n        if (lump == NULL) {\n                return -1;\n        }\n\n        rc = llapi_file_get_stripe(path, lump);\n\n        if (rc != 0) {\n                fprintf(stderr, \"get_stripe failed: %d (%s)\\n\",errno, strerror(errno));\n                return -1;\n        }\n\n        printf(\"Lov magic %u\\n\", lump->lmm_magic);\n        printf(\"Lov pattern %u\\n\", lump->lmm_pattern);\n        printf(\"Lov object id %llu\\n\", lump->lmm_object_id);\n        printf(\"Lov stripe size %u\\n\", lump->lmm_stripe_size);\n        printf(\"Lov stripe count %hu\\n\", lump->lmm_stripe_count);\n        printf(\"Lov stripe offset %u\\n\", lump->lmm_stripe_offset);\n        for (i = 0; i < lump->lmm_stripe_count; i++) {\n                printf(\"Object index %d Objid %llu\\n\", lump->lmm_objects[i].l_ost_idx, lump->lmm_objects[i].l_object_id);\n        }\n\n        free(lump);\n        return rc;\n\n}\n\n/* Ping all OSTs that belong to this filesystem */\nint ping_osts()\n{\n        DIR *dir;\n        struct dirent *d;\n        char osc_dir[100];\n        int rc;\n\n        sprintf(osc_dir, \"/proc/fs/lustre/osc\");\n        dir = opendir(osc_dir);\n        if (dir == NULL) {\n                printf(\"Can't open dir\\n\");\n                return -1;\n        }\n        while((d = readdir(dir)) != NULL) {\n                if ( d->d_type == DT_DIR ) {\n                        if (!", "mimetype": "text/plain", "start_char_idx": 21967, "end_char_idx": 24586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c62bf41-01d7-4136-b751-2040b5a5bd96": {"__data__": {"id_": "0c62bf41-01d7-4136-b751-2040b5a5bd96", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abc006a-7fe9-4567-a652-25157dbb2a76", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c21eb372b7a301be32786e3083e8df28e392bf74bb5f468fd7786a4fa1456448", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b5fb6b8-1035-4281-b23d-892df0040323", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4cb38d6320ffc39d9c51b34d3ac43dd1fa6778dffca8103bc9d769bae017c8be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "strncmp(d->d_name, \"OSC\", 3)) {\n                                printf(\"Pinging OSC %s \", d->d_name);\n                                rc = llapi_ping(\"osc\", d->d_name);\n                                if (rc) {\n                                        printf(\"  bad\\n\");\n                                } else {\n                                        printf(\"  good\\n\");\n                                }\n                        }\n                }\n        }\n        return 0;\n\n}\n\nint main()\n{\n        int file;\n        int rc;\n        char filename[100];\n        char sys_cmd[100];\n\n        sprintf(filename, \"%s/%s\",MY_LUSTRE_DIR, TESTFILE);\n\n        printf(\"Open a file with striping\\n\");\n        file = open_stripe_file();\n        if ( file < 0 ) {\n                printf(\"Exiting\\n\");\n                exit(1);\n        }\n        printf(\"Getting uuid list\\n\");\n        rc = get_my_uuids(file);\n        printf(\"Write to the file\\n\");\n        rc = write_file(file);\n        rc = close_file(file);\n        printf(\"Listing LOV data\\n\");\n        rc = get_file_info(filename);\n        printf(\"Ping our OSTs\\n\");\n        rc = ping_osts();\n\n        /* the results should match lfs getstripe */\n        printf(\"Confirming our results with lfs getstripe\\n\");\n        sprintf(sys_cmd, \"/usr/bin/lfs getstripe %s/%s\", MY_LUSTRE_DIR, TESTFILE);\n        system(sys_cmd);\n\n        printf(\"All done\\n\");\n        exit(rc);\n}\n```\n\n**Makefile for sample application:**\n\n```\ngcc -g -O2 -Wall -o lustredemo libtest.c -llustreapi\nclean:\nrm -f core lustredemo *.o\nrun: \nmake\nrm -f /mnt/lustre/ftest/lustredemo\nrm -f /mnt/lustre/ftest/lustre_dummy\ncp lustredemo /mnt/lustre/ftest/\n```\n### See Also\n\n- [the section called \u201c `llapi_file_create` \u201d](#llapi_file_create)\n- [the section called \u201c`llapi_file_get_stripe`\u201d](#llapi_file_get_stripe)\n- [the section called \u201c `llapi_file_open` \u201d](#llapi_file_open)\n- [the section called \u201c `llapi_quotactl` \u201d](#llapi_quotactl)", "mimetype": "text/plain", "start_char_idx": 24587, "end_char_idx": 26528, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16401bc9-28ee-4f62-8a85-4da9fbcc8032": {"__data__": {"id_": "16401bc9-28ee-4f62-8a85-4da9fbcc8032", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b33121-2589-4655-af52-aff6aa8c7f68", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0dfca469acd512a9141149e2b5ed802e464426c5f0537611c3c9594bccfd7979", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82d40da6-b720-4b2d-81e6-aa3894e7d04e", "node_type": "1", "metadata": {}, "hash": "bbf0805484af01194983a2005a354c38c9bc546aa8816b121f34442dd1b573b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## Configuration Files and Module Parameters\n\n- [Configuration Files and Module Parameters](#configuration-files-and-module-parameters)\n  * [Introduction](#introduction)\n  * [Module Options](#module-options)\n    + [LNet Options](#lnet-options)\n      - [Network Topology](#network-topology)\n      - [networks (\"tcp\")](#networks-tcp)\n      - [routes (\"\")](#routes-)\n      - [forwarding (\"\")](#forwarding-)\n      - [`rnet_htable_size`](#rnet_htable_size)\n    \t\t[`SOCKLND` Kernel TCP/IP LND](#socklnd-kernel-tcpip-lnd)\t\n\nThis section describes configuration files and module parameters and includes the following sections:\n\n- [the section called \u201c Introduction\u201d](#introduction)\n- [the section called \u201c Module Options\u201d](#module-options)\n\n## Introduction\n\nLNet network hardware and routing are now configured via module parameters. Parameters should be specified in the `/etc/modprobe.d/lustre.conf`file, for example:\n\n```\noptions lnet networks=tcp0(eth2)\n```\n\nThe above option specifies that this node should use the TCP protocol on the eth2 network interface.\n\nModule parameters are read when the module is first loaded. Type-specific LND modules (for instance, `ksocklnd`) are loaded automatically by the LNet module when LNet starts (typically upon `modprobe ptlrpc`).\n\nLNet configuration parameters can be viewed under `/sys/module/lnet/parameters/`, and LND-specific parameters under the name of the corresponding LND, for example `/sys/module/ksocklnd/parameters/` for the socklnd (TCP) LND.\n\nFor the following parameters, default option settings are shown in parenthesis. Changes to parameters marked with a W affect running systems. Unmarked parameters can only be set when LNet loads for the first time. Changes to parameters marked with `Wc` only have effect when connections are established (existing connections are not affected by these changes.)\n\n## Module Options\n\n- With routed or other multi-network configurations, use `ip2nets` rather than networks, so all nodes can use the same configuration.\n- For a routed network, use the same 'routes' configuration everywhere. Nodes specified as routers automatically enable forwarding and any routes that are not relevant to a particular node are ignored. Keep a common configuration to guarantee that all nodes have consistent routing tables.\n- A separate `lustre.conf` file makes distributing the configuration much easier.\n- If you set `config_on_load=1`, LNet starts at `modprobe` time rather than waiting for the Lustre file system to start. This ensures routers start working at module load time.\n\n```\n# lctl \n# lctl> net down\n```\n\n- Remember the `lctl ping {nid}` command - it is a handy way to check your LNet configuration.\n\n### LNet Options\n\nThis section describes LNet options.\n\n#### Network Topology\n\nNetwork topology module parameters determine which networks a node should join, whether it should route between these networks, and how it communicates with non-local networks.\n\nHere is a list of various networks and the supported software stacks:\n\n| **Network** | **Software Stack** |\n| ----------- | ------------------ |\n| o2ib        | OFED Version 2     |\n\n**Note**\n\nThe Lustre software ignores the loopback interface (`lo0`), but the Lustre file system uses any IP addresses aliased to the loopback (by default). When in doubt, explicitly specify networks.\n\n###  ip2nets (\"tcp\")\n\n`ip2nets` (\"\") is a string that lists globally-available networks, each with a set of IP address ranges. LNet determines the locally-available networks from this list by matching the IP address ranges with the local IPs of a node. The purpose of this option is to be able to use the same `modules.conf` file across a variety of nodes on different networks. The string has the following syntax.\n\n```\n<ip2nets> :== <net-match> [ <comment> ] { <net-sep> <net-match> }\n<net-match> :== [ <w> ] <net-spec> <w> <ip-range> { <w> <ip-range> }\n[ <w> ]\n<net-spec> :== <network> [ \"(\" <interface-list> \")\" ]\n<network> :== <nettype> [ <number> ]\n<nettype> :== \"tcp\" | \"elan\" | \"o2ib\" | ...\n<iface-list> :== <interface> [ \",\" <iface-list> ]\n<ip-range> :== <r-expr> \".\" <r-expr> \".\"", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4120, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82d40da6-b720-4b2d-81e6-aa3894e7d04e": {"__data__": {"id_": "82d40da6-b720-4b2d-81e6-aa3894e7d04e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b33121-2589-4655-af52-aff6aa8c7f68", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0dfca469acd512a9141149e2b5ed802e464426c5f0537611c3c9594bccfd7979", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16401bc9-28ee-4f62-8a85-4da9fbcc8032", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "dfcd793e43036c8f8a038aa091551ce685bd0135dac9d68e340c3caf5087b2b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67def0d8-13cc-4f6a-9a89-84a851c12563", "node_type": "1", "metadata": {}, "hash": "7ba8c9c804a96a146ec6fdd1ad6fe3883ab5efee1b919676e91dfee5dc9da9ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The purpose of this option is to be able to use the same `modules.conf` file across a variety of nodes on different networks. The string has the following syntax.\n\n```\n<ip2nets> :== <net-match> [ <comment> ] { <net-sep> <net-match> }\n<net-match> :== [ <w> ] <net-spec> <w> <ip-range> { <w> <ip-range> }\n[ <w> ]\n<net-spec> :== <network> [ \"(\" <interface-list> \")\" ]\n<network> :== <nettype> [ <number> ]\n<nettype> :== \"tcp\" | \"elan\" | \"o2ib\" | ...\n<iface-list> :== <interface> [ \",\" <iface-list> ]\n<ip-range> :== <r-expr> \".\" <r-expr> \".\" <r-expr> \".\" <r-expr>\n<r-expr> :== <number> | \"*\" | \"[\" <r-list> \"]\"\n<r-list> :== <range> [ \",\" <r-list> ]\n<range> :== <number> [ \"-\" <number> [ \"/\" <number> ] ]\n<comment :== \"#\" { <non-net-sep-chars> }\n<net-sep> :== \";\" | \"\\n\"\n<w> :== <whitespace-chars> { <whitespace-chars> }\n```\n\n`<net-spec>` contains enough information to uniquely identify the network and load an appropriate LND. The LND determines the missing \"address-within-network\" part of the NID based on the interfaces it can use.\n\n`<iface-list>` specifies which hardware interface the network can use. If omitted, all interfaces are used. LNDs that do not support the `<iface-list>` syntax cannot be configured to use particular interfaces and just use what is there. Only a single instance of these LNDs can exist on a node at any time, and `<iface-list>` must be omitted.\n\n`<net-match>` entries are scanned in the order declared to see if one of the node's IP addresses matches one of the `<ip-range>` expressions. If there is a match, `<net-spec>` specifies the network to instantiate. Note that it is the first match for a particular network that counts. This can be used to simplify the match expression for the general case by placing it after the special cases. For example:\n\n```\nip2nets=\"tcp(eth1,eth2) 134.32.1.[4-10/2]; tcp(eth1) *.*.*.*\"\n```\n\n4 nodes on the 134.32.1.* network have 2 interfaces (134.32.1.{4,6,8,10}) but all the rest have 1.\n\n```\nip2nets=\"o2ib 192.168.0.*; tcp(eth2) 192.168.0.[1,7,4,12]\" \n```\n\nThis describes an IB cluster on 192.168.0.*. Four of these nodes also have IP interfaces; these four could be used as routers.\n\nNote that match-all expressions (For instance, `*.*.*.*`) effectively mask all other\n\n`<net-match>` entries specified after them. They should be used with caution.\n\nHere is a more complicated situation, the route parameter is explained below. We have:\n\n- Two TCP subnets\n- One Elan subnet\n- One machine set up as a router, with both TCP and Elan interfaces\n- IP over Elan configured, but only IP will be used to label the nodes.\n\n```\noptions lnet ip2nets=\u00e2\u20actcp 198.129.135.* 192.128.88.98; \\\n        elan 198.128.88.98 198.129.135.3; \\ \n        routes='cp 1022@elan # Elan NID of router; \\\n        elan  198.128.88.98@tcp # TCP NID of router  '\n```\n\n#### networks (\"tcp\")\n\nThis is an alternative to \"`ip2nets`\" which can be used to specify the networks to be instantiated explicitly. The syntax is a simple comma separated list of `<net-spec>`s (see above). The default is only used if neither 'ip2nets' nor 'networks' is specified.\n\n#### routes (\"\")\n\nThis is a string that lists networks and the NIDs of routers that forward to them.", "mimetype": "text/plain", "start_char_idx": 3584, "end_char_idx": 6770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67def0d8-13cc-4f6a-9a89-84a851c12563": {"__data__": {"id_": "67def0d8-13cc-4f6a-9a89-84a851c12563", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b33121-2589-4655-af52-aff6aa8c7f68", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0dfca469acd512a9141149e2b5ed802e464426c5f0537611c3c9594bccfd7979", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82d40da6-b720-4b2d-81e6-aa3894e7d04e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ec8dc9f41c062d2c30ba2e02dc30a07dafe2429fd68c5f28b79301f3edd8c426", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb996c0a-6f56-49f0-850a-6bb628f2acbe", "node_type": "1", "metadata": {}, "hash": "b7bdb50c177bbdb9357f10be335bd5b749a9f22a4dfd202e6aa2b95e5a489939", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\noptions lnet ip2nets=\u00e2\u20actcp 198.129.135.* 192.128.88.98; \\\n        elan 198.128.88.98 198.129.135.3; \\ \n        routes='cp 1022@elan # Elan NID of router; \\\n        elan  198.128.88.98@tcp # TCP NID of router  '\n```\n\n#### networks (\"tcp\")\n\nThis is an alternative to \"`ip2nets`\" which can be used to specify the networks to be instantiated explicitly. The syntax is a simple comma separated list of `<net-spec>`s (see above). The default is only used if neither 'ip2nets' nor 'networks' is specified.\n\n#### routes (\"\")\n\nThis is a string that lists networks and the NIDs of routers that forward to them.\n\nIt has the following syntax (`<w>` is one or more whitespace characters):\n\n```\n<routes> :== <route>{ ; <route> }\n<route> :== [<net>[<w><hopcount>]<w><nid>[:<priority>]{<w><nid>[:<priority>]}\n```\n\nNote: the priority parameter was added in release 2.5.\n\nSo a node on the network `tcp1` that needs to go through a router to get to the Elan network:\n\n```\noptions lnet networks=tcp1 routes=\"elan 1 192.168.2.2@tcpA\"\n```\n\nThe hopcount and priority numbers are used to help choose the best path between multiply-routed configurations.\n\nA simple but powerful expansion syntax is provided, both for target networks and router NIDs as follows.\n\n```\n<expansion> :== \"[\" <entry> { \",\" <entry> } \"]\"\n<entry> :== <numeric range> | <non-numeric item>\n<numeric range> :== <number> [ \"-\" <number> [ \"/\" <number> ] ]\n```\n\nThe expansion is a list enclosed in square brackets. Numeric items in the list may be a single number, a contiguous range of numbers, or a strided range of numbers. For example, `routes=\"elan 192.168.1.[22-24]@tcp\"` says that network `elan0` is adjacent (hopcount defaults to 1); and is accessible via 3 routers on the `tcp0` network (`192.168.1.22@tcp`, `192.168.1.23@tcp` and `192.168.1.24@tcp`).\n\n`routes=\"[tcp,o2ib] 2 [8-14/2]@elan\"` says that 2 networks (`tcp0` and `o2ib0`) are accessible through 4 routers (`8@elan`, `10@elan`, `12@elan` and `14@elan`). The hopcount of 2 means that traffic to both these networks will be traversed 2 routers - first one of the routers specified in this entry, then one more.\n\nDuplicate entries, entries that route to a local network, and entries that specify routers on a non-local network are ignored.\n\nPrior to release 2.5, a conflict between equivalent entries was resolved in favor of the route with the shorter hopcount. The hopcount, if omitted, defaults to 1 (the remote network is adjacent)..\n\nIntroduced in Lustre 2.5Since 2.5, equivalent entries are resolved in favor of the route with the lowest priority number or shorter hopcount if the priorities are equal. The priority, if omitted, defaults to 0. The hopcount, if omitted, defaults to 1 (the remote network is adjacent).\n\nIt is an error to specify routes to the same destination with routers on different local networks.\n\nIf the target network string contains no expansions, then the hopcount defaults to 1 and may be omitted (that is, the remote network is adjacent). In practice, this is true for most multi-network configurations. It is an error to specify an inconsistent hop count for a given target network. This is why an explicit hopcount is required if the target network string specifies more than one network.\n\n#### forwarding (\"\")\n\nThis is a string that can be set either to \"`enabled`\" or \"`disabled`\" for explicit control of whether this node should act as a router, forwarding communications between all local networks.\n\nA standalone router can be started by simply starting LNet ('`modprobe ptlrpc`') with appropriate network topology options.\n\n#### accept (secure)\n\nThe acceptor is a TCP/IP service that some LNDs use to establish communications.", "mimetype": "text/plain", "start_char_idx": 6166, "end_char_idx": 9846, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb996c0a-6f56-49f0-850a-6bb628f2acbe": {"__data__": {"id_": "bb996c0a-6f56-49f0-850a-6bb628f2acbe", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b33121-2589-4655-af52-aff6aa8c7f68", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0dfca469acd512a9141149e2b5ed802e464426c5f0537611c3c9594bccfd7979", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67def0d8-13cc-4f6a-9a89-84a851c12563", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "808185df11bb9fdeaca94702f81b4487dd7877a0d30bee75dd2e7fb16fee2c14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c479a44-b30b-4aab-9044-346302dd87c6", "node_type": "1", "metadata": {}, "hash": "94505f2e2023756ce0f3200eb15198af06380dcc84ae036eb32af5a54c66acde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is an error to specify routes to the same destination with routers on different local networks.\n\nIf the target network string contains no expansions, then the hopcount defaults to 1 and may be omitted (that is, the remote network is adjacent). In practice, this is true for most multi-network configurations. It is an error to specify an inconsistent hop count for a given target network. This is why an explicit hopcount is required if the target network string specifies more than one network.\n\n#### forwarding (\"\")\n\nThis is a string that can be set either to \"`enabled`\" or \"`disabled`\" for explicit control of whether this node should act as a router, forwarding communications between all local networks.\n\nA standalone router can be started by simply starting LNet ('`modprobe ptlrpc`') with appropriate network topology options.\n\n#### accept (secure)\n\nThe acceptor is a TCP/IP service that some LNDs use to establish communications. If a local network requires it and it has not been disabled, the acceptor listens on a single port for connection requests that it redirects to the appropriate local network. The acceptor is part of the LNet module and configured by the following options:\n\n| **Variable**                  | **Description**                                              |\n| ----------------------------- | ------------------------------------------------------------ |\n| `accept`<br/>`(secure)`       | The type of connections that the acceptor will allow from remote nodes.<br />\u2022 `secure` - Accept connections only from reserved TCP ports (below 1023). This is the default, and prevents userspace processes from trying to connect to the server.<br />\u2022 `all` - Accept connections from any TCP port. This may be needed to allow connections on nonprivileged ports, for example from a client in a virtual machine running in userspace.<br />\u2022 `none` - Do not run the acceptor. This may prevent the client from receiving server RPCs if the TCP connection is lost and the server needs to contact the client for some reason (e.g. LDLM lock callback or size glimpse). |\n| `accept_port`<br/>`(988)`     | Port number on which the acceptor should listen for connection requests. All nodes in a site configuration that require an acceptor must use the same port. |\n| `accept_backlog`<br/>`(127)`  | Maximum length that the queue of pending connections may grow to (see listen(2)). |\n| `accept_timeout`<br/>`(5, W)` | Maximum time in seconds the acceptor is allowed to block while communicating with a peer. |\n| `accept_proto_version`        | Version of the acceptor protocol that should be used by outgoing connection requests. It defaults to the most recent acceptor protocol version, but it may be set to the previous version to allow the node to initiate connections with nodes that only understand that version of the acceptor protocol. The acceptor can, with some restrictions, handle either version (that is, it can accept connections from both 'old' and 'new' peers). For the current version of the acceptor protocol (version 1), the acceptor is compatible with old peers if it is only required by a single local network. |\n\n#### `rnet_htable_size`\n\n`rnet_htable_size` is an integer that indicates how many remote networks the internal LNet hash table is configured to handle. `rnet_htable_size` is used for optimizing the hash table size and does not put a limit on how many remote networks you can have. The default hash table size when this parameter is not specified is: 128.\n\n### `SOCKLND` Kernel TCP/IP LND\n\nThe `SOCKLND` kernel TCP/IP LND (`socklnd`) is connection-based and uses the acceptor to establish communications via sockets with its peers.\n\nIt supports multiple instances and load balances dynamically over multiple interfaces. If no interfaces are specified by the `ip2nets` or networks module parameter, all non-loopback IP interfaces are used. The address-within-network is determined by the address of the first IP interface an instance of the `socklnd` encounters.\n\nConsider a node on the 'edge' of an InfiniBand network, with a low-bandwidth management Ethernet (`eth0`), IP over IB configured (`ipoib0`), and a pair of GigE NICs (`eth1`,`eth2`) providing off-cluster connectivity. This node should be configured with ' `networks=o2ib,tcp(eth1,eth2)`' to ensure that the `socklnd` ignores the management Ethernet and IPoIB.", "mimetype": "text/plain", "start_char_idx": 8905, "end_char_idx": 13272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c479a44-b30b-4aab-9044-346302dd87c6": {"__data__": {"id_": "6c479a44-b30b-4aab-9044-346302dd87c6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93b33121-2589-4655-af52-aff6aa8c7f68", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0dfca469acd512a9141149e2b5ed802e464426c5f0537611c3c9594bccfd7979", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb996c0a-6f56-49f0-850a-6bb628f2acbe", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8f1606dbbae52a0caf476160ab0cb1b922268442d297e40c91f3410bc1734576", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It supports multiple instances and load balances dynamically over multiple interfaces. If no interfaces are specified by the `ip2nets` or networks module parameter, all non-loopback IP interfaces are used. The address-within-network is determined by the address of the first IP interface an instance of the `socklnd` encounters.\n\nConsider a node on the 'edge' of an InfiniBand network, with a low-bandwidth management Ethernet (`eth0`), IP over IB configured (`ipoib0`), and a pair of GigE NICs (`eth1`,`eth2`) providing off-cluster connectivity. This node should be configured with ' `networks=o2ib,tcp(eth1,eth2)`' to ensure that the `socklnd` ignores the management Ethernet and IPoIB.\n\n| **Variable**                                   | **Description**                                              |\n| ---------------------------------------------- | ------------------------------------------------------------ |\n| `timeout``(50,W)`                              | Time (in seconds) that communications may be stalled before the LND completes them with failure. |\n| `nconnds``(4)`                                 | Sets the number of connection daemons.                       |\n| `min_reconnectms``(1000,W)`                    | Minimum connection retry interval (in milliseconds). After a failed connection attempt, this is the time that must elapse before the first retry. As connections attempts fail, this time is doubled on each successive retry up to a maximum of '`max_reconnectms`'. |\n| `max_reconnectms``(6000,W)`                    | Maximum connection retry interval (in milliseconds).         |\n| `eager_ack``(0 on linux,``1 on darwin,W)`      | Boolean that determines whether the `socklnd` should attempt to flush sends on message boundaries. |\n| `typed_conns``(1,Wc)`                          | Boolean that determines whether the `socklnd` should use different sockets for different types of messages. When clear, all communication with a particular peer takes place on the same socket. Otherwise, separate sockets are used for bulk sends, bulk receives and everything else. |\n| `min_bulk``(1024,W)`                           | Determines when a message is considered \"bulk\".              |\n| `tx_buffer_size, rx_buffer_size``(8388608,Wc)` | Socket buffer sizes. Setting this option to zero (0), allows the system to auto-tune buffer sizes.WarningBe very careful changing this value as improper sizing can harm performance. |\n| `nagle``(0,Wc)`                                | Boolean that determines if `nagle` should be enabled. It should never be set in production systems. |\n| `keepalive_idle``(30,Wc)`                      | Time (in seconds) that a socket can remain idle before a keepalive probe is sent. Setting this value to zero (0) disables keepalives. |\n| `keepalive_intvl``(2,Wc)`                      | Time (in seconds) to repeat unanswered keepalive probes. Setting this value to zero (0) disables keepalives. |\n| `keepalive_count``(10,Wc)`                     | Number of unanswered keepalive probes before pronouncing socket (hence peer) death. |\n| `enable_irq_affinity``(0,Wc)`                  | Boolean that determines whether to enable IRQ affinity. The default is zero (0).When set, `socklnd` attempts to maximize performance by handling device interrupts and data movement for particular (hardware) interfaces on particular CPUs. This option is not available on all platforms. This option requires an SMP system to exist and produces best performance with multiple NICs. Systems with multiple CPUs and a single NIC may see increase in the performance with this parameter disabled. |\n| `zc_min_frag``(2048,W)`                        | Determines the minimum message fragment that should be considered for zero-copy sends. Increasing it above the platform's `PAGE_SIZE `disables all zero copy sends. This option is not available on all platforms. |", "mimetype": "text/plain", "start_char_idx": 12584, "end_char_idx": 16474, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88c420f8-1687-4fb9-b6d6-43ae22ea68f1": {"__data__": {"id_": "88c420f8-1687-4fb9-b6d6-43ae22ea68f1", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e1b7b28-d382-4173-9b3d-b397c307eacf", "node_type": "1", "metadata": {}, "hash": "667145127378885d4bff9dbeb02d5c4b9fcc6cb7897ec8d08f5013966e4750ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# System Configuration Utilities\n\n- [System Configuration Utilities](#system-configuration-utilities)\n  * [e2scan](#e2scan)\n    + [Synopsis](#synopsis)\n    + [Description](#description)\n    + [Options](#options)\n  * [l_getidentity](#l_getidentity)\n    + [Synopsis](#synopsis-1)\n    + [Description](#description-1)\n    + [Options](#options-1)\n    + [Files](#files)\n  * [lctl](#lctl)\n    + [Synopsis](#synopsis-2)\n    + [Description](#description-2)\n    + [Setting Parameters with lctl](#setting-parameters-with-lctl)\n    + [Caution](#caution)\n    + [Options](#options-2)\n    + [Examples](#examples)\n    + [See Also](#see-also)\n  * [ll_decode_filter_fid](#ll_decode_filter_fid)\n    + [Synopsis](#synopsis-3)\n    + [Description](#description-3)\n    + [Examples](#examples-1)\n    + [See Also](#see-also-1)\n  * [ll_recover_lost_found_objs](#ll_recover_lost_found_objs)\n    + [Synopsis](#synopsis-4)\n    + [Description](#description-4)\n    + [Options](#options-3)\n    + [Example](#example)\n  * [llobdstat](#llobdstat)\n    + [Synopsis](#synopsis-5)\n    + [Description](#description-5)\n    + [Example](#example-1)\n    + [Files](#files-1)\n  * [llog_reader](#llog_reader)\n    + [Synopsis](#synopsis-6)\n    + [Description](#description-6)\n    + [See Also](#see-also-2)\n  * [llstat](#llstat)\n    + [Synopsis](#synopsis-7)\n    + [Description](#description-7)\n    + [Options](#options-4)\n    + [Example](#example-2)\n    + [Files](#files-2)\n  * [llverdev](#llverdev)\n    + [Synopsis](#synopsis-8)\n    + [Description](#description-8)\n    + [Options](#options-5)\n    + [Examples](#examples-2)\n  * [lshowmount](#lshowmount)\n    + [Synopsis](#synopsis-9)\n    + [Description](#description-9)\n    + [Options](#options-6)\n    + [Files](#files-3)\n  * [lst](#lst)\n    + [Synopsis](#synopsis-10)\n    + [Description](#description-10)\n    + [Modules](#modules)\n    + [Utilities](#utilities)\n    + [Example Script](#example-script)\n  * [lustre_rmmod.sh](#lustre_rmmodsh)\n  * [lustre_rsync](#lustre_rsync)\n    + [Synopsis](#synopsis-11)\n    + [Description](#description-11)\n    + [Options](#options-7)\n    + [Examples](#examples-3)\n    + [See Also](#see-also-3)\n  * [mkfs.lustre](#mkfslustre)\n    + [Synopsis](#synopsis-12)\n    + [Description](#description-12)\n    + [Examples](#examples-4)\n    + [See Also](#see-also-4)\n  * [mount.lustre](#mountlustre)\n    + [Synopsis](#synopsis-13)\n    + [Description](#description-13)\n    + [Options](#options-8)\n    + [Examples](#examples-5)\n    + [See Also](#see-also-5)\n  * [plot-llstat](#plot-llstat)\n    + [Synopsis](#synopsis-14)\n    + [Description](#description-14)\n    + [Options](#options-9)\n    + [Example](#example-3)\n  * [routerstat](#routerstat)\n    + [Synopsis](#synopsis-15)\n    + [Description](#description-15)\n    + [Output](#output)\n    + [Example](#example-4)\n    + [Files](#files-4)\n  * [tunefs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e1b7b28-d382-4173-9b3d-b397c307eacf": {"__data__": {"id_": "6e1b7b28-d382-4173-9b3d-b397c307eacf", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88c420f8-1687-4fb9-b6d6-43ae22ea68f1", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e02405d34a2df46b1548e23f1491704ead3acf4c1cfb2eb70cc9fd3f668b5dad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01b7c2b2-afa3-420b-ad2c-24e6007ad5af", "node_type": "1", "metadata": {}, "hash": "a1cd426def350f58d778627f45a422f45b54d1a907a1079fb073424c8c3582e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "lustre](#mountlustre)\n    + [Synopsis](#synopsis-13)\n    + [Description](#description-13)\n    + [Options](#options-8)\n    + [Examples](#examples-5)\n    + [See Also](#see-also-5)\n  * [plot-llstat](#plot-llstat)\n    + [Synopsis](#synopsis-14)\n    + [Description](#description-14)\n    + [Options](#options-9)\n    + [Example](#example-3)\n  * [routerstat](#routerstat)\n    + [Synopsis](#synopsis-15)\n    + [Description](#description-15)\n    + [Output](#output)\n    + [Example](#example-4)\n    + [Files](#files-4)\n  * [tunefs.lustre](#tunefslustre)\n    + [Synopsis](#synopsis-16)\n    + [Description](#description-16)\n    + [Options](#options-10)\n    + [Examples](#examples-6)\n    + [See Also](#see-also-6)\n  * [Additional System Configuration Utilities](#additional-system-configuration-utilities)\n    + [Application Profiling Utilities](#application-profiling-utilities)\n    + [More /proc Statistics for Application Profiling](#more-proc-statistics-for-application-profiling)\n    + [Testing / Debugging Utilities](#testing--debugging-utilities)\n      - [lr_reader](#lr_reader)\n      - [sgpdd-survey](#sgpdd-survey)\n      - [obdfilter-survey](#obdfilter-survey)\n      - [ior-survey](#ior-survey)\n      - [ost-survey](#ost-survey)\n      - [stats-collect](#stats-collect)\n    + [Fileset Feature](#fileset-feature)\n      - [Examples](#examples-7)\n\n\nThis chapter includes system configuration utilities and includes the following sections:\n\n- [the section called \u201c e2scan\u201d](#e2scan)\n- [the section called \u201c l_getidentity\u201d](#l_getidentity)\n- [the section called \u201c lctl\u201d](#lctl)\n- [the section called \u201c ll_decode_filter_fid\u201d](#ll_decode_filter_fid)\n- [the section called \u201c ll_recover_lost_found_objs\u201d](#ll_recover_lost_found_objs)\n- [the section called \u201c llobdstat\u201d](#llobdstat)\n- [the section called \u201c llog_reader\u201d](#llog_reader)\n- [the section called \u201c llstat\u201d](#llstat)\n- [the section called \u201c llverdev\u201d](#llverdev)\n- [the section called \u201c lshowmount\u201d](#lshowmount)\n- [the section called \u201c lst\u201d](#lst)\n- [the section called \u201c lustre_rmmod.sh\u201d](#lustre_rmmodsh)\n- [the section called \u201c lustre_rsync\u201d](#lustre_rsync)\n- [the section called \u201c mkfs.lustre\u201d](#mkfslustre)\n- [the section called \u201c mount.lustre\u201d](#mountlustre)\n- [the section called \u201c plot-llstat\u201d](#plot-llstat)\n- [the section called \u201c routerstat\u201d](#routerstat)\n- [the section called \u201c tunefs.lustre\u201d](#tunefslustre)\n- [the section called \u201c Additional System Configuration Utilities\u201d](#additional-system-configuration-utilities)\n\n## e2scan\n\nThe e2scan utility is an ext2 file system-modified inode scan program. The e2scan program uses libext2fs to find inodes with ctime or mtime newer than a given time and prints out their pathname. Use e2scan to efficiently generate lists of files that have been modified. The e2scan tool is included in the e2fsprogs package, located at:\n\n<http://downloads.whamcloud.com/public/e2fsprogs/latest/>\n\n### Synopsis\n\n```\ne2scan [options] [-f file] block_device\n```\n\n### Description\n\nWhen invoked, the e2scan utility iterates all inodes on the block device, finds modified inodes, and prints their inode numbers. A similar iterator, using libext2fs(5), builds a table (called parent database) which lists the parent node for each inode. With a lookup function, you can reconstruct modified pathnames from root.", "mimetype": "text/plain", "start_char_idx": 2303, "end_char_idx": 5596, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01b7c2b2-afa3-420b-ad2c-24e6007ad5af": {"__data__": {"id_": "01b7c2b2-afa3-420b-ad2c-24e6007ad5af", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e1b7b28-d382-4173-9b3d-b397c307eacf", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "54189029aaab363dc0f6cdee35557d043254150190144b4b8d96a6e6c028f47a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "342e5a68-3eb2-4671-a622-cdb2a9682656", "node_type": "1", "metadata": {}, "hash": "4a97296be4ff04cb50c53971e2a3507ddbdfa1f52fd051fd0356bf84d46fa1e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The e2scan program uses libext2fs to find inodes with ctime or mtime newer than a given time and prints out their pathname. Use e2scan to efficiently generate lists of files that have been modified. The e2scan tool is included in the e2fsprogs package, located at:\n\n<http://downloads.whamcloud.com/public/e2fsprogs/latest/>\n\n### Synopsis\n\n```\ne2scan [options] [-f file] block_device\n```\n\n### Description\n\nWhen invoked, the e2scan utility iterates all inodes on the block device, finds modified inodes, and prints their inode numbers. A similar iterator, using libext2fs(5), builds a table (called parent database) which lists the parent node for each inode. With a lookup function, you can reconstruct modified pathnames from root.\n\n### Options\n\n| **Option**                 | **Description**                                              |\n| -------------------------- | ------------------------------------------------------------ |\n| `-b *inode buffer blocks*` | Sets the readahead inode blocks to get excellent performance when scanning the block device. |\n| `-o *output file*`         | If an output file is specified, modified pathnames are written to this file. Otherwise, modified parameters are written to stdout. |\n| `-t *inode*| *pathname*`   | Sets the e2scan type if type is inode. The e2scan utility prints modified inode numbers to stdout. By default, the type is set as pathname.The e2scan utility lists modified pathnames based on modified inode numbers. |\n| `-u`                       | Rebuilds the parent database from scratch. Otherwise, the current parent database is used. |\n\n## l_getidentity\n\nThe l_getidentity tool normally handles Lustre user/group mapping upcall.\n\n### Synopsis\n\n```\nl_getidentity { $FSNAME-MDT{xxxx}| -d} {uid}\n```\n\n### Description\n\nThe `l_getidentity` utility is called from the MDS to map a numeric UID value into the list of supplementary group values for that UID, and writes this into the `mdt.*.identity_info` parameter file. The list of supplementary groups is cached in the kernel to avoid repeated upcalls. See [*the section called \u201cUser/Group Upcall\u201d*](06.04-Programming%20Interfaces.md#usergroup-upcall) for more details.\n\nThe `l_getidentity` utility can also be run directly for debugging purposes to ensure that the UID mapping for a particular user is configured correctly, by using the `-d` argument instead of the MDT name.\n\n### Options\n\n| **Option**            | **Description**             |\n| --------------------- | --------------------------- |\n| `${FSNAME}-MDT{xxxx}` | Metadata server target name |\n| `uid`                 | User identifier             |\n\n### Files\n\nThe l_getidentity files are located at:\n\n```\n/proc/fs/lustre/mdt/${FSNAME}-MDT{xxxx}/identity_upcall \n```\n\n## lctl\n\nThe lctl utility is used for root control and configuration. With lctl you can directly control Lustre via an ioctl interface, allowing various configuration, maintenance and debugging features to be accessed.\n\n### Synopsis\n\n```\nlctl [--device devno] command [args]\n```\n\n### Description\n\nThe lctl utility can be invoked in interactive mode by issuing the lctl command. After that, commands are issued as shown below. The most common lctl commands are:\n\n```\ndl\ndk\ndevice\nnetwork up|down\nlist_nids\nping nidhelp\nquit\n```\n\nFor a complete list of available commands, type `help` at the `lctl` prompt. To get basic help on command meaning and syntax, type `help *command*`. Command completion is activated with the TAB key (depending on compile options), and command history is available via the up- and down-arrow keys.\n\nFor non-interactive use, use the second invocation, which runs the command after connecting to the device.\n\n### Setting Parameters with lctl\n\nLustre parameters are not always accessible using the procfs interface, as it is platform-specific. As a solution, lctl {get,set}_param has been introduced as a platform-independent interface to the Lustre tunables. Avoid direct references to /proc/{fs,sys}/{lustre,lnet}. For future portability, use lctl {get,set}_param .\n\nWhen the file system is running, use the `lctl set_param` command on the affected node(s) to *temporarily* set parameters (mapping to items in /proc/{fs,sys}/{lnet,lustre}).", "mimetype": "text/plain", "start_char_idx": 4865, "end_char_idx": 9070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "342e5a68-3eb2-4671-a622-cdb2a9682656": {"__data__": {"id_": "342e5a68-3eb2-4671-a622-cdb2a9682656", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01b7c2b2-afa3-420b-ad2c-24e6007ad5af", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "dd70ce29878d402b36bdd0ae086932f9e41fe7a9b63472b88954f4de690d1cd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc30232d-1547-4cf2-97db-faf8ef9d49da", "node_type": "1", "metadata": {}, "hash": "e351acf0f9260db1dc2d19402ddd5bb76550c6c7da17d0c9b115d6235c5c8acd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Command completion is activated with the TAB key (depending on compile options), and command history is available via the up- and down-arrow keys.\n\nFor non-interactive use, use the second invocation, which runs the command after connecting to the device.\n\n### Setting Parameters with lctl\n\nLustre parameters are not always accessible using the procfs interface, as it is platform-specific. As a solution, lctl {get,set}_param has been introduced as a platform-independent interface to the Lustre tunables. Avoid direct references to /proc/{fs,sys}/{lustre,lnet}. For future portability, use lctl {get,set}_param .\n\nWhen the file system is running, use the `lctl set_param` command on the affected node(s) to *temporarily* set parameters (mapping to items in /proc/{fs,sys}/{lnet,lustre}). The `lctl set_param` command uses this syntax:\n\n```\nlctl set_param [-n] [-P] [-d] obdtype.obdname.property=value\n```\n\nFor example:\n\n```\nmds# lctl set_param mdt.testfs-MDT0000.identity_upcall=NONE\n```\n\nIntroduced in Lustre 2.5Use `-P` option to set parameters permanently. Option `-d `deletes permanent parameters. For example:`mgs# lctl set_param -P mdt.testfs-MDT0000.identity_upcall=NONE mgs# lctl set_param -P -d mdt.testfs-MDT0000.identity_upcall`\n\nMany permanent parameters can be set with `lctl conf_param`. In general, `lctl conf_param` can be used to specify any OBD device parameter settable in a /proc/fs/lustre file. The `lctl conf_param` command must be run on the MGS node, and uses this syntax:\n\n```\nobd|fsname.obdtype.property=value) \n```\n\nFor example:\n\n```\nmgs# lctl conf_param testfs-MDT0000.mdt.identity_upcall=NONE\n$ lctl conf_param testfs.llite.max_read_ahead_mb=16 \n```\n\n### Caution\n\nThe `lctl conf_param` command *permanently* sets parameters in the file system configuration for all nodes of the specified type.\n\nTo get current Lustre parameter settings, use the `lctl get_param` command on the desired node with the same parameter name as `lctl set_param`:\n\n```\nlctl get_param [-n] obdtype.obdname.parameter\n```\n\nFor example:\n\n```\nmds# lctl get_param mdt.testfs-MDT0000.identity_upcall\n```\n\nTo list Lustre parameters that are available to set, use the `lctl list_param` command, with this syntax:\n\n```\nlctl list_param [-R] [-F] obdtype.obdname.*\n```\n\nFor example, to list all of the parameters on the MDT:\n\n```\noss# lctl list_param -RF mdt\n```\n\nFor more information on using lctl to set temporary and permanent parameters, see [the section called \u201cSetting Parameters with `lctl`\u201d](#setting-parameters-with-lctl).\n\n**Network Configuration**\n\n| **Option**                 | **Description**                                              |\n| -------------------------- | ------------------------------------------------------------ |\n| `network up|down|tcp|elan` | Starts or stops LNet, or selects a network type for other `lctl` LNet commands. |\n| `list_nids`                | Prints all NIDs on the local node. LNet must be running.     |\n| `which_nid *nidlist*`      | From a list of NIDs for a remote node, identifies the NID on which interface communication will occur. |\n| `ping *nid*`               | Checks LNet connectivity via an LNet ping. This uses the fabric appropriate to the specified NID. |\n| `interface_list`           | Prints the network interface information for a given *network* type. |\n| `peer_list`                | Prints the known peers for a given *network* type.           |\n| `conn_list`                | Prints all the connected remote NIDs for a given *network* type. |\n| `active_tx`                | This command prints active transmits. It is only used for the Elan *network* type. |\n| `route_list`               | Prints the complete routing table.                           |\n\n**Device Selection**\n\n| **Option**         |      | **Description**                                              |\n| ------------------ | ---- | ------------------------------------------------------------ |\n| `device *devname*` |      | This selects the specified OBD device. All other commands depend on the device being set.", "mimetype": "text/plain", "start_char_idx": 8282, "end_char_idx": 12330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc30232d-1547-4cf2-97db-faf8ef9d49da": {"__data__": {"id_": "fc30232d-1547-4cf2-97db-faf8ef9d49da", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "342e5a68-3eb2-4671-a622-cdb2a9682656", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "dcb3470fc51a2448ecda3e94e99424dda1195d8d0738e3d6e555c424078e9857", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4db9c634-3499-48e7-962b-3ff8f136ad39", "node_type": "1", "metadata": {}, "hash": "815b61160cd494213ebb282f286e7ba9a11d331b5bc0118a2e3d6ff00214c081", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This uses the fabric appropriate to the specified NID. |\n| `interface_list`           | Prints the network interface information for a given *network* type. |\n| `peer_list`                | Prints the known peers for a given *network* type.           |\n| `conn_list`                | Prints all the connected remote NIDs for a given *network* type. |\n| `active_tx`                | This command prints active transmits. It is only used for the Elan *network* type. |\n| `route_list`               | Prints the complete routing table.                           |\n\n**Device Selection**\n\n| **Option**         |      | **Description**                                              |\n| ------------------ | ---- | ------------------------------------------------------------ |\n| `device *devname*` |      | This selects the specified OBD device. All other commands depend on the device being set. |\n| `device_list`      |      | Shows the local Lustre OBDs, a/k/a `dl`.                     |\n\n**Device Operations**\n\n| **Option**                                           | **Description**                                              |                                                              |\n| ---------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| `list_param [-F|-R] *parameter* *[parameter ...]*`   | Lists the Lustre or LNet parameter name.                     |                                                              |\n|                                                      | `-F`                                                         | Adds '/', '@' or '=' for directories, symlinks and writeable files, respectively. |\n|                                                      | `-R`                                                         | Recursively lists all parameters under the specified path. If `param_path` is unspecified, all parameters are shown. |\n| `get_param [-n|-N|-F] *parameter* *[parameter ...]*` | Gets the value of a Lustre or LNet parameter from the specified path. |                                                              |\n|                                                      | `-n`                                                         | Prints only the parameter value and not the parameter name.  |\n|                                                      | `-N`                                                         | Prints only matched parameter names and not the values; especially useful when using patterns. |\n|                                                      | `-F`                                                         | When `-N` is specified, adds '/', '@' or '=' for directories, symlinks and writeable files, respectively. |\n| `set_param [-n] *parameter*=*value*`                 | Sets the value of a Lustre or LNet parameter from the specified path. |                                                              |\n|                                                      | `-n`                                                         | Disables printing of the key name when printing values.      |\n| `conf_param [-d] *device|fsname**parameter*=*value*` | Sets a permanent configuration parameter for any device via the MGS. This command must be run on the MGS node.All writeable parameters under `lctl list_param` (e.g. `lctl list_param -F osc.*.* | grep` =) can be permanently set using `lctl conf_param`, but the format is slightly different. For `conf_param`, the device is specified first, then the obdtype. Wildcards are not supported. Additionally, failover nodes may be added (or removed), and some system-wide parameters may be set as well (sys.at_max, sys.at_min, sys.at_extra, sys.at_early_margin, sys.at_history, sys.timeout, sys.ldlm_timeout). For system-wide parameters, *device* is ignored.For more information on setting permanent parameters and `lctl conf_param`command examples, see [the section called \u201cSetting Permanent Parameters\u201d](03.02-Lustre%20Operations.md#setting-permanent-parameters)(Setting Permanent Parameters). |                                                              |\n|                                                      | `-d *device|fsname*.*parameter*`                             | Deletes a parameter setting (use the default value at the next restart). A null value for *value* also deletes the parameter setting. |\n| `activate`                                           | Re-activates an import after the deactivate operation. This setting is only effective until the next restart (see `conf_param`). |                                                              |\n| `deactivate`                                         | Deactivates an import, in particular meaning do not assign new file stripes to an OSC. Running lctl deactivate on the MDS stops new objects from being allocated on the OST. Running lctl deactivate on Lustre clients causes them to return -EIO when accessing objects on the OST instead of waiting for recovery. |                                                              |\n| `abort_recovery`                                     | Aborts the recovery process on a re-starting MDT or OST.     |                                                              |\n\n**Note**\n\nLustre tunables are not always accessible using the procfs interface, as it is platform-specific. As a solution, `lctl {get,set,list}_param` has been introduced as a platform-independent interface to the Lustre tunables.", "mimetype": "text/plain", "start_char_idx": 11441, "end_char_idx": 16998, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4db9c634-3499-48e7-962b-3ff8f136ad39": {"__data__": {"id_": "4db9c634-3499-48e7-962b-3ff8f136ad39", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc30232d-1547-4cf2-97db-faf8ef9d49da", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f160bf6e692543a9a0037ff4e3f3f51aeeb98de6bbd4a8feed5339c9f71b629d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da9f107b-e586-4aed-8079-16cb8f982e18", "node_type": "1", "metadata": {}, "hash": "af3e5777f0d09296a819ed12c078ce2c5686b6255ee22a1e26c783c80fc7c1a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `activate`                                           | Re-activates an import after the deactivate operation. This setting is only effective until the next restart (see `conf_param`). |                                                              |\n| `deactivate`                                         | Deactivates an import, in particular meaning do not assign new file stripes to an OSC. Running lctl deactivate on the MDS stops new objects from being allocated on the OST. Running lctl deactivate on Lustre clients causes them to return -EIO when accessing objects on the OST instead of waiting for recovery. |                                                              |\n| `abort_recovery`                                     | Aborts the recovery process on a re-starting MDT or OST.     |                                                              |\n\n**Note**\n\nLustre tunables are not always accessible using the procfs interface, as it is platform-specific. As a solution, `lctl {get,set,list}_param` has been introduced as a platform-independent interface to the Lustre tunables. Avoid direct references to `/proc/{fs,sys}/{lustre,lnet}`. For future portability, use `lctl {get,set,list}_param` instead.\n\n**Virtual Block Device Operations**\n\nLustre can emulate a virtual block device upon a regular file. This emulation is needed when you are trying to set up a swap space via the file.\n\n| **Option**                                      | **Description**                                              |\n| ----------------------------------------------- | ------------------------------------------------------------ |\n| `blockdev_attach *filename**/dev/lloop_device*` | Attaches a regular Lustre file to a block device. If the device node does not exist, `lctl`creates it. It is recommend that a device node is created by `lctl` since the emulator uses a dynamical major number. |\n| `blockdev_detach */dev/lloop_device*`           | Detaches the virtual block device.                           |\n| `blockdev_info */dev/lloop_device*`             | Provides information about the Lustre file attached to the device node. |\n\n**Changelogs**\n\n| **Option**                | **Description**                                              |\n| ------------------------- | ------------------------------------------------------------ |\n| `changelog_register`      | Registers a new changelog user for a particular device. Changelog entries are saved persistently on the MDT with each filesystem operation, and are only purged beyond all registered user's minimum set point (see `lfs changelog_clear`). This may cause the Changelog to consume a large amount of space, eventually filling the MDT, if a changelog user is registered but never consumes those records. |\n| changelog_deregister *id* | Unregisters an existing changelog user. If the user's \"clear\" record number is the minimum for the device, changelog records are purged until the next minimum. |\n\n**Debug**\n\n| **Option**                               | **Description**                                              |\n| ---------------------------------------- | ------------------------------------------------------------ |\n| `debug_daemon`                           | Starts and stops the debug daemon, and controls the output filename and size. |\n| `debug_kernel *[file]* [raw]`            | Dumps the kernel debug buffer to stdout or a file.           |\n| `debug_file *input_file**[output_file]*` | Converts the kernel-dumped debug log from binary to plain text format. |\n| `clear`                                  | Clears the kernel debug buffer.                              |\n| `mark *text*`                            | Inserts marker text in the kernel debug buffer.              |\n| `filter *subsystem_id|debug_mask*`       | Filters kernel debug messages by subsystem or mask.          |\n| `show *subsystem_id|debug_mask*`         | Shows specific types of messages.                            |\n| `debug_list *subsystems|types*`          | Lists all subsystem and debug types.                         |\n| `modules *path*`                         | Provides GDB-friendly module information.                    |\n\n### Options\n\nUse the following options to invoke lctl.\n\n| **Option**                        | **Description**                                              |\n| --------------------------------- | ------------------------------------------------------------ |\n| `--device`                        | Device to be used for the operation (specified by name or number). See device_list. |\n| `--ignore_errors | ignore_errors` | Ignores errors during script processing.                     |\n\n### Examples\n\n`lctl`\n\n```\n$ lctl\nlctl > dl \n   0 UP mgc MGC192.168.0.20@tcp btbb24e3-7deb-2ffa-eab0-44dffe00f692 5 \n   1 UP ost OSS OSS_uuid 3 \n   2 UP obdfilter testfs-OST0000 testfs-OST0000_UUID 3 \nlctl > dk /tmp/log Debug log: 87 lines, 87 kept, 0 dropped.", "mimetype": "text/plain", "start_char_idx": 15900, "end_char_idx": 20836, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da9f107b-e586-4aed-8079-16cb8f982e18": {"__data__": {"id_": "da9f107b-e586-4aed-8079-16cb8f982e18", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4db9c634-3499-48e7-962b-3ff8f136ad39", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "764191c117e5c864f37636b9a743f97b502939bae3b6931c3d4e456228387048", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8075030-7004-4aa5-bd47-c77608eecda7", "node_type": "1", "metadata": {}, "hash": "cad1686da1eef5feadf1db67d9bae115668ae5e7b9d8ad045c36c73a85eec6b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n\n### Options\n\nUse the following options to invoke lctl.\n\n| **Option**                        | **Description**                                              |\n| --------------------------------- | ------------------------------------------------------------ |\n| `--device`                        | Device to be used for the operation (specified by name or number). See device_list. |\n| `--ignore_errors | ignore_errors` | Ignores errors during script processing.                     |\n\n### Examples\n\n`lctl`\n\n```\n$ lctl\nlctl > dl \n   0 UP mgc MGC192.168.0.20@tcp btbb24e3-7deb-2ffa-eab0-44dffe00f692 5 \n   1 UP ost OSS OSS_uuid 3 \n   2 UP obdfilter testfs-OST0000 testfs-OST0000_UUID 3 \nlctl > dk /tmp/log Debug log: 87 lines, 87 kept, 0 dropped. \nlctl > quit\n```\n\n### See Also\n\n- [the section called \u201c mkfs.lustre\u201d](#makelustre)\n- [the section called \u201c mount.lustre\u201d](mountlustre)\n- [the section called \u201c lctl\u201d](lctl)\n- [the section called \u201c `lfs` \u201d](06.03-User%20Utilities.md#lfs)\n\n## ll_decode_filter_fid\n\nThe `ll_decode_filter_fid` utility displays the Lustre object ID and MDT parent FID.\n\n### Synopsis\n\n```\nll_decode_filter_fid object_file [object_file ...]\n```\n\n### Description\n\nThe ll_decode_filter_fid utility decodes and prints the Lustre OST object ID, MDT FID, stripe index for the specified OST object(s), which is stored in the \"trusted.fid\" attribute on each OST object. This is accessible to `ll_decode_filter_fid` when the OST file system is mounted locally as type ldiskfs for maintenance.\n\nThe \"trusted.fid\" extended attribute is stored on each OST object when it is first modified (data written or attributes set), and is not accessed or modified by Lustre after that time.\n\nThe OST object ID (objid) may be useful in case of OST directory corruption, though LFSCK can normally reconstruct the entire OST object directory tree, see Section \u201c Checking the file system with LFSCK\u201d for details. The MDS FID can be useful to determine which MDS inode an OST object is (or was) used by. The stripe index can be used in conjunction with other OST objects to reconstruct the layout of a file even if the MDT inode was lost.\n\n### Examples\n\n```\nroot@oss1# cd /mnt/ost/lost+found\nroot@oss1# ll_decode_filter_fid #12345[4,5,8]\n#123454: objid=690670 seq=0 parent=[0x751c5:0xfce6e605:0x0]\n#123455: objid=614725 seq=0 parent=[0x18d11:0xebba84eb:0x1]\n#123458: objid=533088 seq=0 parent=[0x21417:0x19734d61:0x0]\n```\n\nThis shows that the three files in lost+found have decimal object IDs - 690670, 614725, and 533088, respectively. The object sequence number (formerly object group) is 0 for all current OST objects.\n\nThe MDT parent inode FIDs are hexadecimal numbers of the form sequence:oid:idx. Since the sequence number is below 0x100000000 in all these cases, the FIDs are in the legacy Inode and Generation In FID (IGIF) namespace and are mapped directly to the MDT inode = seq and generation = oid values; the MDT inodes are 0x751c5, 0x18d11, and 0x21417 respectively. For objects with MDT parent sequence numbers above 0x200000000, this indicates that the FID needs to be mapped via the MDT Object Index (OI) file on the MDT to determine the internal inode number.\n\nThe idx field shows the stripe number of this OST object in the Lustre RAID-0 striped file.\n\n### See Also\n\n[the section called \u201c ll_recover_lost_found_objs\u201d](#ll_recover_lost_found_objs)\n\n\n\nIntroduced in Lustre 2.8\n\n## ll_recover_lost_found_objs\n\nThe `ll_recover_lost_found_objs` utility was used to help recover Lustre OST objects (file data) from the`lost+found` directory of an OST and return them to their correct locations based on information stored in the`trusted.fid` extended attribute stored on every OST object containing data.", "mimetype": "text/plain", "start_char_idx": 20090, "end_char_idx": 23805, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8075030-7004-4aa5-bd47-c77608eecda7": {"__data__": {"id_": "c8075030-7004-4aa5-bd47-c77608eecda7", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da9f107b-e586-4aed-8079-16cb8f982e18", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0557533af3ce7c3d452e0c4b3d64f71beb7df81ed55cb8fc99302a4af6cfd1dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8342151c-1130-438b-be50-20a5fef97bfc", "node_type": "1", "metadata": {}, "hash": "ab43a0fc6b8ba84a919de20bb6538c579a3db2fa6d138ff5d7a0d0d4046a7942", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For objects with MDT parent sequence numbers above 0x200000000, this indicates that the FID needs to be mapped via the MDT Object Index (OI) file on the MDT to determine the internal inode number.\n\nThe idx field shows the stripe number of this OST object in the Lustre RAID-0 striped file.\n\n### See Also\n\n[the section called \u201c ll_recover_lost_found_objs\u201d](#ll_recover_lost_found_objs)\n\n\n\nIntroduced in Lustre 2.8\n\n## ll_recover_lost_found_objs\n\nThe `ll_recover_lost_found_objs` utility was used to help recover Lustre OST objects (file data) from the`lost+found` directory of an OST and return them to their correct locations based on information stored in the`trusted.fid` extended attribute stored on every OST object containing data.\n\nIntroduced in Lustre 2.6\n\n**Note**\n\nThis utility is not needed with Lustre 2.6 and later, and is removed in Lustre 2.8 since `LFSCK` online scanning will automatically move objects from `lost+found` to the proper place in the OST.\n\n### Synopsis\n\n```\nllobdstat ost_name [interval]\n```\n\n### Description\n\nThe llobdstat utility displays a line of OST statistics for the given ost_name every interval seconds. It should be run directly on an OSS node. Type `CTRL-C` to stop statistics printing.\n\n### Example\n\n```\n# llobdstat liane-OST0002 1\n/usr/bin/llobdstat on /proc/fs/lustre/obdfilter/liane-OST0002/stats\nProcessor counters run at 2800.189 MHz\nRead: 1.21431e+07, Write: 9.93363e+08, create/destroy: 24/1499, stat: 34, p\\\nunch: 18\n[NOTE: cx: create, dx: destroy, st: statfs, pu: punch ]\nTimestamp Read-delta ReadRate Write-delta WriteRate\n--------------------------------------------------------\n1217026053 0.00MB 0.00MB/s 0.00MB 0.00MB/s\n1217026054 0.00MB 0.00MB/s 0.00MB 0.00MB/s\n1217026055 0.00MB 0.00MB/s 0.00MB 0.00MB/s\n1217026056 0.00MB 0.00MB/s 0.00MB 0.00MB/s\n1217026057 0.00MB 0.00MB/s 0.00MB 0.00MB/s\n1217026058 0.00MB 0.00MB/s 0.00MB 0.00MB/s\n1217026059 0.00MB 0.00MB/s 0.00MB 0.00MB/s st:1\n```\n\n### Files\n\n```\n/proc/fs/lustre/obdfilter/ostname/stats\n```\n\n## llog_reader\n\nThe llog_reader utility translates a Lustre configuration log into human-readable form.\n\n### Synopsis\n\n```\nllog_reader filename\n```\n\n### Description\n\nThe llog_reader utility parses the binary format of Lustre's on-disk configuration logs. Llog_reader can only read logs; use tunefs.lustre to write to them.\n\nTo examine a log file on a stopped Lustre server, mount its backing file system as ldiskfs or zfs, then use llog_reader to dump the log file's contents, for example:\n\n```\nmount -t ldiskfs /dev/sda /mnt/mgs \nllog_reader /mnt/mgs/CONFIGS/tfs-client\n```\n\nTo examine the same log file on a running Lustre server, use the ldiskfs-enabled debugfs utility (called debug.ldiskfs on some distributions) to extract the file, for example:\n\n```\ndebugfs -c -R 'dump CONFIGS/tfs-client /tmp/tfs-client' /dev/sda \nllog_reader /tmp/tfs-client\n```\n\n**Caution**\n\nAlthough they are stored in the CONFIGS directory, mountdata files do not use the configuration log format and will confuse the llog_reader utility.\n\n### See Also\n\n[the section called \u201c tunefs.lustre\u201d](#tunefslustre)\n\n## llstat\n\nThe llstat utility displays Lustre statistics.\n\n### Synopsis\n\n```\nllstat [-c] [-g] [-i interval] stats_file\n```\n\n### Description\n\nThe llstat utility displays statistics from any of the Lustre statistics files that share a common format and are updated at `interval` seconds.", "mimetype": "text/plain", "start_char_idx": 23069, "end_char_idx": 26444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8342151c-1130-438b-be50-20a5fef97bfc": {"__data__": {"id_": "8342151c-1130-438b-be50-20a5fef97bfc", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8075030-7004-4aa5-bd47-c77608eecda7", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "82c2bd9f818e14a2aa89819af0b84fa995ea0224bcfd2cfc7383b8be8ba67773", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f5485ff-2186-4340-a583-84eae6d55c51", "node_type": "1", "metadata": {}, "hash": "eb1e442d6373fc6da35e9dc212b7ebba71b44e8f93c20bec67c1596b91c0897d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### See Also\n\n[the section called \u201c tunefs.lustre\u201d](#tunefslustre)\n\n## llstat\n\nThe llstat utility displays Lustre statistics.\n\n### Synopsis\n\n```\nllstat [-c] [-g] [-i interval] stats_file\n```\n\n### Description\n\nThe llstat utility displays statistics from any of the Lustre statistics files that share a common format and are updated at `interval` seconds. To stop statistics printing, use `ctrl`-`c.`\n\n### Options\n\n| **Option**   | **Description**                                              |\n| ------------ | ------------------------------------------------------------ |\n| `-c`         | Clears the statistics file.                                  |\n| `-i`         | Specifies the polling period (in seconds).                   |\n| `-g`         | Specifies graphable output format.                           |\n| `-h`         | Displays help information.                                   |\n| `stats_file` | Specifies either the full path to a statistics file or the shorthand reference, `mds` or `ost` |\n\n### Example\n\nTo monitor /proc/fs/lustre/ost/OSS/ost/stats at 1 second intervals, run;\n\n```\nllstat -i 1 ost\n```\n\n### Files\n\nThe llstat files are located at:\n\n```\n/proc/fs/lustre/mdt/MDS/*/stats\n/proc/fs/lustre/mdt/*/exports/*/stats\n/proc/fs/lustre/mdc/*/stats\n/proc/fs/lustre/ldlm/services/*/stats\n/proc/fs/lustre/ldlm/namespaces/*/pool/stats\n/proc/fs/lustre/mgs/MGS/exports/*/stats\n/proc/fs/lustre/ost/OSS/*/stats\n/proc/fs/lustre/osc/*/stats\n/proc/fs/lustre/obdfilter/*/exports/*/stats\n/proc/fs/lustre/obdfilter/*/stats\n/proc/fs/lustre/llite/*/stats\n```\n\n## llverdev\n\nThe llverdev verifies a block device is functioning properly over its full size.\n\n### Synopsis\n\n```\nllverdev [-c chunksize] [-f] [-h] [-o offset] [-l] [-p] [-r] [-t timestamp] [-v] [-w] device\n```\n\n### Description\n\nSometimes kernel drivers or hardware devices have bugs that prevent them from accessing the full device size correctly, or possibly have bad sectors on disk or other problems which prevent proper data storage. There are often defects associated with major system boundaries such as 2^32 bytes, 2^31 sectors, 2^31 blocks, 2^32 blocks, etc.\n\nThe llverdev utility writes and verifies a unique test pattern across the entire device to ensure that data is accessible after it was written, and that data written to one part of the disk is not overwriting data on another part of the disk.\n\nIt is expected that llverdev will be run on large size devices (TB). It is always better to run llverdev in verbose mode, so that device testing can be easily restarted from the point where it was stopped.\n\nRunning a full verification can be time-consuming for very large devices. We recommend starting with a partial verification to ensure that the device is minimally sane before investing in a full verification.\n\n### Options\n\n| **Option**       | **Description**                                              |\n| ---------------- | ------------------------------------------------------------ |\n| `-c|--chunksize` | I/O chunk size in bytes (default value is 1048576).          |\n| `-f|--force`     | Forces the test to run without a confirmation that the device will be overwritten and all data will be permanently destroyed. |\n| `-h|--help`      | Displays a brief help message.                               |\n| `-o *offset*`    | Offset (in kilobytes) of the start of the test (default value is 0). |\n| `-l|--long`      | Runs a full check, writing and then reading and verifying every block on the disk. |\n| `-p|--partial`   | Runs a partial check, only doing periodic checks across the device (1 GB steps). |\n| `-r|--read`      | Runs the test in read (verify) mode only, after having previously run the test in `-w` mode. |\n| `-t *timestamp*` | Sets the test start time as printed at the start of a previously-interrupted test to ensure that validation data is the same across the entire file system (default value is the current time()). |\n| `-v|--verbose`   | Runs the test in verbose mode, listing each read and write operation.", "mimetype": "text/plain", "start_char_idx": 26091, "end_char_idx": 30106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f5485ff-2186-4340-a583-84eae6d55c51": {"__data__": {"id_": "7f5485ff-2186-4340-a583-84eae6d55c51", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8342151c-1130-438b-be50-20a5fef97bfc", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2f31d08307e6a31aa8cf711f2f4ec9f27f986ddcaebc2c7dc8b3bf075f9a390b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "168e9538-c94a-4889-bbc2-c6b9cb100883", "node_type": "1", "metadata": {}, "hash": "2c4350ad2f68812e37f002894c374a3da099e5a521629d0fc9e24109667f1dca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `-o *offset*`    | Offset (in kilobytes) of the start of the test (default value is 0). |\n| `-l|--long`      | Runs a full check, writing and then reading and verifying every block on the disk. |\n| `-p|--partial`   | Runs a partial check, only doing periodic checks across the device (1 GB steps). |\n| `-r|--read`      | Runs the test in read (verify) mode only, after having previously run the test in `-w` mode. |\n| `-t *timestamp*` | Sets the test start time as printed at the start of a previously-interrupted test to ensure that validation data is the same across the entire file system (default value is the current time()). |\n| `-v|--verbose`   | Runs the test in verbose mode, listing each read and write operation. |\n\n### Examples\n\nRuns a partial device verification on /dev/sda:\n\n```\nllverdev -v -p /dev/sda \nllverdev: permanently overwrite all data on /dev/sda (yes/no)? y \nllverdev: /dev/sda is 4398046511104 bytes (4096.0 GB) in size \nTimestamp: 1009839028 \nCurrent write offset: 4096 kB\n```\n\nContinues an interrupted verification at offset 4096kB from the start of the device, using the same timestamp as the previous run:\n\n```\nllverdev -f -v -p --offset=4096 --timestamp=1009839028 /dev/sda \nllverdev: /dev/sda is 4398046511104 bytes (4096.0 GB) in size \nTimestamp: 1009839028 \nwrite complete \nread complete \n```\n\n## lshowmount\n\nThe lshowmount utility shows Lustre exports.\n\n### Synopsis\n\n```\nlshowmount [-ehlv]\n```\n\n### Description\n\nThe lshowmount utility shows the hosts that have Lustre mounted to a server. This utility looks for exports from the MGS, MDS, and obdfilter.\n\n### Options\n\n| **Option**       | **Description**                                              |\n| ---------------- | ------------------------------------------------------------ |\n| `-e|--enumerate` | Causes lshowmount to list each client mounted on a separate line instead of trying to compress the list of clients into a hostrange string. |\n| `-h|--help`      | Causes lshowmount to print out a usage message.              |\n| `-l|--lookup`    | Causes lshowmount to try to look up the hostname for NIDs that look like IP addresses. |\n| `-v|--verbose`   | Causes lshowmount to output export information for each service instead of only displaying the aggregate information for all Lustre services on the server. |\n\n### Files\n\n```\n/proc/fs/lustre/mgs/server/exports/uuid/nid\n/proc/fs/lustre/mds/server/exports/uuid/nid\n/proc/fs/lustre/obdfilter/server/exports/uuid/nid\n```\n\n## lst\n\nThe lst utility starts LNet self-test.\n\n### Synopsis\n\n```\nlst\n```\n\n### Description\n\nLNet self-test helps site administrators confirm that Lustre Networking (LNet) has been properly installed and configured. The self-test also confirms that LNet and the network software and hardware underlying it are performing as expected.\n\nEach LNet self-test runs in the context of a session. A node can be associated with only one session at a time, to ensure that the session has exclusive use of the nodes on which it is running. A session is create, controlled and monitored from a single node; this is referred to as the self-test console.\n\nAny node may act as the self-test console. Nodes are named and allocated to a self-test session in groups. This allows all nodes in a group to be referenced by a single name.\n\nTest configurations are built by describing and running test batches. A test batch is a named collection of tests, with each test composed of a number of individual point-to-point tests running in parallel. These individual point-to-point tests are instantiated according to the test type, source group, target group and distribution specified when the test is added to the test batch.\n\n### Modules\n\nTo run LNet self-test, load these modules: libcfs, lnet, lnet_selftest and any one of the klnds (ksocklnd, ko2iblnd...). To load all necessary modules, run modprobe lnet_selftest, which recursively loads the modules on which lnet_selftest depends.\n\nThere are two types of nodes for LNet self-test: the console node and test nodes.", "mimetype": "text/plain", "start_char_idx": 29379, "end_char_idx": 33398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "168e9538-c94a-4889-bbc2-c6b9cb100883": {"__data__": {"id_": "168e9538-c94a-4889-bbc2-c6b9cb100883", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f5485ff-2186-4340-a583-84eae6d55c51", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ee480a23adf7bff230729cc52f7d5ec50729508791ecd765c03703e8dd381256", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0664337e-3549-4889-afd1-c0292651989d", "node_type": "1", "metadata": {}, "hash": "45ca37a49a15b1c6a282ca5a4b7f7c678a69379e3e7ae16bd1bcbb47d66c0ffa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nodes are named and allocated to a self-test session in groups. This allows all nodes in a group to be referenced by a single name.\n\nTest configurations are built by describing and running test batches. A test batch is a named collection of tests, with each test composed of a number of individual point-to-point tests running in parallel. These individual point-to-point tests are instantiated according to the test type, source group, target group and distribution specified when the test is added to the test batch.\n\n### Modules\n\nTo run LNet self-test, load these modules: libcfs, lnet, lnet_selftest and any one of the klnds (ksocklnd, ko2iblnd...). To load all necessary modules, run modprobe lnet_selftest, which recursively loads the modules on which lnet_selftest depends.\n\nThere are two types of nodes for LNet self-test: the console node and test nodes. Both node types require all previously-specified modules to be loaded. (The userspace test node does not require these modules).\n\nTest nodes can be in either kernel or in userspace. A console user can invite a kernel test node to join the test session by running lst add_group NID, but the user cannot actively add a userspace test node to the test session. However, the console user can passively accept a test node to the test session while the test node runs lst client to connect to the console.\n\n### Utilities\n\nLNet self-test includes two user utilities, lst and lstclient.\n\nlst is the user interface for the self-test console (run on the console node). It provides a list of commands to control the entire test system, such as create session, create test groups, etc.\n\nlstclient is the userspace self-test program which is linked with userspace LNDs and LNet. A user can invoke lstclient to join a self-test session:\n\n```\nlstclient -sesid CONSOLE_NID group NAME\n```\n\n### Example Script\n\nThis is a sample LNet self-test script which simulates the traffic pattern of a set of Lustre servers on a TCP network, accessed by Lustre clients on an IB network (connected via LNet routers), with half the clients reading and half the clients writing.\n\n```\n#!/bin/bash\nexport LST_SESSION=$$\nlst new_session read/write\nlst add_group servers 192.168.10.[8,10,12-16]@tcp\nlst add_group readers 192.168.1.[1-253/2]@o2ib\nlst add_group writers 192.168.1.[2-254/2]@o2ib\nlst add_batch bulk_rw\nlst add_test --batch bulk_rw --from readers --to servers     brw read check\\\n=simple size=1M\nlst add_test --batch bulk_rw --from writers --to servers     brw write chec\\\nk=full size=4K\n# start running\nlst run bulk_rw\n# display server stats for 30 seconds\nlst stat servers & sleep 30; kill $!\n# tear down\nlst end_session \n```\n\n## lustre_rmmod.sh\n\nThe lustre_rmmod.sh utility removes all Lustre and LNet modules (assuming no Lustre services are running). It is located in /usr/bin.\n\n**Note**\n\nThe lustre_rmmod.sh utility does not work if Lustre modules are being used or if you have manually run the lctl network up command.\n\n## lustre_rsync\n\nThe lustre_rsync utility synchronizes (replicates) a Lustre file system to a target file system.\n\n### Synopsis\n\n```\nlustre_rsync --source|-s src --target|-t tgt \n   --mdt|-m mdt [--user|-u userid]\n   [--xattr|-x yes|no] [--verbose|-v]\n   [--statuslog|-l log] [--dry-run] [--abort-on-err] \n \nlustre_rsync --statuslog|-l log\n \nlustre_rsync --statuslog|-l log --source|-s source\n   --target|-t tgt --mdt|-m mdt\n```\n\n### Description\n\nThe lustre_rsync utility is designed to synchronize (replicate) a Lustre file system (source) to another file system (target). The target can be a Lustre file system or any other type, and is a normal, usable file system. The synchronization operation is efficient and does not require directory walking, as lustre_rsync uses Lustre MDT changelogs to identify changes in the Lustre file system.", "mimetype": "text/plain", "start_char_idx": 32535, "end_char_idx": 36343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0664337e-3549-4889-afd1-c0292651989d": {"__data__": {"id_": "0664337e-3549-4889-afd1-c0292651989d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "168e9538-c94a-4889-bbc2-c6b9cb100883", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c58cc8ce7b35baf03421c9654def85a1b0b3ff81270ac7c721259af046440240", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69d3cd26-bc8d-41f0-99cd-d54bfc9a0517", "node_type": "1", "metadata": {}, "hash": "e5de2813cd7cd86dd0691896652cbc498a41398400033f43d08e7209153da598", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The target can be a Lustre file system or any other type, and is a normal, usable file system. The synchronization operation is efficient and does not require directory walking, as lustre_rsync uses Lustre MDT changelogs to identify changes in the Lustre file system.\n\nBefore using lustre_rsync:\n\n- A changelog user must be registered (see lctl (8) changelog_register)\n\n\\- AND -\n\n- Verify that the Lustre file system (source) and the replica file system (target) are identical before the changelog user is registered. If the file systems are discrepant, use a utility, e.g. regular rsync (not lustre_rsync) to make them identical.\n\n### Options\n\n| **Option**          | **Description**                                              |\n| ------------------- | ------------------------------------------------------------ |\n| `--source=*src*`    | The path to the root of the Lustre file system (source) which will be synchronized. This is a mandatory option if a valid status log created during a previous synchronization operation (--statuslog) is not specified. |\n| `--target=*tgt*`    | The path to the root where the source file system will be synchronized (target). This is a mandatory option if the status log created during a previous synchronization operation (--statuslog) is not specified. This option can be repeated if multiple synchronization targets are desired. |\n| `--mdt=*mdt*`       | The metadata device to be synchronized. A changelog user must be registered for this device. This is a mandatory option if a valid status log created during a previous synchronization operation (--statuslog) is not specified. |\n| `--user=*userid*`   | The changelog user ID for the specified MDT. To use lustre_rsync, the changelog user must be registered. For details, see the changelog_register parameter in the lctl man page. This is a mandatory option if a valid status log created during a previous synchronization operation (--statuslog) is not specified. |\n| `--statuslog=*log*` | A log file to which synchronization status is saved. When lustre_rsync starts, the state of a previous replication is read from here. If the status log from a previous synchronization operation is specified, otherwise mandatory options like --source, --target and --mdt options may be skipped. By specifying options like --source, --target and/or --mdt in addition to the --statuslog option, parameters in the status log can be overridden. Command line options take precedence over options in the status log. |\n| `--xattr*yes|no*`   | Specifies whether extended attributes (xattrs) are synchronized or not. The default is to synchronize extended attributes.NOTE: Disabling xattrs causes Lustre striping information not to be synchronized. |\n| `--verbose`         | Produces a verbose output.                                   |\n| `--dry-run`         | Shows the output of lustre_rsync commands (copy, mkdir, etc.) on the target file system without actually executing them. |\n| `--abort-on-err`    | Shows the output of lustre_rsync commands (copy, mkdir, etc.) on the target file system without actually executing them. |\n\n### Examples\n\nRegister a changelog user for an MDT (e.g., MDT lustre-MDT0000).\n\n```\n$ ssh \n$ MDS lctl changelog_register \\\n           --device lustre-MDT0000 -n \ncl1\n```\n\nSynchronize/replicate a Lustre file system (/mnt/lustre) to a target file system (/mnt/target).\n\n```\n$ lustre_rsync --source=/mnt/lustre --target=/mnt/target \\ \n           --mdt=lustre-MDT0000 --user=cl1 \\ \n           --statuslog replicate.log  --verbose \nLustre filesystem: lustre \nMDT device: lustre-MDT0000 \nSource: /mnt/lustre \nTarget: /mnt/target \nStatuslog: sync.log \nChangelog registration: cl1 \nStarting changelog record: 0 \nErrors: 0 \nlustre_rsync took 1 seconds \nChangelog records consumed: 22\n```\n\nAfter the file system undergoes changes, synchronize the changes with the target file system. Only the statuslog name needs to be specified, as it has all the parameters passed earlier.", "mimetype": "text/plain", "start_char_idx": 36076, "end_char_idx": 40051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69d3cd26-bc8d-41f0-99cd-d54bfc9a0517": {"__data__": {"id_": "69d3cd26-bc8d-41f0-99cd-d54bfc9a0517", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0664337e-3549-4889-afd1-c0292651989d", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f8084c1288a37bf926fb8263ed6401d2bfb255151279050b0a21f12be8b73943", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bca2bd5-6a4e-48e3-b1e5-f9afdace211b", "node_type": "1", "metadata": {}, "hash": "d5beaf46ec441bd82a642ea2c9ee4686718f3ef7a1d7bd1f78992ebe160be94c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\n$ lustre_rsync --source=/mnt/lustre --target=/mnt/target \\ \n           --mdt=lustre-MDT0000 --user=cl1 \\ \n           --statuslog replicate.log  --verbose \nLustre filesystem: lustre \nMDT device: lustre-MDT0000 \nSource: /mnt/lustre \nTarget: /mnt/target \nStatuslog: sync.log \nChangelog registration: cl1 \nStarting changelog record: 0 \nErrors: 0 \nlustre_rsync took 1 seconds \nChangelog records consumed: 22\n```\n\nAfter the file system undergoes changes, synchronize the changes with the target file system. Only the statuslog name needs to be specified, as it has all the parameters passed earlier.\n\n```\n$ lustre_rsync --statuslog replicate.log --verbose \nReplicating Lustre filesystem: lustre \nMDT device: lustre-MDT0000 \nSource: /mnt/lustre \nTarget: /mnt/target \nStatuslog: replicate.log \nChangelog registration: cl1 \nStarting changelog record: 22 \nErrors: 0 \nlustre_rsync took 2 seconds \nChangelog records consumed: 42\n```\n\nSynchronize a Lustre file system (/mnt/lustre) to two target file systems (/mnt/target1 and /mnt/target2).\n\n```\n$ lustre_rsync --source=/mnt/lustre \\ \n   --target=/mnt/target1 --target=/mnt/target2 \\ \n   --mdt=lustre-MDT0000 --user=cl1 \n   --statuslog replicate.log\n```\n\n### See Also\n\n[the section called \u201c `lfs` \u201d](06.03-User%20Utilities.md#lfs)\n\n## mkfs.lustre\n\nThe\u00a0`mkfs.lustre`\u00a0utility formats a disk for a Lustre service.\n\n### Synopsis\n\n```\nmkfs.lustre target_type [options] device\n```\n\nwhere *target_type* is one of the following:\n\n| **Option**            | **Description**                                              |\n| --------------------- | ------------------------------------------------------------ |\n| `--ost`               | Object storage target (OST)                                  |\n| `--mdt`               | Metadata storage target (MDT)                                |\n| `--network=*net,...*` | Network(s) to which to restrict this OST/MDT. This option can be repeated as necessary. |\n| `--mgs`               | Configuration management service (MGS), one per site. This service can be combined with one `--mdt` service by specifying both types. |\n\n### Description\n\n`mkfs.lustre` is used to format a disk device for use as part of a Lustre file system. After formatting, a disk can be mounted to start the Lustre service defined by this command.\n\nWhen the file system is created, parameters can simply be added as a `--param` option to the `mkfs.lustre`command. See [*the section called \u201cSetting Tunable Parameters with `mkfs.lustre`\u201d*](03.02-Lustre%20Operations.md#setting-tunable-parameters-with-mkfslustre).\n\n| **Option**                      | **Description**                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| `--backfstype=*fstype*`         | Forces a particular format for the backing file system such as ldiskfs (the default) or zfs. |\n| `--comment=*comment*`           | Sets a user comment about this disk, ignored by the Lustre software. |\n| `--device-size=*#*>KB`          | Sets the device size for loop devices.                       |\n| `--dryrun`                      | Only prints what would be done; it does not affect the disk. |\n| `--servicenode=*nid,...*`       | Sets the NID(s) of all service nodes, including primary and failover partner service nodes. The `--servicenode` option cannot be used with `--failnode` option. See [*the section called \u201cPreparing a Lustre File System for Failover\u201d*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#preparing-a-lustre-file-system-for-failover) for more details. |\n| `--failnode=*nid,...*`          | Sets the NID(s) of a failover service node for a primary server for a target. The `--failnode` option cannot be used with `--servicenode` option.", "mimetype": "text/plain", "start_char_idx": 39454, "end_char_idx": 43231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8bca2bd5-6a4e-48e3-b1e5-f9afdace211b": {"__data__": {"id_": "8bca2bd5-6a4e-48e3-b1e5-f9afdace211b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69d3cd26-bc8d-41f0-99cd-d54bfc9a0517", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "20333cdeaf6b6ac5ccbc3e90c8f6b1ca2fb2290eff530300ccfdce381ec149d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e785e8c-6599-4349-ac2a-d9579f023c83", "node_type": "1", "metadata": {}, "hash": "b09becee4b45e1bcc9a1bf577d07119ca7b0d868eba0cd207ec4afb6b03f0720", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `--servicenode=*nid,...*`       | Sets the NID(s) of all service nodes, including primary and failover partner service nodes. The `--servicenode` option cannot be used with `--failnode` option. See [*the section called \u201cPreparing a Lustre File System for Failover\u201d*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#preparing-a-lustre-file-system-for-failover) for more details. |\n| `--failnode=*nid,...*`          | Sets the NID(s) of a failover service node for a primary server for a target. The `--failnode` option cannot be used with `--servicenode` option. See [*the section called \u201cPreparing a Lustre File System for Failover\u201d*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#preparing-a-lustre-file-system-for-failover) for more details.                             **Note**                                                                                                                                                                                                         When the `--failnode` option is used, certain restrictions apply (see [*the section called \u201cPreparing a Lustre File System for Failover\u201d*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#preparing-a-lustre-file-system-for-failover)). |\n| `--fsname=*filesystem_name*`    | The Lustre file system of which this service/node will be a part. The default file system name is `lustre`.                                                                                                                        **Note**                                                                                                                                                                                                          The file system name is limited to 8 characters. |\n| `--index=*index_number*`        | Specifies the OST or MDT number (0...N). This allows mapping between the OSS and MDS node and the device on which the OST or MDT is located. |\n| `--mkfsoptions=*opts*`          | Formats options for the backing file system. For example, ext3 options could be set here. |\n| `--mountfsoptions=*opts*`       | Sets the mount options used when the backing file system is mounted.                                                                                                                                                                                    **Warning**                                                                                                                                                                                                      Unlike earlier versions of `mkfs.lustre`, this version completely replaces the default mount options with those specified on the command line, and issues a warning on stderr if any default mount options are omitted.                                                                                                          The defaults for ldiskfs are:                                                                        MGS/MDT: `errors=remount-ro,iopen_nopriv,user_xattr`                                                                                                         OST: `errors=remount-ro,extents,mballoc`                                                                                              Introduced in Lustre 2.5                                                                                                                                                   OST: `errors=remount-ro`                                                                                                                                        Use care when altering the default mount options. |\n| `--network=*net,...*`           | Network(s) to which to restrict this OST/MDT. This option can be repeated as necessary. |\n| `--mgsnode=*nid,...*`           | Sets the NIDs of the MGS node, required for all targets other than the MGS. |\n| `--param *key*=*value*`         | Sets the permanent parameter *key* to value *value*. This option can be repeated as necessary. Typical options might include: |\n| `--param sys.timeout=40`>       | System obd timeout.                                          |\n| `--param lov.stripesize=2M`     | Default stripe size.                                         |\n| `param lov.stripecount=2`       | Default stripe count.                                        |\n| `--param failover.mode=failout` | Returns errors instead of waiting for recovery.              |\n| `--quiet`                       | Prints less information.                                     |\n| `--reformat`                    | Reformats an existing Lustre disk.                           |\n| `--stripe-count-hint=stripes`   | Used to optimize the MDT's inode size.                       |\n| `--verbose`                     | Prints more information.", "mimetype": "text/plain", "start_char_idx": 42648, "end_char_idx": 47502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e785e8c-6599-4349-ac2a-d9579f023c83": {"__data__": {"id_": "7e785e8c-6599-4349-ac2a-d9579f023c83", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bca2bd5-6a4e-48e3-b1e5-f9afdace211b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a465e88eec21458f2a73ffc51e3c6a51c79375021585b6c1db37ddf6bed1f100", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af0b4c87-d136-4e38-862d-9592d10f5d73", "node_type": "1", "metadata": {}, "hash": "f40147270d791e48d6182817726f5870ebb38449dfad4ed620967a305b3192a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `--param *key*=*value*`         | Sets the permanent parameter *key* to value *value*. This option can be repeated as necessary. Typical options might include: |\n| `--param sys.timeout=40`>       | System obd timeout.                                          |\n| `--param lov.stripesize=2M`     | Default stripe size.                                         |\n| `param lov.stripecount=2`       | Default stripe count.                                        |\n| `--param failover.mode=failout` | Returns errors instead of waiting for recovery.              |\n| `--quiet`                       | Prints less information.                                     |\n| `--reformat`                    | Reformats an existing Lustre disk.                           |\n| `--stripe-count-hint=stripes`   | Used to optimize the MDT's inode size.                       |\n| `--verbose`                     | Prints more information.                                     |\n\n### Examples\n\nCreates a combined MGS and MDT for file system `testfs` on, e.g., node `cfs21`:\n\n```\nmkfs.lustre --fsname=testfs --mdt --mgs /dev/sda1\n```\n\nCreates an OST for file system `testfs` on any node (using the above MGS):\n\n```\nmkfs.lustre --fsname=testfs --mgsnode=cfs21@tcp0 --ost --index=0 /dev/sdb\n```\n\nCreates a standalone MGS on, e.g., node `cfs22`:\n\n```\nmkfs.lustre --mgs /dev/sda1\n```\n\nCreates an MDT for file system `myfs1` on any node (using the above MGS):\n\n```\nmkfs.lustre --fsname=myfs1 --mdt --mgsnode=cfs22@tcp0 /dev/sda2\n```\n\n### See Also\n\n- [*the section called \u201c mkfs.lustre\u201d*](#mkfslustre)mkfs.lustre,\n- [*the section called \u201c mount.lustre\u201d*](mountlustre)mount.lustre,\n- [*the section called \u201c `lfs` \u201d*](06.03-User%20Utilities.md#lfs)lfs\n\n## mount.lustre\n\nThe mount.lustre utility starts a Lustre client or target service.\n\n### Synopsis\n\n```\nmount -t lustre [-o options] device mountpoint\n```\n\n### Description\n\nThe mount.lustre utility starts a Lustre client or target service. This program should not be called directly; rather, it is a helper program invoked through mount(8), as shown above. Use the umount command to stop Lustre clients and targets.\n\nThere are two forms for the device option, depending on whether a client or a target service is started:\n\n| **Option**                       | **Description**                                              |\n| -------------------------------- | ------------------------------------------------------------ |\n| `*mgsname*:/*fsname**[/subdir]*` | Mounts the Lustre file system named *fsname* (optionally starting at subdirectory *subdir* within the filesystem, if specified) on the client at the directory*mountpoint*, by contacting the Lustre Management Service at *mgsname*. The format for *mgsname* is defined below. A client file system can be listed in `fstab(5)` for automatic mount at boot time, is usable like any local file system, and provides a full POSIX standard-compliant interface. |\n| *block_device*                   | Starts the target service defined by the `mkfs.lustre(8)` command on the physical disk *block_device*. The *block_device* may be specified using `-L *label*` to find the first block device with that label (e.g. `testfs-MDT0000`), or by UUID using the`-U *uuid*` option. Care should be taken if there is a device-level backup of the target filesystem on the same node, which would have a duplicate label and UUID if it has not been changed with `tune2fs(8)` or similar. The mounted target service filesystem mounted at *mountpoint* is only useful for `df(1)` operations and appears in `/proc/mounts` to show the device is in use. |\n\n### Options\n\n| **Option**                      | **Description**                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| `mgsname=*mgsnode*[:*mgsnode*]` | *mgsname* is a colon-separated list of *mgsnode* names where the MGS service may run.", "mimetype": "text/plain", "start_char_idx": 46583, "end_char_idx": 50518, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af0b4c87-d136-4e38-862d-9592d10f5d73": {"__data__": {"id_": "af0b4c87-d136-4e38-862d-9592d10f5d73", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e785e8c-6599-4349-ac2a-d9579f023c83", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "32c717f0266fb91d9e792fa3005dca38491926848484603699d1fe0a2464de48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc5908e2-5cf8-49a0-bb59-c1fc6b404c91", "node_type": "1", "metadata": {}, "hash": "e97fd6ba7090f34c7f0afc1029d6772a7e57fcd6d3c27897c2bb372c314f7c7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "`testfs-MDT0000`), or by UUID using the`-U *uuid*` option. Care should be taken if there is a device-level backup of the target filesystem on the same node, which would have a duplicate label and UUID if it has not been changed with `tune2fs(8)` or similar. The mounted target service filesystem mounted at *mountpoint* is only useful for `df(1)` operations and appears in `/proc/mounts` to show the device is in use. |\n\n### Options\n\n| **Option**                      | **Description**                                              |\n| ------------------------------- | ------------------------------------------------------------ |\n| `mgsname=*mgsnode*[:*mgsnode*]` | *mgsname* is a colon-separated list of *mgsnode* names where the MGS service may run. Multiple *mgsnode* values can be specified if the MGS service is configured for HA failover and may be running on any one of the nodes. |\n| `mgsnode=*mgsnid*[,*mgsnid*]`   | Each *mgsnode* may specify a comma-separated list of NIDs, if there are different LNet interfaces for that `mgsnode`. |\n| `mgssec=*flavor*`               | Specifies the encryption flavor for the initial network RPC connection to the MGS. Non-security flavors are: `null`, `plain`, and `gssnull`, which respectively disable, or have no encryption or integrity features for testing purposes. Kerberos flavors are: `krb5n`, `krb5a`, `krb5i`, and `krb5p`. Shared-secret key flavors are: `skn`, `ska`,`ski`, and `skpi`, see the [*Configuring Shared-Secret Key (SSK) Security*](03.17-Configuring%20Shared-Secret%20Key%20(SSK)%20Security.md) for more details. The security flavor for client-to-server connections is specified in the filesystem configuration that the client fetches from the MGS. |\n| `skpath=*file|directory*`       | Introduced in Lustre 2.9Path to a file or directory with the keyfile(s) to load for this mount command. Keys are inserted into the `KEY_SPEC_SESSION_KEYRING` keyring in the kernel with a description containing `lustre:` and a suffix which depends on whether the context of the mount command is for an MGS, MDT/OST, or client. |\n| `exclude=*ostlist*`             | Starts a client or MDT with a colon-separated list of known inactive OSTs that it will not try to connect to. |\n\nIn addition to the standard mount(8) options, Lustre understands the following client-specific options:\n\n| **Option**        | **Description**                                              |\n| ----------------- | ------------------------------------------------------------ |\n| `always_ping`     | Introduced in Lustre 2.9The client will periodically ping the server when it is idle, even if the server `ptlrpc` module is configured with the `suppress_pings` option. This allows clients to reliably use the filesystem even if they are not part of an external client health monitoring mechanism. |\n| `flock`           | Enables advisory file locking support between participating applications using the `flock(2)`system call. This causes file locking to be coherent across all client nodes also using this mount option. This is useful if applications need coherent userspace file locking across multiple client nodes, but also imposes communications overhead in order to maintain locking consistency between client nodes. |\n| `localflock`      | Enables client-local `flock(2)` support, using only client-local advisory file locking. This is faster than using the global `flock` option, and can be used for applications that depend on functioning `flock(2)` but run only on a single node. It has minimal overhead using only the Linux kernel's locks. |\n| `noflock`         | Disables `flock(2)` support entirely, and is the default option. Applications calling `flock(2)`get an `ENOSYS` error. It is up to the administrator to choose either the `localflock` or `flock`mount option based on their requirements. It is possible to mount clients with different options, and only those mounted with `flock` will be coherent amongst each other.", "mimetype": "text/plain", "start_char_idx": 49765, "end_char_idx": 53733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc5908e2-5cf8-49a0-bb59-c1fc6b404c91": {"__data__": {"id_": "fc5908e2-5cf8-49a0-bb59-c1fc6b404c91", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af0b4c87-d136-4e38-862d-9592d10f5d73", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7375eaf9541b81cf9b553857a8050ac61f3ec7a7f80ca13422c0927c7b7b9f81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c2bad15-8530-4158-8cf2-3a04faa7baad", "node_type": "1", "metadata": {}, "hash": "cf46336c8bdbed51233874d9d115f50a78e197aec219c6731c7b5a79187c9d17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| `localflock`      | Enables client-local `flock(2)` support, using only client-local advisory file locking. This is faster than using the global `flock` option, and can be used for applications that depend on functioning `flock(2)` but run only on a single node. It has minimal overhead using only the Linux kernel's locks. |\n| `noflock`         | Disables `flock(2)` support entirely, and is the default option. Applications calling `flock(2)`get an `ENOSYS` error. It is up to the administrator to choose either the `localflock` or `flock`mount option based on their requirements. It is possible to mount clients with different options, and only those mounted with `flock` will be coherent amongst each other. |\n| `lazystatfs`      | Allows `statfs(2)` (as used by `df(1)` and `lfs-df(1)`) to return even if some OST or MDT is unresponsive or has been temporarily or permanently disabled in the configuration. This avoids blocking until all of the targets are available. This is the default behavior since Lustre 2.9.0. |\n| `nolazystatfs`    | Requires that `statfs(2)` block until all OSTs and MDTs are available and have returned space usage. |\n| `user_xattr`      | Enables get/set of extended attributes by regular users in the `user.*` namespace. See the`attr(5)` manual page for more details. |\n| `nouser_xattr`    | Disables use of extended attributes in the `user.*` namespace by regular users. Root and system processes can still use extended attributes. |\n| `verbose`         | Enable extra mount/umount console messages.                  |\n| `noverbose`       | Disable mount/umount console messages.                       |\n| `user_fid2path`   | Enable FID-to-path translation by regular users.NoteThis option allows a potential security hole because it allows regular users direct access to a file by its Lustre File ID. This bypasses POSIX path-based permission checks, and could allow the user to access a file in a directory that they do not have access to. Regular POSIX file mode and ACL permission checks are still performed on the file itself, so users cannot access a file to which they have no permission. |\n| `nouser_fid2path` | Disable FID to path translation by regular users. Root and processes with`CAP_DAC_READ_SEARCH` can still perform FID to path translation. |\n\nIn addition to the standard mount options and backing disk type (e.g. ldiskfs) options, Lustre understands the following server-specific mount options:\n\n| **Option**                     | **Description**                                              |\n| ------------------------------ | ------------------------------------------------------------ |\n| `nosvc`                        | Starts the MGC (and MGS, if co-located) for a target service, not the actual service. |\n| `nomgs`                        | Starts only the MDT (with a co-located MGS), without starting the MGS. |\n| `abort_recov`                  | Aborts client recovery on that server and starts the target service immediately. |\n| `max_sectors_kb=*KB*`          | Introduced in Lustre 2.10Sets the block device parameter `max_sectors_kb` limit for the MDT or OST target being mounted to specified maximum number of kilobytes. When `max_sectors_kb` isn't specified as a mount option, it will automatically be set to the `max_hw_sectors_kb` (up to a maximum of 16MiB) for that block device. This default behavior is suited for most users. When `max_sectors_kb=0` is used, the current value for this tunable will be kept. |\n| `md_stripe_cache_size`         | Sets the stripe cache size for server-side disk with a striped RAID configuration. |\n| `recovery_time_soft=*timeout*` | Allows `timeout` seconds for clients to reconnect for recovery after a server crash. This timeout is incrementally extended if it is about to expire and the server is still handling new connections from recoverable clients.The default soft recovery timeout is 3 times the value of the Lustre timeout parameter (see [*the section called \u201cSetting Static Timeouts\u201d*](06.02-Lustre%20Parameters.md#setting-static-timeouts)). The default Lustre timeout is 100 seconds, which would make the soft recovery timeout default to 300 seconds (5 minutes).", "mimetype": "text/plain", "start_char_idx": 53018, "end_char_idx": 57203, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c2bad15-8530-4158-8cf2-3a04faa7baad": {"__data__": {"id_": "1c2bad15-8530-4158-8cf2-3a04faa7baad", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc5908e2-5cf8-49a0-bb59-c1fc6b404c91", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5435e82c56a78443ff4295c58a97980f950a2c63e11edd6dd0cf8649dc4001dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d84abdb-0609-4f2f-84f2-d7bc7af44eb7", "node_type": "1", "metadata": {}, "hash": "110ce83b87defb3b0ef17081fc64869ace42fa9e0e13108ce3e248c87856698e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This default behavior is suited for most users. When `max_sectors_kb=0` is used, the current value for this tunable will be kept. |\n| `md_stripe_cache_size`         | Sets the stripe cache size for server-side disk with a striped RAID configuration. |\n| `recovery_time_soft=*timeout*` | Allows `timeout` seconds for clients to reconnect for recovery after a server crash. This timeout is incrementally extended if it is about to expire and the server is still handling new connections from recoverable clients.The default soft recovery timeout is 3 times the value of the Lustre timeout parameter (see [*the section called \u201cSetting Static Timeouts\u201d*](06.02-Lustre%20Parameters.md#setting-static-timeouts)). The default Lustre timeout is 100 seconds, which would make the soft recovery timeout default to 300 seconds (5 minutes). The soft recovery timeout is set at mount time and will not change if the Lustre timeout is changed after mount time. |\n| `recovery_time_hard=*timeout*` | The server is allowed to incrementally extend its timeout up to a hard maximum of *timeout* seconds.The default hard recovery timeout is 9 times the value of the Lustre timeout parameter (see [*the section called \u201cSetting Static Timeouts\u201d*](06.02-Lustre%20Parameters.md#setting-static-timeouts)). The default Lustre timeout is 100 seconds, which would make the hard recovery timeout default to 900 seconds (15 minutes). The hard recovery timeout is set at mount time and will not change if the Lustre timeout is changed after mount time. |\n| `noscrub`                      | Typically the MDT will detect restoration from a file-level backup during mount. This mount option prevents the OI Scrub from starting automatically when the MDT is mounted. Manually starting LFSCK after mounting provides finer control over the starting conditions. This mount option also prevents OI scrub from occurring automatically when OI inconsistency is detected (see [*the section called \u201cAuto scrub\u201d*](05.02-Troubleshooting%20Recovery.md#auto-scrub)). |\n\n### Examples\n\nStarts a client for the Lustre file system *chipfs* at mount point */mnt/chip*. The Management Service is running on a node reachable from this client via the cfs21@tcp0 NID.\n\n```\nmount -t lustre cfs21@tcp0:/chipfs /mnt/chip\n```\n\nIntroduced in Lustre 2.9Similar to the above example, but mounting a subdirectory under *chipfs* as a fileset.`mount -t lustre cfs21@tcp0:/chipfs/v1_0 /mnt/chipv1_0`\n\nStarts the Lustre metadata target service from /dev/sda1 on mount point /mnt/test/mdt.\n\n```\nmount -t lustre /dev/sda1 /mnt/test/mdt\n```\n\nStarts the testfs-MDT0000 service (using the disk label), but aborts the recovery process.\n\n```\nmount -t lustre -L testfs-MDT0000 -o abort_recov /mnt/test/mdt\n```\n\n### See Also\n\n- [the section called \u201c mkfs.lustre\u201d](mkfslustre)\n- [the section called \u201c tunefs.lustre\u201d](tunefslustre)\n- [the section called \u201c lctl\u201d](lctl)\n- [the section called \u201c `lfs` \u201d](06.03-User%20Utilities.md#lfs)\n\n## plot-llstat\n\nThe plot-llstat utility plots Lustre statistics.\n\n### Synopsis\n\n```\nplot-llstat results_filename [parameter_index]\n```\n\n### Description\n\nThe plot-llstat utility generates a CSV file and instruction files for gnuplot from the output of llstat. Since llstat is generic in nature, plot-llstat is also a generic script. The value of parameter_index can be 1 for count per interval, 2 for count per second (default setting) or 3 for total count.\n\nThe plot-llstat utility creates a .dat (CSV) file using the number of operations specified by the user. The number of operations equals the number of columns in the CSV file. The values in those columns are equal to the corresponding value of parameter_index in the output file.\n\nThe plot-llstat utility also creates a .scr file that contains instructions for gnuplot to plot the graph. After generating the .dat and .scr files, the plot-llstat tool invokes gnuplot to display the graph.", "mimetype": "text/plain", "start_char_idx": 56375, "end_char_idx": 60275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d84abdb-0609-4f2f-84f2-d7bc7af44eb7": {"__data__": {"id_": "7d84abdb-0609-4f2f-84f2-d7bc7af44eb7", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c2bad15-8530-4158-8cf2-3a04faa7baad", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "86ea08ace2a05a7e9293e4ea00f773ad6165c50e1c3416161c5a08576332ffd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c525478c-1612-4cf2-b299-e636e2c0f8fd", "node_type": "1", "metadata": {}, "hash": "7f3d4f376a05987a2971a7ebf61ec72845c7b1952e0fb31be1a7e3737a3f2328", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since llstat is generic in nature, plot-llstat is also a generic script. The value of parameter_index can be 1 for count per interval, 2 for count per second (default setting) or 3 for total count.\n\nThe plot-llstat utility creates a .dat (CSV) file using the number of operations specified by the user. The number of operations equals the number of columns in the CSV file. The values in those columns are equal to the corresponding value of parameter_index in the output file.\n\nThe plot-llstat utility also creates a .scr file that contains instructions for gnuplot to plot the graph. After generating the .dat and .scr files, the plot-llstat tool invokes gnuplot to display the graph.\n\n### Options\n\n| **Option**         | **Description**                                              |\n| ------------------ | ------------------------------------------------------------ |\n| `results_filename` | Output generated by plot-llstat                              |\n| `parameter_index`  | Value of parameter_index can be:1 - count per interval2 - count per second (default setting)3 - total count |\n\n### Example\n\n```\nllstat -i2 -g -c lustre-OST0000 > log\nplot-llstat log 3\n```\n\n## routerstat\n\nThe routerstat utility prints Lustre router statistics.\n\n### Synopsis\n\n```\nrouterstat [interval]\n```\n\n### Description\n\nThe routerstat utility displays LNet router statistics. If no `*interval*` is specified, then statistics are sampled and printed only one time. Otherwise, statistics are sampled and printed at the specified `*interval*` (in seconds).", "mimetype": "text/plain", "start_char_idx": 59589, "end_char_idx": 61127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c525478c-1612-4cf2-b299-e636e2c0f8fd": {"__data__": {"id_": "c525478c-1612-4cf2-b299-e636e2c0f8fd", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d84abdb-0609-4f2f-84f2-d7bc7af44eb7", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "73d6132df7992b9358dcb5e5e4857a8f28c2aa829980960c49169988a4a15303", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bcb5c2f-8833-4c64-868e-9f74ab54d0c4", "node_type": "1", "metadata": {}, "hash": "d4828861122320904a97c627673e22eadc4e861aed51cd1a785e6098756af7c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Options\n\n| **Option**         | **Description**                                              |\n| ------------------ | ------------------------------------------------------------ |\n| `results_filename` | Output generated by plot-llstat                              |\n| `parameter_index`  | Value of parameter_index can be:1 - count per interval2 - count per second (default setting)3 - total count |\n\n### Example\n\n```\nllstat -i2 -g -c lustre-OST0000 > log\nplot-llstat log 3\n```\n\n## routerstat\n\nThe routerstat utility prints Lustre router statistics.\n\n### Synopsis\n\n```\nrouterstat [interval]\n```\n\n### Description\n\nThe routerstat utility displays LNet router statistics. If no `*interval*` is specified, then statistics are sampled and printed only one time. Otherwise, statistics are sampled and printed at the specified `*interval*` (in seconds).\n\n### Output\n\nThe routerstat output includes the following fields:\n\n| **Output** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `M`        | Number of messages currently being processed by LNet (The maximum number of messages ever processed by LNet concurrently) |\n| `E`        | Number of LNet errors                                        |\n| `S`        | Total size (length) of messages sent in bytes/ Number of messages sent |\n| `R`        | Total size (length) of messages received in bytes/ Number of messages received |\n| `F`        | Total size (length) of messages routed in bytes/ Number of messages routed |\n| `D`        | Total size (length) of messages dropped in bytes/ Number of messages dropped |\n\nWhen an `*interval*` is specified, additional lines of statistics are printed including the following fields:\n\n| **Output** | **Description**                                              |\n| ---------- | ------------------------------------------------------------ |\n| `M`        | Number of messages currently being processed by LNet (The maximum number of messages ever processed by LNet concurrently) |\n| `E`        | Number of LNet errors per second                             |\n| `S`        | Rate of data sent in Mbytes per second/ Count of messages sent per second |\n| `R`        | Rate of data received in Mbytes per second/ Count of messages received per second |\n| `F`        | Rate of data routed in Mbytes per second/ Count of messages routed per second |\n| `D`        | Rate of data dropped in Mbytes per second/ Count of messages dropped per second |\n\n### Example\n\n```\n# routerstat 1\nM 0(13) E 0 S 117379184/4250 R 878480/4356 F 0/0 D 0/0\nM   0( 13) E 0 S    7.00/     7 R    0.00/    14 F    0.00/     0 D 0.00/0\nM   0( 13) E 0 S    7.00/     7 R    0.00/    14 F    0.00/     0 D 0.00/0\nM   0( 13) E 0 S    8.00/     8 R    0.00/    16 F    0.00/     0 D 0.00/0\nM   0( 13) E 0 S    7.00/     7 R    0.00/    14 F    0.00/     0 D 0.00/0\nM   0( 13) E 0 S    7.00/     7 R    0.00/    14 F    0.00/     0 D 0.00/0\nM   0( 13) E 0 S    7.00/     7 R    0.00/    14 F    0.00/     0 D 0.00/0\nM   0( 13) E 0 S    7.00/     7 R    0.00/    14 F    0.00/     0 D 0.00/0\nM   0( 13) E 0 S    8.00/     8 R    0.00/    16 F    0.00/     0 D 0.00/0\nM   0( 13) E 0 S    7.00/     7 R    0.00/    14 F    0.00/     0 D 0.00/0\n...\n```\n\n### Files\n\nThe routerstat utility extracts statistics data from:\n\n```\n/proc/sys/lnet/stats\n```\n\n## tunefs.lustre\n\nThe tunefs.lustre utility modifies configuration information on a Lustre target disk.", "mimetype": "text/plain", "start_char_idx": 60277, "end_char_idx": 63776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bcb5c2f-8833-4c64-868e-9f74ab54d0c4": {"__data__": {"id_": "1bcb5c2f-8833-4c64-868e-9f74ab54d0c4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c525478c-1612-4cf2-b299-e636e2c0f8fd", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "babd1ae1b84d232b0d3bf1a16a52e61ccf70b652baf385b169295e46b439c3b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86c097b9-3b5c-434a-87d9-d0b81a3db21e", "node_type": "1", "metadata": {}, "hash": "128b4c8b1a06eb645bd17cdc53b174f92dbf7a54bbfec2fe2b52f04d20bcb7bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Synopsis\n\n```\ntunefs.lustre [options] /dev/device\n```\n\n### Description\n\ntunefs.lustre is used to modify configuration information on a Lustre target disk. This does not reformat the disk or erase the target information, but modifying the configuration information can result in an unusable file system.\n\n**Caution**\n\nChanges made here affect a file system only when the target is mounted the next time.\n\nWith tunefs.lustre, parameters are \"additive\" -- new parameters are specified in addition to old parameters, they do not replace them. To erase all old tunefs.lustre parameters and just use newly-specified parameters, run:\n\n```\n$ tunefs.lustre --erase-params --param=new_parameters \n```\n\nThe tunefs.lustre command can be used to set any parameter settable in a /proc/fs/lustre file and that has its own OBD device, so it can be specified as *{obd|fsname}.obdtype.proc_file_name=value*. For example:\n\n```\n$ tunefs.lustre --param mdt.identity_upcall=NONE /dev/sda1\n```\n### Options\n\nThe tunefs.lustre options are listed and explained below.\n\n| **Option**                   | **Description**                                              |\n| ---------------------------- | ------------------------------------------------------------ |\n| `--comment=*comment*`        | Sets a user comment about this disk, ignored by Lustre.      |\n| `--dryrun`                   | Only prints what would be done; does not affect the disk.    |\n| `--erase-params`             | Removes all previous parameter information.                  |\n| `--servicenode=*nid,...*`    | Sets the NID(s) of all service nodes, including primary and failover partner service nodes. The `--servicenode` option cannot be used with `--failnode` option. See [*the section called \u201cPreparing a Lustre File System for Failover\u201d*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#preparing-a-lustre-file-system-for-failover) for more details. |\n| `--failnode=*nid,...*`       | Sets the NID(s) of a failover service node for a primary server for a target. The `--failnode` option cannot be used with `--servicenode` option. See [*the section called \u201cPreparing a Lustre File System for Failover\u201d*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#preparing-a-lustre-file-system-for-failover) for more details.                                                                                          **Note**                                                                                                              When the `--failnode` option is used, certain restrictions apply (see [*the section called \u201cPreparing a Lustre File System for Failover\u201d*](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#preparing-a-lustre-file-system-for-failover)). |\n| `--fsname=*filesystem_name*` | The Lustre file system of which this service will be a part. The default file system name is `lustre`. |\n| `--index=*index*`            | Forces a particular OST or MDT index.                        |\n| `--mountfsoptions=*opts*`    | Sets the mount options used when the backing file system is mounted.                                                                                                                                                                          **Warning**                                                                                                         Unlike earlier versions of tunefs.lustre, this version completely replaces the existing mount options with those specified on the command line, and issues a warning on stderr if any default mount options are omitted.                                                                                                            The defaults for ldiskfs are:                                                                   MGS/MDT: `errors=remount-ro,iopen_nopriv,user_xattr`                  OST: `errors=remount-ro,extents,mballoc`                                   Introduced in Lustre 2.5                                                                                    OST: `errors=remount-ro`                                                                             Do not alter the default mount options unless you know what you are doing. |\n| `--network=*net,...*`        | Network(s) to which to restrict this OST/MDT. This option can be repeated as necessary. |\n| `--mgs`                      | Adds a configuration management service to this target.      |\n| `--msgnode=*nid,...*`        | Sets the NID(s) of the MGS node; required for all targets other than the MGS. |\n| `--nomgs`                    | Removes a configuration management service to this target.", "mimetype": "text/plain", "start_char_idx": 63778, "end_char_idx": 68439, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86c097b9-3b5c-434a-87d9-d0b81a3db21e": {"__data__": {"id_": "86c097b9-3b5c-434a-87d9-d0b81a3db21e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bcb5c2f-8833-4c64-868e-9f74ab54d0c4", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "026c43a26a0dd96b1813a40f3001952db2a3c41c2a067ad5462a19c882d93abb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77b25d47-0888-437d-ab87-3b6995d19628", "node_type": "1", "metadata": {}, "hash": "f8c456e4bce66d5baab4198b3be2e27d68db1d4b70b85321c382c5ba75e24ab2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The defaults for ldiskfs are:                                                                   MGS/MDT: `errors=remount-ro,iopen_nopriv,user_xattr`                  OST: `errors=remount-ro,extents,mballoc`                                   Introduced in Lustre 2.5                                                                                    OST: `errors=remount-ro`                                                                             Do not alter the default mount options unless you know what you are doing. |\n| `--network=*net,...*`        | Network(s) to which to restrict this OST/MDT. This option can be repeated as necessary. |\n| `--mgs`                      | Adds a configuration management service to this target.      |\n| `--msgnode=*nid,...*`        | Sets the NID(s) of the MGS node; required for all targets other than the MGS. |\n| `--nomgs`                    | Removes a configuration management service to this target.   |\n| `--quiet`                    | Prints less information.                                     |\n| `--verbose`                  | Prints more information.                                     |\n| `--writeconf`                | Erases all configuration logs for the file system to which this MDT belongs, and regenerates them. This is dangerous operation. All clients must be unmounted and servers for this file system should be stopped. All targets (OSTs/MDTs) must then be restarted to regenerate the logs. No clients should be started until all targets have restarted.    The correct order of operations is:                                                        1.Unmount all clients on the file system                                           2.Unmount the MDT and all OSTs on the file system                              3.Run `tunefs.lustre --writeconf *device*` on every server        4.Mount the MDT and OSTs                                                                    5.Mount the clients |\n\n### Examples\n\nChange the MGS's NID address. (This should be done on each target disk, since they should all contact the same MGS.)\n\n```\ntunefs.lustre --erase-param --mgsnode=new_nid --writeconf /dev/sda\n```\n\nAdd a failover NID location for this target.\n\n```\ntunefs.lustre --param=\"failover.node=192.168.0.13@tcp0\" /dev/sda \n```\n\n### See Also\n\n- [the section called \u201c mkfs.lustre\u201d](mkfslustre)\n- [the section called \u201c tunefs.lustre\u201d](tunefslustre)\n- [the section called \u201c lctl\u201d](lctl)\n- [the section called \u201c `lfs` \u201d](06.03-User%20Utilities.md#lfs)\n\n## Additional System Configuration Utilities\n\nThis section describes additional system configuration utilities for Lustre.\n\n### Application Profiling Utilities\n\nThe following utilities are located in /usr/bin.\n\n`lustre_req_history.sh`\n\nThe lustre_req_history.sh utility (run from a client), assembles as much Lustre RPC request history as possible from the local node and from the servers that were contacted, providing a better picture of the coordinated network activity.\n\n### More /proc Statistics for Application Profiling\n\nThe following utilities provide additional statistics.\n\n`vfs_ops_stats`\n\nThe client vfs_ops_stats utility tracks Linux VFS operation calls into Lustre for a single PID, PPID, GID or everything.\n\n```\n/proc/fs/lustre/llite/*/vfs_ops_stats\n/proc/fs/lustre/llite/*/vfs_track_[pid|ppid|gid]\n```\n\n`extents_stats`\n\nThe client extents_stats utility shows the size distribution of I/O calls from the client (cumulative and by process).\n\n```\n/proc/fs/lustre/llite/*/extents_stats, extents_stats_per_process\n```\n\n`offset_stats`\n\nThe client offset_stats utility shows the read/write seek activity of a client by offsets and ranges.\n\n```\n/proc/fs/lustre/llite/*/offset_stats\n```\n\nLustre includes per-client and improved MDT statistics:\n\n- Per-client statistics tracked on the servers\n\nEach MDS and OSS now tracks LDLM and operations statistics for every connected client, for comparisons and simpler collection of distributed job statistics.\n\n```\n/proc/fs/lustre/mds|obdfilter/*/exports/\n```\n\n- Improved MDT statistics\n\nMore detailed MDT operations statistics are collected for better profiling.\n\n```\n/proc/fs/lustre/mdt/*/md_stats\n```\n\n### Testing / Debugging Utilities\n\nLustre offers the following test and debugging utilities.\n\n#### lr_reader\n\nThe lr_reader utility translates the content of the `last_rcvd` and `reply_data` files into human-readable form.\n\nThe following utilities are part of the Lustre I/O kit.", "mimetype": "text/plain", "start_char_idx": 67489, "end_char_idx": 71930, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77b25d47-0888-437d-ab87-3b6995d19628": {"__data__": {"id_": "77b25d47-0888-437d-ab87-3b6995d19628", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86c097b9-3b5c-434a-87d9-d0b81a3db21e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7d51d9295ecba1897b2914638dd35f6bfd7cda7ffafd7ddf3820f8fc03c12cbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df319bff-9b78-482b-9e10-9d9340ac65c8", "node_type": "1", "metadata": {}, "hash": "62cdc276e05ef0b2d31ebbe0db00f41f4990044d285a18370bcd2e9858d5bf8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "```\n/proc/fs/lustre/llite/*/offset_stats\n```\n\nLustre includes per-client and improved MDT statistics:\n\n- Per-client statistics tracked on the servers\n\nEach MDS and OSS now tracks LDLM and operations statistics for every connected client, for comparisons and simpler collection of distributed job statistics.\n\n```\n/proc/fs/lustre/mds|obdfilter/*/exports/\n```\n\n- Improved MDT statistics\n\nMore detailed MDT operations statistics are collected for better profiling.\n\n```\n/proc/fs/lustre/mdt/*/md_stats\n```\n\n### Testing / Debugging Utilities\n\nLustre offers the following test and debugging utilities.\n\n#### lr_reader\n\nThe lr_reader utility translates the content of the `last_rcvd` and `reply_data` files into human-readable form.\n\nThe following utilities are part of the Lustre I/O kit. For more information, see [*Benchmarking Lustre File System Performance (Lustre I/O Kit)*](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md).\n\n#### sgpdd-survey\n\nThe `sgpdd-survey` utility tests 'bare metal' performance, bypassing as much of the kernel as possible. The `sgpdd-survey` tool does not require Lustre, but it does require the sgp_dd package.\n\n**Caution**\n\nThe `sgpdd-survey` utility erases all data on the device. \n\n#### obdfilter-survey\n\nThe `obdfilter-survey` utility is a shell script that tests performance of isolated OSTS, the network via echo clients, and an end-to-end test.\n\n#### ior-survey\n\nThe ior-survey utility is a script used to run the IOR benchmark. Lustre includes IOR version 2.8.6.\n\n#### ost-survey\n\nThe `ost-survey` utility is an OST performance survey that tests client-to-disk performance of the individual OSTs in a Lustre file system.\n\n#### stats-collect\n\nThe stats-collect utility contains scripts used to collect application profiling information from Lustre clients and servers.\n\nIntroduced in Lustre 2.9\n\n### Fileset Feature\n\nWith the fileset feature, Lustre now provides subdirectory mount support. Subdirectory mounts, also referred to as filesets, allow a client to mount a child directory of a parent filesystem, thereby limiting the filesystem namespace visibility on a specific client. A common use case is for a client to use a subdirectory mount when there is a desire to limit the visibility of the entire filesystem namesapce to aid in the prevention of accidental file deletions outside of the subdirectory mount.\n\nIt is important to note that invocation of the subdirectory mount is voluntary by the client and not does prevent access to files that are visible in multiple subdirectory mounts via hard links. Furthermore, it does not prevent the client from subsequently mounting the whole file system without a subdirectory being specified.\n\n**Figure 21.  Lustre fileset**\n\n ![Lustre file system fileset feature](./figures/fileset.png) \n\n#### Examples\n\nThe following example will mount the `chipfs` filesystem on client1 and create a subdirectory `v1_1` within that filesystem. Client2 will then mount only the `v1_1` subdirectory as a fileset, thereby limiting access to anything else in the `chipfs` filesystem from client2.\n\n```\nclient1# mount -t lustre mgs@tcp:/chipfs /mnt/chip\nclient1# mkdir /mnt/chip/v1_1\n```\n\n```\nclient2# mount -t lustre mgs@tcp:/chipfs/v1_1 /mnt/chipv1_1\n```\n\nYou can check the created mounts in /etc/mtab.", "mimetype": "text/plain", "start_char_idx": 71148, "end_char_idx": 74453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df319bff-9b78-482b-9e10-9d9340ac65c8": {"__data__": {"id_": "df319bff-9b78-482b-9e10-9d9340ac65c8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea80abee-1c6e-4eea-8975-8c7fd00b547d", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "1ab8cca93f8d50da28078b7c0711399809601791d73c94cc59aae602a610d664", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77b25d47-0888-437d-ab87-3b6995d19628", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e29d4e63a373176e3612b9ef864c06defaca4eaac807de8b199a311a5679f1af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 21.  Lustre fileset**\n\n ![Lustre file system fileset feature](./figures/fileset.png) \n\n#### Examples\n\nThe following example will mount the `chipfs` filesystem on client1 and create a subdirectory `v1_1` within that filesystem. Client2 will then mount only the `v1_1` subdirectory as a fileset, thereby limiting access to anything else in the `chipfs` filesystem from client2.\n\n```\nclient1# mount -t lustre mgs@tcp:/chipfs /mnt/chip\nclient1# mkdir /mnt/chip/v1_1\n```\n\n```\nclient2# mount -t lustre mgs@tcp:/chipfs/v1_1 /mnt/chipv1_1\n```\n\nYou can check the created mounts in /etc/mtab. It should look like the following:\n\n```\nclient1\nmds@tcp0:/chipfs/ /mnt/chip lustre rw         0       0\n```\n\n```\nclient2\nmds@tcp0:/chipfs/v1_1 /mnt/chipv1_1 lustre rw         0       0\n```\n\nCreate a directory under the /mnt/chip mount, and get its FID\n\n```\nclient1# mkdir /mnt/chip/v1_2\nclient1# lfs path2fid /mnt/chip/v1_2\n[0x200000400:0x2:0x0]\n```\n\nIf you try resolve the FID of the `/mnt/chip/v1_2` path (as created in the example above) on client2, an error will be returned as the FID can not be resolved on client2 since it is not part of the mounted fileset on that client. Recall that the fileset on client2 mounted the `v1_1` subdirectory beneath the top level `chipfs` filesystem.\n\n```\nclient2# lfs fid2path /mnt/chip/v1_2 [0x200000400:0x2:0x0]\nfid2path: error on FID [0x200000400:0x2:0x0]: No such file or directory\n```\n\nSubdirectory mounts do not have the `.lustre` pseudo directory, which prevents clients from opening or accessing files only by FID.\n\n```\nclient1# ls /mnt/chipfs/.lustre\n        fid  lost+found\nclient2# ls /mnt/chipv1_1/.lustre\n        ls: cannot access /mnt/chipv1_1/.lustre: No such file or directory\n```", "mimetype": "text/plain", "start_char_idx": 73862, "end_char_idx": 75591, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3168d81b-22e0-475b-b4b5-bafb597a4158": {"__data__": {"id_": "3168d81b-22e0-475b-b4b5-bafb597a4158", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368eac03-b394-4c04-8d4b-1af2f2abc607", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da7b4967aeaad37cee9cd9bf590f1a0461abf26411c4753e963083c1f6d4180e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceccb3ae-d26a-446d-bece-8c2231aa98a9", "node_type": "1", "metadata": {}, "hash": "ddf5ea8df89f2cb78366f563206ccd37fbf36d90c1e7ab2c76ff45f317f1e653", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# LNet Configuration C-API\n\n- [LNet Configuration C-API](#lnet-configuration-c-api)\n  * [General API Information](#general-api-information)\n    + [API Return Code](#api-return-code)\n    + [API Common Input Parameters](#api-common-input-parameters)\n    + [API Common Output Parameters](#api-common-output-parameters)\n      - [Internal YAML Representation (cYAML)](#internal-yaml-representation-cyaml)\n      - [Error Block](#error-block)\n      - [Show Block](#show-block)\n  * [The LNet Configuration C-API](#the-lnet-configuration-c-api)\n    + [Configuring LNet](#configuring-lnet)\n    + [Enabling and Disabling Routing](#enabling-and-disabling-routing)\n    + [Adding Routes](#adding-routes)\n    + [Deleting Routes](#deleting-routes)\n    + [Showing Routes](#showing-routes)\n    + [Adding a Network Interface](#adding-a-network-interface)\n    + [Deleting a Network Interface](#deleting-a-network-interface)\n    + [Showing Network Interfaces](#showing-network-interfaces)\n    + [Adjusting Router Buffer Pools](#adjusting-router-buffer-pools)\n    + [Showing Routing information](#showing-routing-information)\n    + [Showing LNet Traffic Statistics](#showing-lnet-traffic-statistics)\n    + [Adding/Deleting/Showing Parameters through a YAML Block](#addingdeletingshowing-parameters-through-a-yaml-block)\n    + [Adding a route code example](#adding-a-route-code-example)\n\n\nThis section describes the LNet Configuration C-API library. This API allows the developer to programatically configure LNet. It provides APIs to add, delete and show LNet configuration items listed below. The API utilizes IOCTL to communicate with the kernel. Changes take effect immediately and do not require restarting LNet. API calls are synchronous\n\n- [Configuring LNet](#configuring-lnet)\n- Enabling/Disabling routing](#enabling-and-disabling-routing)\n- Adding/removing/showing Routes\n- Adding/removing/showing Networks\n- Configuring Router Buffer Pools\n\n## General API Information\n\n### API Return Code\n\n```\nLUSTRE_CFG_RC_NO_ERR                 0\nLUSTRE_CFG_RC_BAD_PARAM             -1\nLUSTRE_CFG_RC_MISSING_PARAM         -2\nLUSTRE_CFG_RC_OUT_OF_RANGE_PARAM    -3\nLUSTRE_CFG_RC_OUT_OF_MEM            -4\nLUSTRE_CFG_RC_GENERIC_ERR           -5\n```\n\n------\n\n### API Common Input Parameters\n\nAll APIs take as input a sequence number. This is a number that's assigned by the caller of the API, and is returned in the YAML error return block. It is used to associate the request with the response. It is especially useful when configuring via the YAML interface, since typically the YAML interface is used to configure multiple items. In the return Error block, it is desired to know which items were configured properly and which were not configured properly. The sequence number achieves this purpose.\n\n### API Common Output Parameters\n\n#### Internal YAML Representation (cYAML)\n\nOnce a YAML block is parsed it needs to be stored structurally in order to facilitate passing it to different functions, querying it and printing it. Also it is required to be able to build this internal representation from data returned from the kernel and return it to the caller, which can query and print it. This structure representation is used for the Error and Show API Out parameters. For this YAML is internally represented via this structure:\n\n```\ntypedef enum {\n    EN_YAML_TYPE_FALSE = 0,\n    EN_YAML_TYPE_TRUE,\n    EN_YAML_TYPE_NULL,\n    EN_YAML_TYPE_NUMBER,\n    EN_YAML_TYPE_STRING,\n    EN_YAML_TYPE_ARRAY,\n    EN_YAML_TYPE_OBJECT\n} cYAML_object_type_t;\n\ntypedef struct cYAML {\n    /* next/prev allow you to walk array/object chains. */\n    struct cYAML *cy_next, *cy_prev;\n    /* An array or object item will have a child pointer pointing\n       to a chain of the items in the array/object. */\n    struct cYAML *cy_child;\n    /* The type of the item, as above.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3825, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ceccb3ae-d26a-446d-bece-8c2231aa98a9": {"__data__": {"id_": "ceccb3ae-d26a-446d-bece-8c2231aa98a9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368eac03-b394-4c04-8d4b-1af2f2abc607", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da7b4967aeaad37cee9cd9bf590f1a0461abf26411c4753e963083c1f6d4180e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3168d81b-22e0-475b-b4b5-bafb597a4158", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "8212e71304486b4e3aa04a0adbc4a3edffc00a421116b8a03a699a9bbe40d123", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b71f8bd-15aa-4263-be78-c0e85fda6d71", "node_type": "1", "metadata": {}, "hash": "c434c6590fd0b17b5e58d29a05e22b13b61e28c2d38c33b3eb0762773ad01259", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This structure representation is used for the Error and Show API Out parameters. For this YAML is internally represented via this structure:\n\n```\ntypedef enum {\n    EN_YAML_TYPE_FALSE = 0,\n    EN_YAML_TYPE_TRUE,\n    EN_YAML_TYPE_NULL,\n    EN_YAML_TYPE_NUMBER,\n    EN_YAML_TYPE_STRING,\n    EN_YAML_TYPE_ARRAY,\n    EN_YAML_TYPE_OBJECT\n} cYAML_object_type_t;\n\ntypedef struct cYAML {\n    /* next/prev allow you to walk array/object chains. */\n    struct cYAML *cy_next, *cy_prev;\n    /* An array or object item will have a child pointer pointing\n       to a chain of the items in the array/object. */\n    struct cYAML *cy_child;\n    /* The type of the item, as above. */\n    cYAML_object_type_t cy_type;\n    /* The item's string, if type==EN_YAML_TYPE_STRING */\n    char *cy_valuestring;\n    /* The item's number, if type==EN_YAML_TYPE_NUMBER */\n    int cy_valueint;\n    /* The item's number, if type==EN_YAML_TYPE_NUMBER */\n    double cy_valuedouble;\n    /* The item's name string, if this item is the child of,\n       or is in the list of subitems of an object. */\n    char *cy_string;\n    /* user data which might need to be tracked per object */\n    void *cy_user_data;\n} cYAML;\n```\n\n#### Error Block\n\nAll APIs return a cYAML error block. This error block has the following format, when it's printed out. All configuration errors shall be represented in a YAML sequence\n\n```\n<cmd>:\n  - <entity>:\n    errno: <error number>\n    seqno: <sequence number>\n    descr: <error description>\n\nExample:\nadd:\n  - route\n      errno: -2\n      seqno: 1\n      descr: Missing mandatory parameter(s) - network\n```\n\n#### Show Block\n\nAll Show APIs return a cYAML show block. This show block represents the information requested in YAML format. Each configuration item has its own YAML syntax. The YAML syntax of all supported configuration items is described later in this document. Below is an example of a show block:\n\n```\nnet:\n    - nid: 192.168.206.130@tcp4\n      status: up\n      interfaces:\n          0: eth0\n      tunables:\n          peer_timeout: 10\n          peer_credits: 8\n          peer_buffer_credits: 30\n          credits: 40\n```\n\n## The LNet Configuration C-API\n\n### Configuring LNet\n\n```\n/*\n * lustre_lnet_config_ni_system\n *   Initialize/Uninitialize the LNet NI system.\n *\n *   up - whether to init or uninit the system\n *   load_ni_from_mod - load NI from mod params.\n *   seq_no - sequence number of the request\n *   err_rc - [OUT] struct cYAML tree describing the error. Freed by\n *            caller\n */\nint lustre_lnet_config_ni_system(bool up, bool load_ni_from_mod,\n                                 int seq_no, struct cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_CONFIGURE or IOC_LIBCFS_UNCONFIGURE\n\n**Description:**\n\n**Configuring LNet**\n\nInitialize LNet internals and load any networks specified in the module parameter if `load_ni_from_mod` is set. Otherwise do not load any network interfaces.\n\n**Unconfiguring LNet**\n\nBring down LNet and clean up network itnerfaces, routes and all LNet internals.\n\n**Return Value**\n\n0: if success\n\n-errno: if failure\n\n### Enabling and Disabling Routing\n\n```\n/*\n * lustre_lnet_enable_routing\n *   Send down an IOCTL to enable or disable routing\n *\n *   enable - 1 to enable routing, 0 to disable routing\n *   seq_no - sequence number of the request\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_enable_routing(int enable,\n                                      int seq_no,\n                                      cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_ENABLE_RTR\n\n**Description:**\n\n**Enabling Routing**\n\nThe router buffer pools are allocated using the default values. Internally the node is then flagged as a Router node. The node can be used as a router from this point on.\n\n**Disabling Routing**\n\nThe unused router buffer pools are freed.", "mimetype": "text/plain", "start_char_idx": 3162, "end_char_idx": 7018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b71f8bd-15aa-4263-be78-c0e85fda6d71": {"__data__": {"id_": "0b71f8bd-15aa-4263-be78-c0e85fda6d71", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368eac03-b394-4c04-8d4b-1af2f2abc607", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da7b4967aeaad37cee9cd9bf590f1a0461abf26411c4753e963083c1f6d4180e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceccb3ae-d26a-446d-bece-8c2231aa98a9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d5ec8e759f9b70fed2add67c424f1f60a0e1272b442d7ae82b1774aa274f19c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "835aa024-1a5d-422c-b548-fb5b6aaa1f5c", "node_type": "1", "metadata": {}, "hash": "b4653452ef98495c31b9a0789b39261b16fc6cde78c14fb2f3be002071339f33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Return Value**\n\n0: if success\n\n-errno: if failure\n\n### Enabling and Disabling Routing\n\n```\n/*\n * lustre_lnet_enable_routing\n *   Send down an IOCTL to enable or disable routing\n *\n *   enable - 1 to enable routing, 0 to disable routing\n *   seq_no - sequence number of the request\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_enable_routing(int enable,\n                                      int seq_no,\n                                      cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_ENABLE_RTR\n\n**Description:**\n\n**Enabling Routing**\n\nThe router buffer pools are allocated using the default values. Internally the node is then flagged as a Router node. The node can be used as a router from this point on.\n\n**Disabling Routing**\n\nThe unused router buffer pools are freed. Buffers currently in use are not freed until they are returned to the unused list. Internally the node routing flag is turned off. Any subsequent messages not destined to this node are dropped.\n\n**Enabling Routing on an already enabled node, or vice versa**\n\nIn both these cases the LNet Kernel module ignores this request.\n\n**Return Value**\n\n-ENOMEM: if there is no memory to allocate buffer pools\n\n0: if success\n\n### Adding Routes\n\n```\n/*\n * lustre_lnet_config_route\n *   Send down an IOCTL to the kernel to configure the route\n *\n *   nw - network\n *   gw - gateway\n *   hops - number of hops passed down by the user\n *   prio - priority of the route\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_config_route(char *nw, char *gw,\n                    int hops, int prio,\n                    int seq_no,\n                    cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_ADD_ROUTE\n\n**Description:**\n\nThe LNet Kernel module adds this route to the list of existing routes, if one doesn't already exist. If hop parameter is not specified (IE: -1) then the hop count is set to 1. If the priority parameter is not specified (IE: -1) then the priority is set to 0. All routes with the same hop and priority are used in round robin. Routes with lower number of hops and/or higher priority are preferred. 0 is the highest priority.\n\nIf a route already exists the request to add the same route is ignored.\n\n**Return Value**\n\n-EINVAL: if the network of the route is local\n\n-ENOMEM: if there is no memory\n\n-EHOSTUNREACH: if the host is not on a local network\n\n0: if success\n\n### Deleting Routes\n\n```\n/*\n * lustre_lnet_del_route\n *   Send down an IOCTL to the kernel to delete a route\n *\n *   nw - network\n *   gw - gateway\n */\nextern int lustre_lnet_del_route(char *nw, char *gw,\n                 int seq_no,\n                 cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_DEL_ROUTE\n\n**Description:**\n\nLNet will remove the route which matches the network and gateway passed in. If no route matches, then the operation fails with an appropriate error number.\n\n**Return Value**\n\n-ENOENT: if the entry being deleted doesn't exist\n\n0: if success\n\n### Showing Routes\n\n```\n/*\n * lustre_lnet_show_route\n *   Send down an IOCTL to the kernel to show routes\n *   This function will get one route at a time and filter according to\n *   provided parameters. If no filter is provided then it will dump all\n *   routes that are in the system.\n *\n *   nw - network.  Optional.  Used to filter output\n *   gw - gateway. Optional. Used to filter ouptut\n *   hops - number of hops passed down by the user\n *          Optional.  Used to filter output.\n *   prio - priority of the route.  Optional.  Used to filter output.\n *   detail - flag to indicate whether detail output is required\n *   show_rc - [OUT] The show output in YAML.  Must be freed by caller.\n *   err_rc - [OUT] cYAML tree describing the error.", "mimetype": "text/plain", "start_char_idx": 6181, "end_char_idx": 9968, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "835aa024-1a5d-422c-b548-fb5b6aaa1f5c": {"__data__": {"id_": "835aa024-1a5d-422c-b548-fb5b6aaa1f5c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368eac03-b394-4c04-8d4b-1af2f2abc607", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da7b4967aeaad37cee9cd9bf590f1a0461abf26411c4753e963083c1f6d4180e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b71f8bd-15aa-4263-be78-c0e85fda6d71", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "a93d50544e462dd64ab0b99a4bf13b032a4826abb5ab5bba372c894549592ae3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68cb63d7-e086-4811-99fc-151ed16c7c03", "node_type": "1", "metadata": {}, "hash": "50efe312cf324e7a1cb0249f80651e946643449777cf736cefb15b7959eb1214", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If no filter is provided then it will dump all\n *   routes that are in the system.\n *\n *   nw - network.  Optional.  Used to filter output\n *   gw - gateway. Optional. Used to filter ouptut\n *   hops - number of hops passed down by the user\n *          Optional.  Used to filter output.\n *   prio - priority of the route.  Optional.  Used to filter output.\n *   detail - flag to indicate whether detail output is required\n *   show_rc - [OUT] The show output in YAML.  Must be freed by caller.\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_show_route(char *nw, char *gw,\n                  int hops, int prio, int detail,\n                  int seq_no,\n                  cYAML **show_rc,\n                  cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_GET_ROUTE\n\n**Description:**\n\nThe routes are fetched from the kernel one by one and packed in a cYAML block, after filtering according to the parameters passed in. The cYAML block is then returned to the caller of the API.\n\nAn example with the detail parameter set to 1\n\n```\nroute:\n    net: tcp5\n    gateway: 192.168.205.130@tcp\n    hop: 1.000000\n    priority: 0.000000\n    state: up\n```\n\nAn Example with the detail parameter set to 0\n\n```\nroute:\n    net: tcp5\n    gateway: 192.168.205.130@tcp\n```\n\n**Return Value**\n\n-ENOMEM: If no memory\n\n0: if success\n\n### Adding a Network Interface\n\n```\n/*\n * lustre_lnet_config_net\n *   Send down an IOCTL to configure a network.\n *\n *   net - the network name\n *   intf - the interface of the network of the form net_name(intf)\n *   peer_to - peer timeout\n *   peer_cr - peer credit\n *   peer_buf_cr - peer buffer credits\n *       - the above are LND tunable parameters and are optional\n *   credits - network interface credits\n *   smp - cpu affinity\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_config_net(char *net,\n                  char *intf,\n                  int peer_to,\n                  int peer_cr,\n                  int peer_buf_cr,\n                  int credits,\n                  char *smp,\n                  int seq_no,\n                  cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_ADD_NET\n\n**Description:**\n\nA new network is added and initialized. This has the same effect as configuring a network from the module parameters. The API allows the specification of network parameters such as the peer timeout, peer credits, peer buffer credits and credits. The CPU affinity of the network interface being added can also be specified. These parameters become network specific under Dynamic LNet Configuration (DLC), as opposed to being per LND as it was previously.\n\nIf an already existing network is added the request is ignored.\n\n**Return Value**\n\n-EINVAL: if the network passed in is not recognized.\n\n-ENOMEM: if no memory\n\n0: success\n\n### Deleting a Network Interface\n\n```\n/*\n * lustre_lnet_del_net\n *   Send down an IOCTL to delete a network.\n *\n *   nw - network to delete.\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_del_net(char *nw,\n                   int seq_no,\n                   cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_DEL_NET\n\n**Description:**\n\nThe network interface specified is deleted. All resources associated with this network interface are freed. All routes going over that Network Interface are cleaned up.\n\nIf a non existent network is deleted then the call return -EINVAL.\n\n**Return Value**\n\n-EINVAL: if the request references a non-existent network.\n\n0: success\n\n### Showing Network Interfaces\n\n```\n/*\n * lustre_lnet_show_net\n *   Send down an IOCTL to show networks.\n *   This function will use the nw paramter to filter the output.  If it's\n *   not provided then all networks are listed.\n *\n *   nw - network to show.  Optional.  Used to filter output.\n *   detail - flag to indicate if we require detail output.\n *   show_rc - [OUT] The show output in YAML.", "mimetype": "text/plain", "start_char_idx": 9422, "end_char_idx": 13396, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68cb63d7-e086-4811-99fc-151ed16c7c03": {"__data__": {"id_": "68cb63d7-e086-4811-99fc-151ed16c7c03", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368eac03-b394-4c04-8d4b-1af2f2abc607", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da7b4967aeaad37cee9cd9bf590f1a0461abf26411c4753e963083c1f6d4180e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "835aa024-1a5d-422c-b548-fb5b6aaa1f5c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f54acdf065974a9a9d612beeb770944d1cb7c07a805f6adb98f3b3972a2686dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d561428-2c47-4640-a9a0-032ad23b0f28", "node_type": "1", "metadata": {}, "hash": "0472b7f10fb049b837d81813af06e2feba2ff76d5835334a0b46fcc4ae41b9e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All resources associated with this network interface are freed. All routes going over that Network Interface are cleaned up.\n\nIf a non existent network is deleted then the call return -EINVAL.\n\n**Return Value**\n\n-EINVAL: if the request references a non-existent network.\n\n0: success\n\n### Showing Network Interfaces\n\n```\n/*\n * lustre_lnet_show_net\n *   Send down an IOCTL to show networks.\n *   This function will use the nw paramter to filter the output.  If it's\n *   not provided then all networks are listed.\n *\n *   nw - network to show.  Optional.  Used to filter output.\n *   detail - flag to indicate if we require detail output.\n *   show_rc - [OUT] The show output in YAML.  Must be freed by caller.\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_show_net(char *nw, int detail,\n                int seq_no,\n                cYAML **show_rc,\n                cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_GET_NET\n\n**Description:**\n\nThe network interfaces are queried one at a time from the kernel and packed in a cYAML block, after filtering on the network (EX: tcp). If the detail field is set to 1, then the tunable section of the show block is included in the return.\n\nAn example of the detailed output\n\n```\nnet:\n    nid: 192.168.206.130@tcp4\n    status: up\n    interfaces:\n        intf-0: eth0\n    tunables:\n        peer_timeout: 10\n        peer_credits: 8\n        peer_buffer_credits: 30\n        credits: 40\n```\n\nAn example of none detailed output\n\n```\nnet:\n    nid: 192.168.206.130@tcp4\n    status: up\n    interfaces:\n        intf-0: eth0\n```\n\n**Return Value**\n\n-ENOMEM: if no memory to allocate the error or show blocks.\n\n0: success\n\n### Adjusting Router Buffer Pools\n\n```\n/*\n * lustre_lnet_config_buf\n *   Send down an IOCTL to configure buffer sizes.  A value of 0 means\n *   default that particular buffer to default size. A value of -1 means\n *   leave the value of the buffer unchanged.\n *\n *   tiny - tiny buffers\n *   small - small buffers\n *   large - large buffers.\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_config_buf(int tiny,\n                  int small,\n                  int large,\n                  int seq_no,\n                  cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_ADD_BUF\n\n**Description:**\n\nThis API is used to configure the tiny, small and large router buffers dynamically. These buffers are used to buffer messages which are being routed to other nodes. The minimum value of these buffers per CPT are:\n\n```\n#define LNET_NRB_TINY_MIN     512\n#define LNET_NRB_SMALL_MIN    4096\n#define LNET_NRB_LARGE_MIN    256\n```\n\nThe default values of these buffers are:\n\n```\n#define LNET_NRB_TINY         (LNET_NRB_TINY_MIN * 4)\n#define LNET_NRB_SMALL        (LNET_NRB_SMALL_MIN * 4)\n#define LNET_NRB_LARGE        (LNET_NRB_LARGE_MIN * 4)\n```\n\nThese default value is divided evenly across all CPTs. However, each CPT can only go as low as the minimum.\n\nMultiple calls to this API with the same values has no effect\n\n**Return Value**\n\n-ENOMEM: if no memory to allocate buffer pools.\n\n0: success\n\n### Showing Routing information\n\n```\n/*\n * lustre_lnet_show_routing\n *   Send down an IOCTL to dump buffers and routing status\n *   This function is used to dump buffers for all CPU partitions.\n *\n *   show_rc - [OUT] The show output in YAML.  Must be freed by caller.\n *   err_rc - [OUT] struct cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_show_routing(int seq_no, struct cYAML **show_rc,\n                                    struct cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_GET_BUF\n\n**Description:**\n\nThis API returns a cYAML block describing the values of each of the following per CPT:\n\n1. The number of pages per buffer.", "mimetype": "text/plain", "start_char_idx": 12714, "end_char_idx": 16524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d561428-2c47-4640-a9a0-032ad23b0f28": {"__data__": {"id_": "1d561428-2c47-4640-a9a0-032ad23b0f28", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368eac03-b394-4c04-8d4b-1af2f2abc607", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da7b4967aeaad37cee9cd9bf590f1a0461abf26411c4753e963083c1f6d4180e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68cb63d7-e086-4811-99fc-151ed16c7c03", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "5bc4718b6b15eee36308a8192b79a3cdd3efca930b38da8d3d1e0ab748e33d09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49111234-4189-4cd2-a5b9-cd7694303271", "node_type": "1", "metadata": {}, "hash": "0adb15782ee6b5fea5071575b00930148fd8a9bec3800a39ba91e214a04b7bbc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0: success\n\n### Showing Routing information\n\n```\n/*\n * lustre_lnet_show_routing\n *   Send down an IOCTL to dump buffers and routing status\n *   This function is used to dump buffers for all CPU partitions.\n *\n *   show_rc - [OUT] The show output in YAML.  Must be freed by caller.\n *   err_rc - [OUT] struct cYAML tree describing the error. Freed by caller\n */\nextern int lustre_lnet_show_routing(int seq_no, struct cYAML **show_rc,\n                                    struct cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_GET_BUF\n\n**Description:**\n\nThis API returns a cYAML block describing the values of each of the following per CPT:\n\n1. The number of pages per buffer. This is a constant.\n2. The number of allocated buffers. This is a constant.\n3. The number of buffer credits . This is a real-time value of the number of buffer credits currently available. If this value is negative, that indicates the number of queued messages.\n4. The lowest number of credits ever reached in the system. This is historical data.\n\nThe show block also returns the status of routing, whether enabled, or disabled.\n\nAn exmaple YAML block\n\n```\nrouting:\n    - cpt[0]:\n          tiny:\n              npages: 0\n              nbuffers: 2048\n              credits: 2048\n              mincredits: 2048\n          small:\n              npages: 1\n              nbuffers: 16384\n              credits: 16384\n              mincredits: 16384\n          large:\n              npages: 256\n              nbuffers: 1024\n              credits: 1024\n              mincredits: 1024\n    - enable: 1\n```\n\n**Return Value**\n\n-ENOMEM: if no memory to allocate the show or error block.\n\n0: success\n\n### Showing LNet Traffic Statistics\n\n```\n/*\n * lustre_lnet_show_stats\n *   Shows internal LNet statistics.  This is useful to display the\n *   current LNet activity, such as number of messages route, etc\n *\n *     seq_no - sequence number of the command\n *     show_rc - YAML structure of the resultant show\n *     err_rc - YAML strucutre of the resultant return code.\n */\nextern int lustre_lnet_show_stats(int seq_no, cYAML **show_rc,\n                  cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nIOC_LIBCFS_GET_LNET_STATS\n\n**Description:**\n\nThis API returns a cYAML block describing the LNet traffic statistics. Statistics are continuously incremented by LNet while it's alive. This API retuns the statistics at the time of the API call. The statistics include the following\n\n1. Number of messages allocated\n2. Maximum number of messages in the system\n3. Errors allocating or sending messages\n4. Cumulative number of messages sent\n5. Cumulative number of messages received\n6. Cumulative number of messages routed\n7. Cumulative number of messages dropped\n8. Cumulative number of bytes sent\n9. Cumulative number of bytes received\n10. Cumulative number of bytes routed\n11. Cumulative number of bytes dropped\n\nAn exmaple YAML block\n\n```\nstatistics:\n    msgs_alloc: 0\n    msgs_max: 0\n    errors: 0\n    send_count: 0\n    recv_count: 0\n    route_count: 0\n    drop_count: 0\n    send_length: 0\n    recv_length: 0\n    route_length: 0\n    drop_length: 0\n```\n\n**Return Value**\n\n-ENOMEM: if no memory to allocate the show or error block.\n\n0: success\n\n### Adding/Deleting/Showing Parameters through a YAML Block\n\n```\n/*\n * lustre_yaml_config\n *   Parses the provided YAML file and then calls the specific APIs\n *   to configure the entities identified in the file\n *\n *   f - YAML file\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_yaml_config(char *f, cYAML **err_rc);\n\n/*\n * lustre_yaml_del\n *   Parses the provided YAML file and then calls the specific APIs\n *   to delete the entities identified in the file\n *\n *   f - YAML file\n *   err_rc - [OUT] cYAML tree describing the error.", "mimetype": "text/plain", "start_char_idx": 15844, "end_char_idx": 19626, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49111234-4189-4cd2-a5b9-cd7694303271": {"__data__": {"id_": "49111234-4189-4cd2-a5b9-cd7694303271", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368eac03-b394-4c04-8d4b-1af2f2abc607", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da7b4967aeaad37cee9cd9bf590f1a0461abf26411c4753e963083c1f6d4180e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d561428-2c47-4640-a9a0-032ad23b0f28", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "82d4f48f040ba9220945ba228a96872cf9cf8cf5c11a6b1ab2bd3fe224accd6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e7af7dc-09db-4b5e-b7c0-bdc780cc5dc5", "node_type": "1", "metadata": {}, "hash": "eab568af1b523a34b462033e69298333579232c99a2878ec14ded4a7ec5f4582", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0: success\n\n### Adding/Deleting/Showing Parameters through a YAML Block\n\n```\n/*\n * lustre_yaml_config\n *   Parses the provided YAML file and then calls the specific APIs\n *   to configure the entities identified in the file\n *\n *   f - YAML file\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_yaml_config(char *f, cYAML **err_rc);\n\n/*\n * lustre_yaml_del\n *   Parses the provided YAML file and then calls the specific APIs\n *   to delete the entities identified in the file\n *\n *   f - YAML file\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_yaml_del(char *f, cYAML **err_rc);\n\n/*\n * lustre_yaml_show\n *   Parses the provided YAML file and then calls the specific APIs\n *   to show the entities identified in the file\n *\n *   f - YAML file\n *   show_rc - [OUT] The show output in YAML.  Must be freed by caller.\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_yaml_show(char *f,\n                cYAML **show_rc,\n                cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nDepends on the entity being configured\n\n**Description:**\n\nThese APIs add/remove/show the parameters specified in the YAML file respectively. The entities don't have to be uniform. Multiple different entities can be added/removed/showed in one YAML block.", "mimetype": "text/plain", "start_char_idx": 19035, "end_char_idx": 20392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e7af7dc-09db-4b5e-b7c0-bdc780cc5dc5": {"__data__": {"id_": "4e7af7dc-09db-4b5e-b7c0-bdc780cc5dc5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368eac03-b394-4c04-8d4b-1af2f2abc607", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da7b4967aeaad37cee9cd9bf590f1a0461abf26411c4753e963083c1f6d4180e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49111234-4189-4cd2-a5b9-cd7694303271", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "c83a5ea00c6d0371ad5c80c8c9eb7bd4b30a22df0610d0919fc4da3d949da807", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Freed by caller\n */\nextern int lustre_yaml_del(char *f, cYAML **err_rc);\n\n/*\n * lustre_yaml_show\n *   Parses the provided YAML file and then calls the specific APIs\n *   to show the entities identified in the file\n *\n *   f - YAML file\n *   show_rc - [OUT] The show output in YAML.  Must be freed by caller.\n *   err_rc - [OUT] cYAML tree describing the error. Freed by caller\n */\nextern int lustre_yaml_show(char *f,\n                cYAML **show_rc,\n                cYAML **err_rc);\n```\n\n**IOCTL to Kernel:**\n\nDepends on the entity being configured\n\n**Description:**\n\nThese APIs add/remove/show the parameters specified in the YAML file respectively. The entities don't have to be uniform. Multiple different entities can be added/removed/showed in one YAML block.\n\nAn example YAML block\n\n```\n---\nnet:\n    - nid: 192.168.206.132@tcp\n      status: up\n      interfaces:\n          0: eth3\n      tunables:\n          peer_timeout: 180\n          peer_credits: 8\n          peer_buffer_credits: 0\n          credits: 256\n          SMP: \"[0]\"\nroute:\n   - net: tcp6\n     gateway: 192.168.29.1@tcp\n     hop: 4\n     detail: 1\n     seq_no: 3\n   - net: tcp7\n     gateway: 192.168.28.1@tcp\n     hop: 9\n     detail: 1\n     seq_no: 4\nbuffer:\n   - tiny: 1024\n     small: 2000\n     large: 512\n...\n```\n\n**Return Value**\n\nReturn value will correspond to the return value of the API that will be called to operate on the configuration item, as described in previous sections\n\n### Adding a route code example\n\n```\nint main(int argc, char **argv)\n{\n\tchar *network = NULL, *gateway = NULL;\n\tlong int hop = -1, prio = -1;\n\tstruct cYAML *err_rc = NULL;\n\tint rc, opt;\n\toptind = 0;\n\n\tconst char *const short_options = \"n:g:c:p:h\";\n\tconst struct option long_options[] = {\n\t\t{ \"net\", 1, NULL, 'n' },\n\t\t{ \"gateway\", 1, NULL, 'g' },\n\t\t{ \"hop-count\", 1, NULL, 'c' },\n\t\t{ \"priority\", 1, NULL, 'p' },\n\t\t{ \"help\", 0, NULL, 'h' },\n\t\t{ NULL, 0, NULL, 0 },\n\t};\n\n\twhile ((opt = getopt_long(argc, argv, short_options,\n\t\t\t\t   long_options, NULL)) != -1) {\n\t\tswitch (opt) {\n\t\tcase 'n':\n\t\t\tnetwork = optarg;\n\t\t\tbreak;\n\t\tcase 'g':\n\t\t\tgateway = optarg;\n\t\t\tbreak;\n\t\tcase 'c':\n\t\t\trc = parse_long(optarg, &hop);\n\t\t\tif (rc != 0) {\n\t\t\t\t/* ignore option */\n\t\t\t\thop = -1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 'p':\n\t\t\trc = parse_long(optarg, &prio);\n\t\t\tif (rc != 0) {\n\t\t\t\t/* ingore option */\n\t\t\t\tprio = -1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 'h':\n\t\t\tprint_help(route_cmds, \"route\", \"add\");\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\trc = lustre_lnet_config_route(network, gateway, hop, prio, -1, &err_rc);\n\n\tif (rc != LUSTRE_CFG_RC_NO_ERR)\n\t\tcYAML_print_tree2file(stderr, err_rc);\n\n\tcYAML_free_tree(err_rc);\n\n\treturn rc;\n}       \n```\n\nFor other code examples refer to\n\n```\nlnet/utils/lnetctl.c\n```", "mimetype": "text/plain", "start_char_idx": 19627, "end_char_idx": 22372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9767b782-3f7e-4d39-8cbb-ba918fa1018a": {"__data__": {"id_": "9767b782-3f7e-4d39-8cbb-ba918fa1018a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bb0a221411848abb237ff40820539b64f6f37974dc80162ab6bf1af3467f14c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2a9f83b-5765-4eeb-a7aa-3171d86d8b09", "node_type": "1", "metadata": {}, "hash": "977ee2e7d4b59ee67ebcb82fddf3e47dd71f52dd34bb44c1733af9ae4a6a7164", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Glossary\n\n### A\n\n- ACL\n\n  Access control list. An extended attribute associated with a file that contains enhanced authorization directives.\n\n- Administrative OST failure\n\n  A manual configuration change to mark an OST as unavailable, so that operations intended for that OST fail immediately with an I/O error instead of waiting indefinitely for OST recovery to complete\n\n### C\n\n- Completion callback\n\n  An RPC made by the lock server on an OST or MDT to another system, usually a client, to indicate that the lock is now granted.\n\n- configlog\n\n  An llog file used in a node, or retrieved from a management server over the network with configuration instructions for the Lustre file system at startup time.\n\n- Configuration lock\n\n  A lock held by every node in the cluster to control configuration changes. When the configuration is changed on the MGS, it revokes this lock from all nodes. When the nodes receive the blocking callback, they quiesce their traffic, cancel and re-enqueue the lock and wait until it is granted again. They can then fetch the configuration updates and resume normal operation.\n\n### D\n\n- Default stripe pattern\n\n  Information in the LOV descriptor that describes the default stripe count, stripe size, and layout pattern used for new files in a file system. This can be amended by using a directory stripe descriptor or a per-file stripe descriptor.\n\n- Direct I/O\n\n  A mechanism that can be used during read and write system calls to avoid memory cache overhead for large I/O requests. It bypasses the data copy between application and kernel memory, and avoids buffering the data in the client memory.\n\n- Directory stripe descriptor\n\n  An extended attribute that describes the default stripe pattern for new files created within that directory. This is also inherited by new subdirectories at the time they are created.\n  \n- Distributed Namespace Environment (DNE)\n  \n  A collection of metadata targets serving a single file system namespace. Without DNE, Lustre file systems are limited to a single metadata target for the entire name space. Without the ability to distribute metadata load over multiple targets, Lustre file system performance may be limited. The DNE functionality has two types of scalability. Remote Directories (DNE1) allows sub-directories to be serviced by an independent MDT(s), increasing aggregate metadata capacity and performance for independent sub-trees of the filesystem. This also allows performance isolation of workloads running in a specific sub-directory on one MDT from workloads on other MDTs. In Lustre 2.8 and later Striped Directories (DNE2) allows a single directory to be serviced by multiple MDTs.\n  \n\n### E\n\n- EA\n\n  Extended attribute. A small amount of data that can be retrieved through a name (EA or xattr) associated with a particular inode. A Lustre file system uses EAs to store striping information (indicating the location of file data on OSTs). Examples of extended attributes are ACLs, striping information, and the FID of the file.\n\n- Eviction\n\n  The process of removing a client's state from the server if the client is unresponsive to server requests after a timeout or if server recovery fails. If a client is still running, it is required to flush the cache associated with the server when it becomes aware that it has been evicted.\n\n- Export\n\n  The state held by a server for a client that is sufficient to transparently recover all in-flight operations when a single failure occurs.\n\n- Extent\n\n  A range of contiguous bytes or blocks in a file that are addressed by a {start, length} tuple instead of individual block numbers.\n\n- Extent lock\n\n  An LDLM lock used by the OSC to protect an extent in a storage object for concurrent control of read/write, file size acquisition, and truncation operations.\n\n### F\n\n- Failback\n\n  The failover process in which the default active server regains control from the backup server that had taken control of the service.\n\n- Failout OST\n\n  An OST that is not expected to recover if it fails to answer client requests. A failout OST can be administratively failed, thereby enabling clients to return errors when accessing data on the failed OST without making additional network requests or waiting for OST recovery to complete.\n\n- Failover\n\n  The process by which a standby computer server system takes over for an active computer server after a failure of the active node. Typically, the standby computer server gains exclusive access to a shared storage device between the two servers.\n\n- FID\n\n  Lustre File Identifier. A 128-bit file system-unique identifier for a file or object in the file system.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d2a9f83b-5765-4eeb-a7aa-3171d86d8b09": {"__data__": {"id_": "d2a9f83b-5765-4eeb-a7aa-3171d86d8b09", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bb0a221411848abb237ff40820539b64f6f37974dc80162ab6bf1af3467f14c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9767b782-3f7e-4d39-8cbb-ba918fa1018a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bfbaa3eb2affb0798942370f4b19e8ffa8db31148afca1e8e62f5a7ef141c719", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e11c8992-262b-4b3c-91a7-422ff0264593", "node_type": "1", "metadata": {}, "hash": "a9c9e8faff35c920b90563a58a33faf4eae85482c72e6005067fc48845f68ec5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### F\n\n- Failback\n\n  The failover process in which the default active server regains control from the backup server that had taken control of the service.\n\n- Failout OST\n\n  An OST that is not expected to recover if it fails to answer client requests. A failout OST can be administratively failed, thereby enabling clients to return errors when accessing data on the failed OST without making additional network requests or waiting for OST recovery to complete.\n\n- Failover\n\n  The process by which a standby computer server system takes over for an active computer server after a failure of the active node. Typically, the standby computer server gains exclusive access to a shared storage device between the two servers.\n\n- FID\n\n  Lustre File Identifier. A 128-bit file system-unique identifier for a file or object in the file system. The FID structure contains a unique 64-bit sequence number (see FLDB), a 32-bit object ID (OID), and a 32-bit version number. The sequence number is unique across all Lustre targets (OSTs and MDTs).\n\n- Fileset\n\n  A group of files that are defined through a directory that represents the start point of a file system.\n\n- FLDB\n\n  FID location database. This database maps a sequence of FIDs to a specific target (MDT or OST), which manages the objects within the sequence. The FLDB is cached by all clients and servers in the file system, but is typically only modified when new servers are added to the file system.\n\n- Flight group\n\n  Group of I/O RPCs initiated by the OSC that are concurrently queued or processed at the OST. Increasing the number of RPCs in flight for high latency networks can increase throughput and reduce visible latency at the client.\n\n### G\n\n- Glimpse callback\n\n  An RPC made by an OST or MDT to another system (usually a client) to indicate that a held extent lock should be surrendered. If the system is using the lock, then the system should return the object size and timestamps in the reply to the glimpse callback instead of cancelling the lock. Glimpses are introduced to optimize the acquisition of file attributes without introducing contention on an active lock.\n\n### I\n\n- Import\n\n  The state held held by the client for each target that it is connected to. It holds server NIDs, connection state, and uncommitted RPCs needed to fully recover a transaction sequence after a server failure and restart.\n\n- Intent lock\n\n  A special Lustre file system locking operation in the Linux kernel. An intent lock combines a request for a lock with the full information to perform the operation(s) for which the lock was requested. This offers the server the option of granting the lock or performing the operation and informing the client of the operation result without granting a lock. The use of intent locks enables metadata operations (even complicated ones) to be implemented with a single RPC from the client to the server.\n\n### L\n\n- LBUG\n\n  A fatal error condition detected by the software that halts execution of the kernel thread to avoid potential further corruption of the system state. It is printed to the console log and triggers a dump of the internal debug log. The system must be rebooted to clear this state.\n\n- LDLM\n\n  Lustre Distributed Lock Manager.\n\n- lfs\n\n  The Lustre file system command-line utility that allows end users to interact with Lustre software features, such as setting or checking file striping or per-target free space. For more details, see [the section called \u201c `lfs`\u201d](06.03-User%20Utilities.md#lfs).\n\n- LFSCK\n\n  Lustre file system check. A distributed version of a disk file system checker. Normally, `lfsck` does not need to be run, except when file systems are damaged by events such as multiple disk failures and cannot be recovered using file system journal recovery.\n\n- llite\n\n  Lustre lite. This term is in use inside code and in module names for code that is related to the Linux client VFS interface.\n\n- llog\n\n  Lustre log. An efficient log data structure used internally by the file system for storing configuration and distributed transaction records. An `llog` is suitable for rapid transactional appends of records and cheap cancellation of records through a bitmap.\n\n- llog catalog\n\n  Lustre log catalog. An `llog` with records that each point at an `llog`. Catalogs were introduced to give `llogs`increased scalability. `llogs` have an originator which writes records and a replicator which cancels records when the records are no longer needed.\n\n- LMV\n\n  Logical metadata volume. A module that implements a DNE client-side abstraction device.", "mimetype": "text/plain", "start_char_idx": 3798, "end_char_idx": 8359, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e11c8992-262b-4b3c-91a7-422ff0264593": {"__data__": {"id_": "e11c8992-262b-4b3c-91a7-422ff0264593", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bb0a221411848abb237ff40820539b64f6f37974dc80162ab6bf1af3467f14c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2a9f83b-5765-4eeb-a7aa-3171d86d8b09", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7f1ab29ba88c8ba83cc376cfc1e8342bd8db1ba9b1d85be2b0cb1666b17d0695", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38040e85-accc-42a8-8d37-fd7f6d603588", "node_type": "1", "metadata": {}, "hash": "e5d99f6e71da38f70969ee7f6246814414cd96ca1eb89d87cd5ea6a656ae0671", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- llite\n\n  Lustre lite. This term is in use inside code and in module names for code that is related to the Linux client VFS interface.\n\n- llog\n\n  Lustre log. An efficient log data structure used internally by the file system for storing configuration and distributed transaction records. An `llog` is suitable for rapid transactional appends of records and cheap cancellation of records through a bitmap.\n\n- llog catalog\n\n  Lustre log catalog. An `llog` with records that each point at an `llog`. Catalogs were introduced to give `llogs`increased scalability. `llogs` have an originator which writes records and a replicator which cancels records when the records are no longer needed.\n\n- LMV\n\n  Logical metadata volume. A module that implements a DNE client-side abstraction device. It allows a client to work with many MDTs without changes to the llite module. The LMV code forwards requests to the correct MDT based on name or directory striping information and merges replies into a single result to pass back to the higher `llite` layer that connects the Lustre file system with Linux VFS, supports VFS semantics, and complies with POSIX interface specifications.\n\n- LND\n\n  Lustre network driver. A code module that enables LNet support over particular transports, such as TCP and various kinds of InfiniBand networks.\n\n- LNet\n\n  Lustre networking. A message passing network protocol capable of running and routing through various physical layers. LNet forms the underpinning of LNETrpc.\n\n- Lock client\n\n  A module that makes lock RPCs to a lock server and handles revocations from the server.\n\n- Lock server\n\n  A service that is co-located with a storage target that manages locks on certain objects. It also issues lock callback requests, calls while servicing or, for objects that are already locked, completes lock requests.\n\n- LOV\n\n  Logical object volume. The object storage analog of a logical volume in a block device volume management system, such as LVM or EVMS. The LOV is primarily used to present a collection of OSTs as a single device to the MDT and client file system drivers.\n\n- LOV descriptor\n\n  A set of configuration directives which describes which nodes are OSS systems in the Lustre cluster and providing names for their OSTs.\n\n- Lustre client\n\n  An operating instance with a mounted Lustre file system.\n\n- Lustre file\n\n  A file in the Lustre file system. The implementation of a Lustre file is through an inode on a metadata server that contains references to a storage object on OSSs.\n\n### M\n\n- mballoc\n\n  Multi-block allocate. Functionality in ext4 that enables the `ldiskfs` file system to allocate multiple blocks with a single request to the block allocator.\n\n- MDC\n\n  Metadata client. A Lustre client component that sends metadata requests via RPC over LNet to the metadata target (MDT).\n\n- MDD\n\n  Metadata disk device. Lustre server component that interfaces with the underlying object storage device to manage the Lustre file system namespace (directories, file ownership, attributes).\n\n- MDS\n\n  Metadata server. The server node that is hosting the metadata target (MDT).\n\n- MDT\n\n  Metadata target. A storage device containing the file system namespace that is made available over the network to a client. It stores filenames, attributes, and the layout of OST objects that store the file data.\n\n- MDT0000\n\n  The metadata target storing the file system root directory, as well as some core services such as quota tables. Multiple metadata targets are possible in the same file system. MDT0000 must be available for the file system to be accessible.\n\n- MGS\n\n  Management service. A software module that manages the startup configuration and changes to the configuration. Also, the server node on which this system runs.\n\n- mountconf\n\n  The Lustre configuration protocol that formats disk file systems on servers with the `mkfs.lustre` program, and prepares them for automatic incorporation into a Lustre cluster. This allows clients to be configured and mounted with a simple `mount` command.\n\n### N\n\n- NID\n\n  Network identifier. Encodes the type, network number, and network address of a network interface on a node for use by the Lustre file system.\n\n- NIO API\n\n  A subset of the LNet RPC module that implements a library for sending large network requests, moving buffers with RDMA.\n\n- Node affinity\n\n  Node affinity describes the property of a multi-threaded application to behave sensibly on multiple cores.", "mimetype": "text/plain", "start_char_idx": 7575, "end_char_idx": 12024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38040e85-accc-42a8-8d37-fd7f6d603588": {"__data__": {"id_": "38040e85-accc-42a8-8d37-fd7f6d603588", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bb0a221411848abb237ff40820539b64f6f37974dc80162ab6bf1af3467f14c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e11c8992-262b-4b3c-91a7-422ff0264593", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cd8f58c1d49c0e8e35b20068d4d8cd87ef654e81b928f952534eb14050cf7e09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb938ff6-e589-43f7-999a-fb8f1457defb", "node_type": "1", "metadata": {}, "hash": "5956cd3ae21b32d5ae3c1352431fa34c3e75c6c42108e6d592923c6b56b472d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- MGS\n\n  Management service. A software module that manages the startup configuration and changes to the configuration. Also, the server node on which this system runs.\n\n- mountconf\n\n  The Lustre configuration protocol that formats disk file systems on servers with the `mkfs.lustre` program, and prepares them for automatic incorporation into a Lustre cluster. This allows clients to be configured and mounted with a simple `mount` command.\n\n### N\n\n- NID\n\n  Network identifier. Encodes the type, network number, and network address of a network interface on a node for use by the Lustre file system.\n\n- NIO API\n\n  A subset of the LNet RPC module that implements a library for sending large network requests, moving buffers with RDMA.\n\n- Node affinity\n\n  Node affinity describes the property of a multi-threaded application to behave sensibly on multiple cores. Without the property of node affinity, an operating scheduler may move application threads across processors in a sub-optimal way that significantly reduces performance of the application overall.\n\n- NRS\n\n  Network request scheduler. A subcomponent of the PTLRPC layer, which specifies the order in which RPCs are handled at servers. This allows optimizing large numbers of incoming requests for disk access patterns, fairness between clients, and other administrator-selected policies.\n\n- NUMA\n\n  Non-uniform memory access. Describes a multi-processing architecture where the time taken to access given memory differs depending on memory location relative to a given processor. Typically machines with multiple sockets are NUMA architectures.\n\n### O\n\n- OBD\n\n  Object-based device. The generic term for components in the Lustre software stack that can be configured on the client or server. Examples include MDC, OSC, LOV, MDT, and OST.\n\n- OBD type\n\n  Module that can implement the Lustre object or metadata APIs. Examples of OBD types include the LOV, OSC and OSD.\n\n- Object storage\n\n  Refers to a storage-device API or protocol involving storage objects. The two most well known instances of object storage are the T10 iSCSI storage object protocol and the Lustre object storage protocol (a network implementation of the Lustre object API). The principal difference between the Lustre protocol and T10 protocol is that the Lustre protocol includes locking and recovery control in the protocol and is not tied to a SCSI transport layer.\n\n- opencache\n\n  A cache of open file handles. This is a performance enhancement for NFS.\n\n- Orphan objects\n\n  Storage objects to which no Lustre file points. Orphan objects can arise from crashes and are automatically removed by an `llog` recovery between the MDT and OST. When a client deletes a file, the MDT unlinks it from the namespace. If this is the last link, it will atomically add the OST objects into a per-OST `llog`(if a crash has occurred) and then wait until the unlink commits to disk. (At this point, it is safe to destroy the OST objects. Once the destroy is committed, the MDT `llog` records can be cancelled.)\n\n- OSC\n\n  Object storage client. The client module communicating to an OST (via an OSS).\n\n- OSD\n\n  Object storage device. A generic, industry term for storage devices with a more extended interface than block-oriented devices such as disks. For the Lustre file system, this name is used to describe a software module that implements an object storage API in the kernel. It is also used to refer to an instance of an object storage device created by that driver. The OSD device is layered on a file system, with methods that mimic create, destroy and I/O operations on file inodes.\n\n- OSS\n\n  Object storage server. A server OBD that provides access to local OSTs.\n\n- OST\n\n  Object storage target. An OSD made accessible through a network protocol. Typically, an OST is associated with a unique OSD which, in turn is associated with a formatted disk file system on the server containing the data objects.\n\n### P\n\n- pdirops\n\n  A locking protocol in the Linux VFS layer that allows for directory operations performed in parallel.\n\n- Pool\n\n  OST pools allows the administrator to associate a name with an arbitrary subset of OSTs in a Lustre cluster. A group of OSTs can be combined into a named pool with unique access permissions and stripe characteristics.\n\n- Portal\n\n  A service address on an LNet NID that binds requests to a specific request service, such as an MDS, MGS, OSS, or LDLM. Services may listen on multiple portals to ensure that high priority messages are not queued behind many slow requests on another portal.", "mimetype": "text/plain", "start_char_idx": 11163, "end_char_idx": 15717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb938ff6-e589-43f7-999a-fb8f1457defb": {"__data__": {"id_": "eb938ff6-e589-43f7-999a-fb8f1457defb", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bb0a221411848abb237ff40820539b64f6f37974dc80162ab6bf1af3467f14c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38040e85-accc-42a8-8d37-fd7f6d603588", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b751889b17c80d17e946e2c2abdffc42dd8bc751b443a25983c969702ec46023", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b0a89a1-1320-43ba-a4e6-1b840702a77d", "node_type": "1", "metadata": {}, "hash": "1d66044398447d9aae3fbc62cc1f22d0fb772995658ab826f6632e7f2f333952", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A server OBD that provides access to local OSTs.\n\n- OST\n\n  Object storage target. An OSD made accessible through a network protocol. Typically, an OST is associated with a unique OSD which, in turn is associated with a formatted disk file system on the server containing the data objects.\n\n### P\n\n- pdirops\n\n  A locking protocol in the Linux VFS layer that allows for directory operations performed in parallel.\n\n- Pool\n\n  OST pools allows the administrator to associate a name with an arbitrary subset of OSTs in a Lustre cluster. A group of OSTs can be combined into a named pool with unique access permissions and stripe characteristics.\n\n- Portal\n\n  A service address on an LNet NID that binds requests to a specific request service, such as an MDS, MGS, OSS, or LDLM. Services may listen on multiple portals to ensure that high priority messages are not queued behind many slow requests on another portal.\n\n- PTLRPC\n\n  An RPC protocol layered on LNet. This protocol deals with stateful servers and has exactly-once semantics and built in support for recovery.\n\n### R\n\n- Recovery\n\n  The process that re-establishes the connection state when a client that was previously connected to a server reconnects after the server restarts.\n\n- Remote directory\n\n  A remote directory describes a feature of Lustre where metadata for files in a given directory may be stored on a different MDT than the metadata for the parent directory. This is sometimes referred to as DNE1.\n\n- Replay directory\n\n  The concept of re-executing a server request after the server has lost information in its memory caches and shut down. The replay requests are retained by clients until the server(s) have confirmed that the data is persistent on disk. Only requests for which a client received a reply and were assigned a transaction number by the server are replayed. Requests that did not get a reply are resent.\n\n- Resent request\n\n  An RPC request sent from a client to a server that has not had a reply from the server. This might happen if the request was lost on the way to the server, if the reply was lost on the way back from the server, or if the server crashes before or after processing the request. During server RPC recovery processing, resent requests are processed after replayed requests, and use the client RPC XID to determine if the resent RPC request was already executed on the server.\n\n- Revocation callback\n\n  Also called a \"blocking callback\". An RPC request made by the lock server (typically for an OST or MDT) to a lock client to revoke a granted DLM lock.\n\n- Root squash\n\n  A mechanism whereby the identity of a root user on a client system is mapped to a different identity on the server to avoid root users on clients from accessing or modifying root-owned files on the servers. This does not prevent root users on the client from assuming the identity of a non-root user, so should not be considered a robust security mechanism. Typically, for management purposes, at least one client system should not be subject to root squash.\n\n- Routing\n\n  LNet routing between different networks and LNDs.\n\n- RPC\n\n  Remote procedure call. A network encoding of a request.\n\n### S\n\n- Stripe\n\n  A contiguous, logical extent of a Lustre file written to a single OST. Used synonymously with a single OST data object that makes up part of a file visible to user applications.\n  \n- Striped Directory\n\n  A striped directory is when metadata for files in a given directory are distributed evenly over multiple MDTs. Striped directories are only available in Lustre software version 2.8 or later. A user can create a striped directory to increase metadata performance of large directories by distributing the metadata requests in a single directory over two or more MDTs.", "mimetype": "text/plain", "start_char_idx": 14807, "end_char_idx": 18562, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b0a89a1-1320-43ba-a4e6-1b840702a77d": {"__data__": {"id_": "0b0a89a1-1320-43ba-a4e6-1b840702a77d", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e6d6384-67cd-4be0-a9c8-de3e325d60ca", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "bb0a221411848abb237ff40820539b64f6f37974dc80162ab6bf1af3467f14c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb938ff6-e589-43f7-999a-fb8f1457defb", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7c43bbcf194221faac08fafc8dadfe84b2252d60473766a9897acd205143e106", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Typically, for management purposes, at least one client system should not be subject to root squash.\n\n- Routing\n\n  LNet routing between different networks and LNDs.\n\n- RPC\n\n  Remote procedure call. A network encoding of a request.\n\n### S\n\n- Stripe\n\n  A contiguous, logical extent of a Lustre file written to a single OST. Used synonymously with a single OST data object that makes up part of a file visible to user applications.\n  \n- Striped Directory\n\n  A striped directory is when metadata for files in a given directory are distributed evenly over multiple MDTs. Striped directories are only available in Lustre software version 2.8 or later. A user can create a striped directory to increase metadata performance of large directories by distributing the metadata requests in a single directory over two or more MDTs.\n\n\n- Stripe size\n\n  The maximum number of bytes that will be written to an OST object before the next object in a file's layout is used when writing sequential data to a file. Once a full stripe has been written to each of the objects in the layout, the first object will be written to again in round-robin fashion.\n\n- Stripe count\n\n  The number of OSTs holding objects for a RAID0-striped Lustre file.\n\n### T\n\n- T10 object protocol\n\n  An object storage protocol tied to the SCSI transport layer. The Lustre file system does not use T10.\n\n### W\n\n- Wide striping\n\n  Strategy of using many OSTs to store stripes of a single file. This obtains maximum bandwidth access to a single file through parallel utilization of many OSTs. For more information about wide striping, see [*the section called \u201cLustre Striping Internals\u201d*](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#lustre-striping-internals).", "mimetype": "text/plain", "start_char_idx": 17742, "end_char_idx": 19480, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4130f604-b195-4f36-a755-143f4b00764f": {"__data__": {"id_": "4130f604-b195-4f36-a755-143f4b00764f", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4cc1a85-b6f8-4d82-975a-9969cbf942a9", "node_type": "1", "metadata": {}, "hash": "60a51eecd43705de43da19846c6fb583a12db9282fad3f8c2778e747b4faebf4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Index\n\n## A\nAccess Control List (ACL), [Using ACLs](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#using-acls)<br>\n\t&emsp;examples, [Examples](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#examples)<br>\n\t&emsp;how they work, [How ACLs Work](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#how-acls-work)<br>\n\t&emsp;using, [Using ACLs with the Lustre Software](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#using-acls-with-the-lustre-software)<br>\naudit<br>\n\t&emsp;change logs, [Audit with Changelogs](03.01-Monitoring%20a%20Lustre%20File%20System.md#audit-with-changelogs)<br><br>\n\n## B\nbackup, [Backing up a File System](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-a-file-system)<br>\n\t&emsp;aborting recovery, [Aborting Recovery](03.03-Lustre%20Maintenance.md#aborting-recovery)<br>\n\t&emsp;index objects, [Backing Up an OST or MDT (Backend File System Level)](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-an-ost-or-mdt-backend-file-system-level)<br>\n\t&emsp;MDT file system, [Backing Up an OST or MDT (Backend File System Level)](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-an-ost-or-mdt-backend-file-system-level)<br>\n\t&emsp;MDT/OST device level, [Backing Up and Restoring an MDT or OST (ldiskfs Device Level)](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-and-restoring-an-mdt-or-ost-ldiskfs-device-level)<br>\n\t&emsp;new/changed files, [Backing up New/Changed Files to the Backup File System](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-newchanged-files-to-the-backup-file-system)<br>\n\t&emsp;OST and MDT, [Backing Up an OST or MDT](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-an-ost-or-mdt)<br>\n\t&emsp;OST config, [Backing Up OST Configuration Files](03.03-Lustre%20Maintenance.md#backing-up-ost-configuration-files)<br>\n\t&emsp;OST file system, [Backing Up an OST or MDT (Backend File System Level)](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#backing-up-an-ost-or-mdt-backend-file-system-level)<br>\n\t&emsp;restoring file system backup, [Restoring a File-Level Backup](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#restoring-a-file-level-backup)<br>\n\t&emsp;restoring OST config, [Restoring OST Configuration Files](03.03-Lustre%20Maintenance.md#restoring-ost-configuration-files)<br>\n\t&emsp;rsync, [Lustre_rsync](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#lustre_rsync)<br><br>\n\t\t&emsp;&emsp;examples, [lustre_rsync Examples](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#lustre_rsync-examples)<br>\n\t\t&emsp;&emsp;using, [Using Lustre_rsync](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#using-lustre_rsync)<br><br>\n\n\u200b\t&emsp;using LVM,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4cc1a85-b6f8-4d82-975a-9969cbf942a9": {"__data__": {"id_": "b4cc1a85-b6f8-4d82-975a-9969cbf942a9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4130f604-b195-4f36-a755-143f4b00764f", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "4ba9227382bf1e51a78773ac8e4069262f5abcd1da241d98a9dd60382c7f2136", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d24a66e8-f7ed-4873-9ffb-b73434bf7f30", "node_type": "1", "metadata": {}, "hash": "e9fcabc2cf38be9715cbd6977116a4ad205864cf3bc5bdcf3b7dc5bbff0cae78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "restoring OST config, [Restoring OST Configuration Files](03.03-Lustre%20Maintenance.md#restoring-ost-configuration-files)<br>\n\t&emsp;rsync, [Lustre_rsync](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#lustre_rsync)<br><br>\n\t\t&emsp;&emsp;examples, [lustre_rsync Examples](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#lustre_rsync-examples)<br>\n\t\t&emsp;&emsp;using, [Using Lustre_rsync](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#using-lustre_rsync)<br><br>\n\n\u200b\t&emsp;using LVM, [Using LVM Snapshots with the Lustre File System](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#using-lvm-snapshots-with-the-lustre-file-system)<br><br>\n\n\u200b\t\t&emsp;&emsp;creating, [Creating an LVM-based Backup File System](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#creating-an-lvm-based-backup-file-system)<br>\n\u200b\t\t&emsp;&emsp;creating snapshots, [Creating Snapshot Volumes](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#creating-snapshot-volumes)<br>\n\u200b\t\t&emsp;&emsp;deleting, [Deleting Old Snapshots](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#deleting-old-snapshots)<br>\n\u200b\t\t&emsp;&emsp;resizing, [Changing Snapshot Volume Size](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#changing-snapshot-volume-size)<br>\n\u200b\t\t&emsp;&emsp;restoring, [Restoring the File System From a Snapshot](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#restoring-the-file-system-from-a-snapshot)<br><br>\n\n\u200b\t&emsp;ZFS to ldiskfs, [Migrate from a ZFS to an ldiskfs based filesystem](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#migrate-from-a-zfs-to-an-ldiskfs-based-filesystem), [Migrate from an ldiskfs to a ZFS based filesystem](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#migrate-from-an-ldiskfs-to-a-zfs-based-filesystem)<br>\n\u200b\t&emsp;ZFS ZPL, [Migration Between ZFS and ldiskfs Target Filesystems](03.07-BackingUp%20and%20Restoring%20a%20File%20System.md#migration-between-zfs-and-ldiskfs-target-filesystems)<br><br>\n\nbarrier, [Global Write Barriers](03.19-Lustre%20ZFS%20Snapshots.md#global-write-barriers)<br><br>\n\n\u200b\t&emsp;impose, [Impose Barrier](03.19-Lustre%20ZFS%20Snapshots.md#impose-barrier)<br>\n\u200b\t&emsp;query, [Query Barrier](03.19-Lustre%20ZFS%20Snapshots.md#query-barrier)<br>\n\u200b\t&emsp;remove, [Remove Barrier](03.19-Lustre%20ZFS%20Snapshots.md#remove-barrier)<br>\n\u200b\t&emsp;rescan, [Rescan Barrier](03.19-Lustre%20ZFS%20Snapshots.md#rescan-barrier)<br><br>\n\n###### benchmarking\n\n<br>\n\n\u200b\t&emsp;application profiling, [Collecting Application Profiling Information (stats-collect)](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#collecting-application-profiling-information-stats-collect)<br>\n\u200b\t&emsp;local disk, [Testing Local Disk Performance](04.", "mimetype": "text/plain", "start_char_idx": 2291, "end_char_idx": 5069, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d24a66e8-f7ed-4873-9ffb-b73434bf7f30": {"__data__": {"id_": "d24a66e8-f7ed-4873-9ffb-b73434bf7f30", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4cc1a85-b6f8-4d82-975a-9969cbf942a9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d02547a49016a0dc270d32bc7010f33e5591a8776196ecd14bc612176dddae40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "514bd2a8-4598-4b42-ae08-98c77cfe99ab", "node_type": "1", "metadata": {}, "hash": "4729f25e19e1628b1c6fc9c133c48a8e0d97574465b1900ce7fda915efa5f9fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "query, [Query Barrier](03.19-Lustre%20ZFS%20Snapshots.md#query-barrier)<br>\n\u200b\t&emsp;remove, [Remove Barrier](03.19-Lustre%20ZFS%20Snapshots.md#remove-barrier)<br>\n\u200b\t&emsp;rescan, [Rescan Barrier](03.19-Lustre%20ZFS%20Snapshots.md#rescan-barrier)<br><br>\n\n###### benchmarking\n\n<br>\n\n\u200b\t&emsp;application profiling, [Collecting Application Profiling Information (stats-collect)](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#collecting-application-profiling-information-stats-collect)<br>\n\u200b\t&emsp;local disk, [Testing Local Disk Performance](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-local-disk-performance)<br>\n\u200b\t&emsp;MDS performance, [Testing MDS Performance (mds-survey)](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-mds-performance-mds-survey)<br>\n\u200b\t&emsp;network, [Testing Network Performance](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-network-performance)<br>\n\u200b\t&emsp;OST I/O, [Testing OST I/O Performance (ost-survey)](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-ost-io-performance-ost-survey)<br>\n\u200b\t&emsp;OST performance, [Testing OST Performance (obdfilter-survey)](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-ost-performance-obdfilter-survey)<br>\n\u200b\t&emsp;raw hardware with sgpdd-survey, [Testing I/O Performance of Raw Hardware (sgpdd-survey)](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-io-performance-of-raw-hardware-sgpdd-survey)<br>\n\u200b\t&emsp;remote disk, [Testing Remote Disk Performance](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#testing-remote-disk-performance)<br>\n\u200b\t&emsp;tuning storage, [Tuning Linux Storage Devices](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#tuning-linux-storage-devices)<br>\n\u200b\t&emsp;with Lustre I/O Kit, [Using Lustre I/O Kit Tools](04.02-Benchmarking%20Lustre%20File%20System%20Performance%20(Lustre%20IO%20Kit).md#using-lustre-io-kit-tools)<br><br>\n\n## C\nchange logs (see [monitoring](monitoring))<br>\ncommit on share, [Commit on Share](06.01-Lustre%20File%20System%20Recovery.md#commit-on-share)\t<br><br>\n\n\u200b\t&emsp;tuning, [Tuning Commit On Share](06.01-Lustre%20File%20System%20Recovery.md#tuning-commit-on-share)<br>\n\u200b\t&emsp;working with, [Working with Commit on Share](06.01-Lustre%20File%20System%20Recovery.md#working-with-commit-on-share)<br><br>\n\nconfiglogs, [Lustre Configuration Logs](03.19-Lustre%20ZFS%20Snapshots.md#lustre-configuration-logs)<br>\n\n###### configuring,\n\n [Introduction](06.06-Configuration%20Files%20and%20Module%20Parameters.md#introduction)<br><br>\n\n\u200b\t&emsp;adaptive timeouts, [Configuring Adaptive Timeouts](06.02-Lustre%20Parameters.md#configuring-adaptive-timeouts)<br>\n\u200b\t&emsp;LNet options,", "mimetype": "text/plain", "start_char_idx": 4490, "end_char_idx": 7454, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "514bd2a8-4598-4b42-ae08-98c77cfe99ab": {"__data__": {"id_": "514bd2a8-4598-4b42-ae08-98c77cfe99ab", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d24a66e8-f7ed-4873-9ffb-b73434bf7f30", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "958ff73a8810b57b4fc8644f4a2acd6d095d3633098c138c5fc0ed2a8cce9380", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5b40e25-57f4-4984-afa1-5b32ca670bcc", "node_type": "1", "metadata": {}, "hash": "fbaa2d3907838fab8303d262abac7ffbcf31c355aca23796ce3fe5dd6ed2acdd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "tuning, [Tuning Commit On Share](06.01-Lustre%20File%20System%20Recovery.md#tuning-commit-on-share)<br>\n\u200b\t&emsp;working with, [Working with Commit on Share](06.01-Lustre%20File%20System%20Recovery.md#working-with-commit-on-share)<br><br>\n\nconfiglogs, [Lustre Configuration Logs](03.19-Lustre%20ZFS%20Snapshots.md#lustre-configuration-logs)<br>\n\n###### configuring,\n\n [Introduction](06.06-Configuration%20Files%20and%20Module%20Parameters.md#introduction)<br><br>\n\n\u200b\t&emsp;adaptive timeouts, [Configuring Adaptive Timeouts](06.02-Lustre%20Parameters.md#configuring-adaptive-timeouts)<br>\n\u200b\t&emsp;LNet options, [LNet Options](06.06-Configuration%20Files%20and%20Module%20Parameters.md#lnet-options)<br>\n\u200b\t&emsp;module options, [Module Options](06.06-Configuration%20Files%20and%20Module%20Parameters.md#module-options)<br>\n\u200b\t&emsp;multihome, [Multihome Server Example](02.06-Configuring%20Lustre%20Networking%20(LNet).md#multihome-server-example)<br>\n\u200b\t&emsp;network<br><br>\n\n\u200b\t\t&emsp;&emsp;forwarding, [forwarding (\"\")](06.06-Configuration%20Files%20and%20Module%20Parameters.md#forwarding-)<br>\n\u200b\t\t&emsp;&emsp;rnet_htable_size, [rnet_htable_size](06.06-Configuration%20Files%20and%20Module%20Parameters.md#rnet_htable_size)<br>\n\u200b\t\t&emsp;&emsp;routes, [routes (\"\")](06.06-Configuration%20Files%20and%20Module%20Parameters.md#routes-)<br>\n\u200b\t\t&emsp;&emsp;SOCKLND, [SOCKLND Kernel TCP/IP LND](06.06-Configuration%20Files%20and%20Module%20Parameters.md#socklnd-kernel-tcpip-lnd)<br>\n\u200b\t\t&emsp;&emsp;tcp, [networks (\"tcp\")](06.06-Configuration%20Files%20and%20Module%20Parameters.md#networks-tcp)<br><br>\n\n\u200b\t&emsp;network topology, [Network Topology](06.06-Configuration%20Files%20and%20Module%20Parameters.md#network-topology)<br><br>\n\n## D\ndebug<br><br>\n\n\u200b\t&emsp;utilities, [Testing / Debugging Utilities](06.07-System%20Configuration%20Utilities.md#testing--debugging-utilities)<br><br>\n\ndebugging, [Diagnostic and Debugging Tools](05.03-Debugging%20a%20Lustre%20File%20System.md#diagnostic-and-debugging-tools)<br><br>\n\n\u200b\t&emsp;admin tools, [Tools for Administrators and Developers](05.03-Debugging%20a%20Lustre%20File%20System.md#tools-for-administrators-and-developers)<br>\n\u200b\t&emsp;developer tools, [Tools for Developers](05.03-Debugging%20a%20Lustre%20File%20System.md#tools-for-developers)<br>\n\u200b\t&emsp;developers tools, [Lustre Debugging for Developers](05.03-Debugging%20a%20Lustre%20File%20System.md#lustre-debugging-for-developers)<br>\n\u200b\t&emsp;disk contents, [Looking at Disk Content](05.03-Debugging%20a%20Lustre%20File%20System.md#looking-at-disk-content)<br>\n\u200b\t&emsp;external tools, [External Debugging Tools](05.03-Debugging%20a%20Lustre%20File%20System.md#external-debugging-tools)<br>\n\u200b\t&emsp;kernel debug log, [Controlling Information Written to the Kernel Debug Log](05.03-Debugging%20a%20Lustre%20File%20System.md#controlling-information-written-to-the-kernel-debug-log)<br>\n\u200b\t&emsp;lctl example, [Sample lctl Run](05.", "mimetype": "text/plain", "start_char_idx": 6846, "end_char_idx": 9777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5b40e25-57f4-4984-afa1-5b32ca670bcc": {"__data__": {"id_": "d5b40e25-57f4-4984-afa1-5b32ca670bcc", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "514bd2a8-4598-4b42-ae08-98c77cfe99ab", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "03fa1c93e8048f35d7a91172dc6b0f1e9c5f2f4e69796d048e72a2617eb48397", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acb05411-eff8-48a8-b879-6807ca2fa975", "node_type": "1", "metadata": {}, "hash": "fd68f71e037aa802c8b6f21ef35ee24e00ade38070963f95fb02a5e49c4dc9f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "03-Debugging%20a%20Lustre%20File%20System.md#lustre-debugging-for-developers)<br>\n\u200b\t&emsp;disk contents, [Looking at Disk Content](05.03-Debugging%20a%20Lustre%20File%20System.md#looking-at-disk-content)<br>\n\u200b\t&emsp;external tools, [External Debugging Tools](05.03-Debugging%20a%20Lustre%20File%20System.md#external-debugging-tools)<br>\n\u200b\t&emsp;kernel debug log, [Controlling Information Written to the Kernel Debug Log](05.03-Debugging%20a%20Lustre%20File%20System.md#controlling-information-written-to-the-kernel-debug-log)<br>\n\u200b\t&emsp;lctl example, [Sample lctl Run](05.03-Debugging%20a%20Lustre%20File%20System.md#sample-lctl-run)<br>\n\u200b\t&emsp;memory leaks, [Finding Memory Leaks Using leak_finder.pl](05.03-Debugging%20a%20Lustre%20File%20System.md#finding-memory-leaks-using-leak_finderpl)<br>\n\u200b\t&emsp;message format, [Understanding the Lustre Debug Messaging Format](05.03-Debugging%20a%20Lustre%20File%20System.md#understanding-the-lustre-debug-messaging-format)<br>\n\u200b\t&emsp;procedure, [Lustre Debugging Procedures](05.03-Debugging%20a%20Lustre%20File%20System.md#lustre-debugging-procedures)<br>\n\u200b\t&emsp;tools, [Lustre Debugging Tools](05.03-Debugging%20a%20Lustre%20File%20System.md#lustre-debugging-tools)<br>\n\u200b\t&emsp;using lctl, [Using the lctl Tool to View Debug Messages](05.03-Debugging%20a%20Lustre%20File%20System.md#using-the-lctl-tool-to-view-debug-messages)<br>\n\u200b\t&emsp;using strace, [Troubleshooting with strace](05.03-Debugging%20a%20Lustre%20File%20System.md#troubleshooting-with-strace)<br><br>\n\ndesign (see [setup](#setup)<br>\nDLC<br><br>\n\n\u200b\t&emsp;Code Example, [Adding a route code example](06.08-LNet%20Configuration%20C-API.md#adding-a-route-code-example)<br><br>\n\ndom, [Introduction to Data on MDT (DoM)](03.09-Data%20on%20MDT%20(DoM).md#introduction-to-data-on-mdt-dom), [User Commands](03.09-Data%20on%20MDT%20(DoM).md#user-commands), [DoM Stripe Size Restrictions](03.09-Data%20on%20MDT%20(DoM).md#dom-stripe-size-restrictions), [lfs getstripe for DoM files](03.09-Data%20on%20MDT%20(DoM).md#lfs-getstripe-for-dom-files), [lfs find for DoM files](03.09-Data%20on%20MDT%20(DoM).md#lfs-find-for-dom-files), [The dom_stripesize parameter](03.09-Data%20on%20MDT%20(DoM).md#the-dom_stripesize-parameter), [Disable DoM](03.09-Data%20on%20MDT%20(DoM).md#disable-dom)<br><br>\n\n\u200b\t&emsp;disabledom, [Disable DoM](03.09-Data%20on%20MDT%20(DoM).md#disable-dom)<br>\n\u200b\t&emsp;domstripesize, [DoM Stripe Size Restrictions](03.09-Data%20on%20MDT%20(DoM).md#dom-stripe-size-restrictions)<br>\n\u200b\t&emsp;dom_stripesize, [The dom_stripesize parameter](03.09-Data%20on%20MDT%20(DoM).md#the-dom_stripesize-parameter)<br>\n\u200b\t&emsp;intro, [Introduction to Data on MDT (DoM)](03.09-Data%20on%20MDT%20(DoM).", "mimetype": "text/plain", "start_char_idx": 9204, "end_char_idx": 11912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "acb05411-eff8-48a8-b879-6807ca2fa975": {"__data__": {"id_": "acb05411-eff8-48a8-b879-6807ca2fa975", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5b40e25-57f4-4984-afa1-5b32ca670bcc", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "73f9a13af544403ee1dc8cf93f096725bb3155297716880ae6a0b3594701a638", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c1e4c3d-a0c2-4a73-a9be-c689603744d8", "node_type": "1", "metadata": {}, "hash": "8286da9b361404537f92bd58d6b4e0c16a01a6434d0a8b3e6621c5d00ecc0ca5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "09-Data%20on%20MDT%20(DoM).md#disable-dom)<br><br>\n\n\u200b\t&emsp;disabledom, [Disable DoM](03.09-Data%20on%20MDT%20(DoM).md#disable-dom)<br>\n\u200b\t&emsp;domstripesize, [DoM Stripe Size Restrictions](03.09-Data%20on%20MDT%20(DoM).md#dom-stripe-size-restrictions)<br>\n\u200b\t&emsp;dom_stripesize, [The dom_stripesize parameter](03.09-Data%20on%20MDT%20(DoM).md#the-dom_stripesize-parameter)<br>\n\u200b\t&emsp;intro, [Introduction to Data on MDT (DoM)](03.09-Data%20on%20MDT%20(DoM).md#introduction-to-data-on-mdt-dom)<br>\n\u200b\t&emsp;lfsfind, [lfs find for DoM files](03.09-Data%20on%20MDT%20(DoM).md#lfs-find-for-dom-files)<br>\n\u200b\t&emsp;lfsgetstripe, [lfs getstripe for DoM files](03.09-Data%20on%20MDT%20(DoM).md#lfs-getstripe-for-dom-files)<br>\n\u200b\t&emsp;lfssetstripe, [lfs setstripe for DoM files](03.09-Data%20on%20MDT%20(DoM).md#lfs-setstripe-for-dom-files)<br>\n\u200b\t&emsp;usercommands, [User Commands](03.09-Data%20on%20MDT%20(DoM).md#user-commands)<br><br>\n\n## E\ne2scan, [e2scan](06.07-System%20Configuration%20Utilities.md#e2scan)<br>\nerrors (see [troubleshooting](#t))<br>\n## F\nfailover, [What is Failover?](02-Introducing%20the%20Lustre%20File%20System.md#what-is-failover), [Setting Up a Failover Environment](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#setting-up-a-failover-environment)<br><br>\n\n\u200b\t&emsp;and Lustre, [Failover Functionality in a Lustre File System](02-Introducing%20the%20Lustre%20File%20System.md#failover-functionality-in-a-lustre-file-system)<br>\n\u200b\t&emsp;capabilities, [Failover Capabilities](02-Introducing%20the%20Lustre%20File%20System.md#failover-capabilities)<br>\n\u200b\t&emsp;configuration, [Types of Failover Configurations](02-Introducing%20the%20Lustre%20File%20System.md#types-of-failover-configurations)<br>\n\u200b\t&emsp;high-availability (HA) software, [Selecting High-Availability (HA) Software](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#selecting-high-availability-ha-software)<br>\n\u200b\t&emsp;MDT, [MDT Failover Configuration (Active/Passive)](02-Introducing%20the%20Lustre%20File%20System.md#mdt-failover-configuration-activepassive), [MDT Failover Configuration (Active/Active)](02-Introducing%20the%20Lustre%20File%20System.md#mdt-failover-configuration-activeactive)<br>\n\u200b\t&emsp;OST, [OST Failover Configuration (Active/Active)](02-Introducing%20the%20Lustre%20File%20System.md#ost-failover-configuration-activeactive)<br>\n\u200b\t&emsp;power control device, [Selecting Power Equipment](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#selecting-power-equipment)<br>\n\u200b\t&emsp;power management software, [Selecting Power Management Software](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#selecting-power-management-software)<br>\n\u200b\t&emsp;setup,", "mimetype": "text/plain", "start_char_idx": 11452, "end_char_idx": 14176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c1e4c3d-a0c2-4a73-a9be-c689603744d8": {"__data__": {"id_": "6c1e4c3d-a0c2-4a73-a9be-c689603744d8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acb05411-eff8-48a8-b879-6807ca2fa975", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6ebe8c2b06713be74e61319d95aff538136dcde184e91a74384a5d267473ec5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ef9277f-62d4-47f4-ad44-1d92a5c1f6ba", "node_type": "1", "metadata": {}, "hash": "f59e13bfb2be8e6fe54f34723a07d1c2abab8e37a1afa0334de8107c4782befa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "md#mdt-failover-configuration-activeactive)<br>\n\u200b\t&emsp;OST, [OST Failover Configuration (Active/Active)](02-Introducing%20the%20Lustre%20File%20System.md#ost-failover-configuration-activeactive)<br>\n\u200b\t&emsp;power control device, [Selecting Power Equipment](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#selecting-power-equipment)<br>\n\u200b\t&emsp;power management software, [Selecting Power Management Software](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#selecting-power-management-software)<br>\n\u200b\t&emsp;setup, [Preparing a Lustre File System for Failover](02.08-Configuring%20Failover%20in%20a%20Lustre%20File%20System.md#preparing-a-lustre-file-system-for-failover)<br><br>\n\nfeature overview<br><br>\n\n\u200b\t&emsp;configuration, [Configuration](03.19-Lustre%20ZFS%20Snapshots.md#configuration)<br><br>\n\nfile layout<br><br>\n\n\u200b\t&emsp;See striping, [Lustre File Layout (Striping) Considerations](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#lustre-file-layout-striping-considerations)<br><br>\n\nfilefrag, [filefrag](06.03-User%20Utilities.md#filefrag)<br>\nfileset, [Fileset Feature](06.07-System%20Configuration%20Utilities.md#fileset-feature)<br>\nfragmentation, [Description](06.03-User%20Utilities.md#description-1)<br><br>\n\n## H\nHierarchical Storage Management (HSM)<br><br>\n\n\u200b\t&emsp;introduction, [Introduction](03.15-Hierarchical%20Storage%20Management%20(HSM).md#introduction)<br>\nHigh availability (see [failover](#f))<br>\nHSM<br><br>\n\n\u200b\t&emsp;agents, [Agents](03.15-Hierarchical%20Storage%20Management%20(HSM).md#agents)<br>\n\u200b\t&emsp;agents and copytools, [Agents and copytool](03.15-Hierarchical%20Storage%20Management%20(HSM).md#agents-and-copytool)<br>\n\u200b\t&emsp;archiveID backends, [Archive ID, multiple backends](03.15-Hierarchical%20Storage%20Management%20(HSM).md#archive-id-multiple-backends)<br>\n\u200b\t&emsp;automatic restore, [Automatic restore](03.15-Hierarchical%20Storage%20Management%20(HSM).md#automatic-restore)<br>\n\u200b\t&emsp;changelogs, [change logs](03.15-Hierarchical%20Storage%20Management%20(HSM).md#change-logs)<br>\n\u200b\t&emsp;commands, [Commands](03.15-Hierarchical%20Storage%20Management%20(HSM).md#commands)<br>\n\u200b\t&emsp;coordinator, [Coordinator](03.15-Hierarchical%20Storage%20Management%20(HSM).md#coordinator)<br>\n\u200b\t&emsp;file states, [File states](03.15-Hierarchical%20Storage%20Management%20(HSM).md#file-states)<br>\n\u200b\t&emsp;grace_delay, [grace_delay](03.15-Hierarchical%20Storage%20Management%20(HSM).md#grace_delay)<br>\n\u200b\t&emsp;hsm_control, [hsm_controlpolicy](03.15-Hierarchical%20Storage%20Management%20(HSM).md#hsm_controlpolicy)<br>\n\u200b\t&emsp;max_requests, [max_requests](03.15-Hierarchical%20Storage%20Management%20(HSM).md#max_requests)<br>\n\u200b\t&emsp;policy, [policy](03.15-Hierarchical%20Storage%20Management%20(HSM).md#policy)<br>\n\u200b\t&emsp;policy engine, [Policy engine](03.15-Hierarchical%20Storage%20Management%20(HSM).md#policy-engine)<br>\n\u200b\t&emsp;registered agents, [Registered agents](03.15-Hierarchical%20Storage%20Management%20(HSM).md#registered-agents)<br>\n\u200b\t&emsp;", "mimetype": "text/plain", "start_char_idx": 13627, "end_char_idx": 16687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ef9277f-62d4-47f4-ad44-1d92a5c1f6ba": {"__data__": {"id_": "3ef9277f-62d4-47f4-ad44-1d92a5c1f6ba", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c1e4c3d-a0c2-4a73-a9be-c689603744d8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "67c160130887d4b9eaa705bd00913edb64f0e8a7de812c6bdfb290236aab8e77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "006af9de-dfb0-47bf-9e9b-ab1ee67b4d8c", "node_type": "1", "metadata": {}, "hash": "882cb77f3e60a4b0eefff623ef1bf1fb1b4033c5cc81d740be5bd7fe5cf90cba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "md#grace_delay)<br>\n\u200b\t&emsp;hsm_control, [hsm_controlpolicy](03.15-Hierarchical%20Storage%20Management%20(HSM).md#hsm_controlpolicy)<br>\n\u200b\t&emsp;max_requests, [max_requests](03.15-Hierarchical%20Storage%20Management%20(HSM).md#max_requests)<br>\n\u200b\t&emsp;policy, [policy](03.15-Hierarchical%20Storage%20Management%20(HSM).md#policy)<br>\n\u200b\t&emsp;policy engine, [Policy engine](03.15-Hierarchical%20Storage%20Management%20(HSM).md#policy-engine)<br>\n\u200b\t&emsp;registered agents, [Registered agents](03.15-Hierarchical%20Storage%20Management%20(HSM).md#registered-agents)<br>\n\u200b\t&emsp;request monitoring, [Request monitoring](03.15-Hierarchical%20Storage%20Management%20(HSM).md#request-monitoring)<br>\n\u200b\t&emsp;requests, [Requests](03.15-Hierarchical%20Storage%20Management%20(HSM).md#requests)<br>\n\u200b\t&emsp;requirements, [Requirements](03.15-Hierarchical%20Storage%20Management%20(HSM).md#requirements)<br>\n\u200b\t&emsp;robinhood, [Robinhood](03.15-Hierarchical%20Storage%20Management%20(HSM).md#robinhood)<br>\n\u200b\t&emsp;setup, [Setup](03.15-Hierarchical%20Storage%20Management%20(HSM).md#setup)<br>\n\u200b\t&emsp;timeout, [Timeout](03.15-Hierarchical%20Storage%20Management%20(HSM).md#timeout)<br>\n\u200b\t&emsp;tuning, [Tuning](03.15-Hierarchical%20Storage%20Management%20(HSM).md#tuning)<br><br>\n\n## I\nI/O, [Handling Full OSTs](03.12-Managing%20the%20File%20System%20and%20IO.md#handling-full-osts)<br><br>\n\n\u200b\t&emsp;adding an OST, [Adding an OST to a Lustre File System](03.12-Managing%20the%20File%20System%20and%20IO.md#adding-an-ost-to-a-lustre-file-system)<br>\n\u200b\t&emsp;bringing OST online, [Returning an Inactive OST Back Online](03.12-Managing%20the%20File%20System%20and%20IO.md#returning-an-inactive-ost-back-online)<br>\n\u200b\t&emsp;direct, [Performing Direct I/O](03.12-Managing%20the%20File%20System%20and%20IO.md#performing-direct-io)<br>\n\u200b\t&emsp;disabling OST creates, [Disabling creates on a Full OST](03.12-Managing%20the%20File%20System%20and%20IO.md#disabling-creates-on-a-full-ost)<br>\n\u200b\t&emsp;full OSTs, [Handling Full OSTs](03.12-Managing%20the%20File%20System%20and%20IO.md#handling-full-osts)<br>\n\u200b\t&emsp;migrating data, [Migrating Data within a File System](03.12-Managing%20the%20File%20System%20and%20IO.md#migrating-data-within-a-file-system)<br>\n\u200b\t&emsp;OST space usage, [Checking OST Space Usage](03.12-Managing%20the%20File%20System%20and%20IO.md#checking-ost-space-usage)<br>\n\u200b\t&emsp;pools, [Creating and Managing OST Pools](03.12-Managing%20the%20File%20System%20and%20IO.md#creating-and-managing-ost-pools)<br><br>\n\nimperative recovery, [Imperative Recovery](06.01-Lustre%20File%20System%20Recovery.md#imperative-recovery)<br><br>\n\n\u200b\t&emsp;Configuration Suggestions, [Configuration Suggestions for Imperative Recovery](06.01-Lustre%20File%20System%20Recovery.md#configuration-suggestions-for-imperative-recovery)<br>\n\u200b\t&emsp;MGS role, [MGS role](06.01-Lustre%20File%20System%20Recovery.", "mimetype": "text/plain", "start_char_idx": 16110, "end_char_idx": 18998, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "006af9de-dfb0-47bf-9e9b-ab1ee67b4d8c": {"__data__": {"id_": "006af9de-dfb0-47bf-9e9b-ab1ee67b4d8c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ef9277f-62d4-47f4-ad44-1d92a5c1f6ba", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "e15246ef20de7e9231d3c95bd75c54156e9fd6646453cb7aec81e765a614c3d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8be77cec-7088-48d8-a12e-3fd61f754702", "node_type": "1", "metadata": {}, "hash": "025f22f47e32b9a6abb7fc9bb7d2b7ce6e4958bc3295134e477e9a37b24b8029", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12-Managing%20the%20File%20System%20and%20IO.md#checking-ost-space-usage)<br>\n\u200b\t&emsp;pools, [Creating and Managing OST Pools](03.12-Managing%20the%20File%20System%20and%20IO.md#creating-and-managing-ost-pools)<br><br>\n\nimperative recovery, [Imperative Recovery](06.01-Lustre%20File%20System%20Recovery.md#imperative-recovery)<br><br>\n\n\u200b\t&emsp;Configuration Suggestions, [Configuration Suggestions for Imperative Recovery](06.01-Lustre%20File%20System%20Recovery.md#configuration-suggestions-for-imperative-recovery)<br>\n\u200b\t&emsp;MGS role, [MGS role](06.01-Lustre%20File%20System%20Recovery.md#mgs-role)<br>\n\u200b\t&emsp;Tuning, [Tuning Imperative Recovery](06.01-Lustre%20File%20System%20Recovery.md#tuning-imperative-recovery)<br><br>\n\ninodes<br><br>\n\n\u200b\t&emsp;MDS, [Setting Formatting Options for an ldiskfs MDT](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#setting-formatting-options-for-an-ldiskfs-mdt)<br>\n\u200b\t&emsp;OST, [Setting Formatting Options for an ldiskfs OST](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#setting-formatting-options-for-an-ldiskfs-ost)<br><br>\n\ninstalling, [Steps to Installing the Lustre Software](02.01-Installation%20Overview.md#steps-to-installing-the-lustre-software)<br>\n\n\u200b\t&emsp;preparation, [Preparing to Install the Lustre Software](02.05-Installing%20the%20Lustre%20Software.md#preparing-to-install-the-lustre-software)<br><br>\n\nIntroduction, [Introduction](03.19-Lustre%20ZFS%20Snapshots.md#introduction)<br>\n\n\u200b\t&emsp;Requirements, [Requirements](03.19-Lustre%20ZFS%20Snapshots.md#requirements)<br><br>\n\nior-survey, [ior-survey](06.07-System%20Configuration%20Utilities.md#ior-survey)<br>\nIsolation, [Isolating Clients to a Sub-directory Tree](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#isolating-clients-to-a-sub-directory-tree)<br><br>\n\n\u200b\t&emsp;client identification, [Identifying Clients](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#identifying-clients)<br>\n\u200b\t&emsp;configuring, [Configuring Isolation](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#configuring-isolation)<br>\n\u200b\t&emsp;making permanent, [Making Isolation Permanent](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#making-isolation-permanent)<br><br>\n\n## J\njobstats (see [monitoring](monitoring))<br>\n## L\nlarge_xattr<br><br>\n\n\u200b\t&emsp;ea_inode, [File and File System Limits](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#file-and-file-system-limits), [Upgrading to Lustre Software Release 2.x (Major Release)](03.06-Upgrading%20a%20Lustre%20File%20System.md#upgrading-to-lustre-software-release-2x-major-release)<br><br>\n\nlctl, [lctl](06.07-System%20Configuration%20Utilities.md#lctl)<br>\nldiskfs<br><br>\n\n\u200b\t&emsp;formatting options, [Setting ldiskfs File System Formatting Options](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#setting-ldiskfs-file-system-formatting-options)<br><br>\n\nlfs, [lfs](06.", "mimetype": "text/plain", "start_char_idx": 18408, "end_char_idx": 21465, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8be77cec-7088-48d8-a12e-3fd61f754702": {"__data__": {"id_": "8be77cec-7088-48d8-a12e-3fd61f754702", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "006af9de-dfb0-47bf-9e9b-ab1ee67b4d8c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "335b9d716d20b58cc04bd01cfeda0c63dc051620abb7dd266d6552ef43369f52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88f3d31d-b1ae-453c-9195-e10c6c6cbaa8", "node_type": "1", "metadata": {}, "hash": "79567f4a081f556a8eeb69d70f1d1365769d62846fd40a616ccb5f23443cfb06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#file-and-file-system-limits), [Upgrading to Lustre Software Release 2.x (Major Release)](03.06-Upgrading%20a%20Lustre%20File%20System.md#upgrading-to-lustre-software-release-2x-major-release)<br><br>\n\nlctl, [lctl](06.07-System%20Configuration%20Utilities.md#lctl)<br>\nldiskfs<br><br>\n\n\u200b\t&emsp;formatting options, [Setting ldiskfs File System Formatting Options](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#setting-ldiskfs-file-system-formatting-options)<br><br>\n\nlfs, [lfs](06.03-User%20Utilities.md#lfs)<br>\nlfs_migrate, [lfs_migrate](06.03-User%20Utilities.md#lfs_migrate)<br>\nllodbstat, [llobdstat](06.07-System%20Configuration%20Utilities.md#llobdstat)<br>\nllog_reader, [llog_reader](06.07-System%20Configuration%20Utilities.md#llog_reader)<br>\nllstat, [llstat](06.07-System%20Configuration%20Utilities.md#llstat)<br>\nllverdev, [llverdev](06.07-System%20Configuration%20Utilities.md#llverdev)<br>\nll_decode_filter_fid, [ll_decode_filter_fid](06.07-System%20Configuration%20Utilities.md#ll_decode_filter_fid)<br>\nll_recover_lost_found_objs, [ll_recover_lost_found_objs](06.07-System%20Configuration%20Utilities.md#ll_recover_lost_found_objs)<br>\nLNet, [Introducing LNet](02-Introducing%20the%20Lustre%20File%20System.md#introducing-lnet), [Overview of LNet Module Parameters](02.06-Configuring%20Lustre%20Networking%20(LNet).md#overview-of-lnet-module-parameters), [Dynamically Configuring LNet Routes](03.04-ManagingLustreNetworking(LNet).md#dynamically-configuring-lnet-routes), [lustre_routes_config](03.04-ManagingLustreNetworking(LNet).md#lustre_routes_config), [lustre_routes_conversion](03.04-ManagingLustreNetworking(LNet).md#lustre_routes_conversion), [Route Configuration Examples](03.04-ManagingLustreNetworking(LNet).md#route-configuration-examples) (see [configuring](configuring))<br><br>\n\n\u200b\t&emsp;best practice, [Best Practices for LNet Options](02.06-Configuring%20Lustre%20Networking%20(LNet).md#best-practices-for-lnet-options)<br>\n\u200b\t&emsp;buffer yaml syntax, [Enable Routing and Adjust Router Buffer Configuration](02.06-Configuring%20Lustre%20Networking%20(LNet).md#enable-routing-and-adjust-router-buffer-configuration)<br>\n\u200b\t&emsp;capi general information, [General API Information](06.08-LNet%20Configuration%20C-API.md#general-api-information)<br>\n\u200b\t&emsp;capi input params, [API Common Input Parameters](06.08-LNet%20Configuration%20C-API.md#api-common-input-parameters)<br>\n\u200b\t&emsp;capi output params, [API Common Output Parameters](06.08-LNet%20Configuration%20C-API.md#api-common-output-parameters)<br>\n\u200b\t&emsp;capi return code, [API Return Code](06.08-LNet%20Configuration%20C-API.md#api-return-code)<br>\n\u200b\t&emsp;cli, [Configuring LNet](02.06-Configuring%20Lustre%20Networking%20(LNet).md#configuring-lnet), [Displaying Global Settings](02.06-Configuring%20Lustre%20Networking%20(LNet).md#displaying-global-settings), [Adding, Deleting and Showing Networks](02.06-Configuring%20Lustre%20Networking%20(LNet).md#adding-deleting-and-showing-networks), [Manual Adding, Deleting and Showing Peers](02.06-Configuring%20Lustre%20Networking%20(LNet).", "mimetype": "text/plain", "start_char_idx": 20851, "end_char_idx": 24062, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88f3d31d-b1ae-453c-9195-e10c6c6cbaa8": {"__data__": {"id_": "88f3d31d-b1ae-453c-9195-e10c6c6cbaa8", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8be77cec-7088-48d8-a12e-3fd61f754702", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "55c47726e8c536e7fb04a9c8c2e086895eeab37ad6f559d2e89f9a06667d633c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dd968c3-c348-49f2-ab30-ed398572267c", "node_type": "1", "metadata": {}, "hash": "3a7792ac667b36662cc2605b55ca0b690480d638630421a7f9ef61005b72fd32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "08-LNet%20Configuration%20C-API.md#api-common-output-parameters)<br>\n\u200b\t&emsp;capi return code, [API Return Code](06.08-LNet%20Configuration%20C-API.md#api-return-code)<br>\n\u200b\t&emsp;cli, [Configuring LNet](02.06-Configuring%20Lustre%20Networking%20(LNet).md#configuring-lnet), [Displaying Global Settings](02.06-Configuring%20Lustre%20Networking%20(LNet).md#displaying-global-settings), [Adding, Deleting and Showing Networks](02.06-Configuring%20Lustre%20Networking%20(LNet).md#adding-deleting-and-showing-networks), [Manual Adding, Deleting and Showing Peers](02.06-Configuring%20Lustre%20Networking%20(LNet).md#manual-adding-deleting-and-showing-peers), [Adding, Deleting and Showing routes](02.06-Configuring%20Lustre%20Networking%20(LNet).md#adding-deleting-and-showing-routes), [Enabling and Disabling Routing](02.06-Configuring%20Lustre%20Networking%20(LNet).md#enabling-and-disabling-routing), [Showing routing information](02.06-Configuring%20Lustre%20Networking%20(LNet).md#showing-routing-information), [Configuring Routing Buffers](02.06-Configuring%20Lustre%20Networking%20(LNet).md#configuring-routing-buffers), [Importing YAML Configuration File](02.06-Configuring%20Lustre%20Networking%20(LNet).md#importing-yaml-configuration-file), [Exporting Configuration in YAML format](02.06-Configuring%20Lustre%20Networking%20(LNet).md#exporting-configuration-in-yaml-format), [Showing LNet Traffic Statistics](02.06-Configuring%20Lustre%20Networking%20(LNet).md#showing-lnet-traffic-statistics)<br><br>\n\n\u200b\t\t&emsp;&emsp;asymmetrical route, [Asymmetrical Routes](02.06-Configuring%20Lustre%20Networking%20(LNet).md#asymmetrical-routes)<br>\n\u200b\t\t&emsp;&emsp;dynamic discovery, [Dynamic Peer Discovery](02.06-Configuring%20Lustre%20Networking%20(LNet).md#dynamic-peer-discovery)<br><br>\n\n\u200b\t&emsp;comments, [Including comments](02.06-Configuring%20Lustre%20Networking%20(LNet).md#including-comments)<br>\n\u200b\t&emsp;Configuring LNet, [Configuring LNet via lnetctl](02.06-Configuring%20Lustre%20Networking%20(LNet).md#configuring-lnet-via-lnetctl)<br>\n\u200b\t&emsp;cyaml, [Internal YAML Representation (cYAML)](06.08-LNet%20Configuration%20C-API.md#internal-yaml-representation-cyaml)\n\u200b\t&emsp;error block, [Error Block](06.08-LNet%20Configuration%20C-API.md#error-block)<br>\n\u200b\t&emsp;escaping commas with quotes, [Escaping commas with quotes](02.06-Configuring%20Lustre%20Networking%20(LNet).md#escaping-commas-with-quotes)<br>\n\u200b\t&emsp;features, [Key Features of LNet](02-Introducing%20the%20Lustre%20File%20System.md#key-features-of-lnet)<br>\n\u200b\t&emsp;hardware multi-rail configuration, [Hardware Based Multi-Rail Configurations with LNet](03.04-ManagingLustreNetworking(LNet).md#hardware-based-multi-rail-configurations-with-lnet)<br>\n\u200b\t&emsp;InfiniBand load balancing, [Load Balancing with an InfiniBand* Network](03.04-ManagingLustreNetworking(LNet).md#load-balancing-with-an-infiniband-network)<br>\n\u200b\t&emsp;ip2nets, [Setting the LNet Module ip2nets Parameter](02.06-Configuring%20Lustre%20Networking%20(LNet).md#setting-the-lnet-module-ip2nets-parameter)<br>\n\u200b\t&emsp;lustre.conf, [Setting Up lustre.conf for Load Balancing](03.04-ManagingLustreNetworking(LNet).", "mimetype": "text/plain", "start_char_idx": 23453, "end_char_idx": 26606, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1dd968c3-c348-49f2-ab30-ed398572267c": {"__data__": {"id_": "1dd968c3-c348-49f2-ab30-ed398572267c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88f3d31d-b1ae-453c-9195-e10c6c6cbaa8", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "55e4560c82a5b0736f66da6a07bfad8925a4c16c1fbc93eacf2317ddfaaf1cf3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3de678d-4b9e-44eb-a310-595d5b2b4e81", "node_type": "1", "metadata": {}, "hash": "51e09a58a62c5134cd7184feb2e6b032fb49ca2830f5678e331edc0731f65dc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "md#key-features-of-lnet)<br>\n\u200b\t&emsp;hardware multi-rail configuration, [Hardware Based Multi-Rail Configurations with LNet](03.04-ManagingLustreNetworking(LNet).md#hardware-based-multi-rail-configurations-with-lnet)<br>\n\u200b\t&emsp;InfiniBand load balancing, [Load Balancing with an InfiniBand* Network](03.04-ManagingLustreNetworking(LNet).md#load-balancing-with-an-infiniband-network)<br>\n\u200b\t&emsp;ip2nets, [Setting the LNet Module ip2nets Parameter](02.06-Configuring%20Lustre%20Networking%20(LNet).md#setting-the-lnet-module-ip2nets-parameter)<br>\n\u200b\t&emsp;lustre.conf, [Setting Up lustre.conf for Load Balancing](03.04-ManagingLustreNetworking(LNet).md#setting-up-lustreconf-for-load-balancing)<br>\n\u200b\t&emsp;lustre_lnet_config_buf, [Adjusting Router Buffer Pools](06.08-LNet%20Configuration%20C-API.md#adjusting-router-buffer-pools)<br>\n\u200b\t&emsp;lustre_lnet_config_net, [Adding a Network Interface](06.08-LNet%20Configuration%20C-API.md#adding-a-network-interface)<br>\n\u200b\t&emsp;lustre_lnet_config_ni_system, [Configuring LNet](06.08-LNet%20Configuration%20C-API.md#configuring-lnet)<br>\n\u200b\t&emsp;lustre_lnet_config_route, [Adding Routes](06.08-LNet%20Configuration%20C-API.md#adding-routes)<br>\n\u200b\t&emsp;lustre_lnet_del_net, [Deleting a Network Interface](06.08-LNet%20Configuration%20C-API.md#deleting-a-network-interface)<br>\n\u200b\t&emsp;lustre_lnet_del_route, [Deleting Routes](06.08-LNet%20Configuration%20C-API.md#deleting-routes)<br>\n\u200b\t&emsp;lustre_lnet_enable_routing, [Enabling and Disabling Routing](06.08-LNet%20Configuration%20C-API.md#enabling-and-disabling-routing)<br>\n\u200b\t&emsp;lustre_lnet_show stats, [Showing LNet Traffic Statistics](06.08-LNet%20Configuration%20C-API.md#showing-lnet-traffic-statistics)<br>\n\u200b\t&emsp;lustre_lnet_show_buf, [Showing Routing information](06.08-LNet%20Configuration%20C-API.md#showing-routing-information)<br>\n\u200b\t&emsp;lustre_lnet_show_net, [Showing Network Interfaces](06.08-LNet%20Configuration%20C-API.md#showing-network-interfaces)<br>\n\u200b\t&emsp;lustre_lnet_show_route, [Showing Routes](06.08-LNet%20Configuration%20C-API.md#showing-routes)<br>\n\u200b\t&emsp;lustre_yaml, [Adding/Deleting/Showing Parameters through a YAML Block](06.08-LNet%20Configuration%20C-API.md#addingdeletingshowing-parameters-through-a-yaml-block)<br>\n\u200b\t&emsp;management, [Updating the Health Status of a Peer or Router](03.04-ManagingLustreNetworking(LNet).md#updating-the-health-status-of-a-peer-or-router)<br>\n\u200b\t&emsp;module parameters, [Setting the LNet Module networks Parameter](02.06-Configuring%20Lustre%20Networking%20(LNet).md#setting-the-lnet-module-networks-parameter)<br>\n\u200b\t&emsp;network yaml syntax, [Network Configuration](02.06-Configuring%20Lustre%20Networking%20(LNet).md#network-configuration)<br>\n\u200b\t&emsp;proc, [Monitoring LNet](06.02-Lustre%20Parameters.md#monitoring-lnet)<br>\n\u200b\t&emsp;route checker, [Configuring the Router Checker](02.06-Configuring%20Lustre%20Networking%20(LNet).md#configuring-the-router-checker)<br>\n\u200b\t&emsp;router yaml syntax, [Route Configuration](02.06-Configuring%20Lustre%20Networking%20(LNet).md#route-configuration)<br>\n\u200b\t&emsp;routes, [Setting the LNet Module routes Parameter](02.06-Configuring%20Lustre%20Networking%20(LNet).", "mimetype": "text/plain", "start_char_idx": 25956, "end_char_idx": 29140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3de678d-4b9e-44eb-a310-595d5b2b4e81": {"__data__": {"id_": "f3de678d-4b9e-44eb-a310-595d5b2b4e81", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dd968c3-c348-49f2-ab30-ed398572267c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "d028e904f2ddc24fed374e01f8aa6bcb989018205748c4936fc5f8eb25481a89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b96eef8-c761-4789-a35a-95b2caa517de", "node_type": "1", "metadata": {}, "hash": "672ef7e06047760b68899904874b63bb3f2104f65bf30c83e9bebb0c21ef0323", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "network yaml syntax, [Network Configuration](02.06-Configuring%20Lustre%20Networking%20(LNet).md#network-configuration)<br>\n\u200b\t&emsp;proc, [Monitoring LNet](06.02-Lustre%20Parameters.md#monitoring-lnet)<br>\n\u200b\t&emsp;route checker, [Configuring the Router Checker](02.06-Configuring%20Lustre%20Networking%20(LNet).md#configuring-the-router-checker)<br>\n\u200b\t&emsp;router yaml syntax, [Route Configuration](02.06-Configuring%20Lustre%20Networking%20(LNet).md#route-configuration)<br>\n\u200b\t&emsp;routes, [Setting the LNet Module routes Parameter](02.06-Configuring%20Lustre%20Networking%20(LNet).md#setting-the-lnet-module-routes-parameter)<br>\n\u200b\t&emsp;routing example, [Routing Example](02.06-Configuring%20Lustre%20Networking%20(LNet).md#routing-example)<br>\n\u200b\t&emsp;self-test, [LNet Self-Test Overview](04.01-Testing%20Lustre%20Network%20Performance%20(LNet%20Self-Test).md#lnet-self-test-overview)<br>\n\u200b\t&emsp;show block, [Show Block](06.08-LNet%20Configuration%20C-API.md#show-block), [The LNet Configuration C-API](06.08-LNet%20Configuration%20C-API.md#the-lnet-configuration-c-api)<br>\n\u200b\t&emsp;starting/stopping, [Starting and Stopping LNet](03.04-ManagingLustreNetworking(LNet).md#starting-and-stopping-lnet)<br>\n\u200b\t&emsp;statistics yaml syntax, [Show Statistics](02.06-Configuring%20Lustre%20Networking%20(LNet).md#show-statistics)<br>\n\u200b\t&emsp;supported networks, [Supported Network Types](02-Introducing%20the%20Lustre%20File%20System.md#supported-network-types)<br>\n\u200b\t&emsp;testing, [Testing the LNet Configuration](02.06-Configuring%20Lustre%20Networking%20(LNet).md#testing-the-lnet-configuration)<br>\n\u200b\t&emsp;tuning, [Tuning LNet Parameters](04.03-Tuning%20a%20Lustre%20File%20System.md#tuning-lnet-parameters)<br>\n\u200b\t&emsp;understanding, [Introducing LNet](02-Introducing%20the%20Lustre%20File%20System.md#introducing-lnet)<br>\n\u200b\t&emsp;using NID, [Using a Lustre Network Identifier (NID) to Identify a Node](02.06-Configuring%20Lustre%20Networking%20(LNet).md#using-a-lustre-network-identifier-nid-to-identify-a-node)<br>\n\u200b\t&emsp;yaml syntax, [YAML Syntax](02.06-Configuring%20Lustre%20Networking%20(LNet).md#yaml-syntax)<br><br>\n\nlogs, [Snapshot Logs](03.19-Lustre%20ZFS%20Snapshots.md#snapshot-logs)<br>\nlr_reader, [lr_reader](06.07-System%20Configuration%20Utilities.md#lr_reader)<br>\nlshowmount, [lshowmount](06.07-System%20Configuration%20Utilities.md#lshowmount)<br>\nlsom<br><br>\n\n\u200b\tenablelsom, [Enable LSoM](03.10-Lazy%20Size%20on%20MDT%20(LSoM).md#enable-lsom)<br>\n\u200b\tintro, [Introduction to Lazy Size on MDT (LSoM)](03.10-Lazy%20Size%20on%20MDT%20(LSoM).md#introduction-to-lazy-size-on-mdt-lsom)<br>\n\u200b\tlfsgetsom, [lfs getsom for LSoM data](03.10-Lazy%20Size%20on%20MDT%20(LSoM).md#lfs-getsom-for-lsom-data)<br>\n\u200b\tusercommands, [User Commands](03.10-Lazy%20Size%20on%20MDT%20(LSoM).md#user-commands)<br><br>\n\nlst, [lst](06.07-System%20Configuration%20Utilities.", "mimetype": "text/plain", "start_char_idx": 28555, "end_char_idx": 31425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b96eef8-c761-4789-a35a-95b2caa517de": {"__data__": {"id_": "3b96eef8-c761-4789-a35a-95b2caa517de", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3de678d-4b9e-44eb-a310-595d5b2b4e81", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "2886781ee45b879e0d502b39f27f525ee0ecda300aa9c70d9c5155d04e62710b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6af00e97-5fb3-4028-87aa-8fac7142816c", "node_type": "1", "metadata": {}, "hash": "74c096d28e7d55d86c92bde129e6910f85630e06862826438ec5540e77a926d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Enable LSoM](03.10-Lazy%20Size%20on%20MDT%20(LSoM).md#enable-lsom)<br>\n\u200b\tintro, [Introduction to Lazy Size on MDT (LSoM)](03.10-Lazy%20Size%20on%20MDT%20(LSoM).md#introduction-to-lazy-size-on-mdt-lsom)<br>\n\u200b\tlfsgetsom, [lfs getsom for LSoM data](03.10-Lazy%20Size%20on%20MDT%20(LSoM).md#lfs-getsom-for-lsom-data)<br>\n\u200b\tusercommands, [User Commands](03.10-Lazy%20Size%20on%20MDT%20(LSoM).md#user-commands)<br><br>\n\nlst, [lst](06.07-System%20Configuration%20Utilities.md#lst)<br>\nLustre, [What a Lustre File System Is (and What It Isn't)](02-Introducing%20the%20Lustre%20File%20System.md#what-a-lustre-file-system-is-and-what-it-isnt)<br><br>\n\n\u200b\t&emsp;at scale, [Lustre Cluster](02-Introducing%20the%20Lustre%20File%20System.md#lustre-cluster)<br>\n\u200b\t&emsp;cluster, [Lustre Cluster](02-Introducing%20the%20Lustre%20File%20System.md#lustre-cluster)<br>\n\u200b\t&emsp;components, [Lustre Components](02-Introducing%20the%20Lustre%20File%20System.md#lustre-components)<br>\n\u200b\t&emsp;configuring, [Configuring a Simple Lustre File System](02.07-Configuring%20a%20Lustre%20File%20System.md#configuring-a-simple-lustre-file-system)<br><br>\n\n\u200b\t\t&emsp;&emsp;additional options, [Additional Configuration Options](02.07-Configuring%20a%20Lustre%20File%20System.md#additional-configuration-options)<br>\n\u200b\t\t&emsp;&emsp;for scale, [Scaling the Lustre File System](02.07-Configuring%20a%20Lustre%20File%20System.md#scaling-the-lustre-file-system)<br>\n\u200b\t\t&emsp;&emsp;simple example, [Simple Lustre Configuration Example](02.07-Configuring%20a%20Lustre%20File%20System.md#simple-lustre-configuration-example)<br>\n\u200b\t\t&emsp;&emsp;striping, [Changing Striping Defaults](02.07-Configuring%20a%20Lustre%20File%20System.md#changing-striping-defaults)<br>\n\u200b\t\t&emsp;&emsp;utilities, [Using the Lustre Configuration Utilities](02.07-Configuring%20a%20Lustre%20File%20System.md#using-the-lustre-configuration-utilities)<br><br>\n\n\u200b\t&emsp;features, [Lustre Features](02-Introducing%20the%20Lustre%20File%20System.md#lustre-features)<br>\n\u200b\t&emsp;fileset, [Fileset Feature](06.07-System%20Configuration%20Utilities.md#fileset-feature)<br>\n\u200b\t&emsp;I/O, [Lustre File System Storage and I/O](02-Introducing%20the%20Lustre%20File%20System.md#lustre-file-system-storage-and-io)<br>\n\u200b\t&emsp;LNet, [Lustre Networking (LNet)](02-Introducing%20the%20Lustre%20File%20System.md#lustre-networking-lnet)<br>\n\u200b\t&emsp;MGS, [Management Server (MGS)](02-Introducing%20the%20Lustre%20File%20System.md#management-server-mgs)<br>\n\u200b\t&emsp;Networks, [Lustre Networks](02-Introducing%20the%20Lustre%20File%20System.md#lustre-networks)<br>\n\u200b\t&emsp;requirements, [Lustre File System Components](02-Introducing%20the%20Lustre%20File%20System.md#lustre-file-system-components)<br>\n\u200b\t&emsp;", "mimetype": "text/plain", "start_char_idx": 30958, "end_char_idx": 33681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6af00e97-5fb3-4028-87aa-8fac7142816c": {"__data__": {"id_": "6af00e97-5fb3-4028-87aa-8fac7142816c", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b96eef8-c761-4789-a35a-95b2caa517de", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "085d223b8a22222085fafb46b4eecedcf31dd15f95466dc2c1b112274ce74793", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d38d39e2-e92b-4206-9281-b1ad4d509fc0", "node_type": "1", "metadata": {}, "hash": "524928e81c06e5c98097bd846be30f219eacc1c6b03c2fdc06d976b0ca3130c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "md#lustre-file-system-storage-and-io)<br>\n\u200b\t&emsp;LNet, [Lustre Networking (LNet)](02-Introducing%20the%20Lustre%20File%20System.md#lustre-networking-lnet)<br>\n\u200b\t&emsp;MGS, [Management Server (MGS)](02-Introducing%20the%20Lustre%20File%20System.md#management-server-mgs)<br>\n\u200b\t&emsp;Networks, [Lustre Networks](02-Introducing%20the%20Lustre%20File%20System.md#lustre-networks)<br>\n\u200b\t&emsp;requirements, [Lustre File System Components](02-Introducing%20the%20Lustre%20File%20System.md#lustre-file-system-components)<br>\n\u200b\t&emsp;storage, [Lustre File System Storage and I/O](02-Introducing%20the%20Lustre%20File%20System.md#lustre-file-system-storage-and-io)<br>\n\u200b\t&emsp;striping, [Lustre File System and Striping](02-Introducing%20the%20Lustre%20File%20System.md#lustre-file-system-and-striping)<br>\n\u200b\t&emsp;upgrading (see [upgrading](#u))<br><br>\n\nlustre<br><br>\n\n\u200b\t&emsp;errors (see [troubleshooting](#t))<br>\n\u200b\t&emsp;recovery (see [recovery](#r))<br>\n\u200b\t&emsp;troubleshooting (see [troubleshooting](#t))<br><br>\n\nlustre_rmmod.sh, [lustre_rmmod.sh](06.07-System%20Configuration%20Utilities.md#lustre_rmmodsh)<br>\nlustre_rsync, [lustre_rsync](06.07-System%20Configuration%20Utilities.md#lustre_rsync)<br>\nLVM (see [backup](#b))<br>\nl_getidentity, [l_getidentity](06.07-System%20Configuration%20Utilities.md#l_getidentity)<br><br>\n\n## M\nmaintenance, [Working with Inactive OSTs](03.03-Lustre%20Maintenance.md#working-with-inactive-osts), [Working with Inactive MDTs](03.03-Lustre%20Maintenance.md#working-with-inactive-mdts)<br><br>\n\n\u200b\t&emsp;aborting recovery, [Aborting Recovery](03.03-Lustre%20Maintenance.md#aborting-recovery)<br>\n\u200b\t&emsp;adding a OST, [Adding a New OST to a Lustre File System](03.03-Lustre%20Maintenance.md#adding-a-new-ost-to-a-lustre-file-system)<br>\n\u200b\t&emsp;adding an MDT, [Adding a New MDT to a Lustre File System](03.03-Lustre%20Maintenance.md#adding-a-new-mdt-to-a-lustre-file-system)<br>\n\u200b\t&emsp;backing up OST config, [Backing Up OST Configuration Files](03.03-Lustre%20Maintenance.md#backing-up-ost-configuration-files)<br>\n\u200b\t&emsp;bringing OST online, [Returning an Inactive OST Back Online](03.12-Managing%20the%20File%20System%20and%20IO.md#returning-an-inactive-ost-back-online)<br>\n\u200b\t&emsp;changing a NID, [Changing a Server NID](03.03-Lustre%20Maintenance.md#changing-a-server-nid)<br>\n\u200b\t&emsp;changing failover node address, [Changing the Address of a Failover Node](03.03-Lustre%20Maintenance.md#changing-the-address-of-a-failover-node)<br>\n\u200b\t&emsp;Clearing a config, [Clearing configuration](03.03-Lustre%20Maintenance.md#clearing-configuration)<br>\n\u200b\t&emsp;finding nodes, [Finding Nodes in the Lustre File System](03.03-Lustre%20Maintenance.md#finding-nodes-in-the-lustre-file-system)<br>\n\u200b\t&emsp;full OSTs, [Migrating Data within a File System](03.12-Managing%20the%20File%20System%20and%20IO.md#migrating-data-within-a-file-system)<br>\n\u200b\t&emsp;identifying OST host,", "mimetype": "text/plain", "start_char_idx": 33154, "end_char_idx": 36060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d38d39e2-e92b-4206-9281-b1ad4d509fc0": {"__data__": {"id_": "d38d39e2-e92b-4206-9281-b1ad4d509fc0", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6af00e97-5fb3-4028-87aa-8fac7142816c", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "71654a23b7694ab7035e415ae2e0164f70d7e31d2e24bb51dd24eaaa3c1a1b93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba305577-4101-4af5-94e7-b241d115030e", "node_type": "1", "metadata": {}, "hash": "326db506609cd4590e15e64826f2ec175ac1d56b538a3961aa4841e577fa7a8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "md#changing-a-server-nid)<br>\n\u200b\t&emsp;changing failover node address, [Changing the Address of a Failover Node](03.03-Lustre%20Maintenance.md#changing-the-address-of-a-failover-node)<br>\n\u200b\t&emsp;Clearing a config, [Clearing configuration](03.03-Lustre%20Maintenance.md#clearing-configuration)<br>\n\u200b\t&emsp;finding nodes, [Finding Nodes in the Lustre File System](03.03-Lustre%20Maintenance.md#finding-nodes-in-the-lustre-file-system)<br>\n\u200b\t&emsp;full OSTs, [Migrating Data within a File System](03.12-Managing%20the%20File%20System%20and%20IO.md#migrating-data-within-a-file-system)<br>\n\u200b\t&emsp;identifying OST host, [Determining Which Machine is Serving an OST](03.03-Lustre%20Maintenance.md#determining-which-machine-is-serving-an-ost)<br>\n\u200b\t&emsp;inactive MDTs, [Working with Inactive MDTs](03.03-Lustre%20Maintenance.md#working-with-inactive-mdts)L 2.4<br>\n\u200b\t&emsp;inactive OSTs, [Working with Inactive OSTs](03.03-Lustre%20Maintenance.md#working-with-inactive-osts)<br>\n\u200b\t&emsp;mounting a server, [Mounting a Server Without Lustre Service](03.03-Lustre%20Maintenance.md#mounting-a-server-without-lustre-service)<br>\n\u200b\t&emsp;pools, [Creating and Managing OST Pools](03.12-Managing%20the%20File%20System%20and%20IO.md#creating-and-managing-ost-pools)<br>\n\u200b\t&emsp;regenerating config logs, [Regenerating Lustre Configuration Logs](03.03-Lustre%20Maintenance.md#regenerating-lustre-configuration-logs)<br>\n\u200b\t&emsp;reintroducing an OSTs, [Returning a Deactivated OST to Service](03.03-Lustre%20Maintenance.md#returning-a-deactivated-ost-to-service)<br>\n\u200b\t&emsp;removing an MDT, [Removing an MDT from the File System](03.03-Lustre%20Maintenance.md#removing-an-mdt-from-the-file-system)<br>\n\u200b\t&emsp;removing an OST, [Removing and Restoring MDTs and OSTs](03.03-Lustre%20Maintenance.md#removing-and-restoring-mdts-and-osts), [Removing an OST from the File System](03.03-Lustre%20Maintenance.md#removing-an-ost-from-the-file-system)<br>\n\u200b\t&emsp;restoring an OST, [Removing and Restoring MDTs and OSTs](03.03-Lustre%20Maintenance.md#removing-and-restoring-mdts-and-osts)<br>\n\u200b\t&emsp;restoring OST config, [Restoring OST Configuration Files](03.03-Lustre%20Maintenance.md#restoring-ost-configuration-files)<br>\n\u200b\t&emsp;separate a combined MGS/MDT, [Separate a combined MGS/MDT](03.03-Lustre%20Maintenance.md#separate-a-combined-mgsmdt)<br><br>\n\nMDT<br><br>\n\n\u200b\t&emsp;multiple MDSs, [Upgrading to Lustre Software Release 2.x (Major Release)](03.06-Upgrading%20a%20Lustre%20File%20System.md#upgrading-to-lustre-software-release-2x-major-release)<br><br>\n\nmigrating metadata, [Migrating Metadata within a Filesystem](03.12-Managing%20the%20File%20System%20and%20IO.md#migrating-metadata-within-a-filesystem), [Whole Directory Migration](03.12-Managing%20the%20File%20System%20and%20IO.md#whole-directory-migration), [Striped Directory Migration](03.12-Managing%20the%20File%20System%20and%20IO.md#striped-directory-migration)\nmkfs.lustre, [mkfs.lustre](06.07-System%20Configuration%20Utilities.md#mkfslustre)<br>\n\n###### monitoring, \n\n[Lustre Changelogs](03.", "mimetype": "text/plain", "start_char_idx": 35445, "end_char_idx": 38492, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba305577-4101-4af5-94e7-b241d115030e": {"__data__": {"id_": "ba305577-4101-4af5-94e7-b241d115030e", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d38d39e2-e92b-4206-9281-b1ad4d509fc0", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f7df3f975156cfe0c2011e04d4c6221eff3775a1e409fdee90e2dba90a64061f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3faf249-779b-4a78-a51e-c73518da42b6", "node_type": "1", "metadata": {}, "hash": "5f52c27034742af1af85f2c0d22a9b054574b2e52df94bfe0ad6d2fae9ea0f31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "md#upgrading-to-lustre-software-release-2x-major-release)<br><br>\n\nmigrating metadata, [Migrating Metadata within a Filesystem](03.12-Managing%20the%20File%20System%20and%20IO.md#migrating-metadata-within-a-filesystem), [Whole Directory Migration](03.12-Managing%20the%20File%20System%20and%20IO.md#whole-directory-migration), [Striped Directory Migration](03.12-Managing%20the%20File%20System%20and%20IO.md#striped-directory-migration)\nmkfs.lustre, [mkfs.lustre](06.07-System%20Configuration%20Utilities.md#mkfslustre)<br>\n\n###### monitoring, \n\n[Lustre Changelogs](03.01-Monitoring%20a%20Lustre%20File%20System.md#lustre-changelogs), [Lustre Jobstats](03.01-Monitoring%20a%20Lustre%20File%20System.md#lustre-jobstats)<br><br>\n\n\u200b\t&emsp;additional tools, [Other Monitoring Options](03.01-Monitoring%20a%20Lustre%20File%20System.md#other-monitoring-options)<br>\n\u200b\t&emsp;change logs, [Lustre Changelogs](03.01-Monitoring%20a%20Lustre%20File%20System.md#lustre-changelogs), [Working with Changelogs](03.01-Monitoring%20a%20Lustre%20File%20System.md#working-with-changelogs)<br>\n\u200b\t&emsp;jobstats, [Lustre Jobstats](03.01-Monitoring%20a%20Lustre%20File%20System.md#lustre-jobstats), [How Jobstats Works](03.01-Monitoring%20a%20Lustre%20File%20System.md#how-jobstats-works), [Enable/Disable Jobstats](03.01-Monitoring%20a%20Lustre%20File%20System.md#enabledisable-jobstats), [Check Job Stats](03.01-Monitoring%20a%20Lustre%20File%20System.md#check-job-stats), [Clear Job Stats](03.01-Monitoring%20a%20Lustre%20File%20System.md#clear-job-stats), [Configure Auto-cleanup Interval](03.01-Monitoring%20a%20Lustre%20File%20System.md#configure-auto-cleanup-interval)<br>\n\u200b\t&emsp;Lustre Monitoring Tool, [Lustre Monitoring Tool (LMT)](03.01-Monitoring%20a%20Lustre%20File%20System.md#lustre-monitoring-tool-lmt)<br><br>\n\nmount, [mount](06.03-User%20Utilities.md#mount)<br>\nmount.lustre, [mount.lustre](06.07-System%20Configuration%20Utilities.md#mountlustre)<br>\nMR<br><br>\n\n\u200b\t&emsp;addremotepeers, [Adding Remote Peers that are Multi-Rail Capable](03.05-LNet%20Software%20Multi-Rail%202.10.md#adding-remote-peers-that-are-multi-rail-capable)<br>\n\u200b\t&emsp;configuring, [Configuring Multi-Rail](03.05-LNet%20Software%20Multi-Rail%202.10.md#configuring-multi-rail)<br>\n\u200b\t&emsp;deleteinterfaces, [Deleting Network Interfaces](03.05-LNet%20Software%20Multi-Rail%202.10.md#deleting-network-interfaces)<br>\n\u200b\t&emsp;deleteremotepeers, [Deleting Remote Peers](03.05-LNet%20Software%20Multi-Rail%202.10.md#deleting-remote-peers)<br>\n\u200b\t&emsp;health, [LNet Health](03.05-LNet%20Software%20Multi-Rail%202.10.md#lnet-health)<br>\n\u200b\t&emsp;mrhealth<br><br>\n\n\u200b\t\t&emsp;&emsp;display, [Displaying Information](03.05-LNet%20Software%20Multi-Rail%202.10.md#displaying-information)<br>\n\u200b\t\t&emsp;&emsp;failuretypes,", "mimetype": "text/plain", "start_char_idx": 37923, "end_char_idx": 40700, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3faf249-779b-4a78-a51e-c73518da42b6": {"__data__": {"id_": "c3faf249-779b-4a78-a51e-c73518da42b6", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba305577-4101-4af5-94e7-b241d115030e", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "223af306f8556339e0f30cbbced3d404f2b8b83bbc98b07ba546d787aaeb274c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79a5f98d-8759-43ac-b2fd-7bc8b2b7dd6a", "node_type": "1", "metadata": {}, "hash": "6c24617f70de38fd9cdbe4d895c482c64552065ffa793bd2c34c419b62126077", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "deleteinterfaces, [Deleting Network Interfaces](03.05-LNet%20Software%20Multi-Rail%202.10.md#deleting-network-interfaces)<br>\n\u200b\t&emsp;deleteremotepeers, [Deleting Remote Peers](03.05-LNet%20Software%20Multi-Rail%202.10.md#deleting-remote-peers)<br>\n\u200b\t&emsp;health, [LNet Health](03.05-LNet%20Software%20Multi-Rail%202.10.md#lnet-health)<br>\n\u200b\t&emsp;mrhealth<br><br>\n\n\u200b\t\t&emsp;&emsp;display, [Displaying Information](03.05-LNet%20Software%20Multi-Rail%202.10.md#displaying-information)<br>\n\u200b\t\t&emsp;&emsp;failuretypes, [Failure Types and Behavior](03.05-LNet%20Software%20Multi-Rail%202.10.md#failure-types-and-behavior)<br>\n\u200b\t\t&emsp;&emsp;initialsetup, [Initial Settings Recommendations](03.05-LNet%20Software%20Multi-Rail%202.10.md#initial-settings-recommendations)<br>\n\u200b\t\t&emsp;&emsp;interface, [User Interface](03.05-LNet%20Software%20Multi-Rail%202.10.md#user-interface)<br>\n\u200b\t\t&emsp;&emsp;value, [Health Value](03.05-LNet%20Software%20Multi-Rail%202.10.md#health-value)<br>\n\n\u200b\t&emsp;mrrouting, [Notes on routing with Multi-Rail](03.05-LNet%20Software%20Multi-Rail%202.10.md#notes-on-routing-with-multi-rail)<br><br>\n\n\u200b\t\t&emsp;&emsp;routingex, [Multi-Rail Cluster Example](03.05-LNet%20Software%20Multi-Rail%202.10.md#multi-rail-cluster-example)<br>\n\u200b\t\t&emsp;&emsp;routingmixed, [Mixed Multi-Rail/Non-Multi-Rail Cluster](03.05-LNet%20Software%20Multi-Rail%202.10.md#mixed-multi-railnon-multi-rail-cluster)<br>\n\u200b\t\t&emsp;&emsp;routingresiliency, [Utilizing Router Resiliency](03.05-LNet%20Software%20Multi-Rail%202.10.md#utilizing-router-resiliency)<br><br>\n\n\u200b\t&emsp;multipleinterfaces, [Configure Multiple Interfaces on the Local Node](03.05-LNet%20Software%20Multi-Rail%202.10.md#configure-multiple-interfaces-on-the-local-node)<br>\n\u200b\t&emsp;overview, [Multi-Rail Overview](03.05-LNet%20Software%20Multi-Rail%202.10.md#multi-rail-overview)<br><br>\n\nmultiple-mount protection, [Overview of Multiple-Mount Protection](03.13-Lustre%20File%20System%20Failover%20and%20Multiple-Mount%20Protection.md#overview-of-multiple-mount-protection)<br><br>\n\n## O\nobdfilter-survey, [obdfilter-survey](06.07-System%20Configuration%20Utilities.md#obdfilter-survey)<br>\noperations, [Mounting by Label](03.02-Lustre%20Operations.md#mounting-by-label), [Snapshot Operations](03.19-Lustre%20ZFS%20Snapshots.md#snapshot-operations)<br><br>\n\n\u200b\t&emsp;create, [Creating a Snapshot](03.19-Lustre%20ZFS%20Snapshots.md#creating-a-snapshot)<br>\n\u200b\t&emsp;degraded OST RAID, [Handling Degraded OST RAID Arrays](03.02-Lustre%20Operations.md#handling-degraded-ost-raid-arrays)<br>\n\u200b\t&emsp;delete, [Delete a Snapshot](03.19-Lustre%20ZFS%20Snapshots.md#delete-a-snapshot)<br>\n\u200b\t&emsp;erasing a file system, [Erasing a File System](03.02-Lustre%20Operations.md#erasing-a-file-system)<br>\n\u200b\t&emsp;failover, [Specifying Failout/Failover Mode for OSTs](03.02-Lustre%20Operations.md#specifying-failoutfailover-mode-for-osts), [Specifying NIDs and Failover](03.", "mimetype": "text/plain", "start_char_idx": 40183, "end_char_idx": 43104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79a5f98d-8759-43ac-b2fd-7bc8b2b7dd6a": {"__data__": {"id_": "79a5f98d-8759-43ac-b2fd-7bc8b2b7dd6a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3faf249-779b-4a78-a51e-c73518da42b6", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "ba13e03dfc190f92f14a843e2f7f3e2f8d6ca04eaff7dbed2b6db87b37d071c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1160ec06-94a7-486c-9d76-8e781dbf2e45", "node_type": "1", "metadata": {}, "hash": "52958bff559b7b4917b4dc23c5a2959ca47a3d7ec81d6d37bfc1e9f7f902dfed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "19-Lustre%20ZFS%20Snapshots.md#creating-a-snapshot)<br>\n\u200b\t&emsp;degraded OST RAID, [Handling Degraded OST RAID Arrays](03.02-Lustre%20Operations.md#handling-degraded-ost-raid-arrays)<br>\n\u200b\t&emsp;delete, [Delete a Snapshot](03.19-Lustre%20ZFS%20Snapshots.md#delete-a-snapshot)<br>\n\u200b\t&emsp;erasing a file system, [Erasing a File System](03.02-Lustre%20Operations.md#erasing-a-file-system)<br>\n\u200b\t&emsp;failover, [Specifying Failout/Failover Mode for OSTs](03.02-Lustre%20Operations.md#specifying-failoutfailover-mode-for-osts), [Specifying NIDs and Failover](03.02-Lustre%20Operations.md#specifying-nids-and-failover)<br>\n\u200b\t&emsp;identifying OSTs, [Identifying To Which Lustre File an OST Object Belongs](03.02-Lustre%20Operations.md#identifying-to-which-lustre-file-an-ost-object-belongs)<br>\n\u200b\t&emsp;list, [List Snapshots](03.19-Lustre%20ZFS%20Snapshots.md#list-snapshots)<br>\n\u200b\t&emsp;mkdir, [Creating a directory striped across multiple MDTs](03.02-Lustre%20Operations.md#creating-a-directory-striped-across-multiple-mdts)<br>\n\u200b\t&emsp;modify, [Modify Snapshot Attributes](03.19-Lustre%20ZFS%20Snapshots.md#modify-snapshot-attributes)<br>\n\u200b\t&emsp;mount, [Mounting a Snapshot](03.19-Lustre%20ZFS%20Snapshots.md#mounting-a-snapshot)<br>\n\u200b\t&emsp;mounting, [Mounting a Server](03.02-Lustre%20Operations.md#mounting-a-server)<br>\n\u200b\t&emsp;mounting by label, [Mounting by Label](03.02-Lustre%20Operations.md#mounting-by-label)<br>\n\u200b\t&emsp;multiple file systems, [Running Multiple Lustre File Systems](03.02-Lustre%20Operations.md#running-multiple-lustre-file-systems)<br>\n\u200b\t&emsp;parameters, [Setting and Retrieving Lustre Parameters](03.02-Lustre%20Operations.md#setting-and-retrieving-lustre-parameters)<br>\n\u200b\t&emsp;reclaiming space, [Reclaiming Reserved Disk Space](03.02-Lustre%20Operations.md#reclaiming-reserved-disk-space)<br>\n\u200b\t&emsp;remote directory, [Creating a sub-directory on a given MDT](03.02-Lustre%20Operations.md#creating-a-sub-directory-on-a-given-mdt)<br>\n\u200b\t&emsp;replacing an OST or MDS, [Replacing an Existing OST or MDT](03.02-Lustre%20Operations.md#replacing-an-existing-ost-or-mdt)<br>\n\u200b\t&emsp;setdirstripe, [Creating a directory striped across multiple MDTs](03.02-Lustre%20Operations.md#creating-a-directory-striped-across-multiple-mdts)<br>\n\u200b\t&emsp;shutdownLustre, [Stopping the Filesystem](03.02-Lustre%20Operations.md#stopping-the-filesystem)<br>\n\u200b\t&emsp;starting, [Starting Lustre](03.02-Lustre%20Operations.md#starting-lustre)<br>\n\u200b\t&emsp;striped directory, [Creating a directory striped across multiple MDTs](03.02-Lustre%20Operations.md#creating-a-directory-striped-across-multiple-mdts)<br>\n\u200b\t&emsp;unmount, [Unmounting a Snapshot](03.19-Lustre%20ZFS%20Snapshots.md#unmounting-a-snapshot)<br>\n\u200b\t&emsp;unmounting, [Unmounting a Specific Target on a Server](03.02-Lustre%20Operations.md#unmounting-a-specific-target-on-a-server)<br><br>\n\nost-survey, [ost-survey](06.07-System%20Configuration%20Utilities.", "mimetype": "text/plain", "start_char_idx": 42545, "end_char_idx": 45474, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1160ec06-94a7-486c-9d76-8e781dbf2e45": {"__data__": {"id_": "1160ec06-94a7-486c-9d76-8e781dbf2e45", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79a5f98d-8759-43ac-b2fd-7bc8b2b7dd6a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "79d437ffb457d049292f5a9bd19786f0a650c705833405167334d4da68ad79bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "326e82a4-2e10-47e1-b272-bb135115ad8a", "node_type": "1", "metadata": {}, "hash": "513d8bdd09a09bba75106858b946688a68e5e19b8925cfaef4e1a5882e82fd5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "02-Lustre%20Operations.md#stopping-the-filesystem)<br>\n\u200b\t&emsp;starting, [Starting Lustre](03.02-Lustre%20Operations.md#starting-lustre)<br>\n\u200b\t&emsp;striped directory, [Creating a directory striped across multiple MDTs](03.02-Lustre%20Operations.md#creating-a-directory-striped-across-multiple-mdts)<br>\n\u200b\t&emsp;unmount, [Unmounting a Snapshot](03.19-Lustre%20ZFS%20Snapshots.md#unmounting-a-snapshot)<br>\n\u200b\t&emsp;unmounting, [Unmounting a Specific Target on a Server](03.02-Lustre%20Operations.md#unmounting-a-specific-target-on-a-server)<br><br>\n\nost-survey, [ost-survey](06.07-System%20Configuration%20Utilities.md#ost-survey)<br><br>\n\n## P\nperformance (see [benchmarking](#benchmarking))<br>\npings<br><br>\n\n\u200b\t&emsp;evict_client, [Client Death Notification](06.01-Lustre%20File%20System%20Recovery.md#client-death-notification)\n\u200b\t&emsp;suppress_pings, [\"suppress_pings\" Kernel Module Parameter](06.01-Lustre%20File%20System%20Recovery.md#suppress_pings-kernel-module-parameter)<br><br>\n\nplot-llstat, [plot-llstat](06.07-System%20Configuration%20Utilities.md#plot-llstat)<br>\npools, [Creating and Managing OST Pools](03.12-Managing%20the%20File%20System%20and%20IO.md#creating-and-managing-ost-pools)<br><br>\n\n\u200b\t&emsp;usage tips, [Tips for Using OST Pools](03.12-Managing%20the%20File%20System%20and%20IO.md#tips-for-using-ost-pools)<br><br>\n\nproc<br><br>\n\n\u200b\t&emsp;adaptive timeouts, [Configuring Adaptive Timeouts](06.02-Lustre%20Parameters.md#configuring-adaptive-timeouts)<br>\n\u200b\t&emsp;block I/O, [Monitoring the OST Block I/O Stream](06.02-Lustre%20Parameters.md#monitoring-the-ost-block-io-stream)<br>\n\u200b\t&emsp;client metadata performance, [Tuning the Client Metadata RPC Stream](06.02-Lustre%20Parameters.md#tuning-the-client-metadata-rpc-stream)<br>\n\u200b\t&emsp;client stats, [Monitoring Client Activity](06.02-Lustre%20Parameters.md#monitoring-client-activity)<br>\n\u200b\t&emsp;configuring adaptive timeouts, [Configuring Adaptive Timeouts](06.02-Lustre%20Parameters.md#configuring-adaptive-timeouts)<br>\n\u200b\t&emsp;debug, [Enabling and Interpreting Debugging Logs](06.02-Lustre%20Parameters.md#enabling-and-interpreting-debugging-logs)<br>\n\u200b\t&emsp;free space, [Allocating Free Space on OSTs](06.02-Lustre%20Parameters.md#allocating-free-space-on-osts)<br>\n\u200b\t&emsp;LNet, [Monitoring LNet](06.02-Lustre%20Parameters.md#monitoring-lnet)<br>\n\u200b\t&emsp;locking, [Configuring Locking](06.02-Lustre%20Parameters.md#configuring-locking)<br>\n\u200b\t&emsp;OSS journal, [Enabling OSS Asynchronous Journal Commit](06.02-Lustre%20Parameters.md#enabling-oss-asynchronous-journal-commit)<br>\n\u200b\t&emsp;read cache, [Tuning OSS Read Cache](06.02-Lustre%20Parameters.md#tuning-oss-read-cache)<br>\n\u200b\t&emsp;read/write survey, [Monitoring Client Read-Write Offset Statistics](06.02-Lustre%20Parameters.md#monitoring-client-read-write-offset-statistics), [Monitoring Client Read-Write Extent Statistics](06.02-Lustre%20Parameters.md#monitoring-client-read-write-extent-statistics)<br>\n\u200b\t&emsp;readahead, [Tuning File Readahead and Directory Statahead](06.02-Lustre%20Parameters.md#tuning-file-readahead-and-directory-statahead)<br>\n\u200b\t&emsp;RPC tunables,", "mimetype": "text/plain", "start_char_idx": 44859, "end_char_idx": 47978, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "326e82a4-2e10-47e1-b272-bb135115ad8a": {"__data__": {"id_": "326e82a4-2e10-47e1-b272-bb135115ad8a", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1160ec06-94a7-486c-9d76-8e781dbf2e45", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "0e4beafd379f3193f13c74ab621b2def1e8dd09ad2d602d671bcf860835d043a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ce74c66-f170-4878-8541-cd18ad94df33", "node_type": "1", "metadata": {}, "hash": "9b8167afbd1e542885d7fb6983b4daba8eabd9a8707d6ca6ada4d1097f205e59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Enabling OSS Asynchronous Journal Commit](06.02-Lustre%20Parameters.md#enabling-oss-asynchronous-journal-commit)<br>\n\u200b\t&emsp;read cache, [Tuning OSS Read Cache](06.02-Lustre%20Parameters.md#tuning-oss-read-cache)<br>\n\u200b\t&emsp;read/write survey, [Monitoring Client Read-Write Offset Statistics](06.02-Lustre%20Parameters.md#monitoring-client-read-write-offset-statistics), [Monitoring Client Read-Write Extent Statistics](06.02-Lustre%20Parameters.md#monitoring-client-read-write-extent-statistics)<br>\n\u200b\t&emsp;readahead, [Tuning File Readahead and Directory Statahead](06.02-Lustre%20Parameters.md#tuning-file-readahead-and-directory-statahead)<br>\n\u200b\t&emsp;RPC tunables, [Tuning the Client I/O RPC Stream](06.02-Lustre%20Parameters.md#tuning-the-client-io-rpc-stream)<br>\n\u200b\t&emsp;static timeouts, [Setting Static Timeouts](06.02-Lustre%20Parameters.md#setting-static-timeouts)<br>\n\u200b\t&emsp;thread counts, [Setting MDS and OSS Thread Counts](06.02-Lustre%20Parameters.md#setting-mds-and-oss-thread-counts)<br>\n\u200b\t&emsp;watching RPC, [Monitoring the Client RPC Stream](06.02-Lustre%20Parameters.md#monitoring-the-client-rpc-stream)<br><br>\n\nprofiling (see [benchmarking](#benchmarking))<br>\nprogramming<br><br>\n\n\u200b\t&emsp;upcall, [User/Group Upcall](06.04-Programming%20Interfaces.md#usergroup-upcall)<br>\n\n## Q\nQuotas<br><br>\n\n\u200b\t&emsp;allocating, [Quota Allocation](03.14-Configuring%20and%20Managing%20Quotas.md#quota-allocation)<br>\n\u200b\t&emsp;configuring, [Working with Quotas](03.14-Configuring%20and%20Managing%20Quotas.md#working-with-quotas)<br>\n\u200b\t&emsp;creating, [Quota Administration](03.14-Configuring%20and%20Managing%20Quotas.md#quota-administration)<br>\n\u200b\t&emsp;enabling disk, [Enabling Disk Quotas](03.14-Configuring%20and%20Managing%20Quotas.md#enabling-disk-quotas)<br>\n\u200b\t&emsp;Interoperability, [Quotas and Version Interoperability](03.14-Configuring%20and%20Managing%20Quotas.md#quotas-and-version-interoperability)<br>\n\u200b\t&emsp;known issues, [Granted Cache and Quota Limits](03.14-Configuring%20and%20Managing%20Quotas.md#granted-cache-and-quota-limits)<br>\n\u200b\t&emsp;statistics, [Lustre Quota Statistics](03.14-Configuring%20and%20Managing%20Quotas.md#lustre-quota-statistics)<br>\n\u200b\t&emsp;verifying, [Quota Verification](03.14-Configuring%20and%20Managing%20Quotas.md#quota-verification)<br><br>\n\n## R\nrecovery, [Recovery Overview](06.01-Lustre%20File%20System%20Recovery.md#recovery-overview)<br><br>\n\n\u200b\t&emsp;client eviction, [Client Eviction](06.01-Lustre%20File%20System%20Recovery.md#client-eviction)<br>\n\u200b\t&emsp;client failure, [Client Failure](06.01-Lustre%20File%20System%20Recovery.md#client-failure)<br>\n\u200b\t&emsp;commit on share (see [commit on share](06.01-Lustre%20File%20System%20Recovery.md#commit-on-share))<br>\n\u200b\t&emsp;corruption of backing ldiskfs file system, [Recovering from Errors or Corruption on a Backing ldiskfs File System](05.02-Troubleshooting%20Recovery.md#recovering-from-errors-or-corruption-on-a-backing-ldiskfs-file-system)<br>\n\u200b\t&emsp;corruption of Lustre file system, [Recovering from Corruption in the Lustre File System](05.02-Troubleshooting%20Recovery.", "mimetype": "text/plain", "start_char_idx": 47308, "end_char_idx": 50407, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ce74c66-f170-4878-8541-cd18ad94df33": {"__data__": {"id_": "2ce74c66-f170-4878-8541-cd18ad94df33", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "326e82a4-2e10-47e1-b272-bb135115ad8a", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "93c5f6374b80a0e92acf2c9711ffe1863adcbbacddb6e60a8ff37f64ab248dd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91919c20-4fb5-4920-8685-cf6c29cfd2b7", "node_type": "1", "metadata": {}, "hash": "4675308c11fa7f0adfb1c94782a49c9ca1967a56a141316610f334b69c702283", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "md#client-eviction)<br>\n\u200b\t&emsp;client failure, [Client Failure](06.01-Lustre%20File%20System%20Recovery.md#client-failure)<br>\n\u200b\t&emsp;commit on share (see [commit on share](06.01-Lustre%20File%20System%20Recovery.md#commit-on-share))<br>\n\u200b\t&emsp;corruption of backing ldiskfs file system, [Recovering from Errors or Corruption on a Backing ldiskfs File System](05.02-Troubleshooting%20Recovery.md#recovering-from-errors-or-corruption-on-a-backing-ldiskfs-file-system)<br>\n\u200b\t&emsp;corruption of Lustre file system, [Recovering from Corruption in the Lustre File System](05.02-Troubleshooting%20Recovery.md#recovering-from-corruption-in-the-lustre-file-system)<br>\n\u200b\t&emsp;failed recovery, [Failed Recovery](06.01-Lustre%20File%20System%20Recovery.md#failed-recovery)<br>\n\u200b\t&emsp;LFSCK, [Checking the file system with LFSCK](05.02-Troubleshooting%20Recovery.md#checking-the-file-system-with-lfsck)<br>\n\u200b\t&emsp;locks, [Lock Recovery](06.01-Lustre%20File%20System%20Recovery.md#lock-recovery)<br>\n\u200b\t&emsp;MDS failure, [MDS Failure (Failover)](06.01-Lustre%20File%20System%20Recovery.md#mds-failure-failover)<br>\n\u200b\t&emsp;metadata replay, [Metadata Replay](06.01-Lustre%20File%20System%20Recovery.md#metadata-replay)<br>\n\u200b\t&emsp;network, [Network Partition](06.01-Lustre%20File%20System%20Recovery.md#network-partition)<br>\n\u200b\t&emsp;oiscrub, [Checking the file system with LFSCK](05.02-Troubleshooting%20Recovery.md#checking-the-file-system-with-lfsck)<br>\n\u200b\t&emsp;orphaned objects, [Working with Orphaned Objects](05.02-Troubleshooting%20Recovery.md#working-with-orphaned-objects)<br>\n\u200b\t&emsp;OST failure, [OST Failure (Failover)](06.01-Lustre%20File%20System%20Recovery.md#ost-failure-failover)<br>\n\u200b\t&emsp;unavailable OST, [Recovering from an Unavailable OST](05.02-Troubleshooting%20Recovery.md#recovering-from-an-unavailable-ost)<br>\n\u200b\t&emsp;VBR (see [version-based recovery](06.01-Lustre%20File%20System%20Recovery.md#version-based-recovery))<br><br>\n\nreporting bugs (see [troubleshooting](#t))<br>\nrestoring (see [backup](#b))<br>\nroot squash, [Using Root Squash](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#using-root-squash)<br><br>\n\n\u200b\t&emsp;configuring, [Configuring Root Squash](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#configuring-root-squash)<br>\n\u200b\t&emsp;enabling, [Enabling and Tuning Root Squash](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#enabling-and-tuning-root-squash)<br>\n\u200b\t&emsp;tips, [Tips on Using Root Squash](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#tips-on-using-root-squash)<br><br>\n\nround-robin algorithm, [Managing Free Space](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\nrouterstat, [routerstat](06.07-System%20Configuration%20Utilities.md#routerstat)<br>\nrsync (see [backup](#b))<br><br>\n\n## S\nselinux policy check,", "mimetype": "text/plain", "start_char_idx": 49803, "end_char_idx": 52675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91919c20-4fb5-4920-8685-cf6c29cfd2b7": {"__data__": {"id_": "91919c20-4fb5-4920-8685-cf6c29cfd2b7", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ce74c66-f170-4878-8541-cd18ad94df33", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "6035fd6f8ef16ab221a9f5d882d4b30c8fd3e03a7a50bc40ba409b7d87a59ede", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "598d2917-1902-48c4-9d74-0804cd1ba6c5", "node_type": "1", "metadata": {}, "hash": "8d26e9c91e1a15a35a39c4bdc8e40b4c21995a771744602c45ac7712d34be309", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Enabling and Tuning Root Squash](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#enabling-and-tuning-root-squash)<br>\n\u200b\t&emsp;tips, [Tips on Using Root Squash](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#tips-on-using-root-squash)<br><br>\n\nround-robin algorithm, [Managing Free Space](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\nrouterstat, [routerstat](06.07-System%20Configuration%20Utilities.md#routerstat)<br>\nrsync (see [backup](#b))<br><br>\n\n## S\nselinux policy check, [Checking SELinux Policy Enforced by Lustre Clients](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#checking-selinux-policy-enforced-by-lustre-clients)<br><br>\n\n\u200b\t&emsp;determining, [Determining SELinux Policy Info](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#determining-selinux-policy-info)<br>\n\u200b\t&emsp;enforcing, [Enforcing SELinux Policy Check](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#enforcing-selinux-policy-check)<br>\n\u200b\t&emsp;making permanent, [Making SELinux Policy Check Permanent](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#making-selinux-policy-check-permanent)<br>\n\u200b\t&emsp;sending client, [Sending SELinux Status Info from Clients](03.18-Managing%20Security%20in%20a%20Lustre%20File%20System.md#sending-selinux-status-info-from-clients)<br><br>\n\nsetup, [Hardware Considerations](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#hardware-considerations)<br><br>\n\n\u200b\t&emsp;hardware, [Hardware Considerations](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#hardware-considerations)<br>\n\u200b\t&emsp;inodes, [Setting Formatting Options for an ldiskfs MDT](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#setting-formatting-options-for-an-ldiskfs-mdt)<br>\n\u200b\t&emsp;ldiskfs, [Setting ldiskfs File System Formatting Options](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#setting-ldiskfs-file-system-formatting-options)<br>\n\u200b\t&emsp;limits, [File and File System Limits](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#file-and-file-system-limits)<br>\n\u200b\t&emsp;MDT, [MGT and MDT Storage Hardware Considerations](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#mgt-and-mdt-storage-hardware-considerations), [Determining MDT Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-mdt-space-requirements)<br>\n\u200b\t&emsp;memory, [Determining Memory Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-memory-requirements)<br><br>\n\n\u200b\t\t&emsp;&emsp;client, [Client Memory Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#client-memory-requirements)<br>\n\u200b\t\t&emsp;&emsp;MDS, MDS Memory Requirements, [Calculating MDS Memory Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.", "mimetype": "text/plain", "start_char_idx": 52117, "end_char_idx": 55322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "598d2917-1902-48c4-9d74-0804cd1ba6c5": {"__data__": {"id_": "598d2917-1902-48c4-9d74-0804cd1ba6c5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91919c20-4fb5-4920-8685-cf6c29cfd2b7", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "373ebbf73d4ab5588459e391f9d7cd48bf48126ea4a79d4ec898357e95bce694", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7171d91d-d981-457d-a6a8-ee96bd810183", "node_type": "1", "metadata": {}, "hash": "185cff5c38fab18329d91dba059ba8ae5eca9435966351738012d5c2adb73159", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Determining MDT Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-mdt-space-requirements)<br>\n\u200b\t&emsp;memory, [Determining Memory Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-memory-requirements)<br><br>\n\n\u200b\t\t&emsp;&emsp;client, [Client Memory Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#client-memory-requirements)<br>\n\u200b\t\t&emsp;&emsp;MDS, MDS Memory Requirements, [Calculating MDS Memory Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#calculating-mds-memory-requirements)<br>\n\u200b\t\t&emsp;&emsp;OSS, OSS Memory Requirements, [Calculating OSS Memory Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#calculating-oss-memory-requirements)<br><br>\n\n\u200b\t&emsp;MGT, [Determining MGT Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-mgt-space-requirements)<br>\n\u200b\t&emsp;network, [Implementing Networks To Be Used by the Lustre File System](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#implementing-networks-to-be-used-by-the-lustre-file-system)<br>\n\u200b\t&emsp;OST, [OST Storage Hardware Considerations](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#ost-storage-hardware-considerations), [Determining OST Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-ost-space-requirements)<br>\n\u200b\t&emsp;space, [Determining Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-space-requirements)<br><br>\n\nsgpdd-survey, [sgpdd-survey](06.07-System%20Configuration%20Utilities.md#sgpdd-survey)<br><br>\n\n###### space,\n\n [How Lustre File System Striping Works](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#how-lustre-file-system-striping-works)<br><br>\n\n\u200b\t&emsp;considerations, [Lustre File Layout (Striping) Considerations](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#lustre-file-layout-striping-considerations)<br>\n\u200b\t&emsp;determining MDT requirements, [Determining MDT Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-mdt-space-requirements)<br>\n\u200b\t&emsp;determining MGT requirements, [Determining MGT Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-mgt-space-requirements)<br>\n\u200b\t&emsp;determining OST requirements, [Determining OST Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-ost-space-requirements)<br>\n\u200b\t&emsp;determining requirements, [Determining Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-space-requirements)<br>\n\u200b\t&emsp;free space, [Managing Free Space](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\n\u200b\t&emsp;location weighting, [Adjusting the Weighting Between Free Space and Location](03.", "mimetype": "text/plain", "start_char_idx": 54621, "end_char_idx": 58029, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7171d91d-d981-457d-a6a8-ee96bd810183": {"__data__": {"id_": "7171d91d-d981-457d-a6a8-ee96bd810183", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "598d2917-1902-48c4-9d74-0804cd1ba6c5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "da73d250041f18b04f9a9657e5f9d99e6c5e09030e9395f3ab8e21a6b336d5fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc92b5ab-90a4-400b-9568-5673c771565b", "node_type": "1", "metadata": {}, "hash": "ea85fae5f22901eb36b6413923155acfc3f23342f57838a8fafae973d4de1025", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "md#determining-mgt-space-requirements)<br>\n\u200b\t&emsp;determining OST requirements, [Determining OST Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-ost-space-requirements)<br>\n\u200b\t&emsp;determining requirements, [Determining Space Requirements](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#determining-space-requirements)<br>\n\u200b\t&emsp;free space, [Managing Free Space](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\n\u200b\t&emsp;location weighting, [Adjusting the Weighting Between Free Space and Location](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#adjusting-the-weighting-between-free-space-and-location)<br>\n\u200b\t&emsp;striping, [How Lustre File System Striping Works](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#how-lustre-file-system-striping-works)<br><br>\n\nstats-collect, [stats-collect](06.07-System%20Configuration%20Utilities.md#stats-collect)<br>\nstorage<br><br>\n\n\u200b\t&emsp;configuring, [Selecting Storage for the MDT and OSTs](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#selecting-storage-for-the-mdt-and-osts)<br><br>\n\n\u200b\t\t&emsp;&emsp;external journal, [Choosing Parameters for an External Journal](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#choosing-parameters-for-an-external-journal)<br>\n\u200b\t\t&emsp;&emsp;for best practice, [Reliability Best Practices](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#reliability-best-practices)<br>\n\u200b\t\t&emsp;&emsp;for mkfs, [Computing file system parameters for mkfs](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#computing-file-system-parameters-for-mkfs)<br>\n\u200b\t\t&emsp;&emsp;MDT, [Metadata Target (MDT)](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#metadata-target-mdt)<br>\n\u200b\t\t&emsp;&emsp;OST, [Object Storage Server (OST)](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#object-storage-server-ost)<br>\n\u200b\t\t&emsp;&emsp;RAID options, [Formatting Options for ldiskfs RAID Devices](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#formatting-options-for-ldiskfs-raid-devices)<br>\n\u200b\t\t&emsp;&emsp;SAN, [Connecting a SAN to a Lustre File System](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#connecting-a-san-to-a-lustre-file-system)<br><br>\n\n\u200b\t&emsp;performance tradeoffs, [Performance Tradeoffs](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#performance-tradeoffs)<br><br>\n\nstriping (see [space](#space))<br><br>\n\n\u200b\t&emsp;allocations, [Stripe Allocation Methods](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#stripe-allocation-methods)<br>\n\u200b\t&emsp;configuration, [Setting the File Layout/Striping Configuration (lfs setstripe)](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#setting-the-file-layoutstriping-configuration-lfs-setstripe)<br>\n\u200b\t&emsp;", "mimetype": "text/plain", "start_char_idx": 57360, "end_char_idx": 60367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc92b5ab-90a4-400b-9568-5673c771565b": {"__data__": {"id_": "dc92b5ab-90a4-400b-9568-5673c771565b", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7171d91d-d981-457d-a6a8-ee96bd810183", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "3e26165c1ef5dd527c79485b87d5a5121b420a6477a55c7b05308340263ec3a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ff0ad3f-e0c6-425e-b74b-446a0da0a2f7", "node_type": "1", "metadata": {}, "hash": "dbae58b03e3ed0f87c3db2a9bb312f2315b5416b916cad94e42ea2878efad986", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "performance tradeoffs, [Performance Tradeoffs](02.03-Configuring%20Storage%20on%20a%20Lustre%20File%20System.md#performance-tradeoffs)<br><br>\n\nstriping (see [space](#space))<br><br>\n\n\u200b\t&emsp;allocations, [Stripe Allocation Methods](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#stripe-allocation-methods)<br>\n\u200b\t&emsp;configuration, [Setting the File Layout/Striping Configuration (lfs setstripe)](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#setting-the-file-layoutstriping-configuration-lfs-setstripe)<br>\n\u200b\t&emsp;considerations, [Lustre File Layout (Striping) Considerations](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#lustre-file-layout-striping-considerations)<br>\n\u200b\t&emsp;count, [Setting the Stripe Count](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#setting-the-stripe-count)<br>\n\u200b\t&emsp;getting information, [Retrieving File Layout/Striping Information (getstripe)](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#retrieving-file-layoutstriping-information-getstripe)<br>\n\u200b\t&emsp;how it works, [How Lustre File System Striping Works](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#how-lustre-file-system-striping-works)<br>\n\u200b\t&emsp;metadata, [Creating a directory striped across multiple MDTs](03.02-Lustre%20Operations.md#creating-a-directory-striped-across-multiple-mdts)<br>\n\u200b\t&emsp;on specific OST, [Creating a File on a Specific OST](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#creating-a-file-on-a-specific-ost)<br>\n\u200b\t&emsp;overview, [Lustre File System and Striping](02-Introducing%20the%20Lustre%20File%20System.md#lustre-file-system-and-striping)<br>\n\u200b\t&emsp;per directory, [Setting the Striping Layout for a Directory](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#setting-the-striping-layout-for-a-directory)<br>\n\u200b\t&emsp;per file system, [Setting the Striping Layout for a File System](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#setting-the-striping-layout-for-a-file-system)<br>\n\u200b\t&emsp;PFL, [Progressive File Layout(PFL)](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#progressive-file-layoutpfl)<br>\n\u200b\t&emsp;remote directories, [Locating the MDT for a remote directory](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#locating-the-mdt-for-a-remote-directory)<br>\n\u200b\t&emsp;round-robin algorithm, [Managing Free Space](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\n\u200b\t&emsp;size, [Choosing a Stripe Size](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#choosing-a-stripe-size)<br>\n\u200b\t&emsp;weighted algorithm, [Managing Free Space](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\n\u200b\t&emsp;wide striping, [Lustre Striping Internals](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#lustre-striping-internals)<br>\n\nsuppressing pings, [Suppressing Pings](06.", "mimetype": "text/plain", "start_char_idx": 59806, "end_char_idx": 62854, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ff0ad3f-e0c6-425e-b74b-446a0da0a2f7": {"__data__": {"id_": "8ff0ad3f-e0c6-425e-b74b-446a0da0a2f7", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc92b5ab-90a4-400b-9568-5673c771565b", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "f4c076f9cf1bb1590f37b97bf44c494955a3dbb8ff656b71cd882023638dad32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c252df82-3700-408f-ac0b-f869fb5c27b9", "node_type": "1", "metadata": {}, "hash": "50943a5d84e96458e8fc356156fb4d9a0a1fe183620e972c4333f56023baf403", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\n\u200b\t&emsp;size, [Choosing a Stripe Size](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#choosing-a-stripe-size)<br>\n\u200b\t&emsp;weighted algorithm, [Managing Free Space](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\n\u200b\t&emsp;wide striping, [Lustre Striping Internals](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#lustre-striping-internals)<br>\n\nsuppressing pings, [Suppressing Pings](06.01-Lustre%20File%20System%20Recovery.md#suppressing-pings)<br>\n\n## T\ntroubleshooting, [Lustre Error Messages](05.01-Lustre%20File%20System%20Troubleshooting.md#lustre-error-messages)<br><br>\n\n\u200b\t&emsp;'Address already in use', [Handling/Debugging \"Bind: Address already in use\" Error](05.01-Lustre%20File%20System%20Troubleshooting.md#handlingdebugging-bind-address-already-in-use-error)<br>\n\u200b\t&emsp;'Error -28', [Handling/Debugging Error \"- 28\"](05.01-Lustre%20File%20System%20Troubleshooting.md#handlingdebugging-error---28)<br>\n\u200b\t&emsp;common problems, [Common Lustre File System Problems](05.01-Lustre%20File%20System%20Troubleshooting.md#common-lustre-file-system-problems)<br>\n\u200b\t&emsp;error messages, [Viewing Error Messages](05.01-Lustre%20File%20System%20Troubleshooting.md#viewing-error-messages)<br>\n\u200b\t&emsp;error numbers, [Error Numbers](05.01-Lustre%20File%20System%20Troubleshooting.md#error-numbers)<br>\n\u200b\t&emsp;OST out of memory, [Log Message 'Out of Memory' on OST](05.01-Lustre%20File%20System%20Troubleshooting.md#log-message-out-of-memory-on-ost)<br>\n\u200b\t&emsp;reporting bugs, [Reporting a Lustre File System Bug](05.01-Lustre%20File%20System%20Troubleshooting.md#reporting-a-lustre-file-system-bug)<br>\n\u200b\t&emsp;slowdown during startup, [Slowdown Occurs During Lustre File System Startup](05.01-Lustre%20File%20System%20Troubleshooting.md#slowdown-occurs-during-lustre-file-system-startup)<br>\n\u200b\t&emsp;timeouts on setup, [Handling Timeouts on Initial Lustre File System Setup](05.01-Lustre%20File%20System%20Troubleshooting.md#handling-timeouts-on-initial-lustre-file-system-setup)<br><br>\n\ntunefs.lustre, [tunefs.lustre](06.07-System%20Configuration%20Utilities.md#tunefslustre)<br>\ntuning, [Optimizing the Number of Service Threads](04.03-Tuning%20a%20Lustre%20File%20System.md#optimizing-the-number-of-service-threads) (see [benchmarking](#b))<br><br>\n\n\u200b\t&emsp;for small files, [Improving Lustre I/O Performance for Small Files](04.03-Tuning%20a%20Lustre%20File%20System.md#improving-lustre-io-performance-for-small-files)<br>\n\u200b\t&emsp;Large Bulk IO, [Large Bulk IO (16MB RPC)](04.03-Tuning%20a%20Lustre%20File%20System.md#large-bulk-io-16mb-rpc)<br>\n\u200b\t&emsp;libcfs, [libcfs Tuning](04.03-Tuning%20a%20Lustre%20File%20System.md#libcfs-tuning)<br>\n\u200b\t&emsp;LND tuning, [LND Tuning](04.03-Tuning%20a%20Lustre%20File%20System.", "mimetype": "text/plain", "start_char_idx": 62293, "end_char_idx": 65194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c252df82-3700-408f-ac0b-f869fb5c27b9": {"__data__": {"id_": "c252df82-3700-408f-ac0b-f869fb5c27b9", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ff0ad3f-e0c6-425e-b74b-446a0da0a2f7", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "7bf109bb50d11a1da94dbd51d9364f57af049c06734e618209079f855a7acd3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d654f4a1-0e2b-403d-963c-60923e3152d5", "node_type": "1", "metadata": {}, "hash": "1e0f829f739c94d4a81bee649c44cc86a1f730739b455099f8ca4a8e2f89e33f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "for small files, [Improving Lustre I/O Performance for Small Files](04.03-Tuning%20a%20Lustre%20File%20System.md#improving-lustre-io-performance-for-small-files)<br>\n\u200b\t&emsp;Large Bulk IO, [Large Bulk IO (16MB RPC)](04.03-Tuning%20a%20Lustre%20File%20System.md#large-bulk-io-16mb-rpc)<br>\n\u200b\t&emsp;libcfs, [libcfs Tuning](04.03-Tuning%20a%20Lustre%20File%20System.md#libcfs-tuning)<br>\n\u200b\t&emsp;LND tuning, [LND Tuning](04.03-Tuning%20a%20Lustre%20File%20System.md#lnd-tuning)<br>\n\u200b\t&emsp;LNet, [Tuning LNet Parameters](04.03-Tuning%20a%20Lustre%20File%20System.md#tuning-lnet-parameters)<br>\n\u200b\t&emsp;lockless I/O, [Lockless I/O Tunables](04.03-Tuning%20a%20Lustre%20File%20System.md#lockless-io-tunables)<br>\n\u200b\t&emsp;MDS binding, [Binding MDS Service Thread to CPU Partitions](04.03-Tuning%20a%20Lustre%20File%20System.md#binding-mds-service-thread-to-cpu-partitions)<br>\n\u200b\t&emsp;MDS threads, [Specifying the MDS Service Thread Count](04.03-Tuning%20a%20Lustre%20File%20System.md#specifying-the-mds-service-thread-count)<br>\n\u200b\t&emsp;Network interface binding, [Binding Network Interface Against CPU Partitions](04.03-Tuning%20a%20Lustre%20File%20System.md#binding-network-interface-against-cpu-partitions)<br>\n\u200b\t&emsp;Network interface credits, [Network Interface Credits](04.03-Tuning%20a%20Lustre%20File%20System.md#network-interface-credits)<br>\n\u200b\t&emsp;Network Request Scheduler (NRS) Tuning, [Network Request Scheduler (NRS) Tuning](04.03-Tuning%20a%20Lustre%20File%20System.md#network-request-scheduler-nrs-tuning)<br><br>\n\n\u200b\t\t&emsp;&emsp;client round-robin over NIDs (CRR-N) policy, [Client Round-Robin over NIDs (CRR-N) policy](04.03-Tuning%20a%20Lustre%20File%20System.md#client-round-robin-over-nids-crr-n-policy)<br>\n\u200b\t\t&emsp;&emsp;Delay policy, [Delay policy](04.03-Tuning%20a%20Lustre%20File%20System.md#delay-policy)<br>\n\u200b\t\t&emsp;&emsp;first in, first out (FIFO) policy, [First In, First Out (FIFO) policy](04.03-Tuning%20a%20Lustre%20File%20System.md#first-in-first-out-fifo-policy)<br>\n\u200b\t\t&emsp;&emsp;object-based round-robin (ORR) policy, [Object-based Round-Robin (ORR) policy](04.03-Tuning%20a%20Lustre%20File%20System.md#object-based-round-robin-orr-policy)<br>\n\u200b\t\t&emsp;&emsp;Target-based round-robin (TRR) policy, [Target-based Round-Robin (TRR) policy](04.03-Tuning%20a%20Lustre%20File%20System.md#target-based-round-robin-trr-policy)<br>\n\u200b\t\t&emsp;&emsp;Token Bucket Filter (TBF) policy, [Token Bucket Filter (TBF) policy](04.03-Tuning%20a%20Lustre%20File%20System.md#token-bucket-filter-tbf-policy)<br><br>\n\n\u200b\t&emsp;OSS threads, [Specifying the OSS Service Thread Count](04.03-Tuning%20a%20Lustre%20File%20System.md#specifying-the-oss-service-thread-count)<br>\n\u200b\t&emsp;portal round-robin, [Portal Round-Robin](04.", "mimetype": "text/plain", "start_char_idx": 64734, "end_char_idx": 67470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d654f4a1-0e2b-403d-963c-60923e3152d5": {"__data__": {"id_": "d654f4a1-0e2b-403d-963c-60923e3152d5", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c252df82-3700-408f-ac0b-f869fb5c27b9", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "b691dad13add40bfb394ccc1ce27887d8dcfa72639a024b523fe4ce56676d610", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba75c3ae-77e7-4503-a0d1-e3021ad87dc4", "node_type": "1", "metadata": {}, "hash": "1a8428ae793bc9e1d3cb4ba9ac2f8e98a2130afbada04fcc7c44917b74be2e97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "&emsp;Target-based round-robin (TRR) policy, [Target-based Round-Robin (TRR) policy](04.03-Tuning%20a%20Lustre%20File%20System.md#target-based-round-robin-trr-policy)<br>\n\u200b\t\t&emsp;&emsp;Token Bucket Filter (TBF) policy, [Token Bucket Filter (TBF) policy](04.03-Tuning%20a%20Lustre%20File%20System.md#token-bucket-filter-tbf-policy)<br><br>\n\n\u200b\t&emsp;OSS threads, [Specifying the OSS Service Thread Count](04.03-Tuning%20a%20Lustre%20File%20System.md#specifying-the-oss-service-thread-count)<br>\n\u200b\t&emsp;portal round-robin, [Portal Round-Robin](04.03-Tuning%20a%20Lustre%20File%20System.md#portal-round-robin)<br>\n\u200b\t&emsp;router buffers, [Router Buffers](04.03-Tuning%20a%20Lustre%20File%20System.md#router-buffers)<br>\n\u200b\t&emsp;service threads, [Optimizing the Number of Service Threads](04.03-Tuning%20a%20Lustre%20File%20System.md#optimizing-the-number-of-service-threads)<br>\n\u200b\t&emsp;with lfs ladvise, [Server-Side Advice and Hinting](04.03-Tuning%20a%20Lustre%20File%20System.md#server-side-advice-and-hinting)<br>\n\u200b\t&emsp;write performance, [Understanding Why Write Performance is Better Than Read Performance](04.03-Tuning%20a%20Lustre%20File%20System.md#understanding-why-write-performance-is-better-than-read-performance)<br>\n\n## U\nupgrading, [Release Interoperability and Upgrade Requirements](03.06-Upgrading%20a%20Lustre%20File%20System.md#release-interoperability-and-upgrade-requirements)<br><br>\n\n\u200b\t&emsp;2.X.y to 2.X.y (minor release), [Upgrading to Lustre Software Release 2.x.y (Minor Release)](03.06-Upgrading%20a%20Lustre%20File%20System.md#upgrading-to-lustre-software-release-2xy-minor-release)<br>\n\u200b\t&emsp;major release (2.x to 2.x), [Upgrading to Lustre Software Release 2.x (Major Release)](03.06-Upgrading%20a%20Lustre%20File%20System.md#upgrading-to-lustre-software-release-2x-major-release)<br>\n\nutilities<br><br>\n\n\u200b\t&emsp;application profiling, [Application Profiling Utilities](06.07-System%20Configuration%20Utilities.md#application-profiling-utilities)<br>\n\u200b\t&emsp;debugging, [Testing / Debugging Utilities](06.07-System%20Configuration%20Utilities.md#testing--debugging-utilities)\n\u200b\t&emsp;system config, [Additional System Configuration Utilities](06.07-System%20Configuration%20Utilities.md#additional-system-configuration-utilities)<br>\n\n## V\nversion<br><br>\n\n\u200b\t&emsp;which version of Lustre am I running?, [Revisions](01-Preface.md#revisions)<br>\n\nVersion-based recovery (VBR), [Version-based Recovery](06.01-Lustre%20File%20System%20Recovery.md#version-based-recovery)<br><br>\n\n\u200b\t&emsp;messages, [VBR Messages](06.01-Lustre%20File%20System%20Recovery.md#vbr-messages)<br>\n\u200b\t&emsp;tips, [Tips for Using VBR](06.01-Lustre%20File%20System%20Recovery.md#tips-for-using-vbr)<br>\n\n## W\nweighted algorithm, [Managing Free Space](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\nwide striping, [File and File System Limits](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#file-and-file-system-limits), [Upgrading to Lustre Software Release 2.", "mimetype": "text/plain", "start_char_idx": 66924, "end_char_idx": 69980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba75c3ae-77e7-4503-a0d1-e3021ad87dc4": {"__data__": {"id_": "ba75c3ae-77e7-4503-a0d1-e3021ad87dc4", "embedding": null, "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb6edc1-9346-4f89-a73c-9c90bb50fa63", "node_type": "4", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "04358e329a196100d8b5d85422f3ca8cf4302b1a105e0519de8432c6b2b65bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d654f4a1-0e2b-403d-963c-60923e3152d5", "node_type": "1", "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}, "hash": "cb576c1b33bd1346ee76648a6be4749f09a3cb2ff1657ea161c7f2362af25ff7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "md#version-based-recovery)<br><br>\n\n\u200b\t&emsp;messages, [VBR Messages](06.01-Lustre%20File%20System%20Recovery.md#vbr-messages)<br>\n\u200b\t&emsp;tips, [Tips for Using VBR](06.01-Lustre%20File%20System%20Recovery.md#tips-for-using-vbr)<br>\n\n## W\nweighted algorithm, [Managing Free Space](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#managing-free-space)<br>\nwide striping, [File and File System Limits](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#file-and-file-system-limits), [Upgrading to Lustre Software Release 2.x (Major Release)](03.06-Upgrading%20a%20Lustre%20File%20System.md#upgrading-to-lustre-software-release-2x-major-release), [Lustre Striping Internals](03.08-Managing%20File%20Layout%20(Striping)%20and%20Free%20Space.md#lustre-striping-internals)<br>\n\n\u200b\t&emsp;large_xattr<br><br>\n\u200b\t\t&emsp;&emsp;ea_inode, [File and File System Limits](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#file-and-file-system-limits), [Upgrading to Lustre Software Release 2.x (Major Release)](03.06-Upgrading%20a%20Lustre%20File%20System.md#upgrading-to-lustre-software-release-2x-major-release)<br>\n\n## X\nxattr<br><br>\n\u200b\t&emsp;See wide striping, [File and File System Limits](02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md#file-and-file-system-limits)<br>", "mimetype": "text/plain", "start_char_idx": 69400, "end_char_idx": 70803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"ca9783fc-3fbd-4905-a578-d7fe4a4cf7bb": {"node_ids": ["3c0710af-28b3-4a98-8400-67e28de0a539", "39559a7a-3157-4f68-bf4c-2eb74bd984ab", "1a671d38-8d1e-4b2e-ae6d-79ceb2a86a5e", "70f9d054-2875-4a2e-af9b-10976de8a456"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/0-Lustre Software Release 2.x.md", "file_name": "0-Lustre Software Release 2.x.md", "file_type": "text/markdown", "file_size": 12931, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "bf8a1ec0-568b-47b0-a7c5-21a48cec8b13": {"node_ids": ["16c1f261-9688-46f2-a408-fa12cb0ff908", "edabca5c-8221-41cb-ab61-c15aa7d90653"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/00-Preface.md", "file_name": "00-Preface.md", "file_type": "text/markdown", "file_size": 4867, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "a65ed932-3b3c-455b-b428-909f95d3dbc2": {"node_ids": ["99e6d9a4-143d-4488-9c4d-b91deaa9ea7f", "990e0472-9948-4c0d-a91e-3e755b9eb13a", "84fb2fa1-598c-46c1-8441-1fed67455e51", "d084acc6-9fdb-409a-9709-a6f5bd2dc2f1", "87c980c0-e858-404a-adcc-7852870d6793", "05262992-1418-4e39-8e59-f9a354c0d2c7", "88998759-6bbf-4408-bb53-ff4522aef293", "098fb67f-4fff-4e8e-8338-9623ed9f1eb4", "0a6634ef-a9c0-43d7-bdac-46e915c27c8d", "1eb984ae-fec3-48e9-8460-7d505fbb44b8", "f61e0631-69fd-49ed-9675-f2c7c6f1aa61", "3a62f12c-182e-48b8-b6e4-9bc24101a989", "e72d7a24-b2fb-4a3c-a4d0-7ad61ba79564"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/01-03. Introducing the Lustre File System.md", "file_name": "01-03. Introducing the Lustre File System.md", "file_type": "text/markdown", "file_size": 40276, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "47977575-fffa-4357-a1c0-cff50282b3fe": {"node_ids": ["60e316ed-db9f-4fbb-b44c-fb21d7823af9"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/04-Installation Overview.md", "file_name": "04-Installation Overview.md", "file_type": "text/markdown", "file_size": 2886, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "22a09abf-8c64-40ef-b0e4-3f0c680c6528": {"node_ids": ["8f8561b6-a25c-4035-aeb6-b7263285504b", "e6882cf8-971f-4033-b3e2-b60f093dd580", "1e5c261f-01e6-4679-a9ba-020d587fe252", "997875b0-8b68-4eb2-b1d0-a77464f21cf6", "bd95e015-3f83-4b82-8cdf-06160241bc94", "56f9f000-2de4-4757-bbbb-36f169265b26", "0b805162-992c-4dbb-a3e5-cf7f71c3236e", "aa4c1ab0-c77f-4351-a209-42bc80883a18", "ac8b0e86-593f-454f-ac21-95982f436246", "4c3f0732-6917-44a4-a33e-a6956ed7de57", "b09a0ca5-72e1-4e96-851e-d212de48b481", "a18efd4e-6f06-4fd7-b994-d2fb42626c28"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_name": "05-Determining Hardware Configuration Requirements and Formatting Options.md", "file_type": "text/markdown", "file_size": 42804, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "777b7a2a-a9f3-4979-b600-eec8a49b8e7d": {"node_ids": ["e105c176-31c7-49fd-ba76-6cbc88c67273", "fbac1618-9d37-4eaf-afef-7bad5a39609a", "ad928673-1a9a-4b70-835b-9bd579452e5c"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/06-Configuring Storage on a Lustre File System.md", "file_name": "06-Configuring Storage on a Lustre File System.md", "file_type": "text/markdown", "file_size": 10504, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "67237423-af56-4120-aa24-70eb93f29297": {"node_ids": ["d0aee2b2-4853-43f7-b674-3ac18a01e8d9", "27217ae4-0599-4cbb-8c2d-d39754903de9", "6a15ccb5-78ba-499f-a01a-3b43a9295015", "9ba2013c-de54-4185-9c98-baee14e2b2ed", "81966ca0-1c09-44d1-a78e-3cba76b65ef3"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/07-Setting Up Network Interface Bonding.md", "file_name": "07-Setting Up Network Interface Bonding.md", "file_type": "text/markdown", "file_size": 12170, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "a6e49f6e-2fe2-46be-a390-6cd9582a5cb2": {"node_ids": ["bb24aeb7-99f2-404e-b3c1-47f2bc628112", "6d9b8bbd-faf9-4797-9672-b1d414b96f9b", "3d8ab82b-4a62-4e06-9299-c7661a4834a2", "f01f2a99-fd5b-4648-aa8e-4f0c579664d9"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/08-Installing the Lustre Software.md", "file_name": "08-Installing the Lustre Software.md", "file_type": "text/markdown", "file_size": 11897, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "2f911bd0-c44f-44b5-b226-560182781bd4": {"node_ids": ["3ff5ddcc-8cb9-4cc3-bb88-ae1986126506", "d8466c53-c24f-4e28-a96d-352cdaa4595e", "efcb45fd-4838-4ab9-925e-deb45eeec8ca", "0cdbcef2-c1ef-4d54-be5b-075258d9e220", "fd807025-95a4-4292-8c9f-949027f31ad9", "f286cdc6-54f9-458d-857e-071b1c754f2f", "edb03e85-e1e6-4883-84c5-7f04c1f7d52e", "f58d8f3c-6a5c-4be9-a51b-579b5193fbbd", "1d2226d4-a3c0-4b0f-b6a8-80d6c7e5b9a7", "e52c40f6-7d2f-4ae2-b851-16552b84da0b", "47085eab-6f62-411e-b472-740c6000cb8f", "45c925c1-251d-4c47-a62e-234c7405e77a", "8007cc36-c2f8-4633-a86c-143f8511294c"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/09-Configuring Lustre Networking (LNet).md", "file_name": "09-Configuring Lustre Networking (LNet).md", "file_type": "text/markdown", "file_size": 39613, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "a9366dfb-6705-4f16-8ac5-76e1945f8dc8": {"node_ids": ["45c0fee2-cc78-40e7-af23-53ec99edf098", "34e65364-5276-4335-ba80-7937a322d9e9", "688b7b6e-0c00-4fa6-88d6-dcf3fbf89bb1", "3de7168f-247a-4265-9390-1eaf79e5baf3", "e069d5ff-744a-41ba-b927-8ffb9781fa71", "f3492345-b073-4336-838a-371332162bbf", "580f4f20-b533-451e-9f4d-41040256ad76", "b3066eb5-9faf-476c-a0f6-d17bcd552755"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/10-Configuring a Lustre File System.md", "file_name": "10-Configuring a Lustre File System.md", "file_type": "text/markdown", "file_size": 22606, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "0bbbec1f-efdf-4525-ba8a-93c7d136290f": {"node_ids": ["af8294bb-953e-4ba7-a128-25c0962b4deb", "61e1d57a-0e7f-44eb-ac51-dd2150203ec5", "8df0a542-59a5-4039-b9e7-70c5ed180bb3"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/11-Configuring Failover in a Lustre File System.md", "file_name": "11-Configuring Failover in a Lustre File System.md", "file_type": "text/markdown", "file_size": 7933, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "bb3d8a82-59bb-4143-bfee-fdc2379b11d6": {"node_ids": ["c225c7ad-7af7-47bb-b644-a6ff481ae9fd", "cfe753d7-fc48-4b93-a301-6f81e7047022", "87ba4671-b829-4c5d-8cfb-a3f6e74c1006", "2b765fd0-82a0-44ad-980d-aec425c5d8a7", "6e5f49d9-4a1e-4cf2-9044-bd2b7c53a4e1", "97ceb643-957e-4ba3-8375-750a353ea863", "2f3efa13-85c7-423c-b003-cc10f0e145fd", "41c9842f-3d0a-4a2c-b5bb-9df0586049a8", "a382b835-c6ef-4ac1-89e1-dcf054ed463d", "a6a1833c-0d62-4f9c-ab24-55d14624bdd9", "0676830f-743b-40b4-af9d-0948b4818846"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/12-Monitoring a Lustre File System.md", "file_name": "12-Monitoring a Lustre File System.md", "file_type": "text/markdown", "file_size": 28375, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "6c068f47-9062-4cc9-803d-e6acfb6c66a0": {"node_ids": ["850eb63d-3f73-47be-a4f8-194429b25a53", "fff02b91-315c-4bda-8221-b5ee521682f3", "5a480064-3ecd-4233-8b6a-e9cd659bd8a1", "4ae89cdf-3a48-417e-be0d-ae50a557964b", "1c7f22bd-f4e4-48a6-9398-339099bcd19b", "89a49ef1-3931-4d51-a21a-0c4aa7385ee8", "00576cf0-08da-4d13-a604-ced094435a92", "169f02aa-7d6b-4ae2-bd3e-6a78d9255178", "55670111-0771-4ded-b490-742332a58dee", "85f1a93c-3f49-4333-88f6-0812e8f8df1d", "8d094437-13ef-4076-bf8c-37dda3427653", "e8fda46a-e6bb-44fe-b59c-cc89ba5e41a0", "d45ccdc5-24bc-4705-aa6e-c099b00aac72"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/13-Lustre Operations.md", "file_name": "13-Lustre Operations.md", "file_type": "text/markdown", "file_size": 32889, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "645b2691-3c1f-4a87-b41a-d620be00ec85": {"node_ids": ["78efda29-5e61-436b-aad0-505d1d6cd97d", "2ae5ac95-2db8-4620-aa1f-092ac89f40ac", "23c3446b-a8c9-4e32-9c40-27dd5bcffeab", "324c4d1a-38ad-4a4a-995e-8879cae54099", "36e2b789-7df7-4f6c-b28d-de01b984a2ae", "6e96e415-7fc6-4a95-ba0c-f59072f0ffdc", "a33c3387-2dd9-4966-ac12-ed75207f98a5", "9911285a-b813-4946-9702-07251119218c", "3c52e911-1f68-4bb8-b478-64ad1e88846e", "57ee31a2-4529-40bc-8a6c-de04c79d481c", "88f18a19-e019-4711-bafe-3477f3895402"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/14-Lustre Maintenance.md", "file_name": "14-Lustre Maintenance.md", "file_type": "text/markdown", "file_size": 34106, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "9fd8cda8-7499-403c-92ee-4b4b4fdbf23a": {"node_ids": ["0c7ef801-325d-4e42-990e-9ee4c2ba11b3", "fc4bb9cf-43f3-4229-8486-2eb33d659acc", "c235149f-381c-49c5-8309-e774a4457fb0", "ab43e416-226a-481a-bdc2-4b92ede3a636", "8f5a2669-15f7-4dc7-a7db-3a253df3c03f"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/15-ManagingLustreNetworking(LNet).md", "file_name": "15-ManagingLustreNetworking(LNet).md", "file_type": "text/markdown", "file_size": 12734, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "fa7edd7f-b6d5-4bd0-97c5-f442424b8fbb": {"node_ids": ["1c7f8342-aff1-4a66-ab67-02c98c9aff6f", "26a1788e-c342-47a4-ae23-22c00611b98c", "afcf90c1-6db2-4858-9311-cb671723f0f6", "3c919a47-5f29-41c5-b7d1-b15d78e8dd8a", "0a0c745a-4ce6-46c7-9097-3ef9d8a4d38b", "0965d9b9-8345-4ac2-9fce-fc4b9eee8918", "95914297-72b6-4758-a223-358c6e8ebb2d", "e7a41d50-04ce-4a68-9d7e-b3ec356957ea", "529c9648-92dd-4acb-8e9e-4abc64947169"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/16-LNet Software Multi-Rail 2.10.md", "file_name": "16-LNet Software Multi-Rail 2.10.md", "file_type": "text/markdown", "file_size": 30971, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "0888add2-b651-4598-9893-5922248a97c9": {"node_ids": ["6ba914da-d7e6-4b83-bc5f-d4afc270dad4", "758c62da-76ab-480e-92bd-39b87a48a406", "e271d3c8-a8e4-41c2-a214-c9e3fbeb81ab", "04ffd1d6-c418-4587-9df1-d372db06dc6f"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/17-Upgrading a Lustre File System.md", "file_name": "17-Upgrading a Lustre File System.md", "file_type": "text/markdown", "file_size": 13071, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "761cdce3-575f-4bdb-94ac-ae187e5ef7e9": {"node_ids": ["97a87b7f-003a-4b97-92eb-623de84a0ada", "78706d88-6079-4cfa-9df0-2cc951081685", "80e00c7f-06a2-48a8-9108-7845ba032f76", "42eceeee-6cc2-4570-8be8-f6e1ff1b2531", "e7f781d4-ccb1-4fdf-a7e8-14684c118690", "6e0cf0d9-6ebf-451c-af03-a5fb0566f715", "4928e89b-90e4-4ebd-a5f5-8fb076e35b35", "14af0eac-4713-406f-9899-10e23567b00d", "09e7ffba-5ab5-45a3-a9c7-6f06464de225", "d746be72-d5ee-4fad-8fb6-185be883418b", "b89bed59-d1d4-43f1-84c2-3e484f521bf8", "f70c5049-1c69-4717-b0a8-f9e55b96637b", "d6077241-ef53-4122-a268-1c30a2f954d5"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/18-BackingUp and Restoring a File System.md", "file_name": "18-BackingUp and Restoring a File System.md", "file_type": "text/markdown", "file_size": 36402, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "5b3ff7df-5b84-46ae-b374-90e4f2cfed78": {"node_ids": ["4ba4f0b0-9b74-4cea-a015-9a6d8e8e1710", "e9059c12-5b98-4e65-9ded-730980f9276d", "06055661-3e92-48f2-83cf-ddf96872f49c", "a73df900-f4b5-4c92-94a1-b35c0c18ef5d", "bfcc1e97-ef78-4850-b7ac-1f9ffa217a33", "5204ea24-9dd2-465e-b720-79d81bdf750b", "426b4faf-6ad8-473a-850a-4c5e3dcf22c9", "d0c601e0-ba51-45f0-b2c8-6c6cccc23221", "afa03b52-1847-426a-8a76-e9f29182f7c6", "49378cae-63ec-4e61-aa89-bef5e683103b", "f8a22a3c-0adf-4397-a82b-3657c343eb35", "f1a2369d-52cd-4b13-9254-b0d43ee89149", "ef7feccf-90c7-4254-8868-1b9aee2e1759", "5109ff20-66d7-4c3d-b305-b5688f11b425", "6d1bcdd4-6ff1-4e18-b37e-d6a8cfad0c7f", "fa08f408-8d6c-4741-80e2-31141bde438d", "67d36f7e-f87b-452a-8490-2a1583a45a4d", "d80970f5-c462-4381-a340-5a1bf000238b", "f83d6f6b-5703-4cd1-bcf9-3db13dcf4a3a", "89481538-57aa-4755-a40f-ca883786125e", "bcd1e421-103e-4735-a200-17803c811991", "3225dfe7-8db0-4070-afb6-7ec70b4a26de", "5994e311-edfc-439b-939b-9df3e60d27f6", "ccff69a2-86d9-4206-8eae-47ab9df1fd31", "59290b1e-1817-4bf7-bee4-ebfac7d14792", "5c543585-de51-4e1a-9275-308e98d43465", "8e2adddf-5564-47d7-812c-c57416a423b7", "01caa303-9e21-42b3-bff5-44b86af5fa73", "2178baf0-2e71-4958-9144-1216f2f1f9c9", "bdcd0665-afef-4d12-84a3-80eb97def906", "80c0a361-b35b-40b6-b798-65ce79ea5f35"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/19-Managing File Layout (Striping) and Free Space.md", "file_name": "19-Managing File Layout (Striping) and Free Space.md", "file_type": "text/markdown", "file_size": 78215, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "a83a9ded-1bf5-47a3-b6d1-9aba91141757": {"node_ids": ["99d3d5ba-d4d0-4259-bc8c-705a0c3678e9", "d5bd9449-2f9f-4ecd-8977-7ba6aab35435", "e24f2a49-8d1b-4530-942b-7167a89bf90b", "f0d07f09-f859-493e-b6cb-5c55c56d668c", "bef8dbb8-829d-49e4-b438-81772ad0d899", "f4aa9d5d-217f-4f40-9bc7-3812d8ce8aa6"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/20-Data on MDT (DoM).md", "file_name": "20-Data on MDT (DoM).md", "file_type": "text/markdown", "file_size": 15631, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "fa0d9664-d941-45d1-8d3c-afe24618fbb8": {"node_ids": ["543bf6bc-1eac-4ae5-bf47-56de26276ade", "5b003578-7bd1-4359-af6c-e8c6b9d49041", "c8f8d992-cf93-4c9e-81de-9729f18de176"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/21-Lazy Size on MDT (LSoM).md", "file_name": "21-Lazy Size on MDT (LSoM).md", "file_type": "text/markdown", "file_size": 7262, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "37222704-0a59-4da0-925a-e2f4ab7c0729": {"node_ids": ["bbad918d-30ae-4ff3-8757-5e7eaa4cfaaf", "bee065b7-2b55-4c0f-840e-80159963033b", "15170e97-1ce3-4a72-843d-4eaa53b9cfaa", "eab2a285-0e6e-4f09-a752-c24cc6fe9a82", "74b23b5e-9aac-44f5-825c-3abf739e1aac", "ee3f4a17-ccb1-4467-ac74-247980d83251", "73c8af7c-febb-4309-bd71-f09475fdfd85", "436b5466-ee41-4372-9565-a3e3131c2949", "8283437d-d295-44bf-8eff-70868a7153d6", "ce4ce2f7-8911-437a-86c3-50d1e21497f3", "41054fef-3fab-4adb-9ac4-88a24e989a66", "cfdd6e3e-9c2a-4e29-8c73-7564c5397aa4", "c7448192-91d4-4829-bfaf-af421667b63c", "35fd6598-17c4-4583-8762-ac9e23dcaee4", "25514e69-c941-4475-95c5-dfa182eaee1c", "2c2b1a7a-31ac-49c0-a2ed-38379e4df154", "9a00cc16-ae52-423a-a5a1-6d33128bd6a1", "7a760f3d-88a7-4517-8486-c51269a38888"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/22-File Level Redundancy (FLR).md", "file_name": "22-File Level Redundancy (FLR).md", "file_type": "text/markdown", "file_size": 43562, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "a35258f4-eb58-4e5a-b66b-4814eca7f5b0": {"node_ids": ["0cdb5fde-7a5d-442b-a88e-6ad85806fad2", "9bc4fd67-9769-4da1-bbab-12760a299a4d", "737c143f-2576-4f45-a854-e39a5ade0454", "6ff60224-6fb9-41f0-8c34-92ce4ede4172", "b6e6edbe-b8e5-4c4f-bf19-7b8a1ff405b3", "ac88c352-89cb-4475-8269-132388625273", "6b800ea5-0623-4e7e-a5d5-ae6b0f83adb8", "7518710f-9ca7-4291-9ba3-f007d085ef0d"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/23-Managing the File System and IO.md", "file_name": "23-Managing the File System and IO.md", "file_type": "text/markdown", "file_size": 24097, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "67c64319-6584-4bdc-b600-62c718831df6": {"node_ids": ["6ec64c04-8560-473f-b0f8-a38b2e3251fe"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/24-Lustre File System Failover and Multiple-Mount Protection.md", "file_name": "24-Lustre File System Failover and Multiple-Mount Protection.md", "file_type": "text/markdown", "file_size": 3302, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "f3322b57-2eb6-4c07-87d5-5d7ec98e81cf": {"node_ids": ["70dfe8e7-61f0-4329-8e86-bba6c99b3384", "c2e3055a-18ce-4694-9f0d-96b35b5732b0", "be88f882-9e95-4063-b60b-ea860976d866", "ae7ea765-7866-4f26-9f1b-64e116e1241f", "ac99226f-31a3-4bad-9db9-363ac8bac564", "8ae994c4-2e69-45a9-bfcf-029cb6b69e20", "a64024c2-d8b2-40a9-9b41-165050b39e6b", "5f4c9d8e-4d88-4a1f-90ea-d8721ff2c572", "47c151d5-aa4f-482d-8278-9923dd87a84c", "d0234c91-95c9-48eb-9c53-dcdbbd4be96a"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/25-Configuring and Managing Quotas.md", "file_name": "25-Configuring and Managing Quotas.md", "file_type": "text/markdown", "file_size": 30551, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "f01c8240-32d1-4591-ac57-6a7a39cc34a8": {"node_ids": ["4694d615-158c-4a50-9edb-cf2069562aad", "b8f89da6-3dfc-4280-adb3-fec694077ca5", "b8a9f542-64c3-4e43-8ed0-89b3db7c222b", "6a2a71f4-2a66-490a-9864-9ceed73b855d", "a0985e58-e9bd-406a-a937-97aa977d2707"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/26-Hierarchical Storage Management (HSM).md", "file_name": "26-Hierarchical Storage Management (HSM).md", "file_type": "text/markdown", "file_size": 13966, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "43e7b8fa-6a6d-454e-acf9-0a63a9e35e29": {"node_ids": ["cd554e57-cbe8-43fd-8a3d-99e34ba1d830", "7b331903-ef04-46bf-ad5f-2c7cabe52273", "7012fb65-3f50-4799-8bd5-f011dfb15c31", "a0d96c8d-593b-4d86-add1-ef04496a7171", "14493710-ca01-4a67-aae9-b89acc28ca0b"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/27-Persistent Client Cache (PCC).md", "file_name": "27-Persistent Client Cache (PCC).md", "file_type": "text/markdown", "file_size": 14470, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "984e3dd9-2de7-40aa-bcc8-6b78d92cf2d2": {"node_ids": ["f93ab89e-cda3-4f5a-afde-101d5349ad25", "a2053816-b9eb-4574-8b06-7bbf7faaa21e", "374d567e-9228-4288-bc7d-9b9d5118cc3d", "ea62810b-4b18-4659-b0e5-c7d34e7ee8f4", "c91a1b06-ae3e-41f3-b7d2-b3d65353b4a3", "47c61460-18e3-4c24-8063-e675db1a7b93"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/28-Mapping UIDs and GIDs with Nodemap.md", "file_name": "28-Mapping UIDs and GIDs with Nodemap.md", "file_type": "text/markdown", "file_size": 16830, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "8fe9bbcf-6085-4523-a8c0-b662c60ba81d": {"node_ids": ["500cba4f-d799-4781-8e18-44990af993bf", "97520c7d-6b47-4583-bdf0-c111eb12421b", "2e53743b-fa8b-4b25-9e82-67f189d4cfbe", "4ecbf791-d6cf-4d54-8cd0-017baa7c34e3", "e12e8e0f-3b9c-43ec-af99-5dd69231a90b", "4da52265-8c66-44ba-be49-47e16fcf8b71", "955d252e-8d72-4be9-87eb-66d9f5f08f2e", "4f990239-3787-458e-a0ed-09b3d54d702c", "727175eb-c8a0-421c-8c00-82eea3ed13bc", "4f4d9103-e696-4ce0-8ad8-a0b921a123b3", "6d1f4446-4ad9-4743-9cdb-333265814353", "18d32423-84bc-46dc-9aa5-046a8cdb2759", "e67619a3-0fd3-416d-899f-58d0a2a848cf", "e68caf36-73a2-4ceb-8f72-07334ca8152e"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/29-Configuring Shared-Secret Key (SSK) Security.md", "file_name": "29-Configuring Shared-Secret Key (SSK) Security.md", "file_type": "text/markdown", "file_size": 35388, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "5b986ca8-7311-4014-8513-80f735259c79": {"node_ids": ["45af6caf-a2a2-43c7-b8ec-1d951d70e6b1", "3232d1da-0ba6-40fc-905c-67797e14dedd", "b17e48e7-1e41-4576-a65a-b2141bca4d26", "1b0b8101-feaf-4b95-8b18-3bfbcb7c7576", "260f57d6-fdf5-4982-aeae-0088bc3a6fe9", "7e05ad97-7c80-4f0e-9930-928a5a937f64", "81e0d73d-72fd-4060-a924-6b0f880dcec1", "07494a65-fa94-43c6-bb5d-505af52af2d4", "79e428fe-eceb-4dcc-8653-142fadd62c84", "07914103-c94a-439a-95d2-f2f9081034d8", "325566b4-d0f8-4a1a-a101-523b573da843", "d2497dfb-87e4-469e-b917-5938bf600c1e", "e04316a3-9431-4a00-815f-eea66d935b72", "aca219f7-5935-40fe-9e19-1336f1cc7867"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/30-Managing Security in a Lustre File System.md", "file_name": "30-Managing Security in a Lustre File System.md", "file_type": "text/markdown", "file_size": 45293, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "b88264eb-e9c4-4f08-9be9-eb423dd36b19": {"node_ids": ["20bc7b11-c15c-4eb1-91db-fd05353848af", "3072836d-b160-46b4-87dc-275d9713e2a6", "6162b0e9-1cf7-451b-86db-b807f7a0d50a", "c17f761a-5422-43e5-8923-77324d6f9c98", "ea23b2d1-7e3c-4af0-a085-16cbfa1cce2a"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/31-Lustre ZFS Snapshots.md", "file_name": "31-Lustre ZFS Snapshots.md", "file_type": "text/markdown", "file_size": 17362, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "7d617e61-6c89-4bb9-a949-0006b8fd89f5": {"node_ids": ["b73853a3-4cab-444d-9984-5ebe0d6ebbc4", "035e36cf-d71e-413b-9e80-0285118eb183", "0c613bba-1dde-460e-b307-9f46b7238ef5", "69f23c0a-4913-42a8-98dc-e6dc594da9db", "29be9a0a-7c85-4056-becb-ff538025c8eb", "af4d7fa8-d26a-4baa-9ed0-be56a752cfa7", "2a8d8bc9-fe55-4333-b713-daa2a965a34e", "492b0191-9870-4e04-9a6d-ef25b9b490d5", "f01f6609-2e15-4b2b-90cd-ac68bbd9c800", "9c781917-1bea-42f6-ba8a-619a13dcd7ad"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/32-Testing Lustre Network Performance (LNet Self-Test).md", "file_name": "32-Testing Lustre Network Performance (LNet Self-Test).md", "file_type": "text/markdown", "file_size": 28899, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "dea0950f-ca76-4df0-9c7c-70e604fd5f14": {"node_ids": ["464a55e4-48af-405c-a8d6-ec93289db24f", "bb42bff2-8e2c-4867-9311-c53e6ffa5aed", "e31f2ba0-fdb2-4dbb-9e16-a62dc367f0cc", "2c41aa1c-442c-4a2a-84cf-8cd0800036c8", "f3376098-ec3d-4c5b-9f7e-10a1b51a3d52", "61c1da22-0d52-4893-a2de-6ee03d524084", "0ca26635-e8e4-488b-bf27-eae09c16899d", "9686f81e-bf5a-4a13-9a37-f9e79a87bcb9", "de46db37-e19d-4f14-89a4-5bb1253f3241", "a613ba78-a4bc-4cf6-9c1a-659ad2d7dcc3"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_name": "33-Benchmarking Lustre File System Performance (Lustre IO Kit).md", "file_type": "text/markdown", "file_size": 32441, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "dcc81bc0-236c-4ad3-a95f-a0ec7a928645": {"node_ids": ["a2a6219e-cedd-431e-bd8c-250bb45f0f18", "ec13075d-7b85-4b22-a524-49e499df6cde", "53b652f1-4066-4ad9-84dd-830c5689f4e5", "77bc8d10-ae76-446f-becf-c5068e71c580", "ec42e894-1708-49ed-a740-fb33442aaa14", "ae54a7f7-6482-480d-bba9-74d9a63a8d5b", "4be836c2-5785-4d22-b85e-ee403b72b180", "3db5f6a8-287f-4a01-9514-4cd65c4fe0e3", "98cdaeb3-bb01-4f21-9979-b891f090eb0e", "b1e9a8b8-f0c1-4d86-a7c6-350eaca359b3", "e1190fe6-b130-40a3-a14e-d54ec696349c", "75157706-cebc-48b7-b47a-4033eaa07997", "a7f8500a-8f5b-46d6-98f8-0f7b5f35b110", "dbc4d4d3-4a74-4974-a6b1-35f212b59ddc", "c7aea255-df11-43fe-9485-423bea33c358", "75128155-3564-440e-b39f-e13abf77c8e6", "769ef2a2-22b4-41ed-bb8d-6482c4c95574", "4fa1029f-9ae1-45c1-82d9-196f4786204f", "2f158caa-8625-4ee3-96ba-4789d5ee53e4", "6a59ad66-2d74-48ad-ae9c-2a458860adb8", "459f8772-f83c-49fb-863e-ebd29a3bdb03", "b9d963b6-9a11-4bd3-adcf-86a510dc2ac8", "653444f8-2f28-4c86-84be-fa5755f9ad79", "24b29f16-da27-4188-8e59-c27e3184fb61"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/34-Tuning a Lustre File System.md", "file_name": "34-Tuning a Lustre File System.md", "file_type": "text/markdown", "file_size": 69579, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "5172067a-b18b-4bb7-8d1a-9cf62a4cfa18": {"node_ids": ["f3bfbce9-4dc3-4acd-b10e-f42c3a0c12e2", "25fb1c16-d6cc-4b16-80bb-9e1f45c5a2a2", "b345a187-792c-4486-9fc0-d0c08c7b3d1a", "6d97ea8f-d920-425a-aae5-f1e3acfb4cf0", "8c9b5cf8-8726-4a64-9026-bb3c3516edc8", "4ddb765e-bef5-46aa-810d-c49fd0426b9d", "53a165a5-b007-4d8b-b848-d1d192a391aa", "0c8b38db-8a7c-492b-879a-e70e1a4336db", "093a0a9d-5f90-45fa-b381-86e79679e187", "e7fd9b77-7787-4c1f-a1ed-8a7afda3ffd8"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/35-Lustre File System Troubleshooting.md", "file_name": "35-Lustre File System Troubleshooting.md", "file_type": "text/markdown", "file_size": 30635, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "cff75fac-b53b-4236-96bd-53e27bb85409": {"node_ids": ["a5fcd38d-e8ef-4c3d-b3e3-489a82ce624e", "52afaf24-82ed-453d-8253-c5839bbceddb", "4280eac5-bc8c-42ec-954f-1743ecd86131", "cbc2e97c-2895-43e8-b4f6-877f684f93e5", "1291463a-80ab-4f81-8935-7df59e9a2c64", "3f189d9f-5d20-419b-841b-d7f2b85a3235", "b6eb837f-45bd-42d5-929a-82e505e2c47c", "0426c5b3-ac50-4a0b-bf30-28c4ef7c0c88", "3f72eb89-bf6c-42e7-9738-de311fde642d", "8f6e846f-cad9-40e6-bb21-8ffbd5615435"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/36-Troubleshooting Recovery.md", "file_name": "36-Troubleshooting Recovery.md", "file_type": "text/markdown", "file_size": 44560, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "c98af996-876a-4161-b507-b7f760731c52": {"node_ids": ["79fd59a1-0f67-4259-95d0-9593eddb2979", "e6bd3b87-622a-4b94-bd18-1cf5490321f2", "771620ec-5238-4304-aa8d-540848257809", "fd7da413-0f58-4831-8192-83066ed725b5", "4c469705-e1c5-418e-b1e0-e40a215c9857", "55c2ee46-422a-47fb-ba08-a90f897c30e6", "03c25273-8887-4cda-976f-c061b6d442be", "68180694-3e1d-4002-9d96-522b76ecd18f", "8ab42aa1-762a-43b6-ae4f-9241ac5b56db", "94780ac9-a5d4-4faa-ac54-be9ac8408ca2", "de6b6565-4653-4051-b0fd-509673094b31"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/37-Debugging a Lustre File System.md", "file_name": "37-Debugging a Lustre File System.md", "file_type": "text/markdown", "file_size": 35810, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "d8fcf2da-aa3c-4baa-8756-3a8bfdcb03d1": {"node_ids": ["55347ab6-1108-482d-ab43-548ed534e865", "8575c6da-600e-4aee-9e5e-c3d974cc49b8", "8c26a9ce-334c-4348-8f53-a4d55a0b7d12", "8069e7d9-0b7d-418f-9b52-4da09682b7d2", "158dd72e-689f-4485-b4cd-6cc55f14dffa", "0cddf58d-b096-4684-8060-9c47d1338342", "e21dba46-ecd5-4089-8416-247339c9b6b2", "acdcc031-14ca-4291-b81d-2312b00d3d13", "dd4654a1-8ea5-4074-8cac-3d77074b1767", "f043b67b-01a5-4b06-b43c-d1ee02c25170", "62d79756-d8c9-422b-acd9-0ba998c6d162", "fb72eaaf-ade5-4c07-8b12-df384333a32b"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/38-Lustre File System Recovery.md", "file_name": "38-Lustre File System Recovery.md", "file_type": "text/markdown", "file_size": 45255, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "f13aa227-5c7b-43f8-8e13-41665aef4267": {"node_ids": ["7a81acd2-ae13-4df8-b38b-46b2a7069503", "f90c1baa-80d1-4e8d-88ff-db4fa0baee38", "b198a0b8-1dee-44bf-8fd2-1e7a6cb721e0", "711b0b2a-c247-48ab-b9ae-0b2f7ef3376e", "f41449d3-0812-4d39-8a70-b2dc1f0baf70", "bf25aa15-c988-428b-b362-e61342ad4e63", "0a58cacc-7968-44c4-afcb-78c33ab1f5b6", "63168870-1025-44bc-b22f-87ac8190c3b8", "c8cecca9-eafc-4bf0-b3ed-e7f26954e26f", "d1812762-419a-45d7-876c-35ea1f2cd008", "7288a503-3adf-425f-b755-ede9169970dc", "418eaabd-57dd-45c3-8d13-02e5a47d941d", "c61082d9-b1cb-4af1-96cc-845e7027aae8", "c212ba32-d78c-4e9e-b894-68a0fd6363a9", "e36592a5-5f66-4fb7-9b35-78e111ec7192", "92f7a61e-d750-4a9e-a119-75ad62738585", "ce9db1af-3c90-40d9-bfe0-be3aaca2470d", "e1912527-e3e0-4212-8be5-fbe640d49244", "16042130-d987-4e09-ab98-c62f1032782a", "985ee1c8-15ae-4481-b610-1452034f970b", "e31e850a-a069-44c2-a904-0d8f38b57c45", "99e9aaa6-a99c-4893-8e80-494b3ab92314", "0ecf32d5-1f81-4826-a762-dd2328a8f8c0", "afbef011-e553-4f2e-8b82-95c02380243b", "1428a947-0b91-4c1b-9042-05dc93c55c0f", "8f98d5f2-3512-45bc-8316-047df59c3bfe", "a3d99e67-2928-4caf-8e15-b2e796767ce5", "8421f599-df9f-4940-b7e8-102d2fe80f2a", "f8dbfe61-bcfd-43c4-a0dc-892717c00341", "ef810dae-afea-41ef-9c96-acb5c588b910", "341f7f39-897d-431f-a822-05346c2f06a9"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/39-Lustre Parameters.md", "file_name": "39-Lustre Parameters.md", "file_type": "text/markdown", "file_size": 89074, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "2abb3b47-73ec-4f1c-8aa3-be8b9d0cc479": {"node_ids": ["cf8e5da9-8aff-43dd-a423-661803a1bf5c", "ef22e630-71f4-4fb3-9927-54e882dc31d0", "bff3eb95-0e97-49d4-b008-e836adaa76b2", "a2f4a512-beb1-4d9a-8509-6d0f9a4c4d48", "b3291a1b-e1f9-438f-be38-11bc31513976", "8bd52d44-4e46-401e-bf97-41825fcb2955", "40d42af7-e390-4057-a7d4-d9e25e6d22b3", "7a9c618f-fd1e-4ead-98e4-58cfbb669a04", "074dd052-0636-443f-9ac8-53eaa76ddfdd", "10e6fb7d-a3ca-4e23-91e2-288b52556e32"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/40-User Utilities.md", "file_name": "40-User Utilities.md", "file_type": "text/markdown", "file_size": 35577, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "2a4f6215-823c-41aa-9fdb-a39e2944433f": {"node_ids": ["f9a34abb-43de-4671-b277-a483dacf3813", "50681729-1b26-434e-a7a1-b3374e83150b"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/41-Programming Interfaces.md", "file_name": "41-Programming Interfaces.md", "file_type": "text/markdown", "file_size": 4426, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "4abc006a-7fe9-4567-a652-25157dbb2a76": {"node_ids": ["9e570bbb-dda6-43c6-a96e-7482602f3c63", "6ed96385-8787-408a-89de-630559f85dde", "805e0cb0-a020-4fab-88d1-3575f69b6684", "ed6544c1-11ec-45a5-bf0d-9d4b7ebfcbb3", "f2890be5-78c3-4a3b-a90d-816749ed643c", "97ce037c-aa05-4ddf-b60f-406f02a30279", "083b98a1-e73e-45b1-a255-ba723e128e20", "6b5fb6b8-1035-4281-b23d-892df0040323", "0c62bf41-01d7-4136-b751-2040b5a5bd96"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/42-Setting Lustre Properties in a C Program (llapi).md", "file_name": "42-Setting Lustre Properties in a C Program (llapi).md", "file_type": "text/markdown", "file_size": 26575, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "93b33121-2589-4655-af52-aff6aa8c7f68": {"node_ids": ["16401bc9-28ee-4f62-8a85-4da9fbcc8032", "82d40da6-b720-4b2d-81e6-aa3894e7d04e", "67def0d8-13cc-4f6a-9a89-84a851c12563", "bb996c0a-6f56-49f0-850a-6bb628f2acbe", "6c479a44-b30b-4aab-9044-346302dd87c6"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/43-Configuration Files and Module Parameters.md", "file_name": "43-Configuration Files and Module Parameters.md", "file_type": "text/markdown", "file_size": 16493, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "ea80abee-1c6e-4eea-8975-8c7fd00b547d": {"node_ids": ["88c420f8-1687-4fb9-b6d6-43ae22ea68f1", "6e1b7b28-d382-4173-9b3d-b397c307eacf", "01b7c2b2-afa3-420b-ad2c-24e6007ad5af", "342e5a68-3eb2-4671-a622-cdb2a9682656", "fc30232d-1547-4cf2-97db-faf8ef9d49da", "4db9c634-3499-48e7-962b-3ff8f136ad39", "da9f107b-e586-4aed-8079-16cb8f982e18", "c8075030-7004-4aa5-bd47-c77608eecda7", "8342151c-1130-438b-be50-20a5fef97bfc", "7f5485ff-2186-4340-a583-84eae6d55c51", "168e9538-c94a-4889-bbc2-c6b9cb100883", "0664337e-3549-4889-afd1-c0292651989d", "69d3cd26-bc8d-41f0-99cd-d54bfc9a0517", "8bca2bd5-6a4e-48e3-b1e5-f9afdace211b", "7e785e8c-6599-4349-ac2a-d9579f023c83", "af0b4c87-d136-4e38-862d-9592d10f5d73", "fc5908e2-5cf8-49a0-bb59-c1fc6b404c91", "1c2bad15-8530-4158-8cf2-3a04faa7baad", "7d84abdb-0609-4f2f-84f2-d7bc7af44eb7", "c525478c-1612-4cf2-b299-e636e2c0f8fd", "1bcb5c2f-8833-4c64-868e-9f74ab54d0c4", "86c097b9-3b5c-434a-87d9-d0b81a3db21e", "77b25d47-0888-437d-ab87-3b6995d19628", "df319bff-9b78-482b-9e10-9d9340ac65c8"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/44-System Configuration Utilities.md", "file_name": "44-System Configuration Utilities.md", "file_type": "text/markdown", "file_size": 75801, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "368eac03-b394-4c04-8d4b-1af2f2abc607": {"node_ids": ["3168d81b-22e0-475b-b4b5-bafb597a4158", "ceccb3ae-d26a-446d-bece-8c2231aa98a9", "0b71f8bd-15aa-4263-be78-c0e85fda6d71", "835aa024-1a5d-422c-b548-fb5b6aaa1f5c", "68cb63d7-e086-4811-99fc-151ed16c7c03", "1d561428-2c47-4640-a9a0-032ad23b0f28", "49111234-4189-4cd2-a5b9-cd7694303271", "4e7af7dc-09db-4b5e-b7c0-bdc780cc5dc5"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/45-LNet Configuration C-API.md", "file_name": "45-LNet Configuration C-API.md", "file_type": "text/markdown", "file_size": 22374, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "3e6d6384-67cd-4be0-a9c8-de3e325d60ca": {"node_ids": ["9767b782-3f7e-4d39-8cbb-ba918fa1018a", "d2a9f83b-5765-4eeb-a7aa-3171d86d8b09", "e11c8992-262b-4b3c-91a7-422ff0264593", "38040e85-accc-42a8-8d37-fd7f6d603588", "eb938ff6-e589-43f7-999a-fb8f1457defb", "0b0a89a1-1320-43ba-a4e6-1b840702a77d"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Glossary.md", "file_name": "Glossary.md", "file_type": "text/markdown", "file_size": 19490, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}, "4fb6edc1-9346-4f89-a73c-9c90bb50fa63": {"node_ids": ["4130f604-b195-4f36-a755-143f4b00764f", "b4cc1a85-b6f8-4d82-975a-9969cbf942a9", "d24a66e8-f7ed-4873-9ffb-b73434bf7f30", "514bd2a8-4598-4b42-ae08-98c77cfe99ab", "d5b40e25-57f4-4984-afa1-5b32ca670bcc", "acb05411-eff8-48a8-b879-6807ca2fa975", "6c1e4c3d-a0c2-4a73-a9be-c689603744d8", "3ef9277f-62d4-47f4-ad44-1d92a5c1f6ba", "006af9de-dfb0-47bf-9e9b-ab1ee67b4d8c", "8be77cec-7088-48d8-a12e-3fd61f754702", "88f3d31d-b1ae-453c-9195-e10c6c6cbaa8", "1dd968c3-c348-49f2-ab30-ed398572267c", "f3de678d-4b9e-44eb-a310-595d5b2b4e81", "3b96eef8-c761-4789-a35a-95b2caa517de", "6af00e97-5fb3-4028-87aa-8fac7142816c", "d38d39e2-e92b-4206-9281-b1ad4d509fc0", "ba305577-4101-4af5-94e7-b241d115030e", "c3faf249-779b-4a78-a51e-c73518da42b6", "79a5f98d-8759-43ac-b2fd-7bc8b2b7dd6a", "1160ec06-94a7-486c-9d76-8e781dbf2e45", "326e82a4-2e10-47e1-b272-bb135115ad8a", "2ce74c66-f170-4878-8541-cd18ad94df33", "91919c20-4fb5-4920-8685-cf6c29cfd2b7", "598d2917-1902-48c4-9d74-0804cd1ba6c5", "7171d91d-d981-457d-a6a8-ee96bd810183", "dc92b5ab-90a4-400b-9568-5673c771565b", "8ff0ad3f-e0c6-425e-b74b-446a0da0a2f7", "c252df82-3700-408f-ac0b-f869fb5c27b9", "d654f4a1-0e2b-403d-963c-60923e3152d5", "ba75c3ae-77e7-4503-a0d1-e3021ad87dc4"], "metadata": {"file_path": "/custom-install/PFSAgent/LustreManualIndex/data/md_manual/Index.md", "file_name": "Index.md", "file_type": "text/markdown", "file_size": 71594, "creation_date": "2025-02-08", "last_modified_date": "2025-02-08"}}}}