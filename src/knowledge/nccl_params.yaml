NCCL_ALGO:
  description: "Collective algorithm selection. Controls which communication pattern is used: Ring (optimal bandwidth, O(k) latency), Tree (O(log k) latency, full bandwidth via double binary tree), CollnetDirect/CollnetChain (SHARP in-network reduction), NVLS (NVLink SHARP on NVSwitch), NVLSTree (NVLS intra + Tree inter). Since NCCL 2.24, supports per-function syntax: allreduce:tree;broadcast:ring. Exclude syntax: ^tree means all except tree."
  mechanism: "algorithm_choice"
  risk: "low"
  default: "auto (NCCL cost model selects based on topology, message size, GPU count)"
  valid_values: "Tree, Ring, CollnetDirect, CollnetChain, NVLS, NVLSTree, or comma-separated list"
  tuning_increase: "Use Ring for large messages >100MB where bandwidth dominates. Use Tree for small/medium messages or large GPU counts where latency matters."
  tuning_decrease: "N/A (categorical parameter)"
  interactions: "PROTO (Tree+LL128 synergistic for medium msgs; Ring+Simple synergistic for large msgs), NCHANNELS (Ring benefits more from channels than Tree), message_size (determines optimal algorithm)"
  topology_sensitivity: "NVLink: both work well. IB inter-node: Tree preferred at 32+ GPUs. Ethernet: Tree preferred (lower per-step latency)."
  scaling_note: "2-8 GPU: either. 8-32: Ring large / Tree small. 32-128: Tree increasingly preferred. 128-1000: Tree+CollNet. 1000+: CollNet/SHARP essential."
  anti_pattern: "Forcing Ring at 128+ GPUs for small messages (O(k) latency prohibitive). Forcing Tree for >100MB messages (suboptimal bandwidth)."

NCCL_PROTO:
  description: "Protocol selection for collectives. Controls data synchronization method: Simple (memory fences, ~6us overhead, ~95% bandwidth, best >64KB), LL (flag-polling per 8B, ~1us/hop, 25-50% bandwidth, best <64KB, forces host memory), LL128 (flag-polling per 128B, ~2us/hop, ~95% bandwidth, best 64KB-100MB, requires 128-byte atomic write hardware support). Since NCCL 2.24, supports per-function and exclude syntax."
  mechanism: "protocol_choice"
  risk: "low"
  default: "auto (LL,LL128,Simple on platforms supporting LL128; LL,Simple otherwise)"
  valid_values: "Simple, LL, LL128, or comma-separated list. Exclude: ^LL128"
  tuning_increase: "Use Simple for large messages >64KB where bandwidth matters. Use LL for tiny messages <64KB where latency matters. Use LL128 for medium messages on supported platforms."
  tuning_decrease: "N/A (categorical parameter)"
  interactions: "ALGO (Tree+LL128 synergistic; Ring+Simple synergistic), NET_GDR_LEVEL (LL forces host memory, disabling GDR), hardware (LL128 requires 128-byte atomic writes)"
  topology_sensitivity: "NVLink: all three work well, LL128 only 5% slower than Simple at large msgs. IB: Simple dominates for large msgs, LL for small. Ethernet: Simple preferred (minimize small sends)."
  scaling_note: "Protocol choice mainly depends on message size, not GPU count. At extreme scale, LL128 can underperform due to accumulated per-128B sync overhead."
  anti_pattern: "Forcing LL128 on unsupported hardware causes SILENT DATA CORRUPTION. Use ^LL128 to safely disable."

NCCL_NTHREADS:
  description: "Threads per CUDA block (per channel). Each channel launches one CUDA block. Valid values: 64, 128, 256, 512. Default: 512 for recent GPUs (Hopper/Ada/Ampere), 256 for older. WARNING: Since NCCL 2.24, manual settings may be IGNORED and can cause incorrect behavior. Use NCCL_MIN_CTAS/MAX_CTAS instead."
  mechanism: "parallelism"
  risk: "med"
  default: "512 (recent GPUs), 256 (older GPUs)"
  valid_values: "64, 128, 256, 512"
  tuning_increase: "When GPU clocks are low and more thread-level parallelism needed."
  tuning_decrease: "When reducing GPU communication workload for better compute overlap."
  interactions: "NCHANNELS (more channels + more threads = more SM pressure), compute workload (threads compete for SM resources)"
  topology_sensitivity: "Impact is uniform across topologies. Primary effect is SM resource consumption."
  scaling_note: "Prefer NCCL_MIN_CTAS/MAX_CTAS on NCCL 2.24+. NTHREADS is effectively deprecated for modern versions."
  anti_pattern: "Setting NTHREADS on NCCL 2.24+ may be silently ignored or cause incorrect behavior."

NCCL_BUFFSIZE:
  description: "Communication buffer size in bytes between pairs of GPUs. Default: 4194304 (4 MiB). Divided into 8 slots for pipelining. Total memory per communicator = BUFFSIZE x NCHANNELS x NPEERS. Since NCCL 2.29, adaptive routing threshold is tied to BUFFSIZE."
  mechanism: "buffering"
  risk: "med"
  default: "4194304 (4 MiB)"
  valid_values: "Powers of 2 recommended. Minimum practical: 1048576 (1 MiB)."
  tuning_increase: "For large messages: larger pipeline chunks improve Ring throughput. For high-latency networks: larger buffers amortize latency."
  tuning_decrease: "When memory-constrained (many communicators active). When adaptive routing is needed (high BUFFSIZE may disable it in NCCL 2.29+)."
  interactions: "NCHANNELS (total memory = BUFFSIZE x NCHANNELS x NPEERS, multiplicative), ALGO (Ring benefits more from larger buffers than Tree), adaptive_routing (NCCL 2.29: high BUFFSIZE may disable adaptive routing)"
  topology_sensitivity: "PCIe: buffer size matters more (higher latency). NVLink: minimal impact. IB: moderate impact on large message throughput. Ethernet: larger buffers help for high-latency links."
  scaling_note: "Keep default (4 MiB) for most cases. Increase to 8-16 MiB only for very large messages on high-bandwidth links. Watch memory budget at scale."
  anti_pattern: "Setting BUFFSIZE too high disables adaptive routing (NCCL 2.29+). Setting too low (<1 MiB) with Ring and large messages causes extreme fragmentation and timeouts."

NCCL_MIN_NCHANNELS:
  description: "Minimum communication channels NCCL will use. Each channel = 1 CUDA block on separate SM, operating on disjoint buffer chunks. Useful when NCCL defaults to too few channels (e.g., aggregated collectives default to 1 channel)."
  mechanism: "channel_parallelism"
  risk: "med"
  default: "varies by topology (typically 1-4)"
  valid_values: "1-32"
  tuning_increase: "When NCCL auto-selects too few channels for high-bandwidth links. When large messages need parallel bandwidth saturation. When multiple NICs need balanced traffic."
  tuning_decrease: "Rarely decreased (it's a minimum). Set to 1 to let NCCL use single channel for small messages."
  interactions: "MAX_NCHANNELS (MIN must be <= MAX), BUFFSIZE (more channels x buffsize = more memory), message_size (small msgs don't benefit from many channels)"
  topology_sensitivity: "NVLink: can support higher minimums (high bandwidth). PCIe: keep low (limited bandwidth). IB: 2 channels per NIC per peer is default baseline."
  scaling_note: "8-32 GPU: 4 is reasonable minimum. 128+: may reduce if Tree algorithm preferred. Multi-NIC: minimum should ensure all NICs are utilized."
  anti_pattern: "Setting MIN_NCHANNELS=16 for small-message workloads wastes NIC FIFO capacity and SM resources."

NCCL_MAX_NCHANNELS:
  description: "Maximum communication channels. Each channel = 1 CUDA block on separate SM. Reducing this frees CUDA compute resources for model computation. Critical for communication-compute overlap in training."
  mechanism: "channel_parallelism"
  risk: "med"
  default: "varies by topology (typically 8-16)"
  valid_values: "1-32"
  tuning_increase: "When large messages need to saturate high-bandwidth links (NVLink). When benchmark shows communication is the bottleneck."
  tuning_decrease: "When compute is bottleneck and needs more SMs. When workload is dominated by small messages. For PCIe topology (lower bandwidth limit)."
  interactions: "BUFFSIZE (memory = BUFFSIZE x NCHANNELS x NPEERS), NTHREADS (channels x threads = total SM pressure), ALGO (Ring benefits more from channels than Tree), message_size (per-channel chunk must exceed 512KB NIC FIFO)"
  topology_sensitivity: "NVLink: high impact (can support 8-16 channels). PCIe: medium (4-8 sufficient). IB inter-node: medium (per-channel chunk must exceed 512KB NIC FIFO)."
  scaling_note: "8-32 GPU: 8 is typical max. 128+: Tree + fewer channels often better than Ring + many channels. Compute-heavy workloads: reduce to free SMs."
  anti_pattern: "MAX_NCHANNELS=32 for small messages: per-channel chunk < 512KB NIC FIFO causes partial buffer sends and PCIe/network waste."

NCCL_MIN_CTAS:
  description: "Minimum CTAs (Cooperative Thread Arrays / CUDA blocks) per collective operation. Since NCCL 2.24, preferred over NCCL_NTHREADS for controlling communication resource allocation. NCCL auto-selects within MIN-MAX range based on operation size and concurrency."
  mechanism: "cta_parallelism"
  risk: "med"
  default: "auto (NCCL selects based on operation)"
  valid_values: "1-64 (NCCL 2.27+ supports up to 64 simultaneous CTAs)"
  tuning_increase: "When communication throughput needs to be higher. When link bandwidth is not saturated."
  tuning_decrease: "Rarely decreased (it's a minimum)."
  interactions: "MAX_CTAS (MIN must be <= MAX), compute workload (CTAs compete for SMs), group_calls (concurrent operations share CTA budget)"
  topology_sensitivity: "High-bandwidth links need more CTAs. SHARP-enabled: fewer CTAs needed (offload to switch)."
  scaling_note: "SHARP (NCCL 2.27+) reduces CTA needs from 16+ to 6. Group calls dynamically reduce per-operation CTAs."
  anti_pattern: "Setting MIN_CTAS too high when many operations run concurrently via group calls."

NCCL_MAX_CTAS:
  description: "Maximum CTAs per collective operation. NVIDIA guidance: take just enough CTAs to saturate line rate at large message sizes, but no more. Reducing frees SMs for model compute, enabling better overlap."
  mechanism: "cta_parallelism"
  risk: "med"
  default: "auto (NCCL selects based on operation)"
  valid_values: "1-64"
  tuning_increase: "When isolated communication benchmark needs higher throughput. When links are under-utilized."
  tuning_decrease: "When compute-bound workload needs SMs freed. When benchmark improves but training slows (SM starvation). When SHARP is available (fewer CTAs needed)."
  interactions: "MIN_CTAS (MAX must be >= MIN), compute workload (key trade-off), SHARP (reduces CTA needs dramatically)"
  topology_sensitivity: "Same as MIN_CTAS. Primary effect is SM management."
  scaling_note: "Critical insight: benchmark improvement != workload improvement. More CTAs always improve nccl-tests but may hurt end-to-end training."
  anti_pattern: "Maximizing CTAs based on nccl-tests benchmarks, then seeing slower training due to SM starvation for compute."

NCCL_P2P_LEVEL:
  description: "Controls peer-to-peer transport level. Determines which P2P paths NCCL uses: NVL (NVLink direct), PHB (PCIe bridge), SYS (across NUMA), LOC (local only). NCCL auto-detects optimal level from topology."
  mechanism: "transport_topology"
  risk: "low"
  default: "auto-detected from hardware topology"
  valid_values: "LOC, PHB, SYS, NVL"
  tuning_increase: "N/A (categorical). Set to NVL to force NVLink P2P. Set to PHB for PCIe bridge P2P."
  tuning_decrease: "Set to LOC to restrict to local device only (diagnostic use)."
  interactions: "SHM_DISABLE (if SHM disabled, P2P path must work), topology (auto-detected, override rarely needed)"
  topology_sensitivity: "NVLink: NVL auto-selected. PCIe: PHB auto-selected. Cross-socket: SYS with SHM fallback."
  scaling_note: "Rarely needs override. NCCL auto-detection is reliable on standard platforms."
  anti_pattern: "Forcing NVL on systems without NVLink. Forcing SYS P2P across sockets without SHM fallback."

NCCL_NET_GDR_LEVEL:
  description: "Controls GPUDirect RDMA activation distance threshold. GDR allows NIC to read/write GPU memory directly, bypassing host memory copy. Critical when GPU and NIC share PCIe switch. Values: LOC (same device), PHB (same PCIe switch), SYS (across NUMA)."
  mechanism: "transport"
  risk: "med"
  default: "auto-detected. GDR read enabled by default on NVLink platforms since NCCL 2.4.2."
  valid_values: "LOC, PHB, SYS"
  tuning_increase: "Set PHB or SYS when GPU-NIC are on same PCIe switch but GDR not auto-detected. 30-50% improvement for large IB messages with GDR."
  tuning_decrease: "Set LOC to disable GDR (diagnostic use only)."
  interactions: "PROTO (LL forces host memory, disabling GDR benefit), IB topology (GDR most beneficial at PHB level), PCIe topology (requires NIC-GPU proximity)"
  topology_sensitivity: "IB: critical (30-50% improvement). NVLink+IB: auto-enabled. Ethernet: not applicable (no RDMA)."
  scaling_note: "GDR benefit is per-transfer, scales linearly. Ensure NIC-GPU affinity is correct at all nodes."
  anti_pattern: "Not enabling GDR when GPU and NIC share PCIe switch: massive performance loss for IB transfers."

NCCL_SOCKET_NTHREADS:
  description: "Number of CPU threads for socket I/O. Each thread manages its assigned sockets. Critical parameter for Ethernet/TCP clusters where TCP throughput is the bottleneck. More threads = better CPU utilization for socket operations."
  mechanism: "network_threads"
  risk: "low"
  default: "1"
  valid_values: "1-16"
  tuning_increase: "Always increase for Ethernet clusters (default 1 is too low). 4-8 threads typical for high-bandwidth Ethernet."
  tuning_decrease: "Only if CPU overhead is excessive (rare)."
  interactions: "NSOCKS_PERTHREAD (total TCP streams = NTHREADS x NSOCKS), available CPU cores (need enough cores for threads)"
  topology_sensitivity: "Ethernet: most impactful parameter. IB: not applicable. NVLink: not applicable."
  scaling_note: "More threads always help up to CPU core count. Diminishing returns beyond 8-16 total streams."
  anti_pattern: "Using default NTHREADS=1 on Ethernet clusters: severely under-utilizes available TCP bandwidth."

NCCL_NSOCKS_PERTHREAD:
  description: "Number of TCP sockets per socket thread. Multiple sockets per thread enable parallel TCP streams. Total TCP parallelism = SOCKET_NTHREADS x NSOCKS_PERTHREAD."
  mechanism: "network_sockets"
  risk: "low"
  default: "1"
  valid_values: "1-16"
  tuning_increase: "For Ethernet clusters to increase total TCP stream count. 4 sockets per thread is typical."
  tuning_decrease: "If too many sockets cause kernel resource exhaustion (very rare)."
  interactions: "SOCKET_NTHREADS (total streams = NTHREADS x NSOCKS), kernel TCP buffer limits"
  topology_sensitivity: "Ethernet: critical. IB/NVLink: not applicable."
  scaling_note: "Total streams of 16-32 (4 threads x 4 sockets) usually sufficient. Beyond that, diminishing returns."
  anti_pattern: "Using defaults (1x1=1 stream) on 25+ Gbps Ethernet: single TCP stream cannot saturate the link."

NCCL_IB_QPS_PER_CONNECTION:
  description: "Number of Queue Pairs per InfiniBand connection. Multiple QPs provide different source ports, improving ECMP (Equal-Cost Multi-Path) routing entropy on multi-level IB fabrics (fat-tree, dragonfly). Each QP independently routes through the fabric."
  mechanism: "network_qp"
  risk: "med"
  default: "1"
  valid_values: "1-8"
  tuning_increase: "On multi-level IB fabrics (fat-tree) where ECMP routing entropy matters. 2-4 QPs typical for multi-switch topologies."
  tuning_decrease: "On single-switch IB (ECMP not relevant). If QP creation fails due to HCA resource limits."
  interactions: "CROSS_NIC (combined NIC diversity + QP diversity maximizes routing entropy), fabric topology (only helps on multi-level fabrics)"
  topology_sensitivity: "Multi-switch IB: significant improvement. Single-switch IB: no benefit. Not applicable to NVLink/Ethernet."
  scaling_note: "Benefit scales with fabric complexity. 2 QPs for 2-level fat-tree, 4 for 3-level. Diminishing returns beyond 4."
  anti_pattern: "Setting high QPS on single-switch IB fabric: wastes HCA resources with no routing benefit."

NCCL_SHM_DISABLE:
  description: "Disable shared memory transport for intra-node communication. Shared memory uses pinned host memory as intermediary: GPU -> host -> GPU. Critical fallback for cross-socket PCIe communication where direct P2P is poorly handled."
  mechanism: "transport"
  risk: "low"
  default: "0 (SHM enabled)"
  valid_values: "0 (enabled), 1 (disabled)"
  tuning_increase: "N/A (boolean). Set to 1 only on NVLink-only systems where SHM is unnecessary."
  tuning_decrease: "Keep at 0 (enabled) for PCIe systems, especially cross-socket."
  interactions: "P2P_LEVEL (if SHM disabled, P2P must work for all GPU pairs), NVLink (SHM unnecessary when NVLink available), PCIe topology (SHM critical for cross-socket)"
  topology_sensitivity: "PCIe cross-socket: disabling SHM is DANGEROUS (forces broken P2P path). NVLink: SHM not needed but harmless."
  scaling_note: "No scaling effect. Binary choice based on topology."
  anti_pattern: "Disabling SHM on cross-socket PCIe systems: forces direct PCIe P2P across CPU interconnect, dramatically slower."

NCCL_CROSS_NIC:
  description: "Controls whether different rings/trees can use different NICs. Value 2 (default) allows cross-NIC assignment, improving aggregate bandwidth on multi-NIC nodes by distributing traffic across all available NICs."
  mechanism: "nic_assignment"
  risk: "low"
  default: "2"
  valid_values: "0 (same NIC for all), 1 (round-robin), 2 (topology-aware, default)"
  tuning_increase: "Keep at 2 for multi-NIC systems (default is optimal)."
  tuning_decrease: "Set to 0 only if NIC-specific debugging needed."
  interactions: "IB_QPS_PER_CONNECTION (combined NIC + QP diversity maximizes routing), multi-NIC topology (only relevant with multiple NICs)"
  topology_sensitivity: "Multi-NIC nodes (DGX/HGX): important for bandwidth balancing. Single-NIC: no effect."
  scaling_note: "Default value 2 is appropriate at all scales."
  anti_pattern: "Setting CROSS_NIC=0 on multi-NIC systems: underutilizes available NIC bandwidth."

NCCL_NVLS_ENABLE:
  description: "Enable/disable NVLink SHARP (NVLS) on NVSwitch systems. NVLS offloads collective reduction to NVSwitch hardware, reducing GPU SM usage and improving intra-node performance. Requires NVSwitch (DGX H100, DGX B200, GB200 NVL)."
  mechanism: "hardware_offload"
  risk: "low"
  default: "1 (enabled on supported systems since NCCL 2.17)"
  valid_values: "0 (disabled), 1 (enabled)"
  tuning_increase: "Keep at 1 on NVSwitch systems (default is optimal)."
  tuning_decrease: "Set to 0 only if memory-constrained (NVLS uses additional GPU memory for multicast buffers)."
  interactions: "CollNet/SHARP (NVLS intra + SHARP inter = maximum hardware offload), memory budget (NVLS allocates multicast buffers)"
  topology_sensitivity: "NVSwitch: highly beneficial. Non-NVSwitch: not applicable."
  scaling_note: "Always beneficial on NVSwitch systems regardless of scale."
  anti_pattern: "Disabling NVLS unnecessarily on NVSwitch systems: leaves performance on the table."

NCCL_IB_TIMEOUT:
  description: "InfiniBand timeout value. Actual timeout = 4.096 microseconds x 2^TIMEOUT. Controls how long IB waits before declaring a connection failed. Must be high enough for large clusters where fabric congestion can cause legitimate delays."
  mechanism: "reliability"
  risk: "med"
  default: "22 (approximately 17 seconds)"
  valid_values: "1-31"
  tuning_increase: "For large clusters (1000+ GPUs) where congestion causes delays. For adaptive routing where path changes add latency."
  tuning_decrease: "For small clusters where faster failure detection is preferred."
  interactions: "Cluster scale (larger clusters need higher timeout), adaptive routing (path changes add latency), fabric congestion (congested networks need higher timeout)"
  topology_sensitivity: "IB only. Larger fabrics need higher timeouts."
  scaling_note: "Default 22 is sufficient for most clusters. Increase to 24+ for 1000+ GPU jobs. Maximum 31."
  anti_pattern: "Low timeout on large clusters: spurious disconnections during congestion events, causing NCCL errors and job failures."
